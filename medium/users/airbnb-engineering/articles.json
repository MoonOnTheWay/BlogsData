[
    {
        "url": "https://medium.com/airbnb-engineering/listing-embeddings-for-similar-listing-recommendations-and-real-time-personalization-in-search-601172f7603e?source=---------0",
        "title": "Listing Embeddings in Search Ranking \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Airbnb\u2019s marketplace contains millions of diverse listings which potential guests explore through search results generated from a sophisticated Machine Learning model that uses more than hundred signals to decide how to rank a particular listing on the search page. Once a guest views a home they can continue their search by either returning to the results or by browsing the Similar Listing Carousel, where listing recommendations related to the current listing are shown. Together, Search Ranking and Similar Listings drive 99% of our booking conversions.\n\nIn this blog post we describe a Listing Embedding technique we developed and deployed at Airbnb for the purpose of improving Similar Listing Recommendations and Real-Time Personalization in Search Ranking. The embeddings are vector representations of Airbnb homes learned from search sessions that allow us to measure similarities between listings. They effectively encode many listing features, such as location, price, listing type, architecture and listing style, all using only 32 float numbers. We believe that the embedding approach for personalization and recommendation is very powerful and useful for any type of online marketplace on the Web.\n\nIn a number of Natural Language Processing (NLP) applications classic methods for language modeling that represent words as high-dimensional, sparse vectors have been replaced by Neural Language models that learn word embeddings, i.e. low-dimensional representations of words, through the use of neural networks. The networks are trained by directly taking into account the word order and their co-occurrence, based on the assumption that words frequently appearing together in the sentences also share more statistical dependence. With the development of highly scalable continuous bag-of-words and Skip-gram language models for word representation learning, the embedding models have been shown to obtain state-of-the-art performance on many traditional language tasks after training on large text data.\n\nMore recently, the concept of embeddings has been extended beyond word representations to other applications outside of NLP domain. Researchers from the Web Search, E-commerce and Marketplace domains have realized that just like one can train word embeddings by treating a sequence of words in a sentence as context, the same can be done for training embeddings of user actions by treating sequence of user actions as context. Examples include learning representations of items that were clicked or purchased or queries and ads that were clicked. These embeddings have subsequently been leveraged for a variety of recommendations on the Web.\n\nLet us assume we are given a data set of click sessions obtained from N users, where each session s=(L\u2081,\u2026,Ln)\u2208 S is defined as an uninterrupted sequence of n listing ids that were clicked by the user. A new session is started whenever there is a time gap of more than 30 minutes between two consecutive user clicks. Given this data set, the aim is to learn a 32-dimensional real-valued representation v(Li) \u2208 R\u00b3\u00b2 of each unique listing Li, such that similar listings lie nearby in the embedding space.\n\nThe dimensionality of the listing embeddings was set to d = 32, as we found that to be a good trade-off between offline performance (discussed in following section) and the memory needed to store vectors in RAM memory of search machines for the purposes of real-time similarity calculations.\n\nThere exist several different ways of training embeddings. We will focus on a technique called Negative Sampling. It starts by initializing the embeddings to random vectors, and proceeds to update them via stochastic gradient descent by reading through search sessions in a sliding window manner. At each step the vector of the central listing is updated by pushing it closer to vectors of positive context listings: listings that were clicked by the same user before and after central listing, within a window of length m (m = 5), and pushing it away from negative context listings: randomly sampled listings (as chances are these are not related to the central listing).\n\nFor brevity we\u2019ll skip over the details of the training procedure and focus on explaining several modifications we made to adapt this approach for our own use-case:\n\nConsidering all of the above, the final optimization objective can be formulated as\n\nUsing the described optimization procedure we learned listing embeddings for 4.5 million active listings on Airbnb using more than 800 million search clicks sessions, resulting in high quality listing representations.\n\nCold-start Embeddings. Every day new listings are created by hosts and made available on Airbnb. At that point these listings do not have an embedding because they were not present our training data. To create embeddings for a new listing we find 3 geographically closest listings that do have embeddings, and are of same listing type and price range as the new listing, and calculate their mean vector.\n\nTo evaluate what characteristics of listings were captured by the embeddings, we examine them in several ways. First, to evaluate if geographical similarity is encoded we performed k-means clustering on learned embeddings. The figure on the left, which shows resulting 100 clusters in California, confirms that listings from similar locations are clustered together. Next, we evaluated average cosine similarities between listings of different types (Entire Home, Private Room, Shared Room) and price ranges and confirmed that cosine similarities between listings of same type and price ranges are much higher compared to similarities between listings of different type and price ranges. Therefore, we can conclude that those two listing characteristics are well encoded in the learned embeddings as well.\n\nWhile some listing characteristics, such as price, do not need to be learned because they can be extracted from listing meta-data, other types of listing characteristics, such as architecture, style and feel are much harder to extract in form of listing features. To evaluate these characteristics and to be able to conduct fast and easy explorations in the embedding space we developed an internal Similarity Exploration Tool demonstrated in a video below.\n\nThe video provides many examples of embeddings being able to find similar listings of the same unique architecture, including houseboats, treehouses, castles, etc.\n\nBefore testing embeddings in recommendation applications on real search traffic, we conducted several offline tests. We also used these tests to compare several different trained embeddings to reach quick decisions regarding embedding dimensionality, different ideas on algorithm modifications, training data construction, choice of hyperparameters, etc.\n\nOne way of evaluating trained embeddings is to test how good they are in recommending listings that the user would book, based on their most recent click.\n\nMore specifically, let us assume we are given the most recently clicked listing and the listing candidates that need to be ranked, which contain the listing that user eventually booked. By calculating cosine similarities between embeddings of the clicked listing and the candidate listings we can rank the candidates and observe the rank position of the booked listing.\n\nIn the figure below we show results of one such evaluation where listings in search were re-ranked based on similarities in embedding space and rankings of booked listing are averaged for each click leading to booking, going as far back as 17 clicks before the booking to the last click before booking.\n\nWe compared several embedding versions 1) d32 regular trained without any modifications to the original embedding algorithm, 2) d32 booking global trained with bookings as global context and 3) d32 booking global + market negatives trained with bookings as global context and explicit negatives from same market (from our formula). From the consistent lower average ranking of booked listing we can conclude that d32 booking global + market negatives outperforms the other two embedding models.\n\nEvery Airbnb home listing page contains a Similar Listings carousel that recommends listings which are similar to it and available for the same set of dates.\n\nAt the time of testing embeddings the existing algorithm for Similar Listings consisted of calling our main Search Ranking model for the same location as the given listing followed by filtering on same price range and listing type as the given listing.\n\nWe conducted an A/B test where we compared the existing Similar Listings algorithm at the time to our embedding-based solution, in which similar listings were produced by finding k-nearest neighbors in listing embedding space. More precisely, given learned listing embeddings, similar listings for a given listing l are found by calculating cosine similarity between its vector v(l) and vectors v(lj) of all listings from the same market that are available for the same set of dates (if check-in and check-out dates are set). The k=12 listings with the highest similarity were shown as similar listings.\n\nThe A/B test showed that embedding-based solution lead to a 21% increase in Similar Listing carousel CTR and 4.9% more guests discovering the listing they ended up booking in the Similar Listing carousel.\n\nSo far we\u2019ve seen that embeddings can be used to efficiently calculate similarities between listings. Our next idea was to leverage this capability in Search Ranking for real-time in-session personalization, where the aim is to show to the guest more listings that are similar to the ones we think they liked since starting the search session and fewer listings similar to the ones we think they did not like.\n\nTo achieve this, for each user we collect and maintain in real-time (using Kafka) two sets of short-term history events:\n\nNext, each time the user conducts a search we calculate 2 similarity measures for each candidate listing l\u1d62 returned in search.\n\nSpecifically, we calculate similarity between market-level centroids from Hc and pick the maximum similarity. For example if Hc contained listings from New York and Los Angeles, embeddings of listings for each of these two markets would be averaged out to form a market-level centroids.\n\nThese two similarity measures were next introduced as additional signal that our Search Ranking Machine Learning model considers when ranking candidate listings.\n\nThis was done by first logging the two embedding similarity features along with other search ranking features so we can create a new labeled data set for model training and then proceeding to train a new ranking model that we can be tested against the current production ranking model in a A/B test.\n\nTo evaluate if the new model learned to use the embedding similarity features as we intended, we plot their partial dependency plots below. These plots show what would happen to candidate listing ranking score if we fix values of all but a single feature (one we are examining).\n\nIn the left subgraph it can be seen that larger values of EmbClickSim (listing similar to listings user recently click on) lead to higher model score.\n\nIn the right subgraph it can be seen that larger values of EmbSkipSim (listing similar to listings user skipped, i.e. did not like) lead to lower model score.\n\nObservations from partial dependency plots confirmed that features behavior matches what we intuitively expected the model will learn. In addition, the new embedding features ranked high in ranking model feature importances and our offline tests showed an improvement in performance metrics on a hold-out set when the embedding features were added to the model. This was enough for us to make a decision to proceed to an online experiment, which was successful and lead to the launch of embedding features for real-time personalization to production in Summer of 2017."
    },
    {
        "url": "https://medium.com/airbnb-engineering/fighting-financial-fraud-with-targeted-friction-82d950d8900e?source=---------1",
        "title": "Fighting Financial Fraud with Targeted Friction \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "David from the Trust team walks us through how Airbnb battles chargebacks while minimizing impact to good guests.\n\nOn any given night, nearly two million people are staying in Airbnb listings in 191 countries around the world. The rapid growth of our global community is predicated on one thing: trust.\n\nWe take a comprehensive approach to trust, which consists of both robust proactive measures and reactive support, but today I want to focus on some of the work we do ahead of time \u2014 and often in the background \u2014 to prevent fraudsters from using stolen credit cards on our site.\n\nIn this post, I\u2019ll walk through how we leverage machine learning, experimentation, and analytics to identify and block fraudsters while minimizing impact on the overwhelming majority of good users. First, I\u2019ll introduce our use of machine-learning (ML) models to trigger frictions targeted at blocking fraudsters. Then, I\u2019ll outline how we choose the model\u2019s threshold by minimizing a loss function, and dive into each term in the loss function: the costs of false positives, false negatives, and true positives. Finally, I\u2019ll walk through a numerical example comparing the optimization of blocking transactions versus applying a friction.\n\nLike all online businesses, Airbnb faces fraudsters who attempt to use stolen credit cards. When the true cardholder realizes their card has been stolen and notices unauthorized charges on their bill, the credit card company issues what\u2019s called a \u201cchargeback,\u201d and the merchant (in our case, Airbnb) returns the money.\n\nUnlike some of our competitors, we absorb the cost of these chargebacks and do not pass any financial liability to our hosts. In order to better protect our community and reduce our own exposure to chargeback costs, we actively work to block stolen credit cards from being used in the first place.\n\nWe detect financial fraud in a number of ways, but our workhorse method uses machine-learning (ML) models trained on past examples of confirmed good and confirmed fraudulent behavior. Because no model can be perfect, we will always have false positives: \u201cgood\u201d events that a model or rule classifies as \u201cbad\u201d (above the threshold). In some situations, we block actions outright, but in most situations we allow the user the opportunity to satisfy an additional verification called a friction. A friction is ideally something that blocks a fraudster, yet is easy for a good user to satisfy.\n\nTo stop the use of stolen credit cards, our chargeback model triggers a number of frictions to ensure that the guest is in fact authorized to use that card, including micro-authorization (placing two small authorizations on the credit card, which the cardholder must identify by logging into their online banking statement), 3-D Secure (which allows credit card companies to directly authenticate cardholders via a password or SMS challenge), and billing-statement verification (requiring the cardholder to upload a copy of the billing statement associated with the card).\n\nWe train the chargeback model on positive (fraud) and negative (non-fraud) examples from past bookings, with the goal of predicting the probability that a booking is fraudulent. Because fraud is extremely rare, this is an imbalanced classification problem with scarce positive labels. We characterize our model\u2019s performance at identifying fraudulent versus good bookings at various thresholds in terms of the true-positive rate and false-positive rate, then evaluate the total cost associated with each threshold using a loss function that depends on those rates.\n\nSpecifically, our goal is to minimize the overall loss function L, which we can write as:\n\nIn this equation FP is the number of false positives, G is the dropout rate of good users when exposed to the friction, V is the good user lifetime value, FN is the number of false negatives, C is the cost of a fraud event, TP is the number of true positives, and F is the friction\u2019s efficacy against fraudsters. (The cost of a \u201ctrue negative\u201d \u2014 i.e., the model correctly identifying a good booking as good \u2014 is zero, so does not appear in the loss function.) In the following sections, we\u2019ll examine how we estimate each of these terms.\n\nIf we incorrectly apply friction to a good booking (a false positive), we incur a cost because there is a chance the good user will not complete the friction and then will not use Airbnb. This probability of the good user dropping out is given by G.\n\nFor each good guest that is unable to complete the friction and drops from the funnel, we approximate that the guest has churned for life \u2014 that is, we lose their entire lifetime value V. We won\u2019t go into the details of how we calculate lifetime values in this post, but it is a concept common to many businesses.\n\nThe loss from good users dropping out is given by multiplying the number of false positives by the expected loss from each false positive: FP * G * V.\n\nIn order to measure the impact of each friction on good users G, we run an A/B test using our Experiment Reporting Framework. We assign users with low model scores (who are very unlikely to be fraudsters) to the experiment at the same stage in the funnel where we will apply the friction against fraudsters.\n\nWhat do we measure in these good user dropout experiments? For simplicity\u2019s sake, we tend to choose a single, end-of-funnel metric. For anti-chargeback frictions, we measure the number of guests who successfully complete a booking. We are looking to find the good-user friction dropout rate G, which we\u2019ll define as: 1 \u2014 (success rate in friction group)/(success rate in control group).\n\nGood user dropout experiments are costly to run. We need to cause enough good users to drop out that we can measure how big that dropout is with a reasonable degree of confidence \u2014 which means some good guests won\u2019t end up booking! To minimize the total number of good users exposed to the friction, while still measuring G to within a given confidence interval, we use highly imbalanced assignments such as 95% control (no friction) / 5% treatment (friction). To see why, consider that we can use the Delta method to calculate the the variance on a ratio metric as:\n\nIs this equation \ud835\udf07_c and \ud835\udf07_t are the means of the control and treatment statistics respectively, \ud835\udf0e_c\u00b2 and \ud835\udf0e_t\u00b2 are their respective variances, and G = 1-\ud835\udf07_t/\ud835\udf07_c. Since \ud835\udf0e_t\u00b2 is set by the costly treatment (friction) group size, we want \ud835\udf0e_c\u00b2 to be as small as possible \u2014 which is achievable by increasing the control group size with an imbalanced experiment. In many cases, if G\u226a1 then the imbalanced experiment allows us to expose roughly half as many users to friction compared to a 50/50 experiment to achieve the same confidence interval on G.\n\nThe disadvantage of running an imbalanced experiment is that the statistic takes longer to converge (i.e., requires a larger total sample size) than a 50/50 experiment. The exact treatment fraction needs to be tailored to the number of users eligible for the experiment each day, the expected magnitude of G, and the amount of time we are willing to run the experiment.\n\nNext, we need to know the cost of false negatives \u2014 that is, the cost of a fraudulent event that scored below the model\u2019s threshold. The total loss from false negatives is given simply by multiplying the number of false negatives by the cost of each fraud event: FN * C.\n\nAirbnb absorbs all costs associated with chargebacks, and we never pass them through to our hosts. Thus, the total cost is the full amount of the payment made by the fraudster, plus an overhead factor associated with processor fees and increased card decline rates.\n\nA true positive is when the model correctly identifies a fraudster with a score above the threshold. Here we apply friction to achieve our ultimate goal: preventing this fraudster from using Airbnb. If the friction successfully blocks the fraudster, we have achieved this goal and have no loss.\n\nHowever, no friction is perfect, and if the fraudster somehow manages to pass the friction, then we do incur a loss. The total loss from true positives if given by TP*(1-F)*C, where F is the fraudster dropout rate when challenged by the friction.\n\nWe measure F using another A/B test, where we assign risky users who score above the model threshold. This time the imbalance is flipped around because not applying friction to a fraudster is very expensive. We subject all high-risk events that skipped the friction due to the experiment to a manual review in order to prevent harm to our community and minimize loss. We track a metric measuring fraud events and compare the number of successful fraud events in the friction group (hopefully small!) to the number of fraud events caught by manual review in the control group to get the fraudster dropout rate F. A value of F=1 would signify that the friction is 100% effective at blocking fraudsters, so we want F to be as close to 1 as possible.\n\nLet\u2019s walk through a fictional example. First, let\u2019s imagine that we trained a machine-learning model on past examples of fraud \u2014 for this example we\u2019ll use a dummy ROC curve defined by TPR = \u2075\u221a[1-(1- FPR)\u2075]\n\nLet\u2019s say that 1% of events are attempted by fraudsters. Each instance of fraud has a fixed cost C=10, and each good user has a value V=1. If we use our model to block transactions directly (Figure 3(a)), the loss function is minimized at ~6 by blocking roughly 1% of transactions. If we instead use the model to trigger a friction that is F=95% effective against fraudsters with a good-user dropout rate G=10%, then total loss is minimized to ~3 by applying friction to 11% of transactions. We can cut total losses by nearly half by using the friction rather than hard-blocking transactions!\n\nNote that we could go a step further by hard-blocking the riskiest events and applying friction to the medium-risk events. This approach maps onto a two-dimensional optimization of the same loss function above.\n\nFighting fraud is an adversarial business by nature. The more aggressively we apply frictions, the less likely fraudsters are to try attacking again \u2014 and the framework described above doesn\u2019t explicitly account for this feedback loop. For this reason, we tend to action slightly more aggressively than the optimal point on our loss function curve. We also avoid updating our operating point too quickly when we see fraud rates drop, though we do act and update quickly if rates rise.\n\nThe fact is that fraudsters will never stop looking for new ways around our defenses. Machine learning and targeted frictions are just one of the many ways we work to keep our community safe, and our team is constantly working to improve our systems in order to earn your trust and stay ahead of those who might be looking to take advantage of our community."
    },
    {
        "url": "https://medium.com/airbnb-engineering/contextual-calendar-reminder-key-to-successful-hosting-9be89e1a32fd?source=---------2",
        "title": "Contextual Calendar Reminders \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "To help improve hosts\u2019 calendar accuracy and avoid canceling bookings, we\u2019ve created handy calendar reminder emails to give hosts extra reassurance. Hosts can opt out of the reminders if they like.\n\nAirbnb\u2019s mission is to create a world where anyone can belong anywhere. So when hosts forget to update their calendars and are booked by guests on dates that don\u2019t work, the host has to cancel the reservation. This definitely does not make the guests feel like they belong. Inaccurate calendars are also detrimental to the business. Being cancelled on is a frustrating experience that makes guests think twice about using Airbnb for a future reservation. Airbnb also compensates the guest in these situations. An accurate host calendar saves all parties a lot of frustration and advances the company mission of enabling anybody to belong anywhere.\n\nWe want to provide the host with appropriately timed reminders such that the host maintains a substantially up-to-date calendar, but is not overwhelmed with excessive reminders such that the host unsubscribes or opts out from receiving calendar reminders to update his or her calendar.\n\nWe developed two strategies to send calendar reminder:\n\nWe want to provide \u201csmart\u201d reminders to hosts based on how often each host typically checks his/her calendar. We first look at how often hosts look at their calendar by plotting the distribution of two adjacent calendar checks. It appears that the distribution follows an exponential distribution, which indicates the calendar check may follow a Poisson process.\n\nThe probability that a host checks his/her calendar t days from previous calendar check follows the distribution\n\nwhere a is the average interval between two adjacent calendar checks.\n\nThis indicates the calendar action ratio\n\ncould be a good indicator of whether a host\u2019s calendar is outdated. For example, if a host typically looks at their calendar every 5 days and he/she hasn\u2019t looked at their calendar for 15 days, it is very likely that the calendar is outdated.\n\nThe average interval of two adjacent calendar checks is based on a few samples, which may not be reliable. For example, if we only see one sample that the user checks their calendar 5 days apart, we cannot say he/she will check his/her calendar every 5 days. We can show that the estimated average interval follows a chi-squared distribution\n\nwhere n is the number of samples used to estimate the average interval a and c is the estimated interval.\n\nThis indicates the probability, that a host checks his calendar t days from previous calendar check over estimated interval c is greater than some threshold, is given by\n\nTherefore, given a probability reliability and number of samples, we can estimate the real interval as the product of estimated interval with a multiplier. The figure below shows this multiplier as a function of the number of samples and reliability.\n\nWe look at the impact of calendar action ratio on booking decline and cancellation. We can clearly see that decline and cancellation rate increases monotonically as calendar action ratio increases, which shows that the ratio is a good signal to predict decline/cancellation.\n\nNow the question remains on how we should set the calendar ratio threshold to send reminder emails. Sending calendar reminder emails has two impacts. On one hand, calendar reminder emails may help hosts keep an updated calendar and reduce cancellation. On the other hand, excessive calendar reminders may make hosts unsubscribe our emails, which may lead to an increase in reservation cancellation. Since cancellations are closely tied to revenue, we obtained a final threshold by maximizing revenue through balancing potential increases and decreases in revenue.\n\nAs stated above, maintaining an accurate calendar is crucial to all parties on Airbnb. However, new hosts may neglect the importance of having an accurate calendar and do not connect their earnings with calendar accuracy. Moreover, the smart calendar reminder does not work for new hosts as we do not have data to learn their typical behavior. Therefore, it is important to help new hosts to form a habit to check calendar regularly.\n\nRegular calendar reminders can help hosts form the habit, and we can send reminders to hosts if they forget to check their calendar in the past week. Now the question becomes exactly when we should send the emails so that they are the most likely reacted by the hosts.\n\nWe analyze data and find the probability that a host will view their calendar on each weekday in different countries in Fig. 4 below. Many hosts are likely to check their calendars near the weekend, such as on Friday, Saturday or Sunday. The rationale is that hosts may want to check their calendar to prepare hosting in the next week.\n\nFor the calendar reminder emails we sent, we would like hosts to open them as much as possible. We analyze the data and find the probability that a host will open an email from Airbnb cross weekday over different countries in Fig. 5 below. Interestingly, hosts from different countries do not have a common email opening behavior.\n\nFinally, to determine the optimal time for sending reminders, we want to select the time that corresponds to the highest likelihood that a host will read a calendar reminder email and access the calendars in response to receiving the reminder. We multiply the probabilities in Fig. 4 and Fig. 5 and get the probability in Fig. 6 below. We can see that the highest likelihood is hit on Sunday across all countries. Therefore, we send calendar reminder emails on Sundays.\n\nWe build a data pipeline using the platform Airflow open sourced at Airbnb. The data pipeline is run daily to process the data logged in the previous day. It takes several hours for the data pipeline to finish and the finish time also varies from day to day. However, hosts may have already checked their calendars when the pipeline is running. It will be awkward if we send a calendar reminder email to a host who has just looked at calendar.\n\nTo solve this problem, we adopt the computation framework AirStream developed at Airbnb. Airstream is a real time stream computation framework built on top of Spark Streaming and Spark SQL. It allows engineers and data scientists at Airbnb to easily leverage Spark Streaming and SQL to get real time insights and to build real time feedback loops.\n\nThe architecture of Airstream is given in Fig. 7. Airstream takes a very simple config. It defines sources (e.g. kafka), computation (e.g. SQL query), and sinks (e.g. kafka, redis, hdfs, hbase).\n\nIn our implementation, we config an Airstream job to collect calendar view impressions. We define the source as kafka and the sink as a hbase table with TTL 24 hours so that it will only keep data in the past 24 hours. The Airstream job is run every minute. As soon as the main calendar reminder pipeline finishes, we join the generated host ids with the host ids that appear in the Airstream sink table where the later host ids are excluded in the final list of sending calendar reminder. Finally, we send reminder emails by the Mochi, which is a service developed at Airbnb to send high volume marketing emails.\n\nOur experiment results from this calendar reminder email have shown great impact on the Airbnb business. Calendar reminders have helped hosts with successful hosting and lead to guests feeling like they belong. This is also a good example on how real time data can be useful to address users\u2019 feedback immediately.\n\nFor regular calendar reminders, we are sending emails at a particular day of a week for all hosts that need to be reminded. We are working on a machine learning model to find personalized email sending time that is the best for each individual host. We are also working on integrating the data from this project into other products. Please stay tuned!\n\nNew technologies have made it easier to keep each other at a distance. We are using them to bring people together and to make people feel more belonging."
    },
    {
        "url": "https://medium.com/airbnb-engineering/measuring-transactional-integrity-in-airbnbs-distributed-payment-ecosystem-a670d6926d22?source=---------3",
        "title": "Measuring Transactional Integrity in Airbnb\u2019s Distributed Payment Ecosystem",
        "text": "With rapidly emerging payments technologies, merchants face a continually evolving landscape when it comes to processing payment transactions. The advent of new value-add entities, such as payment gateways, has offered increasingly large benefits to merchants, providing simplification by offering vaulting services and single API integration for payments processing. By integrating with payment gateways, Airbnb has been able to rapidly scale worldwide through various processing entities with minimal changes to our online transaction processing.\n\nThis being said, integration with any new value-add entity comes at a cost. Each new entity in the payments process adds an additional layer where a transaction\u2019s integrity can be effected. A breakdown in a transaction\u2019s integrity can create headaches for community members, increased workload for customer support, and an operational efficiency overhead.\n\nGenerally, a transaction represents a unit of work performed in RDMS (Relational Database Management System) with atomic, consistent, isolated and durable properties. When it comes to payments, transactional integrity represents a no-surprise, accurate money movement. Accuracy of a financial transaction can be verified by various attributes, such as amount, currency, status, payment method, and even timeliness.\n\nAt Airbnb, transactional integrity encompasses both internal and external money movements. This means that we not only expect full consistency of payment attributes between our internal systems, but also across all external partners that the payment touches.\n\nAirbnb is an accommodation platform connecting guests and hosts to enable travel worldwide. Payments at Airbnb is a key factor to enable community trust, both on- and off-platform. To maintain this trust, it is crucial that we properly handle payments between our guest and host communities with the utmost accuracy.\n\n \n\nAirbnb is a global brand operating in over 190 countries and 40 currencies around the world, including markets that are hard to reach and not commonly supported in the payments ecosystem. A singular processor relationship to move money globally simply does not exist in today\u2019s world. As a result, Airbnb integrates with a handful of gateways and dozens of processors to achieve global coverage and payments redundancy. This system includes processors with varying degrees of maturity in their systems, different modes of integration (API call, batch and URL redirect), and widely differing transaction settlement periods. For example, Airbnb supports the processing of completely synchronous payment methods, such as major credit/debit card networks, as well as payment methods that can take up to several days to settle, such as Brazilian Boletos.\n\nFor a traditional online merchant, money flows into the system as a result of online purchases. More recently however, with the sharing economy on the rise, we have started to witness an increase in two-sided marketplaces. Airbnb is one such example \u2014 connecting travelers and hosts worldwide. As a result, Payments at Airbnb handles bi-directional money flow, not only handling payments into our platform, but also all payment outflows to hosts.\n\nA large portion of the money flowing out to our hosts occurs with direct integration with banks via batch processing. Batch transaction processing involves two steps. First, we send a collection of transaction requests to a bank in a compliant format. We then process the response file(s) that bank sends us back containing responses to the transaction requests. While batch processes are suitable for larger transaction volume processing, the batching process, by nature, leaves our systems out-of-sync until batch response files are processed. Because this process can take up to several hours to complete, batch processing can prove to be a difficult barrier in maintaining transactional integrity.\n\nEven in the most simple example of an Airbnb transaction, a reservation between guest and host, there are at least two financial transactions associated with it. The first financial transaction occurs when the guest pays for the reservation to Airbnb, and the second occurs when Airbnb pays the host within hours of rendering the service. However, travel plans change more often than we think. Additional payment features such as alterations, deposits/installments, group travel, tax withholding, VAT etc. all dramatically increase the number and complexity of financial transactions associated with a reservation. Additionally, the fact that many reservations on Airbnb platform are cross border, involving multiple currencies, further increases the complexity of our money movements.\n\nAirbnb has seen an explosive growth in its marketplace in recent years, with payments being a critical underpinning of the expansion. Until recent years, most of Airbnb\u2019s business and financial transaction logic was performed in a monolithic rails application.\n\n \n\nTo improve scalability, Airbnb is making a significant investment in Service Oriented Architecture. As part of this strategy, we set out to build an internal payment gateway to encapsulate all network communication to/from various processors and handle the \u201cburden\u201d of executing money movements for the application. Our new payment gateway is a Java Service with a dedicated datastore. This datastore hosts various payment methods and serves as a system of records for financial transactions.\n\n \n\nThis new service represents two distinct challenges with transactional integrity. First, an additional internal gateway increases the number of hops made during payment execution and if it behaves in an inconsistent manner or fails to process a gateway or processor response, it will create an \u201cout-of-sync\u201d transaction. Secondly, while we ramp up traffic on the new payment gateway there will be two transaction stores with Airbnb internal system \u2014 a legacy transaction store and a new payment gateway transaction store. We cannot afford any drift in consistency within our two data stores for any significant amount of time. These two challenges warranted additional consideration for transactional integrity.\n\nIf any system in the payments processing chain fails to respond and/or its subsequent system fails to properly consume the response of money movement it creates an \u201cout-of-sync\u201d transaction. Additionally, incorrect treatment of API responses by any entity in the chain can lead to \u201cout-of-sync\u201d transactions.\n\nMeasuring transactional integrity across this maze of distributed systems in a timely fashion, and subsequently detecting and responding to any anomalies is a challenge. Using many systems of varying maturity makes it nearly impossible to instantaneously track a transaction through various states in different systems \u2014 platform, payment gateway, payment processor, etc. \u2014 at all times.\n\nOne potential solution that comes to mind is to use payment gateway APIs for transaction record comparison. The downside to this approach is that it\u2019s harder to scale the comparison between internal transaction stores and the external transaction stores using APIs. In fact, some external entities do not even offer this information via API. Furthermore, a dedicated comparison system may be needed to execute API calls to the entity for transactional analysis to avoid any potential impact on live traffic, a scenario which most web-services cannot tolerate.\n\nAlmost all processors and gateways offers transaction reports and details to the merchant via secure file transfer protocol (SFTP). These transaction reports are offered as part of settlement to the merchant within an agreed upon SLA. Typically, merchants reconcile all the money movement on the platform via 3-way reconciliation between platform transaction records, processor settlement files, and bank statements. Airbnb has a dedicated service that imports, extracts and exports every processor file. In detail, the service:\n\nAdditionally, it has logic to detect duplicate files and avoid repeated processing of the same file. Many processors/gateways offer these reports in CSV format and every entity uses their own vocabulary in the reports. These transaction details are then stored in a datastore and as part of the export they are sent for reconciliation."
    },
    {
        "url": "https://medium.com/airbnb-engineering/server-rendering-code-splitting-and-lazy-loading-with-react-router-v4-bfe596a6af70?source=---------4",
        "title": "Server Rendering, Code Splitting, and Lazy Loading with React Router v4",
        "text": "Historically, Airbnb has been a Rails app. A few years back that started to change, we began using Rails simply as a data layer, and all render logic started migrating into JavaScript in the form of React. In order to maintain server rendering, we created and open sourced Hypernova, a JavaScript Rendering as a Service\u2026 service.\n\nTaking this a step further, we introduced client side routing and route based code-splitting with React Router v3 as part of our architecture revamp. This is what enabled the smooth page transitions, and smaller initial page loads.\n\nServer rendering + code splitting boils down to a single requirement. In order for them to work together you need to be able to match against your current route before rendering.\n\nThe problem then, is that React Router v4 switched from a centralized route configuration (with a function for async loading) to a decentralized version. Routes are now defined inline like so:\n\nDefining routes in this manner means you won\u2019t know which routes/components are needed to render your page until you are actually rendering. To demonstrate why this is a problem, imagine we\u2019re async loading the component.\n\nWhen we server render, we\u2019ll want to render all of the content, so the html for the component will be generated and inserted into the DOM tree. On the client side though, we won\u2019t know to match the route nor would we know to load the component, until after we\u2019ve already entered the render cycle. This will cause a client/server mismatch error since without having the component loaded, the client won\u2019t create the same html as the server. This also likely means a flash of content and a wasted render, meaning a worse experience for your users.\n\nTo address the unique problems with inlining routes, they\u2019ve created . This let\u2019s you continue to define your routes in a centralized location, and match against them before triggering your initial render. Using the library, our routes definition might look something like this:\n\nUsing is great, but there is still a bit of work left to do. doesn\u2019t seem to have support for loading components in an async manner, and child routes are required to be too explicit. Notice that all the path values are defined as full paths.\n\nThis can get a little unwieldy, and limits reuse. To address this issue, we\u2019ve implemented a mapping function that allows components to define child routes.\n\nDoing this gives us a little bit more freedom during development. The grandchild route no longer needs to know its full path, and can be inserted into your central configuration at any location to create deeply nested routes.\n\nThis is extremely powerful, especially in large code bases. Routes can be reused in multiple locations, and component logic can be route agnostic, so their reusability increases as well. This is what makes our current transition into a large single page app (SPA) possible, as we can add additional routes without bloating our core flow. As our SPA grows over time to include more product pages, we can rest assured that any individual page only contains what is needed.\n\nWe need to ensure that all the components, for whichever route a user hits, are loaded before we make the call to render.\n\nTo start, we\u2019ll create an async component definition, then we\u2019ll change our grandchild route to use the new component. Note the static load function, this will be used later to ensure that we\u2019re ready to render.\n\nAlso, let\u2019s update our grandchild route to leverage the new helper function.\n\nNow we\u2019re exporting a route config that contains a route with component that defines a static function. All that\u2019s left to do before rendering is ensure that everything is loaded.\n\nNow that everything\u2019s in place, we\u2019re finally ready to render our application. This is what everything looks like when tied together (minus the definitions for the helper functions , , and )\n\nThis post is a bit heavy with code, I thought it would be helpful to see all of this in action. Check out the demo repository to see everything put together, and feel free to explore the code!\n\nWe\u2019re always looking for ways to improve, both in our core products and in the open source libraries that we depend upon."
    },
    {
        "url": "https://medium.com/airbnb-engineering/smart-instant-book-filter-book-with-confidence-ab3d6dace0b2?source=---------5",
        "title": "The Smart Instant Book Filter \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "People like it when things are fast. We want our deliveries within an hour, our downloads within a minute, and our news in an instant. That is why we need to help our Airbnb community to book listings they want instantly. Instant Book is a key lever in our product to empower the Airbnb community to book places immediately without prior host approval and increase acceptance.\n\nOur landscape of Instant Book has changed incredibly over the past few years and today 2 out of 3 booking are made using Instant Book. Since 2016, we have been steadily improving Instant Book, and one of the biggest levers is the smart Instant Book Filter. In the following sections, we will provide an overview of the smart Instant Book filter and shed light on how it helps our Airbnb community to book instantly and belong anywhere.\n\nAt Airbnb, we are always aiming to offer a convenient and successful booking experience. Before we offered Instant Book, guests using reservation requests can often get multiple rejections, and the situation may turn out worse for travelers who want to make their plans quickly. With Instant Book, guests can filter on our search page to find places where reservations can be confirmed instantly. Still, we\u2019ve realized some guests select Request-to-Book listings without knowing the chance of rejection and possible delay in response.\n\nMotived by this, we initiated a smart Instant Book Filter \u2014 intelligently applying the Instant Book filter to searches to encourage a better user experience.\n\nWe were faced with the following two challenges before we could move forward:\n\nWithin our Search service, a broad knowledge of idiosyncrasies of all listings and market dynamics are available. We built a decision engine embedded in search service leveraging this data to to answer question #1. In addition, we have a powerful channel in platform to deliver informative insights to our guests \u2014 Market Insight. This has addressed question #2.\n\nNext we will explain the workflow and experience of the smart Instant Book Filter.\n\nIn this section, we start by conceptualizing the model, then we explain the data, algorithm and model. Finally, we will describe the user experience.\n\nAt a high level, when a guest uses Airbnb to search for homes, the API server first collects basic query information and performs an initial eligibility check. Next, the API server queries the Search service and talks with the Market Insights service to decide if the Instant Book filter should be triggered, generating corresponding candidate market insights in real-time. Finally, the API server combines the information collected from both services and reflects the result in the UI.\n\nAs Fig. 1 illustrated, the workflow is as follows:\n\nOur decision engine inside the Search service incorporates heterogeneous sources of data\u2014user data, listing data, platform data, and query dynamics\u2014to make a deliberative decision.\n\nWe aim to shift Request-to-Book to Instant Book by smartly applying the Instant Book filter. As a result, we only show Instant Book results for our guests under certain circumstances. Yet the biggest challenge here is how to avoid booking loss during this procedure due to reduced supply size. An analogy of this scenario is shown in Fig. 2.\n\nSo we consider the following problem setup. Each search is associated with a ranking score s, which is composed of a Instant Book ranking score and a Request-to-Book ranking score. We use a function g to serve as the proxy between booking Bs and ranking score s so we have: Bs = g(s). Note here we don\u2019t assume any form of function g. Thus the problem is reduced to increase Instant Book without decreasing Bs.\n\nLet s0 be the ranking score without the smart Instant Book Filter, then we can have a Taylor expansion of function g about s0 in equation 1.\n\nBy approximating this to the first order, we have equation 2. Then the booking change percentage can be represented as equation 3, where \u03b1 is a constant depending on platform and seasonality. Similarly, we can calculate the Instant Book percentage change in equation 4. Here \u03b2 depends on proportion of Instant Book results in current search results.\n\nWe can backfill the values of \u03b1 and \u03b2 by analyzing previous Instant Book ranking experiments. With \u03b1 and \u03b2, we can reliably predict future experiment\u2019s performance based on offline computed ranking score change. With all these statistics, we can optimize our deterministic decision tree based decision engine according to fixed strategy. More formally, we want to:\n\nwhere w is our feature vector, which is described in data section and servers as the branching factors in our deterministic decision tree, and s1 is our target booking score.\n\nWe can interpret equation 5 as maximizing Instant Book ranking score while keep the ranking score for overall booking slightly positive or neutral. We can also maximize the overall booking ranking score given target Instant Book ranking score vice versa. We\u2019ve computed our feature vector values offline by searching in the dimension space using Python toolbox to best promote Instant Book. One example of the dimension space distribution is illustrated in Fig. 3, where we search the best value for Instant Book score gain given distance ratio dimension and discounted utility value dimension.\n\nNext, we will illustrate the user experience of smart Instant Book filter.\n\nWhen the Search service finds it eligible to turn on Instant Book filter, with a sufficient supply of high quality, we will automatically turn on Instant Book filter and alert guests with a tooltip.\n\nIf guests keep zooming in the map to narrow down searching area while Instant Book filter is on, which consequently find fewer and fewer Instant Book results, a market insight card will replace a single listing card to remind guests they can release smart Instant Book filter with one click.\n\nOver the past year, we\u2019ve closely partnered with user experience researchers, product specialists and data scientists on how to promote Instant Book and convey the message \u2014 Instant Book is the future, to our Airbnb community. A tremendous amount of effort has been dedicated to host side. This project is where we started on guest side and landed with good results. As a result, our work has driven Instant Book percentage by more than 5%, blending in a substantial way of company growth.\n\nMost bookings are made via Instant Book today than Request-to-Book. As guests are finding it easier to leverage the Instant Book filter, a second order effect we have identified in experiment is the tendency of hosts to adopt Instant Book, which is comparable to strong host-side Instant Book adoption campaigns. The boundary between guests and hosts is blurry \u2014 many hosts are guests, as well. This signifies that the interaction between guests and hosts is remarkably important, which paves the way for more advances in this area.\n\nMarket dynamics are ever-changing. Airbnb has been a pioneer in exploring and leveraging advanced technologies, including building automated machine learning infrastructure and workflow management platform. As we are constantly iterating on smart Instant Book filter, in addition to the current state of model, we are planning to leverage automated ML infrastructure and integrate system with an online scoring model to capture evolving conditions."
    },
    {
        "url": "https://medium.com/airbnb-engineering/building-services-at-airbnb-part-1-c4c1d8fa811b?source=---------6",
        "title": "Building Services at Airbnb, Part 1 \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Airbnb is moving its infrastructure at an accelerated pace towards a SOA (Service-Oriented Architecture), but moving from a monolithic Rails service towards a SOA while building out new products and features is not without its challenges.\n\nIn this post, we share what we have designed and built to scale the development of services \u2014 using the same number engineer-hours to build more backend services that are more robust, performant, and easier to maintain. This is the first in a series of posts on this topic; in this post we present a bird\u2019s-eye view of the approach and overall architecture, and subsequent posts will drill down into specific components.\n\nAt Airbnb, backend services are mostly written in Java using the Dropwizard web service framework, and a number of Airbnb-specific custom Dropwizard filters and modules standardize server-side best practices. In addition, a make-me-a-service tool helps engineers to bootstrap a ready-to-deploy Dropwizard application skeleton, on top of which engineers add RESTful and JSON-over-HTTP service resource endpoints.\n\nHowever, the above combination leaves much to be desired; major shortcomings service developers bring up include:\n\nA service\u2019s API includes both the service interface and request/response data schemas. In a REST service world, tools such as Swagger can be used for automating some boilerplate tasks like documenting the REST API and providing basic client code generation. However, compared to a full-fledged service RPC framework such as Apache Thrift and Google gRPC, most of these REST tools lack rich features (e.g. strong type support, request/response validation, efficient payload encoding and transport protocols) that would accelerate development of robust and performant services.\n\nThrift and gRPC are two popular RPC systems that have wide industry adoptions. However, replacing the HTTP service stack at Airbnb with either one means an big infrastructure overhaul that would require a large amount of engineering resources and cause major disruptions to productivity.\n\nAfter weighing pros and cons of different technical approaches, we decided to take one that does not disrupt the current Java service development process while opening up the possibility of incrementally improving and evolving the services architecture in the future. Our approach is to keep the Dropwizard service framework, add customized Thrift service IDL (interface definition language) to the framework, switch the transport protocol from JSON-over-Http to Thrift-over-Http, and build tools to generate RPC clients in different languages.\n\nWe explain in more detail below how this approach works to scale development while minimizing disruption.\n\nWe use Thrift IDL to define a service\u2019s API \u2014 its interface and request/response data schema. In the service IDL-based Java service development flow, the developer defines the service API in .thrift files, from which service-side code and RPC clients are generated with Airbnb service platform-standard instrumentations that help enforce infrastructure best practices. This simplifies service development and allows engineers to focus on writing service business logic rather than on plumbing and monitoring work (e.g., inter-service transports, metrics, alerts, and backward compatible API management).\n\nService developers are familiar with the Dropwizard service framework and have built many common libraries for it. Dropwizard is JSON-over-Http (using Jersey and Jackson libraries), but we want to bring Thrift service IDL to the service development. Since it is important that we do not disrupt the current service framework and inter-service communication protocol, we extended Dropwizard with Thrift payload type support to make the inter-service communication protocol transition seamlessly from JSON-over-Http to Thrift-over-Http.\n\nThe use of Thrift as service IDL inherently provides clear definition and documentation for the service API; but moreover, every field in a data schema have clearly defined types that are strictly enforced in the generated Java and Ruby data classes. Managing API backward-compatibility is made simpler and easier with built-in schema enforcement and carefully thought-out server-side and client-side generated code with API changes and backward-compatibility in mind.\n\nWe also extended types beyond vanilla Apache Thrift IDL with additions such as Date and DateTime. The rationale behind the type extension is based on past experience \u2014 if service developers were restricted to too small a set of types they would end up reshaping the types they want through existing types (e.g, date as a string), bypassing the automated type-checking and validation, and defeat the purpose of having a strong-typed schema.\n\nWriting a service entails more work than simply implementing server-side business logic. Implementing service RPC clients in different languages, setting up and executing inter-service communication, adding service-side and client-side metrics for monitoring, and adopting standard service platform practices are tedious to do, easy-to-get-wrong, and incur significant engineering costs. Before service IDL, the cost of creating and managing services resulted in engineers and management shying away from SOA under time pressure.\n\nWe want service development with service IDL to be simple, cost-efficient, and provide a step function in terms of added benefits. For that, vanilla Apache Thrift is not enough. We made extensive customization to existing Thrift IDL compilers to generate service boilerplate code and RPC clients with many additional features. We believe it is key in accelerating Airbnb technical stack\u2019s move toward SOA.\n\nThe diagram below shows an example Java service, called Banana, using the service IDL, and Ruby service and another Java service using the generated service client to make RPC calls. The components in red are automatically generated/instrumented based on Banana\u2019s service IDL.\n\nA service developer, besides writing their service IDL, can also declare service dependency by pointing to other service IDL files. Both a service\u2019s IDL files and ones from services it depends on are seamlessly integrated into the existing build process and build tools. Service developers do what they have been doing previously and all the IDL-related code they need is automatically generated and shows up in their IDE without additional steps.\n\nWe extended Apache Thrift code generator with Dropwizard resource generation. The generated service resource class and methods come with necessary Jersey and Jackson annotations to support both JSON and Thrift media types in the HTTP payload \u2014 the service can receive and return both JSON and Thrift request/response data. Thrift binary transport is for performance, and JSON is for compatibility with older services and for easy testing and debugging.\n\nSince the glue-code between business logic and Dropwizard is generated, service developers can write code that is agnostic of the underlying web server framework; so if we upgrade Dropwizard or move away from Dropwizard it will be an easier migration.\n\nBefore service IDL, engineers had to implement clients for other applications that talk to their service. Not only it is time-consuming to write clients in more than one language, they can become quickly outdated and lose parity (e.g., between Ruby and Java clients). With service IDL, ready-to-use Java and Ruby API clients are completely generated. These generated clients are not the Thrift client for Ruby or Java that come with vanilla Apache Thrift. They are RPC clients that provide many features (e.g., mTLS, retry, validation, request context) not found in vanilla Apache Thrift.\n\nAs engineers build more services, it is important that all services adopt consistent best practices. For example, a service request has contextual information associated with it, and the request context should be propagated on the service RPC call chain. This would be difficult to manage if different services had different ways of propagating this context (or not at all), or if this context was defined and obtained in different ways. It would also be a headache for SRE to monitor and debug SOA issues if each service emitted different metrics and had different types of alerts. Emitting standard request and response metrics on both server-side and client-side not only allows for better monitoring in and outside the service-owning team, it can be leveraged for creating automated service dashboards and alerts. Service clients should also adopt standard RPC timeout, retry, and circuit breaker logic; our generated RPC clients makes it so service developers don\u2019t have to implement these RPC resilience features themselves.\n\nAdding Thrift service IDL to our Java service framework is an important first step that towards a reliable, performant, and developer-friendly service platform. It is a centerpiece structure on which many more development-scaling features can be built as Airbnb moves its infrastructure towards SOA.\n\nThe new service IDL-driven Java service development process has been widely embraced by product teams since its release. It has dramatically increased development velocity; several new Java services built with service IDL, from the service inception to taking production traffic, took only three weeks \u2014 we estimate it to have saved 2\u20133 weeks of engineering time per Java service over the previous development process. We plan to extend service IDL support to Ruby services as well.\n\nIn this post we presented our approach for scaling service development while minimizing disruption, and introduced the service IDL as a centerpiece for this approach. We left out much of the design and implementation details of various components of the service IDL; they will be covered in more depth in subsequent posts in this series. Stay tuned!"
    },
    {
        "url": "https://medium.com/airbnb-engineering/recent-web-performance-fixes-on-airbnb-listing-pages-6cd8d93df6f4?source=---------7",
        "title": "React Performance Fixes on Airbnb Listing Pages \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "There may be a lot of low-hanging fruit \ud83e\udd5d affecting performance in areas you might not track very closely\u2026but are still very important.\n\nNot only did this hurt performance, it also caused an extra label to be visibly rendered and then removed from the page every time. Janky! I fixed this by moving the rendering of this content into React state and set it in componentDidMount , which is not run until the client renders. \ud83e\udd42\n\nIt turned out that we used some feature detection to make sure the placeholder was visible in older browsers, like Internet Explorer, by rendering the input differently if placeholders were not supported in the current browser. Feature detection is the right way to do this (as opposed to user agent sniffing), but since there is no browser to feature detect against when server rendering, the server would always render a little bit of extra content than what most browsers will render.\n\nThis narrowed down my search pretty quickly to something called o2/PlaceHolderLabel.jsx , which is the component that is rendered at the top of the reviews section for searching. \ud83d\udd0d\n\nUnfortunately, the error message isn\u2019t super clear about exactly where this happens or what the cause might be, but we do have some clues. \ud83d\udd0e I noticed a bit of text that looked like a CSS class, so I hit the terminal with:\n\nThis is the dreaded server/client mismatch, which happens when the server renders something differently than what the client renders on the initial mount. This forces your web browser to do work that it shouldn\u2019t have to do when using server rendering, so React gives you this handy \u270b warning whenever it happens.\n\nNormally, I advocate for profiling on mobile hardware like a Moto C Plus or with CPU throttling set to 6x slowdown, to understand what folks on slower devices experience. However, since these problems were bad enough it was plainly obvious what the opportunities were on my super fast laptop even without throttling.\n\nThrough a process of profiling, making a fix, and profiling again, we dramatically improved the interaction performance of this critical page, which makes the booking experience smoother and more satisfying. In this post, you\u2019ll learn about the techniques I used to profile this page, the tools I used to optimize it, and see the scale of this impact in the flame charts produced by my profiling.\n\nAs part of this migration into our single-page app, I wanted to investigate any lingering performance issues affecting interactions on the listing page (e.g. scrolling, clicking, typing). This fits with our goal to make pages start fast and stay fast, and generally just makes people feel better about using the site.\n\nThis is the page you visit when deciding which listing to book. Throughout your search, you might visit this page many times to view different listings. This is one of the most visited and most important pages on airbnb.com, so it is critical that we nail all of the details!\n\nWe have been hard at work migrating the airbnb.com core booking flow into a single-page server-rendered app using React Router and Hypernova . At the beginning of the year, we rolled this out for the landing page and search results with good success. Our next step is to expand the single-page app to include the listing detail page .\n\nThis ends up re-rendering a , two , and a when it updates. However, none of these have any differences, so we can make this operation significantly cheaper by using on these three components. This was about as straightforward as changing this:\n\nUp next, we can see that also goes through a re-render on the initial pageload. According to the flame \ud83d\udd25 chart, most of the time is spent rendering and .\n\nThe funny thing here is that these components aren\u2019t even visible \ud83d\udc7b unless the guest input is focused.\n\nThe fix for this is to not render these components when they are not needed. This speeds up the initial render as well as any re-renders that may end up happening. \ud83d\udc0e If we go a little further and drop in some more PureComponents, we can make this area even faster.\n\nWhile doing some work to modernize a smooth scrolling animation we sometimes use on the listing page, I noticed the page felt very janky when scrolling. \ud83d\udcdc People usually get an uncomfortable and unsatisfying feeling when animations aren\u2019t hitting a smooth 60 fps (Frames Per Second), and maybe even when they aren\u2019t hitting 120 fps. Scrolling is a special kind of animation that is directly connected to your finger movements, so it is even more sensitive to bad performance than other animations.\n\nAfter a little profiling, I discovered that we were doing a lot of unnecessary re-rendering of React components inside our scroll event handlers! This is what really bad jank looks like:\n\nI was able to resolve most of this problem by converting three components in these trees to use : , , and . This dramatically reduced the cost of these re-renders. While we aren't quite at 60 fps (Frames Per Second) yet, we are much closer:\n\nHowever, there is still more opportunity to improve. Zooming \ud83d\ude97 into the flame chart a little, we can see that we still spend a lot of time re-rendering . And, if we look down component stack, we notice that there are four similar looking chunks of this:\n\nThe is the part of the listing page that sticks to the top of the viewport. As you scroll between sections, it highlights the section that you are currently inside of. Each of the chunks in the flame \ud83d\ude92 chart corresponds to one of the four links that we render in the sticky navigation. And, when we scroll between sections, we highlight a different link, so some of it needs to re-render. Here's what it looks like in the browser.\n\nNow, I noticed that we have four links here, but only two change appearance when transitioning between sections. But still, in our flame chart, we see that all four links re-render every time. This was happening because our component was creating a new function in render and passing it down to as a prop every time, which de-optimizes pure components.\n\nWe can fix this by ensuring that the always receives the same function every time it is rendered by :\n\nAnd then in :\n\nProfiling after this change, we see that only two links are re-rendered! That's half \ud83c\udf17 the work! And, if we use more than four links here, the amount of work that needs to be done won\u2019t increase much anymore.\n\nDounan Shi at Flexport has been working on Reflective Bind, which uses a Babel plugin to perform this type of optimization for you. It\u2019s still pretty early so it might not be ready for production just yet, but I\u2019m pretty excited about the possibilities here.\n\nLooking down at the Main panel in the Performance recording, I notice that we have a very suspicious-looking block that eats up 19ms on every scroll event. Since we only have 16ms if we want to hit 60 fps, this is way too much. \ud83c\udf2f\n\nThe culprit seems to be somewhere inside of . Through some code searching, I track this down to the . And looking a little closer at these call stacks, I notice that most of the time spent is actually inside of React's , but the weird thing is that we aren't actually seeing any re-renders happening here. Hmm...\n\nDigging into a little more, I notice that we are using React state \ud83d\uddfa to track some information on the instance.\n\nHowever, we never use this state in the render path at all and never need these state changes to cause re-renders, so we end up paying an extra cost. \ud83d\udcb8 Converting all of these uses of React state to be simple instance variables really helps us speed up these scrolling animations.\n\nI also noticed that the was re-rendering, which caused an expensive \ud83d\udcb0 and unnecessary re-render of the component.\n\nThis ended up being partly caused by our higher-order component which we use to help us run experiments. This HOC was written in a way that it always passes down a newly created object as a prop to the component it wraps\u2014deoptimizing anything in its path.\n\nI fixed this by bringing in reselect for this work, which memoizes the previous result so that it will remain referentially equal between successive renders.\n\nThe second part of the problem was similar. In this code path we were using a function called which took an array as its first argument and returned a filtered version of that array, similar to:\n\nAlthough this looks innocent enough, this will create a new instance of the array every time it is run, even if it produces the same result, which will deoptimize any pure components receiving this array as a prop. I fixed this as well by bringing in reselect to memoize the filtering. I don\u2019t have a flame chart for this one because the entire re-render completely disappeared! \ud83d\udc7b\n\nThere\u2019s probably still some more opportunity here (e.g. CSS containment), but scrolling performance is already looking much better!\n\nInteracting with the page a little more, I felt some noticeable lag \u2708\ufe0f when clicking on the \u201cHelpful\u201d button on a review.\n\nMy hunch was that clicking this button was causing all of the reviews on the page to be re-rendered. Looking at the flame chart, I wasn\u2019t too far off:"
    },
    {
        "url": "https://medium.com/airbnb-engineering/congratulations-to-three-inspiring-women-at-airbnb-being-recognized-by-the-women-of-color-in-stem-131b6ace2c85?source=---------8",
        "title": "Congratulations to three inspiring women at Airbnb being recognized by the Women of Color in STEM\u2026",
        "text": "At Airbnb, we value the diversity of our workforce, our guests, our hosts, and the world around us. Today, we\u2019d like to congratulate three inspiring women who will be recognized as rising stars in their fields at the upcoming Women of Color in STEM Conference. This conference is the forum of choice for recognizing the significant contributions by women in STEM fields. Every year, they recognize women who truly excel in their field and inspire their peers with dedication and passion for their work.\n\nWe couldn\u2019t be prouder of our team members who are being awarded this year.\n\nTheresa Johnson is a data scientist who leads a team building analytics products at Airbnb. Theresa is also the co-founder of StandardDeviation, an employee resource group for under-represented minorities in technical roles. Her early fascination with technology for social good led her to earn undergraduate degrees in Science, Technology and Society and Computer Science from Stanford University. Dr. Johnson came to data science after earning a PhD in Aeronautics and Astronautics from Stanford University. She spent her graduate school summers pointing radar at the Greenland morning sky, searching for elusive meteoroids. When she wasn\u2019t examining shooting stars, she mentored rising star students on entrepreneurship and innovation. She is a now a founding board member of StreetCode Academy, a non-profit dedicated to high touch technical training for inner city youth. She has been featured in TechCrunch and USAToday as a mentor to others seeking careers in tech.\n\nFun fact: Theresa enjoys doing yoga with with her dog and 1 year old daughter, usually at the same time.\n\nDiane is a front end software engineer, leading web accessibility at Airbnb. At Airbnb, Diane has been passionate about building the foundation for accessibility and teaching others about the impact of their work on individuals with disabilities. She is also the President of Nerdettes, an employee resource group for those who identify as a woman in tech. After graduating from UC Berkeley in Electrical Engineering and Computer Sciences, Diane went on to work at Intuit, where she was first exposed to the importance of accessibility. She then went on to work at SurveyMonkey, where she was a member of their Technical Leadership Council, a group that guides technical decision making across the company.\n\nFun fact: Outside of work Diane loves to rock climb and play ice hockey.\n\nCuky Perez is a data science manager at Airbnb. She currently leads two teams of data scientists. One of her teams, People Analytics, applies data science methodologies to support Airbnb\u2019s HR department, which includes an emphasis on improving opportunities for women and underrepresented minorities at Airbnb. The other team she leads is currently enabling the creation of a new product line on our site that you will see soon. Before Airbnb, Cuky was an Assistant Professor at the University of Washington where she taught graduate students advanced quantitative methodologies. Cuky completed her Ph.D. in the Economics of Education program and MA in Economics at Stanford University prior to that. Before grad school, she worked at the American Institute for Research where she participated in evaluation projects related to immigrant and Latino students, low performing schools, and students with disabilities.\n\nFun fact: Besides data, her passions include boxing and painting. The only practical use of boxing (so far) has been for fundraising campaigns while at UW \u2014 students would basically pay to box Cuky. The fundraising campaign was a smashing success."
    },
    {
        "url": "https://medium.com/airbnb-engineering/prototyping-with-react-vr-4d5ab91b6f5a?source=---------9",
        "title": "Prototyping with React VR \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "A long history with React on Web and Native led to a very natural exploration in the Virtual space. We would love to tell you about it!\n\nReact VR provided a unique opportunity to leverage our expertise to rapidly test and iterate many of the VR ideas we\u2019ve been playing with. To do this, we brought together a small team of React and VR experts at Airbnb Samara \u2019s VR Lab to see what we could accomplish in the 3 weeks leading up to the React VR launch at F8. Here\u2019s an overview of the technology that powers React VR and the similarities between React, React Native, and React VR.\n\nAt Airbnb, we\u2019ve been using React to build user interfaces for our website since 2014. React has radically changed our approach to building UI, and we\u2019ve been active contributors to the open source community with projects such as Enzyme , Hypernova , react-sketchapp , react-with-styles , react-dates , and react-native-maps .\n\nOne of React\u2019s biggest innovations is that it enables developers to describe a system, such as the UI of a web or mobile app, as a set of declarative components. The power of this declarative approach is that the description of the UI is decoupled from its implementation, allowing authors to build custom \u201crenderers\u201d that target more platforms than just web browsers, such as hardware, terminal applications, music synthesizers, and Sketch.app.\n\nBecause React VR is built on top of React Native, let\u2019s start with a look at how it works under the hood. React Native is built on a renderer that controls native UI on iOS and Android. The React application code runs in a JavaScript virtual machine in a background thread on the mobile device, leaving the main thread free to render the native UI. React Native provides a bridge for communication between the native layer and the JavaScript layer of the app. When the React components in your application are rendered, the React Native renderer serializes all UI changes that need to happen into a JSON-based format and sends this payload asynchronously across the bridge. The native layer receives and deserializes this payload, updating the native UI accordingly.\n\nOver the past year at Airbnb, we\u2019ve invested heavily in React Native because we recognize the power of being able to share knowledge, engineers, and code across platforms. In November, we launched our new Experiences platform, which is largely written in React Native on our iOS and Android apps, and we formed a full-time React Native Infrastructure team to continue this investment.\n\nReact VR\u2019s architecture mirrors that of React Native, with the React application code running in a background thread \u2014 in this case, a Web Worker in the web browser. When the application\u2019s React components are rendered, React VR utilizes the React Native bridge to serialize any necessary UI changes and pass them to the main thread, which in this case is the browser\u2019s main JavaScript runtime. Here, React VR utilizes a library from Oculus called OVRUI to translate the payload of pending UI updates into Three.js commands, rendering a 3D scene using WebGL.\n\nFinally, React VR utilizes WebVR\u2019s new API to send the 3D scene to the user\u2019s head mounted display, such as an Oculus Rift, HTC Vive or Samsung Gear VR. WebVR, a new standard being spearheaded by Mozilla, is supported in recent builds of major web browsers. Check out webvr.info for the latest information on browser support.\n\nBecause React VR implements a lot of the same public APIs that React Native implements, we have access to the same vast ecosystem of patterns, libraries, and tools. It will feel familiar for any developer who has built React or React Native apps. We were able to get a VR prototype up and running quickly; in no time at all, we scaffolded a basic React application, set up Redux, and began hitting our production JSON API for sample data.\n\nWith hot module reloading and Chrome Dev tools debugging, we could iterate nearly as fast as in React web and React Native development, which allowed us to throw a bunch of UI ideas at the proverbial wall to see what would stick.\n\nReact VR inherits React Native\u2019s flexbox-based layout and styling engine, with a few tweaks to allow transforms in 3 dimensions.\n\nFlexbox support for React Native is provided by Yoga, a cross-platform layout engine created by Facebook to simplify mobile development by translating flexbox directives into layout measurements. Because it\u2019s written in C, Yoga (n\u00e9e css-layout) can be embedded natively in Objective-C and Java mobile apps. React VR also uses Yoga for flexbox layout. \u201cBut how?\u201d you ask, \u201cIt\u2019s written in C!\u201d The React VR team has accomplished this by using Emscripten to cross-compile the Yoga C code into JavaScript. How cool is that?\n\nThis is a powerful feature of React VR: developers can use the same styling and layout system across web, React Native, and VR, which opens the doors to directly sharing layout styles across these platforms.\n\nLike React Native, React VR provides a set of basic primitives used to construct UI\u2014 , , , \u2014in addition to adding of its own VR-specific primitives, such as , , among others. This allowed us to drop in some of our existing React Native components into VR, rendered on a 2D surface.\n\nThis is hugely exciting because we\u2019ve built our UI component system upon , a library we developed for sharing React components across platforms by providing the basic , , , etc. primitives for a variety of platforms, including web, native, and Sketch.app (via our react-sketchapp project).\n\nThis means we can use the buttons, rows, icons and more directly in VR, keeping Airbnb design language consistent without having to rewrite it all from scratch.\n\nCheck out our engineer Leland Richardson\u2019s talk at React Europe for a more in-depth look at the promise of , below.\n\nAs you could imagine, placing 2D content onto a flat plane in 3D space often falls short of an optimal viewing experience. Currently, many VR apps solve this by rendering 2D UI onto a cylindrical plane curved in front of the viewer, giving it a \u201c2.5D\u201d feel.\n\nReact VR provides the component for this, which we found to be effective for displaying menus and other interactive content in our app, and was the perfect way to integrate the existing 2D React components from our UI library into the VR environment."
    },
    {
        "url": "https://medium.com/airbnb-engineering/accelerating-services-at-airbnb-by-building-a-blazing-fast-thrift-binding-for-ruby-8f63044ba149",
        "title": "Accelerating Services at Airbnb by Building \u2014 and Open Sourcing \u2014 a Blazing Fast Thrift Binding for\u2026",
        "text": "Writing performant code in Ruby can be difficult due to its dynamic nature: unlike lower-level languages where the idiom is zero-cost-abstractions, pretty much everything in a dynamic language is expensive. As such, it was no surprise that the Ruby Thrift Binding took advantage of an important tool that is often used to improve the performance of libraries in dynamic languages: C extensions.\n\n \n\nAs a kind of abstraction, C extensions hide all the detail of a performant implementation with an elegant interface in a high-level language. However, simply using C extensions does not guarantee high performance. In fact, many patterns we observed in the Ruby Thrift Binding are heavily detrimental to its performance. We will discuss some of these patterns and show how Sparsam is able to avoid them.\n\nCreating a new Ruby string is slow, and it\u2019s not much faster when you do it in C either. One of the reasons the Thrift Ruby Binding was slow was the excessive object allocation it does when deserializing data. For each field it reads/writes, a ruby string needs to be allocated and interned through . This pattern is problematic and adds significant overhead to accessing each field. In one of our experiments, simply caching the interned ID of a string resulted in 25% speedup. In early versions of Sparsam, we store every field inside a hash map of . This way, we avoid the cost of creating strings and string interning completely.\n\nAn important reason C extensions are fast is that they circumvent the Ruby VM. By doing so, C extensions do not share the overhead of a dynamic language. Calls that cross the language barrier are not free, especially when calling a ruby function from C, so the best practice is to handle as much as possible inside a big C function.\n\nAlthough the Thrift Ruby Binding handles a large portion of serialization inside C, it also relies on the dynamic dispatch of Ruby VM in the runtime. As a result, a significant chunk of time was spent resolving the correct method to call in Ruby VM. This trait diminishes the point of using a C extension and can cause performance regressions when the message either contains a large number of fields or has a deeply-nested structure. Sparsam, on the other hand, does not rely on the Ruby VM for dispatching. By doing so, we minimize the number of Ruby VM calls in serialization and greatly improves the performance.\n\nOne of the bottlenecks we identified was accessing Thrift\u2019s struct definitions in the serializer. Thrift\u2019s highly compact binary format requires both ends of the communication to have the schema of the struct that\u2019s being serialized. For example, in Ruby, thrift compiles a definition for a struct into a ruby hash like this:\n\nThe schema is stored inside , a constant defined under the Ruby class, and such objects are only accessible through the Ruby VM. This means that for every read/write of a field, the C extension needs to access such schema and perform type conversion between Ruby and C data types to determine which method to use. This problem is made worse by Thrift\u2019s nested struct support, as nested structs will result in nested hash objects. To alleviate this effect, we cache the schema information of structs inside a C++ . Besides being faster in itself, we also avoid the cost of invoking functions in Ruby VM and type conversion.\n\nOne of the problems of using our map was that ruby has to constantly grow the hash map: each time a value is read, the hash map\u2019s capacity needs to be expanded to store another pair of data, result in an expensive call. Furthermore, when accessing a field, two hash lookups are involved: from Field Name to Field ID, and from Field ID to Value. Therefore, we replaced this design with using instance variables directly to store the data. The benefits to this approach are tri-fold: ruby\u2019s hash-growing behavior for instance variables are different from that of hash maps, making it more suitable for storing deserialized data; a layer of indirection is avoided when accessing data; and an object created by Sparsam is much closer to a PORO (Plain Old Ruby Object). This optimization gained us almost 3x speedup on the read path, with no impact on the write performance.\n\nTo test the speed of Sparsam, we compared the speed of several serializers with a simple schema that we\u2019re using in production at Airbnb:\n\nThis schema is simple, yet complex enough to have both required fields and container types. Results of items/second is shown below (higher is better):\n\nThrough optimizations, Sparsam achieved 25x speedup on the writing path and 8x speedup on the read path, accelerating Thrift in Ruby to be as fast as MessagePack, and significantly faster than JSON, allowing us to move more of our endpoints from legacy JSON endpoints to newer Thrift endpoints without hurting performance.\n\nBesides being fast, Sparsam also provides extensive validation of Thrift structs. By default, Thrift\u2019s only checks for required fields; in Sparsam, we provide two additional validation modes: \u201cstrict\u201d and \u201crecursive\u201d.\n\nOpen-source software plays an important role at Airbnb. Faster serialization reduces the overhead of Service Oriented Architecture, and thereby improves the experience of the Airbnb community. By open sourcing Sparsam, we hope to contribute back to the community."
    },
    {
        "url": "https://medium.com/airbnb-engineering/binaryalert-real-time-serverless-malware-detection-ca44370c1b90",
        "title": "BinaryAlert: Real-time Serverless Malware Detection",
        "text": "YARA is a powerful pattern-matching tool for binary analysis. Unlike a simple hash-based signature, YARA rules can classify entire families of malware according to common patterns. As YARA sees more widespread use within the security community, we wanted to find a way to leverage YARA rules to scan for malicious files across our entire organization.\n\nOther security tools support YARA rule integration, but we could not find a private, low-cost, scalable, batteries-included solution that was easy to deploy and maintain. For example, VirusTotal supports YARA rule matching against file submissions, but it is a public service and not designed for analyzing internal files and documents with varying levels of confidentiality and sensitivity.\n\nBinaryAlert is our solution: a serverless framework for scalable YARA analysis that you can deploy in your own AWS account cheaply and easily!\n\nTime is of the essence when responding to a threat, so BinaryAlert analyzes files almost immediately after being uploaded. In our deployment, analysis is usually completed within 1\u20132 minutes of file discovery.\n\nWhen the YARA ruleset is updated, BinaryAlert will automatically re-analyze your entire file corpus to find any new matches. This allows you to identify threats in the past with information you receive in the future, and it provides an easy mechanism for testing the efficacy of new rules.\n\nBinaryAlert includes several of our own YARA rules and also makes it easy to clone rules from other open-source projects like YaraRules. Each included rule has been tested against more than 2 million executable binaries from Airbnb\u2019s environment to verify its effectiveness.\n\nLike StreamAlert, BinaryAlert utilizes AWS Lambda functions for analysis instead of a traditional server. This provides a number of benefits, including stronger security (no servers to patch or maintain) and lower cost (pay only for what you use).\n\nAgain following StreamAlert\u2019s example, BinaryAlert uses Terraform to manage its underlying infrastructure. This considerably simplifies the deployment process: a single command creates and configures all of the necessary AWS components. Deployments are simple, safe, and repeatable.\n\nBinaryAlert uploads custom metrics about its processing throughput and automatically creates CloudWatch alarms to monitor the health of your deployment. Alarm thresholds are easily configurable to accommodate different workloads.\n\nFortunately, Terraform automatically configures all of these services so you don\u2019t have to!\n\nA future version of BinaryAlert will add support for file pre-processing, including decompressing and unpacking, prior to the YARA analysis. We can leverage AWS Step Functions to better orchestrate the different stages of the pipeline.\n\nAirbnb is also committed to supporting the YARA community. We will continue to contribute our own YARA rules as well as to source, test, and provide feedback on rules from other open-source projects.\n\nServerless architectures have proven effective for security tools due to the lower cost, simpler management, and scalability associated with serverless designs. BinaryAlert represents our next contribution in the open-source serverless security space, allowing others to more quickly and easily detect malicious files within their own organization."
    },
    {
        "url": "https://medium.com/airbnb-engineering/building-mixed-language-ios-project-with-buck-8a903b0e3e56",
        "title": "Building Mixed-Language iOS Project with Buck \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "The build flag causes implicit imports of Objective-C files into Swift, within the same module. This flag unfortunately doesn\u2019t work with Buck.\n\nWhen Xcode generates frameworks, it generates and header map files to indicate header locations. The tool later uses these files to import Objective-C headers. However, since Buck doesn\u2019t generate independent frameworks, it doesn\u2019t generate these files. Thus, the flag doesn\u2019t work in the tool.\n\nThis means that we have to explicitly pass in bridging headers to the tool. Doing this, however, results in a few more problems.\n\nConsider this example: contains the line , but is put under . This works perfectly with Xcode with the help of . But in Buck, this doesn\u2019t work since it is not able to locate .\n\nIn this PR, we updated Buck to allow it to generate header maps for use by the tool, allowing the tool to locate header files and import them.\n\nWhen the tool generates files, it explicitly imports bridging headers for Objective-C definitions. This breaks Buck builds.\n\nAccording to Apple\u2019s code, when using the flag, the generated files import project headers. For example:\n\nWhen providing bridging headers explicitly however (as we need to do with Buck) the generated files end up importing bridging header files directly:\n\nAs you can see, the imported path is a relative one. When another file imports this file, it won't be able to locate the bridging header.\n\nIn this commit, we updated Buck to pass as a compiler argument. That specifically tells the tool to look for the bridging header files at .\n\nThere are two ways to import header files into Objective-C, and . doesn\u2019t work in Buck since Buck doesn\u2019t generate .\n\nThis actually requires us to replace with and/or . This is simple enough for our own source code. However, it's a bit trickier for generated code. files, for instance, always use .\n\nTo address this, we used an admittedly hacky solution, introducing this script to perform the replacement on the fly. In this commit, we added to Buck\u2019s build rule a new parameter, which allowed us to invoke the replacement script on all the files.\n\nThis change knocked down our last big blocker for using Buck."
    },
    {
        "url": "https://medium.com/airbnb-engineering/using-machine-learning-to-predict-value-of-homes-on-airbnb-9272d3d4739d",
        "title": "Using Machine Learning to Predict Value of Homes On Airbnb",
        "text": "Data products have always been an instrumental part of Airbnb\u2019s service. However, we have long recognized that it\u2019s costly to make data products. For example, personalized search ranking enables guests to more easily discover homes, and smart pricing allows hosts to set more competitive prices according to supply and demand. However, these projects each required a lot of dedicated data science and engineering time and effort.\n\nRecently, advances in Airbnb\u2019s machine learning infrastructure have lowered the cost significantly to deploy new machine learning models to production. For example, our ML Infra team built a general feature repository that allows users to leverage high quality, vetted, reusable features in their models. Data scientists have started to incorporate several AutoML tools into their workflows to speed up model selection and performance benchmarking. Additionally, ML infra created a new framework that will automatically translate Jupyter notebooks into Airflow pipelines.\n\nIn this post, I will describe how these tools worked together to expedite the modeling process and hence lower the overall development costs for a specific use case of LTV modeling \u2014 predicting the value of homes on Airbnb.\n\nCustomer Lifetime Value (LTV), a popular concept among e-commerce and marketplace companies, captures the projected value of a user for a fixed time horizon, often measured in dollar terms.\n\nAt e-commerce companies like Spotify or Netflix, LTV is often used to make pricing decisions like setting subscription fees. At marketplace companies like Airbnb, knowing users\u2019 LTVs enable us to allocate budget across different marketing channels more efficiently, calculate more precise bidding prices for online marketing based on keywords, and create better listing segments.\n\nWhile one can use past data to calculate the historical value of existing listings, we took one step further to predict LTV of new listings using machine learning.\n\nData scientists are typically accustomed to machine learning related tasks such as feature engineering, prototyping, and model selection. However, taking a model prototype to production often requires an orthogonal set of data engineering skills that data scientists might not be familiar with.\n\nLuckily, At Airbnb we have machine learning tools that abstract away the engineering work behind productionizing ML models. In fact, we could not have put our model into production without these amazing tools. The remainder of this post is organized into four topics, along with the tools we used to tackle each task:\n\nOne of the first steps of any supervised machine learning project is to define relevant features that are correlated with the chosen outcome variable, a process called feature engineering. For example, in predicting LTV, one might compute the percentage of the next 180 calendar dates that a listing is available or a listing\u2019s price relative to comparable listings in the same market.\n\nAt Airbnb, feature engineering often means writing Hive queries to create features from scratch. However, this work is tedious and time consuming as it requires specific domain knowledge and business logic, which means the feature pipelines are often not easily sharable or even reusable. To make this work more scalable, we developed Zipline \u2014 a training feature repository that provides features at different levels of granularity, such as at the host, guest, listing, or market level.\n\nThe crowdsourced nature of this internal tool allows data scientists to use a wide variety of high quality, vetted features that others have prepared for past projects. If a desired feature is not available, a user can create her own feature with a feature configuration file like the following:\n\nWhen multiple features are required for the construction of a training set, Zipline will automatically perform intelligent key joins and backfill the training dataset behind the scenes. For the listing LTV model, we used existing Zipline features and also added a handful of our own. In sum, there were over 150 features in our model, including:\n\nWith our features and outcome variable defined, we can now train a model to learn from our historical data.\n\nAs in the example training dataset above, we often need to perform additional data processing before we can fit a model:\n\nIn this step, we don\u2019t quite know what is the best set of features to use, so writing code that allows us to rapidly iterate is essential. The pipeline construct, commonly available in open-source tools like Scikit-Learn and Spark, is a very convenient tool for prototyping. Pipelines allow data scientists to specify high-level blueprints that describe how features should be transformed, and which models to train. To make it more concrete, below is a code snippet from our LTV model pipeline:\n\nAt a high level, we use pipelines to specify data transformations for different types of features, depending on whether those features are of type binary, categorical, or numeric. FeatureUnion at the end simply combines the features column-wise to create the final training dataset.\n\nThe advantage of writing prototypes with pipelines is that it abstracts away tedious data transformations using data transforms. Collectively, these transforms ensure that data will be transformed consistently across training and scoring, which solves a common problem of data transformation inconsistency when translating a prototype into production.\n\nFurthermore, pipelines also separates data transformations from model fitting. While not shown in the code above, data scientists can add a final step to specify an estimator for model fitting. By exploring different estimators, data scientists can perform model selection to pick the best model to improve the model\u2019s out of sample error.\n\nAs mentioned in the previous section, we need to decide which candidate model is the best to put into production. To make such a decision, we need to weigh the tradeoffs between model interpretability and model complexity. For example, a sparse linear model might be very interpretable but not complex enough to generalize well. A tree based model might be flexible enough to capture non-linear patterns but not very interpretable. This is known as the Bias-Variance tradeoff.\n\nIn applications such as insurance or credit screening, a model needs to be interpretable because it\u2019s important for the model to avoid inadvertently discriminating against certain customers. In applications such as image classification, however, it is much more important to have a performant classifier than an interpretable model.\n\nGiven that model selection can be quite time consuming, we experimented with using various AutoML tools to speed up the process. By exploring a wide variety of models, we found which types of models tended to perform best. For example, we learned that eXtreme gradient boosted trees (XGBoost) significantly outperformed benchmark models such as mean response models, ridge regression models, and single decision trees.\n\nGiven that our primary goal was to predict listing values, we felt comfortable productionizing our final model using XGBoost, which favors flexibility over interpretability.\n\nAs we alluded to earlier, building a production pipeline is quite different from building a prototype on a local laptop. For example, how can we perform periodic re-training? How do we score a large number of examples efficiently? How do we build a pipeline to monitor model performance over time?\n\nAt Airbnb, we built a framework called ML Automator that automagically translates a Jupyter notebook into an Airflow machine learning pipeline. This framework is designed specifically for data scientists who are already familiar with writing prototypes in Python, and want to take their model to production with limited experience in data engineering.\n\nHere is a code snippet demonstrating how the fit and transform functions are defined in our LTV model. The fit function tells the framework that a XGBoost model will be trained, and that data transformations will be carried out according to the pipeline we defined previously.\n\nOnce the notebook is merged, ML Automator will wrap the trained model inside a Python UDF and create an Airflow pipeline like the one below. Data engineering tasks such as data serialization, scheduling of periodic re-training, and distributed scoring are all encapsulated as a part of this daily batch job. As a result, this framework significantly lowers the cost of model development for data scientists, as if there was a dedicated data engineer working alongside the data scientists to take the model into production!\n\nNote: Beyond productionization, there are other topics, such as tracking model performance over time or leveraging elastic compute environment for modeling, which we will not cover in this post. Rest assured, these are all active areas under development.\n\nIn the past few months, data scientists have partnered very closely with ML Infra, and many great patterns and ideas arose out of this collaboration. In fact, we believe that these tools will unlock a new paradigm for how to develop machine learning models at Airbnb.\n\nWe are very excited about the future of this framework and the new paradigm it brought along. By bridging the gap between prototyping and productionization, we can truly enable data scientists and engineers to pursue end-to-end machine learning projects and make our product better."
    },
    {
        "url": "https://medium.com/airbnb-engineering/helping-guests-make-informed-decisions-with-market-insights-8b09dc904353",
        "title": "Helping Guests Make Informed Decisions with Market Insights",
        "text": "Two common decisions that our guests are making are:\n\nAs the service provider, we have broad views of the entire market and guest behaviors that individual guests do not necessarily have. This information usually provides helpful insights to solve guests\u2019 puzzles.\n\nMarket insights are one channel where we interact with our guests at various stages of the booking flow. We provide dynamically-generated information to assist our guests in planning their trips. This information includes market and listing availability trends, supply, pricing discounts, community activities, etc. It is a critical component of the booking flow and has demonstrated its utility to the Airbnb community by enabling a larger variety of people to make wiser booking decisions and belong everywhere.\n\nFigure 1 illustrates the architecture of our Market Insight service. As a guest interacts with the website, the Market Insight backend system talks with the Search and Pricing services to collect market availability and pricing information. It queries the key-value store for data that are relevant to the search or listing view\u2014along with user information\u2014to generate candidate insights in real-time. Then it ranks the insights according to their values to the guest and powers the front-end on the final insights to display.\n\nWe are determined to generate market insights that are genuine, informational, and timely.\n\nWhen a guest types in a search query that consists of location, dates, guest counts, and possibly room type and additional amenity constraints, the Search backend system retrieves available homes. The frontend automatically zooms in the map to a level that best covers the guest\u2019s interests with enough context. For one such map view, the market insights server aggregates the exact number of available places, and warns the guest if the number is low. Similarly, we have an insight on the percentage of available places.\n\nTo support heterogeneous types of insights, the server retrieves two major sources of data from the key-value store.\n\nAs the service platform, we have more data than individual guests. For instance, Airbnb keeps track of how frequently homes are booked. This information is a good indication of popularity. When a guest views a place that is rarely vacant, we remind our guests with the following insight.\n\nThis insight is supported by two data pipelines \u2014 one that aggregates availability information of an individual listing and the other for the availability of all markets. A \u201cRare Find\u201d insight is for listings that have a high long-term availability ratio compared against the market X percentile \u2014 a value that trades off insight value and scarcity that we determined by live experiments. Both of the data pipelines are updated on a daily basis so our server will deliver accurate and timely insight to our guests.\n\nSince Market Insights\u2019 inception in 2015, we have gradually added more insight types. Our work has increased booking conversion by more than 5%. With a large Airbnb community and our extension to more verticals, our work compounds in a substantial way for company growth.\n\nPersonalization has been an evolving theme for many service platforms, and Airbnb has been a pioneer in adopting new technology, applying machine learning to personalize search results and detect host preferences. We have taken several steps in personalizing market insights.\n\nIt happens quite often that multiple insights are eligible, but we are only able to show one each time. Our current strategy is to use a deterministic and static vetting rule. However, guests parse information differently. For a guest who is sensitive to time, an insight reminding her can be very effective in getting a trip booked soon. Yet, for a last-minute traveller, the number or percentage of search results may sound more informative, providing them with signals to book as availability is running low. On a listing detail page, the mentality of a listing is \u201cusually booked\u201d may have different implications than \u201c10 others are looking at this place\u201d for various people. Not to mention that there may be sophisticated guests who would like to make decisions purely based on their chemistry with the listings, thus preferring no market insights at all.\n\nIn 2016, we have added extensive logging in our booking flow, about which types of insight guests see, and how they react when seeing these insights, such as how much longer they spend on a listing page, whether they wishlist a listing, make a booking request, or go back to search. We implemented a couple of randomization strategies, equalizing the odds of impression for every eligible insight. Showing different and increased variety of insights helps us acquire data to understand user preferences.\n\nAfter collecting user interaction data, we join it with listing information, such as occupancy rate and number of views, along with search parameters, such as trip lead days and length, and perform data analysis. Our goal is to learn smart insight vetting rules that maximize desired outcome. We believe guests are more likely to book with advanced user experience, so we created a utility function that evaluates their progress. For example, requesting to book is worth one point and contacting host is worth half a point, etc.\n\nWe segment guests based on guest features, such as the number of searches and bookings they have done in the past, and come up with a insight vetting rule for each user segment. We are experimenting on our hypothesis that personalized insights deliver improved user experience and in return improves booking conversion."
    },
    {
        "url": "https://medium.com/airbnb-engineering/writing-fast-deterministic-and-accurate-android-integration-tests-c56811bd14e2",
        "title": "Writing fast, deterministic and accurate Android Integration tests",
        "text": "Our choice for Android UI tests is Espresso, which is arguably the best, most popular and recommended library for writing integration tests. However, the extra confidence provided by a solid test suite can be quickly affected by flaky tests creeping up and undermining everyone\u2019s trust on them. In fact, flaky tests are poison!\n\nIntegration tests are at the very top of the Test Pyramid, but that doesn\u2019t mean that they have to be slow, brittle or expensive to write. They should be complemented by an even bigger coverage of service and unit level tests.\n\nAs an example of Service layer tests, one could build a set of APIs that provide test fixtures that can construct fake domain models for use during the test execution (eg.: Ruby\u2019s FactoryGirl and Forgery).\n\nBack to UI tests, flows that rely heavily on the network (eg.: typically performing many API requests) are often affected by network instability during the test execution. This was observed through high flip rates, that is, the rate at which a test will switch from failed to successful and vice/versa between invocations. One of the ways to expose the flakiness was to schedule tests to run every hour, for example, with or without code changes in order to measure their stability. Based on our findings, we started looking into options on how to remove the network variable from this equation. Some of us were familiar with the awesome VCR library, popular among Ruby/Rails folks, but nothing similar seemed to exist for Android. Thus OkReplay was born.\n\nSince all the Airbnb network traffic goes through OkHttp, creating an Interceptor that does the network recording and replaying seemed like the easiest way forward. Interceptors are very powerful and can modify network calls as they see fit, which is exactly what we needed. While looking for similar solutions, we also found the awesome Betamax project, which aims to solve a very similar problem, but was not designed with Android or OkHttp in mind. Regardless, since most of that project\u2019s goals were similar to our needs, we decided to work off of its codebase and modify as needed to make sure it\u2019s Android and OkHttp friendly. Some of these goals were:\n\nBetamax checked almost all the boxes, except for numbers 4 and 6. Another option was Wiremock, however it did not satisfy item 6. Thankfully, extending Betamax was simple enough due to its awesome modular architecture and test coverage. Additionally, having clearly defined constraints, especially the hard dependency on OkHttp, allowed us to build a much simpler solution since we don\u2019t need to solve it for every HTTP client out there!\n\nIn a nutshell, OkReplay boils down to a simple OkHttp Interceptor that, when turned on, looks at every outgoing request and either matches them against a pre-recorded set of \u201cnetwork interactions\u201d and replays them, or captures the network response as soon as it comes down the wire and saves them. These recording are called \u201ctapes\u201d. This term was inherited from VCR where tapes are simple files in YAML format where the network interactions are stored. You can think of these files as simple test fixtures.\n\nFor Android Espresso tests, tapes are stored by default in and loaded (read) during test execution as regular assets using Android's . For recording (writing), they are stored in the device's external storage directory and automatically pulled out of the device after the test execution using a simple Gradle Plugin that comes with OkReplay. This distinction happens because the test APK is not able to overwrite its own package contents to modify the YAML asset files during runtime, so our only option was to write it to external storage instead and overwrite the files using the Gradle plugin. The tapes should be easily readable and may be modified as needed.\n\nThere's a few other key ingredients to OkReplay. First, is the , which consists of a JUnit responsible for loading the appropriate test tape, starting and stopping the Interceptor and writing the tape file after the test is finished. Second, the , as the name says, loads and writes (YAML) tape files. Finally, the , which define ways for matching requests against each other based on, either a predefined set of built in rules, or a custom logic based on each application's specific needs.\n\nUsing OkReplay is just as simple as annotating your test method(s) with , eg.:\n\nThe annotation allows you to optionally specify a few configuration options like the tape name, and .\n\nBefore it can be used, you'll need to add the to your instance and register the OkReplay JUnit :\n\nThat\u2019s it! Any network requests being made while running the with OkHttp will automatically go through OkReplay now. By default, the is , which means it will fail any requests until you've recorded a tape for them. In that case, while watching the device's Logcat output, you'd see an error message like the one below:\n\nWhile running, by default, OkReplay will not allow any network requests to hit the network unless the is \u201cwriteable\u201d (either or ). This is to ensure tests are repeatable and deterministic.\n\nOnce you have all your interactions recorded, you could even run your tests while the device is in Airplane Mode and still see them work fine, assuming you don\u2019t have any other hard dependencies on the network being available.\n\nFinally, since OkReplay is a library meant to be used only while testing, you can use its variant as a dependency to make sure your release application ships with a stubbed version of the OkReplayInterceptor.\n\nBy open sourcing this library and sharing it with the broader Android community, we hope to incentivize developers to write more tests and ship quality apps with more confidence!"
    },
    {
        "url": "https://medium.com/airbnb-engineering/unlocking-test-performance-migrating-from-mocha-to-jest-2796c508ec50",
        "title": "Migrating from Mocha to Jest \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Running our test suite with Mocha took 12+ minutes. In CI with our beefy build machines we\u2019re now able to run the entire Jest suite in 4 minutes 30 seconds.\n\nWe\u2019d been using Mocha at Airbnb since September 2013, but due to increasing growing pains, we\u2019ve recently migrated from Mocha to Jest . The migration actually turned out to require minimal changes to our tests and infrastructure, and provided a myriad of benefits.\n\nWe\u2019ve put significant effort into ensuring that our tests follow best practices and a fairly strict coding style. This has helped us to mitigate flakiness and enable cross team collaboration, because tests look the same across different sections of our code. It was pretty important to us that our test files look more or less the same before/after the migration.\n\nMost teams will not have to change the contents of their test files. In fact the below snippet was enough at Airbnb to paper over the differences between our usage of Mocha and Jest.\n\nThis is possible because the Jest API and the Mocha API are similar with only slightly different function names. Which functions you need to control for (or change in your tests) depends greatly on which functions you\u2019re using. We also had a few calls, which were 1:1 replaceable with which is what we\u2019ve decided as standard. Below is a quick example of what our tests look like before and after our migration to Jest.\n\nWe initially rolled out code coverage using Istanbul and Mocha in January 2016, but we discovered that instrumenting our source files was expensive, and added an unreasonable amount of time to our tests.\n\nTo solve this problem we wrote some custom logic to batch our tests into chunks, run them in separate processes, collect coverage on each, and then merge the coverage into a single report at the end.\n\nCoverage was collected (and enforced) in a different CI job than our test suite. The test suite itself was later parallelized by dispatching chunks of the test suite to different worker machines and aggregating the results at the end.\n\nJest automatically handles splitting tests across processes, collecting and aggregating coverage. This was a major perk for us. By utilizing this fact we were able to remove our custom logic in both jobs, and rely solely on Jest to handle this intelligently for us.\n\nFor projects with many test files, Jest will get you improved performance out of the box. It\u2019s able to do this through a couple of mechanisms.\n\nWith Mocha our suite took ~45 minutes to run locally, and sometimes it wouldn\u2019t complete at all due to the memory pressure of running our full suite in a single thread. With Jest it\u2019s down to 14.5 minutes locally. We saw a similar improvement on our build server with Mocha clocking it at 12+ minutes (after our work to parallelize across machines) and Jest finishing in 4.5 minutes.\n\nWhen you\u2019ve got a test suite as large as ours (several thousand test files), running your tests in a single thread will inevitably lead to flakiness. When we started working on the migration, roughly 12% of our builds would need to be rerun due to flake, and we had tests in our suite that required other tests to run first or they wouldn\u2019t pass.\n\nBeing run in isolation means that it is impossible for a test to fail due to side effects of other test files in your suite. This is especially helpful for errors that are thrown in calls that happen after a test has completed. Now it\u2019s much easier for us to investigate a flaky test by checking the test and source file for any asynchronous code.\n\nAfter migrating to Jest and fixing the tests that failed in isolation we were able to reduce our flake rate to ~1%. This saves our developers hours of time per work day as they no longer have to wait for a build to fail, and repeatedly rerun the test suite until it passes. Additionally on the rare occasions that flake does happen, we\u2019re able to more accurately identify where it is coming from. It\u2019s easier to identify, because of file runs in its own process, so it is guaranteed that flakiness is coming from within that file. With Mocha, a bad timer in file x, could cause a test to fail in file y.\n\nIf you\u2019re dedicated to reducing flakiness, you can take this a step further and reduce flake in your tests even more by killing timers that are set in your tests:\n\nJest was much faster for us out of the box, but initially we weren\u2019t seeing the sort of improvement that we expected. After profiling a few runs, we found that our global file was the culprit. This is a file that we set up with Mocha to configure some global helpers that made our tests more convenient to write. For example, we use Enzyme for testing our React code, and to make writing tests easier, we include chai-enzyme. Rather than having all of our developers hook up this library manually in every test file, we hook it up in which is run before all of our tests.\n\nThis turns out to be really problematic in Jest. Because each test file is run in a clean virtual machine, Jest reruns the file once for each test file. In the case of the above example, importing chai-enzyme starts a chain that imports all of enzyme, which then imports all of React and ReactDOM. This takes 480ms even for tests that do not include React. In our case 480ms * several thousand files meant that we were spending over a minute just setting up this library. With Mocha, we didn\u2019t feel the pain from this because it isn\u2019t parallelized, and only runs the file one time.\n\nTo get around this, we got a little bit creative with Jest\u2019s mocking capabilities. By utilizing the callback on we were able to intercept enzyme imports, and load chai-enzyme only for tests that need it.\n\nOur ultimate goal with this migration was to improve our developer experience, both when writing their tests and when running them. We\u2019ve only been on Jest a few weeks, but so far we\u2019ve seen nothing but positive feedback:"
    },
    {
        "url": "https://medium.com/airbnb-engineering/selection-bias-in-online-experimentation-c3d67795cceb",
        "title": "Selection Bias in Online Experimentation \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "In online experimentation platforms, we choose the experiments with significant successful results to launch to the product. When estimating the aggregated impact of the launched features, we investigate a statistical selection bias in this process and propose a correction method of getting unbiased estimation.\n\nAs a statistician who moved to work in data science in industry less than two years ago, it is inspiring to see the almost universal adoption of experimentation to guide product and business decisions. Statistical inference and hypothesis testing are ingrained in our daily work, and an appreciation of randomness informs much of our decision making. However, the era of big data and A/B testing at scale present new challenges to our old methodologies.\n\nIf you have experience running A/B testing on a large scale experimentation platform, this is something you might have seen before: numbers don\u2019t always add up. Here is an example. Over the period of a couple of months, our team at Airbnb ran several experiments sequentially. Six of the experiments were launched to all users after showing a statistically significant lift on our target metric, with the exception of a small holdout group (split-holdout in the graph). After the spur of successful experiments, we tallied their effect on our target metric and found this:\n\nIn the bottoms-up calculation, each number is the point estimate of the effect on the target metric, as measured by each successful A/B experiment. Naively, both the bottoms-up sum and the result of the \u201cmeta-experiment\u201d of holdout are valid measurements of the aggregated total effect. How do we explain the gap between the two approaches?\n\nYour first impression is probably, \u201cWait, are these effects truly additive?\u201d YES, because we designed the process as a series of experiments conducted one after one. Let\u2019s also assume that the percentage lift of each change is small enough that the cumulative effect can be found by either addition or multiplication (log(1+x)~x when x is small).On the other hand, the answer is also NO, as it does not take a lot of domain knowledge to understand that there are many potential issues:\n\nEach of the above issues could be a separate article at length. However, beside all of these plausible factors to cause the gap, there is a more fundamental one that exists in almost all online experimentation platforms: selection bias, or what we call the Winner\u2019s Curse.\n\nThe Winner\u2019s Curse is a phenomenon in common value auctions where the winner tends to overpay for the value of the item. Analogously, the observed effects of selected experiments tend to overestimate the true effects of experiments. Here is a simplified illustration of the idea. Suppose we run 10 experiments, with the same fixed standard deviation of 1%. We collect enough samples in each experiment and observe their effects individually, as shown in the graph below in the first row. We also pretend not to know the underlying true effects, which are represented in the second row. Everything is in percentage scale.\n\nWe conduct student t-tests for each experiment individually using the observed results. If we set the Type-I error level at \u03b1 = 0.05, it is easy to see that the t-test statistic is just the observation itself, and it needs to be greater than 1.96 to have significant result. Thus, three experiments become significant as circled out in red:\n\nThus, if we add up the observed effects from these three experiments, the total effect would be 2.7% + 2.6% + 3.3% = 8.6%. However, since we know the underlying true effect, we see the aggregation of observed results is indeed larger than the summation of the true effects: 1% + 1% + 4% = 6%. The upward bias is 2.6% in the case.\n\nIn fact, the above illustration is not just a bad example, and the reason behind it can be explained formally through statistical formulation with several simplified assumptions. Suppose that X1, \u2026, Xn are random variables defined on a same probability space, and each X\u1d62 follows a distribution with finite mean a\u1d62 and finite variance \u03c3\u1d62\u00b2(the distributions are not necessarily identical.) We regard a\u1d62 as the unknown true effect and usually estimate it by the unbiased estimate X\u1d62.\n\nConsider a vanilla setting of A/B testing, where we conduct simple two-sample t-test for each experiment. We then select the \u201csignificant\u201d experiments that had the testing statistic larger than a threshold, which is equivalent to when the observed positive estimated effects are greater than a threshold. Suppose we use significance level \u03b1\u1d62 for each experiment i. For now, let us assume that \u03c3\u1d62\u00b2 is known. We choose experiments such that X\u1d62 /\u03c3\u1d62 > b\u1d62, where b\u1d62 is the cut-off from the reference distribution for significance level \u03b1\u1d62, usually set at 0.05.\n\nLet us define the set of significant experiments A= {i|X\u1d62 /\u03c3\u1d62 > b\u1d62}. Then, the total true effect of A is T_A = \u2211{i \u2208 A} a\u1d62. If we add up the effects of positive significant experiments, the total estimated effect of A is S_A= \u2211{i \u2208 A} X\u1d62 . Note that since A is a random set, therefore ES_A \u2260 ET_A in general. Also, the total true effect T_A is random. We can define the expected total true effect as E[T_A] = E[\u2211{i \u2208 A} a\u1d62].\n\nWe can show that ES_A \u2265 ET_A. In fact,\n\nAll the summands are all nonnegative because the mean of lower truncated mean-zero distribution is always positive, and therefore the selection bias is always positive. As an illustration, we can plot the individual terms in the above summation as a function of true effect a\u1d62 under Normal distribution, if \u03c3\u1d62 = 1and b\u1d62 is the two standard deviation cutoff of a t-statistic, i.e. 1.96:\n\nWe can also plot the bias in terms of effect and p-values. Note that the bias increases linearly as the true effect increases.\n\nNow, it is easy to see that we have a way to quantify the total bias E[S_A -T_A] if there is an unbiased estimation for each term within the summation.\n\nWinner\u2019s curse occurs in the process of \u201cselection\u201d, where we pick the winning experiments and attribute their total effect by aggregating over the individual observed effect of these selected experiments.\n\nA reasonable simulation is better than a thousand words. We can show that the bottoms-up estimates (y-axis) are indeed substantially greater than the true effects (x-axis). Suppose that we have n=30 experiments and measure the incremental percentages of the effects. For each i, we sample a\u1d62 from a truncated Gaussian distribution a\u1d62 ~ Z\u1d62|(-1.5 < Z\u1d62 < 2) where Z\u1d62 ~ N(0.2, 0.7\u00b2) and \u03c3\u1d62\u00b2 from the inverse gamma distribution with shape parameter 3 and scale parameter 1. A left-skewed distribution is chosen to be the prior of true effects, because we would like to have more positive true effects in the simulation, a reasonable assumption in the context of a product team seeking to improve a metric. Among the 1000 simulation instances almost all cases indicate the naive bottoms-up estimate is over the 45-degree diagonal line of true effect.\n\nThis might sound familiar to you if you have heard about \u201cmultiple testing\u201d. Admittedly, we are testing multiple hypotheses and instead of using the standard p-value threshold 0.05 (somewhat arbitrary as well), we can adjust it intelligently to control for the inflated false discovery rate or family-wise error rate. There are well-studied methods such as the Bonferroni correction or the Benjamini-Hochberg procedures to address this problem. By using a universal p-value threshold, we\u2019re highly likely to have a few winning experiments even when all of them are just pure noise.\n\nHowever, what we are trying to explain is more than the false positives. There are actually two aspects of selection bias we hope to address. First, even when there is no false positives, i.e. all the experiments we run indeed have positive true effects, the measurement of the aggregated effect will be an overestimation. In addition, the experiments not being selected, i.e. for i \u2209 A, also contribute to the overall bias.\n\nFor one single selected experiment, the observed result in the A/B test is expected to overstate its true effect. Usually we know that the sample mean is an unbiased estimate for the population mean. However, because one becomes interested in this particular experiment after observing that it is significantly positive, we are actually looking at an estimation conditional on that this experiment has been selected, or its observation exceeds a given threshold, hence upward bias has been introduced.\n\nSecondly, in the above formulation we can see that even the non-significant experiments will contribute to the bias, which is a little bit counterintuitive. Why are we interested in such an estimation ET_A particularly? If one focuses only on the selected ones, we can correct for T_A|A; instead of doing this, however, we are integrating out the selection set and measuring the bias of the process as a whole. This is fundamentally a different correction, and we think it represents the actual experimentation process better. Let us consider an intuitive example with the following two cases:\n\nIntuitively, if we estimate the total true effect as 1, case 2) has a higher risk in overestimating the total true effect due to the selection bias. Correcting according to ET_A will take into account this risk. On the other hand, conditioning by the fact that only the first experiment is selected, the bias corrections are the same for case 1) and 2).\n\nIf the set of experiments being aggregated A is fixed from the beginning, then no bias would be introduced. In the end, the bias exists because of the process of selecting the successful ones among many experiments we run, and we do this every day in a large scale experimentation platform.\n\nIs this something you do as a team for attribution, i.e. declaring the total impact based on experimentation results? If so, we should consider accounting for the bias or try best to mitigate it by better experiment design.\n\nWith the above formulation, we have come up with a straightforward unbiased estimation for the Winner\u2019s Curse bias. If we assume the true parameters a\u1d62 and \u03c3\u1d62\u00b2 are known, one can derive the bias E[S_A -T_A] to be\n\nSince a\u1d62 and \u03c3\u1d62 are usually unknown, we use the estimates X\u1d62 and W\u1d62, where W\u1d62 is the estimated standard deviation of X\u1d62, to define the bias estimate\n\nSubtracting this bias estimate from the bottoms-up estimate will provide the adjusted unbiased estimate for the aggregated effect. We then built a feature in ERF, our Experimentations Reporting Framework, using this formula to calculate the bias automatically. We are able to choose a set of experiments that are being selected, specify the metric to be analyzed as well as the selection rules. One can also build confidence interval for the bias-adjusted estimation using the bootstrap method. In the earlier example, we adjusted the bias to get a total effect of 5.3% rather than 7.2%. As you can see, the confidence interval is also quite large.\n\nThis de-biasing method relies on very few assumptions, especially compared to Bayesian methods which requires specific knowledge for the priors. Over the course of developing the method and its implementation into ERF we have several learnings.\n\nMeasurement plays a crucial role in data informed decision making. When online experiments are costly and have to be performed efficiently, we inevitably carry out measurements on the same data used for both inference and model selection. There has been a long ongoing discussion in both academia and industry around \u201cp-hacking\u201d and similar ideas. An extensive literature exists trying to tackle this problem in various applications in econometrics or genome-wide association studies. Our approach, although with simplified assumptions about the selection rule, is a quick and effective way to account for the selection bias without many additional assumptions or prior knowledge, especially in large scale online experimentation platforms.\n\nIn practice, various characteristics of online experiments make application of the theoretical work very challenging. Not every decision-making process follows the same rules. It is only when we keep both statistical rigor and practical concerns in mind that we will be able to move the frontier of product development forward. On the Data Science team at Airbnb, we\u2019re excited about how many interesting problems and undefined opportunities await us in the future."
    },
    {
        "url": "https://medium.com/airbnb-engineering/how-airbnb-democratizes-data-science-with-data-university-3eccc71e073a",
        "title": "How Airbnb Democratizes Data Science With Data University",
        "text": "Data is essential to us at Airbnb. We characterize data as the voice of our users at scale. Thus, data science plays the role of an interpreter \u2014 we use data and statistics to understand our users and translate it to a voice that people or machines can understand. We leverage these quantitative insights, paired together with qualitative insights (e.g. in-person user research) to make the best possible decisions for both the business and our community of hosts and guests.\n\nTo that, we have built a world-class Data Science Team that has scaled to nearly 100 people strong working on everything from experimentation to data analysis & visualization to modeling & machine learning. We also built a stable, reliable and scalable data infrastructure to serve as the foundation for our data, as well as a powerful suite of data tools to empower data scientists and knowledge workers all across Airbnb:\n\nAnother one of our fundamental beliefs is that every employee should be empowered to make data informed decisions. This applies to all parts of Airbnb\u2019s organization \u2014 from deciding whether to launch a new product feature to analyzing how to provide the best possible employee experience. Our Data Science team firmly believes that part of our goal is to empower the company to understand and work with data. In order to inform every decision with data, it wouldn\u2019t be possible to have a data scientist in every room \u2014 we needed to scale our skillset. Additionally, our rapid international growth made the situation even more challenging. We expanded from one office in San Francisco in 2011 to 22 offices internationally today, many of which do not have Data Science presence. Furthermore, we believe that people have the capability to think critically and understand the data on their own, and we wanted to give them the tools to do it.\n\nTo address this challenge, we thought deeply about how to democratize data science and scale data informed decision making during the 2nd half of 2016. We used a metric of weekly active users (WAUs) of our data platform as a proxy to how \u201cdata informed\u201d we were as an organization. At the beginning of Q3 2016, only about 30% of Airbnb employees were a WAU of our data platform, which was significantly lower than other hypergrowth internet company peers we benchmarked with like Facebook and Dropbox.\n\nWe then thought about what might be holding back our company from looking at data themselves.\n\nThe key ingredients needed for data informed decision making included having accessibility to data, a comprehensive set of data tools and user knowledge of how to utilize the data and tools. As we spoke to people throughout Airbnb, it became more and more apparent that the bottleneck to scaling data informed decisions was actually data education for users. Our data tools were serving Data Scientists well. Also, we had already made huge strides in making data more accessible through efforts like Core Data, our single source of truth for product data, as well as SQL Lab, a new SQL editor we built into Superset. The gap was that we didn\u2019t have any formal programs to equip employees with the knowledge to use our tools and how to work with Core Data. Thus, we decided to create Data University.\n\nData University is data education for anyone at Airbnb that scales by role and team. Our vision is to empower every employee to make data informed decisions. Our approach is unique since organizations offering data education typically focus just on their technical employees. Our approach is also intentional because we believe that every person at Airbnb should and can utilize data in his/her role to make better decisions. Thus, we designed the program to make it accessible and relevant to anyone at Airbnb.\n\nCreating \u201ccitizen data scientists\u201d is powerful \u2014 not only does it help ensure that decisions are grounded in data, but it enables people to make decisions autonomously. This is important because the person asking the question always has the best context on the question they are trying to answer, and it reduces the feedback loop to answering questions. This also has the side benefit of freeing up some of the Data Science Team\u2019s time. We had considered leveraging existing resources from MOOCs such as Coursera and Udacity, however many of our data tools are unique, and we believe there is tremendous value in educating people in the context of Airbnb\u2019s data.\n\nThe curriculum consists of over 30 classes covering an array of different topics. The 100-level series provides the foundation for data informed decision making at Airbnb and was designed to be accessible to everyone. The 200-level series equips people with the applied skills for accessing data using SQL, or analyzing and visualizing data using tools such as Superset, Tableau and ERF in the context of Airbnb data. Then, the 300-level series is targeted primarily towards engineers and data scientists. It exposes people to advanced data techniques such as machine learning and tools such as Airflow for writing data pipelines. We also cover popular languages such as R, Python and Hive for analyzing and manipulating data.\n\nMany of the initial classes were developed and taught by Erin Coffman, our most tenured Data Scientist at Airbnb. However, since then we have amassed more than 30 volunteer faculty (many pictured below) from across the Data Science and Engineering organizations to help create course content as well as teach classes. We are incredibly grateful for all the volunteers who make Data University possible!\n\nData University has been a huge success thus far at Airbnb. In the first half year since it launched, more than 500 unique people have participated in at least one class (or about 1/8th of Airbnb). Depth of engagement is high as each employee who has participated has taken more than 4 classes on average as we have had a total of over 2,100 \u201cbutts in seats.\u201d Every class offered thus far has an NPS score of +55 or higher.\n\nFurthermore, it has completely transformed Airbnb\u2019s data culture as 45% of Airbnb is now a WAU of the data platform. Ad hoc data requests that used to go to Data Scientists or Analysts are now often being self-serviced or addressed by other Data University graduates. We routinely hear anecdotes about employees being empowered with data, from employees in recruiting creating Tableau dashboards to Product Managers writing their own SQL and interpreting their own experiments. Most recently, we have begun scaling the program to other offices including Dublin, Portland, Singapore and Seoul.\n\nOur team is really encouraged about the initial results of Data University, and we will continue to iterate on the content as well as scale the program both in terms of breadth of classes (the 300-level series is next) and locations where the curriculum is offered. We will also be experimenting with different learning formats such as online and/or streamed courses.\n\nIn sharing our experience, we hope to inspire other organizations working on the same kinds of problems of scale and data democratization that we are trying to solve, as well as share learnings so that we can collaboratively produce best practices. If you\u2019re interested in exchanging notes, or have follow up questions about our approach, please reach out!\n\nCheck out all of our open source projects over at airbnb.io and follow us on Twitter: @AirbnbData + @AirbnbEng."
    },
    {
        "url": "https://medium.com/airbnb-engineering/rearchitecting-airbnbs-frontend-5e213efc24d2",
        "title": "Rearchitecting Airbnb\u2019s Frontend \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Airbnb sees more than 75 million searches each day, which makes the search page our highest traffic page. For nearly ten years, engineers have evolved, enhanced, and optimized the way that Rails delivers the page.\n\nRecently, we moved into verticals beyond Homes, introducing Experiences and Places. As a part of bringing these new products to web, we took the time to rethink the search experience itself.\n\nRather than navigating from our landing page at www.airbnb.com (1) to a search results page (2) to a single listing (3) to the booking flow (4)\u2014 each page delivered standalone via Rails \u2014 we want the user experience to be fluid, adjusting what the user is experiencing as they explore and narrow their search.\n\nNavigating across tabs and interacting with listings should feel luxurious and effortless. In fact, today there is nothing stopping us from delivering an experience on par with native applications on small and medium screens.\n\nTo tee up this type of experience, we needed to break free of the legacy page-by-page approach that got us here, and in the end we wound up with a fundamental rearchitecting of our Frontend code.\n\nLeland Richardson recently spoke at React Conf about React Native in the \u201cbrownfield\u201d of an existing, high traffic native application. This article will examine how we undertook a dramatic upgrade with similar constraints, but on the web. Hopefully you find it useful if you find yourself in a similar place!\n\nBefore firing up the barbecue for all the fun Progressive Web App work on our roadmap, we needed to separate from Rails (or at least the way we use Rails at Airbnb in delivering standalone pages).\n\nUnfortunately, only a matter of months ago, our search page contained some very old code\u2026like, Lord of the Rings, touch-that-at-your-peril old. Fun fact: I once replaced a small Handlebars template backed by a Rails presenter with a simple React component, and suddenly things were breaking in entirely separate parts of the page \u2014 even in our API response! Turns out, the presenter was mutating the backing Rails model, which had been impacting all downstream data for years, even when the UI wasn\u2019t being rendered.\n\nIn short, we were in this project like Indiana Jones swapping the idol for a bag of sand, and immediately the temple starts collapsing, and we\u2019re running from a boulder.\n\nWhen Rails is server-rendering your page, you can get away with throwing data at your server-rendered React components any way you like. Controllers, helpers, and presenters can produce data of any shape, and even as you migrate sections of the page to React, each component can consume whatever data it requires.\n\nBut once you endeavor to render the route client-side, you need to be able to request the data you need dynamically and in a predetermined shape. In the future, we may crack this problem with something like GraphQL, but let\u2019s set that aside for now, as it wasn\u2019t an option when this refactor took place. Rather, we chose to align on a \u201cv2\u201d of our API, and we needed all our components to begin consuming that canonical data shape.\n\nIf you find yourself in similar waters with a large application, you might find as we did that planning for the migration of existing server-side data plumbing was the easy part. Simply step through any place Rails is rendering a React component, and ensure that data inputs are API shapes. You can further validate compliance with API V2 shapes used as React PropTypes on the client.\n\nThe tricky bit for us was working with all the teams who interact with the guest booking flow: our Business Travel, Growth, and Vacation Rentals teams; our China and India market-specific teams, Disaster Recovery\u2026the list goes on, and we needed to reeducate all these folks that even though it was technically possible to pass data directly to the component being rendered (\u201cyes, I understand it\u2019s just an experiment, but\u2026\u201d), all data needs to go through the API.\n\nThere is a separate class of data from what we would think of as API data, and it includes application config, user-specific experiment assignment, internationalization, localization, and similar concerns. Over the years, Airbnb has built up some incredible tooling to support all these functions, but the mechanisms for delivering them to the Frontend were a bit under-baked (or possibly fully-baked when built, before the ground began shifting under foot!).\n\nWe use Hypernova to server-render React, but before we went deep on this refactor, it was a bit nebulous whether experiment delivery in a React component would blow up during server-rendering or if string translations available on the client would all be reliably available on the server. Critically, if the server and client output don\u2019t match to the bit, the page not only flashes the diff but also re-renders the entire page after load, which is terrible for performance.\n\nWorse yet, we had some magical Rails functions written long ago, for instance , which could ostensibly be called anywhere in Rails to make data available on the client globally via (though, again, not necessarily for Hypernova). What began as a helpful utility for a small team became a source of untraceable witchcraft for a large application and team. The \u201cdata laundering\u201d crimes became increasingly tricky to unwind, as each team owns a different page or feature, and therefore each team cultivated a different mechanism for loading config, each suiting their unique needs.\n\nClearly, this was already breaking down, so we converged on a canonical mechanism for bootstrapping non-API data, and we began migrating all apps/pages to this handoff between Rails and React/Hypernova.\n\nThis higher order component does two very important things:\n\nIn a single shot, we eliminated and prevented engineers from passing arbitrary keys through to top level React components. Order was restored to the shire, and before long we were navigating to routes dynamically in the client and rendering content of material complexity without Rails to prop it up (pun intended).\n\nServer rework in hand, we now turn our gaze to the client.\n\nGone are the days, friends, of the monster Single Page App (SPA) with a gruesome loading spinner on initialization. This dreaded loading spinner was the objection many folks raised when we pitched the idea of client-side routing with React Router.\n\nBut if you look above, you\u2019ll see the impact of code-splitting and lazy-loading bundles by route. In essence, we server render the page and deliver just the bare minimum JavaScript required to make it interactive in the browser, then we begin proactively downloading the rest when the browser is idle.\n\nOn the Rails side, we have one controller for all routes delivered via the SPA. Each action is simply responsible for (1) making whatever API request the client would have made on client-side navigation, then (2) bootstrapping that data to Hypernova along with config. We went from thousands of lines of Ruby code per action (between the controller, helpers, and presenters) down to ~20\u201330 lines. Yahtzee.\n\nBut it\u2019s not just code that is noticeably different\u2026\n\n\u2026now transitions between routes are smooth as butter and a step change (~5x) faster, and we can break ground on the animations featured at the beginning of this post.\n\nPrior to React, we would render an entire page at a time, and this practice carried over into our early React days. But we use an AsyncComponent similar to this as a way to load sections of the component hierarchy after mount.\n\nThis is particularly useful for heavy elements that aren\u2019t initially visible, like Modals and Panels. Our explicit goal is to ship precisely the JavaScript required to initially render the visible portion of the page and make it interactive, not one line more. This has also meant that if, for example, teams want to use D3 for a chart in a modal on a page that doesn\u2019t otherwise use D3, they can weigh the \u201ccost\u201d of downloading that library as part of their modal code in isolation from the rest of the page.\n\nBest of all, it is this simple to use anywhere it is needed:\n\nHere we can simply swap out the synchronous version of our map for an async version, which is particularly useful on small breakpoint, where the map is displayed via user interaction with a button. Since most of these users are on phones, getting them to interactive before worrying about Google Maps comes with a tasty boost in page load time.\n\nAlso, note the utility, which requests the bundle in advance of user interaction. Since the map is so frequently used, we don\u2019t need to wait for user interaction to request it. Instead, we can enqueue it when you get to the Homes Search route. If the user does request it prior to download, they see a reasonable until the component is available. No sweat.\n\nThe final benefit of this approach is that becomes a named bundle that the browser can cache. As we disaggregate larger route-based bundles, the slowly-changing sections of the app remain untouched across updates, further saving JavaScript download time.\n\nDoubtless it warrants a dedicated post, but we have begun building our internal component library with Accessibility enforced as a hard constraint. In the coming months, we will have replaced all UI across the guest flow that is compatible with screen readers.\n\nThe UI is rich enough that we want to associate a CheckBox not only with a title, but also a subtitle using . To achieve this requires a unique identifier in the DOM, which means enforcing a required ID as a prop that any calling parents need to provide. These are the types of hard constraints the UI can impose to ensure that if a component is used in the product, it is delivered with accessibility built in.\n\nThe code above also demonstrates our responsive utilities HideAt and ShowAt, which allow us to dramatically alter what the user experiences at different screen sizes without having to hide and show using CSS. This leads to much leaner pages.\n\nNo Frontend post would be complete without touching on the debate about how to handle app state.\n\nWe use Redux for all API data and \u201cglobals\u201d like authentication state and experiment configurations. Personally, I like redux-pack for async. Your mileage may vary.\n\nHowever, with all the complexity on the page\u2014particularly around Search\u2014it doesn\u2019t work to use Redux for low-level user interactions like form elements. We found that no matter how we optimized, the Redux loop was going to make typing in inputs feel inadequately responsive.\n\nSo we use component local state for everything the user does up until it triggers a route change or network interaction, and we haven\u2019t had any problems.\n\nAt the same time, I like the feel of a Redux container component, and we found that even with local state, we could build Higher Order Components that could be shared. A great example is with our filters. Search for homes in Detroit, and you\u2019ll find a few different panels on the page, each operating independently, that can modify your search. Across various breakpoints, there are actually dozens of components that need to know the currently-applied search filters and how to update them, both temporarily during user interaction and officially once accepted by the user.\n\nHere we have a neat trick. Every component that needs to interact with filters can be wrapped with this HOC, and you\u2019re done. It even comes with prop types. Each component wires into the responseFilters (those associated with the currently-displayed results) from Redux but keeps a local stagedFilters object available for modification.\n\nBy tackling state this way, interacting with our Price Slider has no impact on the rest of the page, so performance is great. But all filters panels are implemented with the same function signatures, so development is simple.\n\nNow that the grizzly legwork of catching the Frontend up with the present is largely in hand, we can turn our attention to the future.\n\nTune in next time as we chase down these opportunities. Since so many of the wins will have immediate quantitative impact, we will try to capture some of the specific wins in subsequent posts."
    },
    {
        "url": "https://medium.com/airbnb-engineering/democratizing-data-at-airbnb-852d76c51770",
        "title": "Democratizing Data at Airbnb \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Like many startups, the number of employees at Airbnb has grown significantly over the past several years. In parallel we have seen explosive growth in both the amount of data and the number of internal data resources: data tables, dashboards, reports, metrics definitions, etc. On one hand, the growth in data resources is healthy and reflects our heavy investment in data tooling to promote data-informed decision making. However it also creates a new challenge: effectively navigating a sea of data resources of varying quality, complexity, relevance, and trustworthiness. In this post we describe our observation of this problem and the Dataportal, a novel data resource search and discovery tool that addresses this issue.\n\nThe overarching goal of the Dataportal is to democratize data and empower Airbnb employees to be data informed by aiding with data exploration, discovery, and trust.\n\nOur team consists of a collection of data misfits: recovering data scientists who understand the numerous pain points associated with data, and visualization engineers who specialize in data communication. We\u2019ve spent time in the trenches, often working in a reactionary, time-critical space. We wanted to design and build proactive solutions that help alleviate common, well-defined data problems.\n\nOur view of the data landscape is merely one of many. To ensure that we developed a data product that provides universal value we talked to employees across departments, roles, tenure, and data literacy levels, to better understand their pain points and concerns around data.\n\nA constant theme appeared: users often had to ask others where to find the appropriate resource as it was difficult to navigate the data landscape. Additionally, the lack of metadata and context made it hard to trust data. This lack of trust prevented employees from using resources outside of their sphere of knowledge, making them afraid of accidentally using outdated or incorrect information. This resulted in people creating additional resources, further muddying the landscape.\n\nAs Airbnb grows so do the challenges around the volume, complexity, and obscurity of data. Information and people become siloed which necessitates navigating an invisible landscape of tribal knowledge. This is an inefficient use of time for people on the journey and for those providing directions.\n\nScale aside, data is often isolated by tool or team, each providing a myopic localized view of the data-space while lacking global context. For example a dashboard is naive with regards to where the data originated from, and a data table lacks context of its relevance to downstream visualization tools. Furthermore, many data tools have complex permission rules which further fragments sharing and understanding.\n\nThe understanding of the entire data ecosystem, from the production of an event log to its consumption in a visualization, provides more value than the sum of its parts.\n\nIt was apparent that we needed to develop a system that enabled a shift in thinking. Relying solely on tribal knowledge stifles data discovery and thus we sought to develop a self-service system which provided transparency to our complex and often-obscure data landscape.\n\nWe hope to shift people from thinking of an individual datasource towards the concept of an integrated data-space; the data-space presents a holistic view of the data and thus provides the necessary context for people to be data informed.\n\nThe Dataportal provides a framework for best practices with data, providing guard rails where necessary. Our hope is that any employee, regardless of role, can easily find or discover data and feel confident about its trustworthiness and relevance.\n\nFrom a transparency perspective we set out with the intent to have a single lens into our data-space by providing as much context as possible while observing per-tool access controls to the underlying data.\n\nOur ecosystem is best represented as a graph, which we leverage in the Dataportal as described below. Nodes are the various resources: data tables, dashboards, reports, users, teams, business outcomes, etc. Their connectivity reflects their relationships: consumption, production, association, etc.\n\nIn our model the relationships are just as pertinent as the nodes. Knowing who produced or consumed a resource is just as valuable as the resource itself. Relationships provide the necessary linkages between our siloed data components and the ability to understand the entire data-space.\n\nPeople are data resources too. Finding employees who have used or own a given data resource can increase the efficiency of knowledge sharing.\n\nA graph of the ecosystem has value far beyond tracking lineage and cross-functional information. Data is a proxy for the operations of a company. Analyzing the network helps to surface lines of communication and identify facets or disconnected information.\n\nTo address the pain points above we built a data tool to improve data discoverability and exploration within Airbnb. To achieve this we built out four major product features:\n\nThe most important feature of the Dataportal is a unified search across our entire data ecosystem. Employees can search logging schemas, data tables, charts, dashboards, employees, and teams. We surface as much metadata about resources as possible in search cards to build context and trust.\n\nWe leverage the topology of the graph to boost search relevance using PageRank for promoting high-quality relevant resources. Well-documented and frequently-consumed resources will result in a higher score which helps to ensure that search drives users to the most desirable entities.\n\nFrom search a user can further explore a resource by visiting its detailed content page. \n\nData without context is often meaningless and can lead to ill-informed and costly decisions. Therefore content pages surface all of the information we have for a resource across data tools to show how it fits into the entire data ecosystem: who has consumed a resource, who created it, when it was created or updated, which other resources it\u2019s related to, etc.\n\nMore metadata translates to being more data informed. This is especially true for data tables, the foundation of any data warehouse. Easily editable metadata promotes the updating of table descriptions and column comments, bypassing complicated and user restricted commands. Surfacing table lineage provides additional context of the data landscape.\n\nWe also surface column popularity and the distribution of column values for low-cardinality columns, which helps to bring context to the forefront. Commonly joined tables, related queries, etc. are on our roadmap and will provide even more context.\n\nEmployees are the ultimate holders of tribal knowledge so we created a dedicated user page to reflect this. We consolidate all of the data resources an employee has created, consumed, or favorited. Any employee in the company can view the page of any other employee, which promotes transparency from both production and consumption standpoints.\n\nAnother source of tribal knowledge within Airbnb is teams. Teams have tables they query, dashboards they create and view, team metrics they track, etc. We found that employees spend a lot of time telling people about the same resources so we designed a way for them to link to and curate these popular items.\n\nGiven our ecosystem is represented as a graph, it was both logical and performant to use a graph database to store the data. We choose Neo4j because it integrates well with Elasticsearch (commonplace at Airbnb), and it has Python bindings. We use Flask as a lightweight Python web framework for the API, which is consistent with a number of open source Airbnb data tools like Airflow, The Knowledge Repository, and Superset. The single page web app leverages React and Redux.\n\nOne of our project goals is to help build trust in data. We therefore embraced a product mindset from the start to ensure a thoughtful UI and UX, and ultimately build a delightful data product to achieve this goal.\n\nOur work creating a self-service data culture is not over. It takes time for employees to change habits and incorporate a new tool into their workflows. Although asking a colleague about data is easy, it is highly inefficient at scale. It can be hard to re-train yourself to search for data on your own. Education about data and the Dataportal are ongoing efforts.\n\nOur future work will focus on improving discovery by surfacing insights from the graph in the form of user-specific recommendations and notifications. To further address the problem of trust, we plan on creating a data certification process with a rubric for both certifiers and content. Certified content will be boosted in rankings to help promote its relevance."
    },
    {
        "url": "https://medium.com/airbnb-engineering/automated-machine-learning-a-paradigm-shift-that-accelerates-data-scientist-productivity-airbnb-f1f8a10d61f8",
        "title": "Automated Machine Learning \u2014 A Paradigm Shift That Accelerates Data Scientist Productivity @ Airbnb",
        "text": "At Airbnb, we are always searching for ways to improve our data science workflow. A fair amount of our data science projects involve machine learning, and many parts of this workflow are repetitive. These repetitive tasks include, but are not limited to:\n\nExploratory Data Analysis: Visualizing data before embarking on a modeling exercise is a crucial step in machine learning. Automating tasks such as plotting all your variables against the target variable being predicted as well as computing summary statistics can save lots of time.\n\nFeature Transformations: There are many choices in how you can encode categorical variables, impute missing values, encode sequences and text, etc. Many of these feature transformations are canonical such that they can be reliably applied to many problems.\n\nAlgorithm Selection & Hyper-parameter Tuning: There are a dizzying number of algorithms to choose from and related hyper-parameters that can be tuned. These tasks are very amenable to automation.\n\nModel Diagnostics: Learning curves, partial dependence plots, feature importances, ROC and other diagnostics are extremely useful to generate automatically.\n\nThere is a growing community around creating tools that automate the tasks outlined above, as well as other tasks that are part of the machine learning workflow. The paradigm that encapsulates this idea is often referred to as automated machine learning, which I will abbreviate as \u201cAML\u201d for the rest of this post.\n\nThere is no universally agreed upon scope of AML, however the folks who routinely organize the AML workshop at the annual ICML conference define a reasonable scope on their website, which includes automating all of the repetitive tasks defined above.\n\nThe scope of AML is ambitious, however, is it really effective? The answer is it depends on how you use it. Our view is that it is difficult to perform wholesale replacement of a data scientist with an AML framework, because most machine learning problems require domain knowledge and human judgement to set up correctly.\n\nAlso, we have found AML tools to be most useful for regression and classification problems involving tabular datasets, however the state of this area is quickly advancing. In summary, we believe that in certain cases AML can vastly increase a data scientist\u2019s productivity, often by an order of magnitude.\n\nWe have leveraged AML at Airbnb in the following ways:\n\nThere is a wide array of commercial and open source tools that address the AML paradigm. We have experimented with the following tools:\n\nAt Airbnb, we use machine learning to build customer lifetime value models (LTV) for guests and hosts. These models allow us to improve our decision making and interactions with our community at very granular levels (down to the user, level if we like).\n\nLTV models are setup as a standard regression problem for guests, where the target variable is the spend of each guest over a time horizon. The features of this model include demographic, location and activity information from our web and mobile applications. There are many moving parts in this model that account for supply and demand elasticity, expected costs, and other variables.\n\nDuring the course of building a model, it is important for a data scientist to stay objective with regards to their choice of algorithm. For example a complex model may only offer a small incremental benefit over a simple model and this trade-off should be made deliberately. For example, during the course of building the LTV model we succumbed to a bias towards one of our favorite algorithms, eXtreme gradient boosted trees (XGBoost). The reason for our biases were the following:\n\nBeing aware of our bias, we fed our raw training data through an AML platform to perform a sanity check and to benchmark our model\u2019s error.\n\nAn illustration of these benchmarks are in the chart below. This chart displays the distribution of RMSE (Root Mean Squared Error) of a variety of models across out of time cross validation folds. The y-axis corresponds to distinct \u201cblueprints\u201d which are a combination of algorithms and feature engineering steps. While it is impossible to go into the details of each of these blueprints, the below chart gives you a sense of the breadth of exploration that modern AML systems are capable of.\n\nUsing AML, we quickly got an alternate point of view: linear models are very competitive for this problem. It turned out that the AML platform tested a plethora of alternate feature engineering steps as well as performed more rigorous hyper-parameter tuning that we did not have time to explore manually. Furthermore, these findings allowed us to make changes to our algorithm and reduce model error by over 5%, which translated into a material impact.\n\nAML is a powerful set of techniques for faster data exploration as well as improving model accuracy through model tuning and better diagnostics. The above case study highlights AML\u2019s capability to improve model accuracy, however we have realized AMLs other benefits as well. For problems that are amenable to AML, we view the use of this paradigm as good modeling hygiene as it is cheap to try once you have already composed your training data. AML is not guaranteed to improve your results, but we find that it often does if used skillfully.\n\nIntrigued? We\u2019re always looking for talented people to join our Data Science and Analytics team!\n\nSpecial thanks to the following editors: Michael, Robert Chang, Krishna Puttaswamy"
    },
    {
        "url": "https://medium.com/airbnb-engineering/https-medium-com-jonathan-parks-scaling-erf-23fd17c91166",
        "title": "Scaling Airbnb\u2019s Experimentation Platform \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "At Airbnb, we are constantly iterating on the user experience and product features. This can include changes to the look and feel of the website or native apps, optimizations for our smart pricing and search ranking algorithms, or even targeting the right content and timing for our email campaigns. For the majority of this work, we leverage our internal A/B Testing platform, the Experimentation Reporting Framework (ERF), to validate our hypotheses and quantify the impact of our work. Read about the basics of ERF and our philosophy on interpreting experiments. \n\n \n\nIntroduced in 2014, ERF began as a tool that allowed users to define metrics in a common configuration language, and then compute and surface those metrics for a small set of experiments. Since then, the number of concurrent experiments running in ERF has grown from a few dozen (in 2014) to about 500 concurrent experiments (at the time of this post). More impressively, the number of metrics computed per day has grown exponentially, starting with a few dozen (in 2014). Today we compute ~2500 distinct metrics per day and roughly 50k distinct experiment/metric combinations. We also have introduced several advanced features (e.g. dimensional cuts, global coverage, pre-assignment bias checking) that add to our scaling challenge. This post explores the architectural approach that we have taken to solve these problems at Airbnb, and the tradeoffs that we\u2019ve had to consider along the way. ERF results were originally computed via a Ruby script that executed daily. This script was a simple query generator that assembled and ran a single huge, monolithic Hive query for each experiment. There were several issues with this approach: The monolithic nature of the queries did not support \u201ccheckpointing\u201d; thus all work had to be reprocessed when failures were encountered. This created a feedback loop. As queries required more time to complete, this increased the likelihood of failure. This, in turn, resulted in more cluster congestion, and ultimately, longer running queries. Dependency checking required all metric tables to land before an experiment could be processed. As adoption grew, the original ERF pipeline did not scale, resulting in a swamped Hadoop cluster and dissatisfied end users. To address these issues, ERF was re-written in Python to run in Airflow, a workload orchestrator developed and open-sourced at Airbnb that leverages \u201cconfiguration as code\u201d principles. This enabled the ERF pipelines to be constructed as dynamically generated Airflow pipelines (a.k.a. DAGs) composed of smaller units of work. Today, we leverage 5 Airflow pipelines to compute ERF assignments, sources, metrics and dimensions, each comprised of thousands of tasks. During the transition to Airflow, we shifted away from computing all of an experiment\u2019s metrics at once, moving the processing unit to the \u201cEvent Source\u201d (i.e. a query defining several metrics). This approach had the key advantage of only scanning the source table once, resulting in enormous computational savings. This also resolved the \u201cdependency issue\u201d where a single broken metric sometimes could stall all metrics for a particular experiment. Today, most of an experiment\u2019s metrics are computed independently. After migrating to Airflow and rearchitecting our data pipelines, the run-time of ERF was reduced from 24+ hours to about 45 minutes. \n\n \n\nBelow is an example of an Event Source definition for \u201ctrips by guest\u201d. This file defines two events that are used to compute metrics, and creates two additional aggregations on top of these events. On a given day, ERF independently processes ~350 of these \u201cEvent Sources\u201d. This is far more scalable than executing a query for each metric (2.5k) or each metric/experiment combination (50k). The chart below shows an Airflow representation of tasks required to compute a single Event Source. Each Airflow pipeline contains about 350 of these data flows that are executed in parallel.\n\nWith the stability of ERF improved and a more flexible/intuitive configuration syntax, adoption immediately began to thrive. This led to more experiments and a huge influx of new metrics. Things quickly got out of hand with some experiments adopting 100 or more metrics. High-level users didn\u2019t know which metrics were important, and UI overcrowding made it difficult to quickly find top-level metrics. This also made it difficult to ensure SLAs on critical business metrics. To address these issues, we introduced a metric hierarchy. Core Metrics: Each experiment is required to adopt Core Metrics, a predefined list of critical business metrics. This ensures that all experiment owners are made aware of their impact (even if none is anticipated). Target Metrics: Each experiment can specify a list of Target Metrics. These are pinned to the top of the UI to improve visibility for key metrics. Certified Metrics: The Data Engineering team certifies around 50% of ERF metrics. These metrics have an SLA guarantee and changes to their definitions are closely audited. All core metrics are guaranteed to be certified metrics. Dimensional cuts are one of the most useful features in ERF. This allows users to slice and dice metrics by user attributes (e.g. geography, language, platform) and attributes of the event\u2019s themselves (e.g \u201cinstant book\u201d vs non-\u201cinstant book\u201d reservations). There are important distinctions between these two types of attributes, which are discussed below.\n\n \n\nSubject-level Dimensions Subject-level attributes enable us to observe response within cohorts/segments of the experiment population. As mentioned before, these dimensions are defined at the subject-level, and correspond to some attribute of each guest or host in the experiment. For instance, we can look specifically at treatment performance for guests from a particular geography (e.g. Europe, France, Paris). \n\n \n\n Event-level Dimensions\n\n \n\nAs ERF adoption began to grow, we noticed a pattern where users were creating variations on metrics, embedding dimensional detail in the definitions themselves. For instance, users created 3 separate booking metrics to independently measure bookings on Android, iOS, and Web. This was in addition to the already existing bookings metric. This approach proved to be unscalable, and contributed to overcrowding in the UI. To resolve this issue, we introduced event-level dimensions.\n\n \n\n Event-level attributes are atomic to the event, allowing us to subdivide a metric into its composing parts (e.g. we can explore bookings by the device used to make the booking, or the geography of the destination). One key difference with event-level dimensions is that metrics are computed against the entire experiment population. Subject-level dimensions segment the event and assignments by the dimension, whereas all event-level cuts for an experiment use the same population size. For example, consider the event-level dimensional cut of bookings by whether or not it is instant bookable. In this case, Bookings and Instant Bookings are essentially considered different metrics that are computed against the same population.\n\n \n\n Pre-computation and UI\n\n \n\n ERF currently precomputes around 50 dimensions for each metric and serves this information instantly in the ERF UI (see gif below). This is the most resource intensive computation in ERF, as data is typically exploded by dimension, and aggregated using GROUPING SETs. The vast quantity of statistical data generated also requires special attention, as this data must be loaded into the MySQL database serving the UI, and optimized for query performance.\n\nThe introduction of precomputed dimensional cuts was hugely successful, but left users wanting to dive deeper into metrics. To accommodate this need, we launched ERF Explorer, a specialized UI that supports drill down and filtering on dimensions. ERF Explorer leverages intermediate datasets (left behind by the main ERF pipelines) to compute stats on the fly via Presto. The tool supports multiple levels of drill down/filtering and other advanced analyses such as quantifying experiment interaction. This enables users to answer virtually any question about their experiment without writing a single database query. The diagram below describes the entities required to compute ERF metrics and dimensional cuts. The diamond figures represent ETL jobs that join in contextual information (e.g. assignments, dimensions) and aggregate data to the desired grain. In this case, the goal is to compute the first and second moments for each experiment treatment. This is in turn used to compute delta and p-value statistics.\n\n \n\n The primary objective of this pipeline architecture is to organize data to minimize computational requirements and prevent duplicate processing. For instance, we compute event-level dimensions from the experiment_events table (see step 5), but choose to compute subject-level dimensions from the subject_summary table. This offers major efficiencies because data has already been aggregated on the join key (i.e. subject_id, experiment). We\u2019ve made enormous strides in experimentation at Airbnb in the past few years. This has been an important lever for the engineering organization, enabling teams to accurately assess the quality of new features and iterate with higher frequency. Despite these gains, there is still a lot of ongoing work in this field at Airbnb. One major project is to leverage metric definitions in ERF to drive company-wide dashboards, supporting slice and dice functionality (powered by Druid). We also have an ongoing project to build support for real-time metrics and custom tools for analyzing Site Performance data. Special thanks to Maxime Beauchemin, Adrian Kuhn, Jeff Feng, and Gurer Kiratli. Check out all of our open source projects over at airbnb.io and follow us on Twitter: @AirbnbEng + @AirbnbData"
    },
    {
        "url": "https://medium.com/airbnb-engineering/messaging-sync-scaling-mobile-messaging-at-airbnb-659142036f06",
        "title": "Messaging Sync \u2014 Scaling Mobile Messaging at Airbnb",
        "text": "With more than 100k messages being sent on mobile per hour, messaging is the most engaged with feature on the Airbnb App. However, with the old inbox fetch method on mobile, the inbox was slow to load, suffered from data inconsistency, and required a network connection to read messages. All of this results in a poor experience for hosts and guests using the mobile inbox. In order to make the inbox faster, more reliable, and more consistent, the Hosts and Homes team at Airbnb built Messaging Sync.\n\nThe old inbox fetch was implemented similar to a turn-of-the-century webmail client, doing a network fetch for a screenful of information at every user tap. With messaging sync, new messages and thread updates are fetched only when data change, which greatly reduces the number of network requests. This means navigation between the inbox and message thread screens is much faster, hitting the local mobile storage most of the time, instead of issuing a network request with each screen change. Messaging sync also reduces the response size for each network fetch, which results in a 2x improvement in terms of API latency. These user experience gains are magnified in areas with slow networks.\n\nThis is the most common scenario:\n\nThis is when there are too many threads and/or messages updates to return. For example, when a user first downloads the App, the server needs to send 10 thread objects and 30 messages for a full delta sync. The full delta sync response payload would be huge, which would result in longer load times and subpar user experience. Instead, we only return a full screen of threads.\n\nThreads sometimes need to be removed from the mobile App. For example, when a cohost is no longer managing the listing for the host, the server removes the threads between the cohost and the guest. In this case, the API sends down an array of thread ids deleted after last sync.\n\nWhile migrating to the new messaging sync API, there were couple caveats to watch out for:\n\nIn order to catch any discrepancies between the data returned by the old messaging API and the messaging sync API, a small percentage of mobile Apps run both the old and the new API at the same time to spot check. It logs all attribute values and the thread order returned from the two APIs. This allows us to catch regressions with the new API and iterate quickly. Whenever a bug is caught, the server marks corresponding thread objects as modified so that it would correct the discrepancy in the next sync.\n\n2. Messaging Sync makes it possible for users to read messages under airplane mode and bad network conditions.\n\nAfter rolling out Messaging Sync together with other messaging improvements, we see more messages are being sent from mobile (+3.8% and +5.3% for Android and iOS), fewer messages are being sent from web (-4.6% and -4.2%). And daily inbox visits are way up (+200% and +96%) as it is now the main screen on the hosting mode. The launch is a big win for our hosting community as messaging is the most used feature by hosts on Airbnb.\n\nLast but not least, if you are interested in creating a thriving community of engaged hosts who provide amazing hospitality everywhere in the world, the Hosts and Homes engineering team is always looking for talented people to join!"
    },
    {
        "url": "https://medium.com/airbnb-engineering/4-principles-for-making-experimentation-count-7a5f1a5268a",
        "title": "4 Principles for Making Experimentation Count \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "For over two years I\u2019ve been a Data Scientist on the Growth team at Airbnb. When I first started at the company we were running fewer than 100 experiments in a given week; we\u2019re now running about 700. As those of us in Growth know all too well, such growth does not happen organically. Instead, it comes about through cultivation. For us, this has meant not just building the right tools, such as our internal Experiment Reporting Framework [ERF], but actively shaping a robust culture of experimentation across all functions. Here I summarize four key principles that underlie our work and have led to step changes in the impact of experimentation on our business:\n\nPracticing these principles will not only save your org a ton of engineering time and money, but will allow incredible insight into your users and product.\n\nWe have incredible engineering talent at Airbnb. This means that it\u2019s easy to build a feature, but that doesn\u2019t mean it should be built, or that product will necessarily be better because of it. On the Growth team at Airbnb, we always start with the question, \u201cWhat do the data say?\u201d If you are not asking that question you are pursuing an incredibly inefficient product optimization strategy. If you have to guess, you\u2019re putting the cart before the horse and should do more work before experimenting.\n\nWhy do hypotheses matter? Without them you\u2019re untethered, easily distracted by what appear to be positive results, but could well be statistical flukes. In this situation it\u2019s easy to make up a story that fits your findings as opposed to doing the hard work of understanding what\u2019s going on. Things may surprise you \u2014 let them! If there is something we do not understand, we typically update our hypotheses and add metrics for clarity mid-flight, as our experimentation pipeline incorporates them with ease.\n\nAs an example, my team ran an experiment using a new translation service on our web and native apps. Naturally, we assumed that this new and improved service would increase conversion for both platforms. We saw that booking conversion in our native apps was jumping up in the experiment group, but we couldn\u2019t understand why something similar was not happening on the web. After puzzling over it we hypothesized this was due to a product change by another team whereby visitors on our native apps were more likely to be using the translation service than those on web. We added a measure for this and were right! A higher rate of users in our experiment group were using translation services in the first place, and our new translation service encouraged them to use it more often. We couldn\u2019t detect a change in our web group because they weren\u2019t getting the exogenous lift that the native group had. Learning this opened up a whole area of strategic opportunity for us. If we hadn\u2019t updated our hypotheses, who knows where we would have ended up.\n\nDon\u2019t just launch a feature or set up an experiment, and wait for the magic to happen. In more cases than not, there will be no magic. This does not mean you are not awesome, but is a reminder that our job is hard. One area where I\u2019ve seen countless teams struggle in experimentation is properly defining the exposed population. The exposed population defines who should see a feature and who should not, and is distinct from the exposure rate, which determines how much of the exposed population is going to be included in your experiment.\n\nFor example, my team wanted to launch a message translation feature for guests who speak a different language than their host, with the hypothesis that this feature would improve conversion to book. Determining language is easy enough, but it\u2019s not enough. If we launched this feature for all guests who message and speak a different language than their host, we\u2019d be over-exposing our experiment- because not all guests who message are doing so to book. Some are on a trip and need to ask where the towels are. Others may have left a phone charger behind and would like it returned. While this may seem simple, you\u2019d be surprised how often this can be glossed over or even ignored, since preventing it often takes seasoned knowledge about your business\u2019 APIs. As a result, the first question I ask when chatting with an engineer to understand what\u2019s going on in an experiment is simple: \u2018When does exposure happen and how is it determined?\u2019. Make people draw it out for you if necessary. This conversation will pay dividends for both parties later. (If they acknowledge that it may be too much work to properly expose users, first push back, then see the Hail Mary option below.)\n\nSanity metrics can be helpful here. If your experiment is limited to current users, add some metrics that would indicate if there are non-users (visitors) in your experiment, like signups. If you see significant numbers of signups happening in your experiment, you are probably not exposing it correctly. Another response is to compute global coverage for your metrics. If you expect an entire population to see your feature, confirm that they are. Overexposure will dilute metrics, with implications for power. It\u2019s really awful to build a great feature but not be able to detect its impact!\n\nHail Mary: If you cannot accurately expose your experiment, make sure you have a way to identify the users who shouldn\u2019t be in the experiment and drop them in the analysis stage. At Airbnb we do this by uploading an \u201cexclusion table\u201d to our experimentation pipeline, which includes all users that should be dropped from analysis due to improper exposure. Identifying these users can sometimes be incredibly onerous. If you are doing this work, make sure you share this with your partners as it is in the best interest of your whole team to understand data challenges and resolve them in scaleable ways.\n\nPower determines your ability to detect an effect in your experiment if there is one. You should not be running experiments if you do not understand this. You can, and should do better than guessing.\n\nIt\u2019s sometimes tempting to use experimentation as a way to prove that you can move metrics, and if (when) you don\u2019t, move on (to another awesome moonshot idea). You should be moving metrics, and when you do, you should be able to show it. But if you only focus on the wins you are going to miss a ton of insight, and risk being blind to mistakes.\n\nWhen this happens, make it your job to understand why. Some questions to get you started include:\n\nThere is a learning curve here. But like most things, it gets easier. It is incredible what you can learn by routinely starting from a place of curiosity.\n\nExperimentation is hard work. A sophisticated experimentation tool is just that- a tool. It does no work on its own. That leaves all of us who experiment with incredible opportunity to shape the businesses we care about.\n\nIntrigued? We\u2019re always looking for talented people to join our Data Science and Analytics team!"
    },
    {
        "url": "https://medium.com/airbnb-engineering/alerting-framework-at-airbnb-35ba48df894f",
        "title": "Alerting Framework at Airbnb \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "At Airbnb, we do not have an engineering operations team, so individual teams are responsible for configuring monitoring and responding to problems for their service. We use Datadog to monitor our infrastructure and alert on its health. While Datadog works well and provides many features, we had some specific requirements around alerting:\n\nThese requirements meant taking alert configuration out of the Datadog UI and into a configuration repository. Fortunately, Datadog does provide an API which we could build the tools that we need to easily templatize our alerts based on our infrastructure and track changes to them.\n\nInterferon is our solution to the alerting requirements we described. It uses a Ruby DSL to define alerts and interacts with an alerting system such as Datadog. Interferon reads host information from various pluggable sources and make the data about the hosts accessible to the Ruby DSL. You can, therefore, write a host source that will dynamically read your infrastructure data (such as querying information from the AWS API) and create alerts based on the various attributes you have available.\n\nIn addition, you can configure your host sources to include metadata such as ownership information along with each host. This allows you to write a generic alert and create multiple instances of the alert which route to the owners of each host.\n\nFor example, here is our standard memory alert:\n\nHere, we have which is a Ruby Hash that contains information dynamically generated by one of the host sources (in our case, Optica). The hash contains which is the name of the Chef role of the host and :owner_groups and which contain the ownership metadata.\n\nThe name attribute corresponds to the Datadog monitor name and is used as the primary key for Interferon. We can leverage the attribute here to create a memory alert for each different role.\n\nThe for this particular alert is generic; however, it is helpful to be able to write alerts with descriptive messages and actionable steps. This aids in first responders being able to quickly begin to triage the issue when they are alerted, especially for obscure problems or edge-cases.\n\nWith a little bit of Ruby code, we can filter out hosts that are exempt from this alerts. The attribute tells Interferon to only include hosts where expression evaluates to true. In this alert, we only want hosts that have roles attach to them but skipping test hosts.\n\nUsing the metadata provided in , we tag the users in in the Datadog message so they are notified when the alert goes off. Interferon also ships with a simple way to define groups using YAML. Those groups in are expanded into people to also tag in the message.\n\nFinally we have the attribute which corresponds to the Datadog query syntax for defining the metric and alert parameters.\n\nThe alerts framework consists of the alerting gem, Interferon, as well as an alerts repository where the actual alert definitions as well as custom host sources, group sources, and custom destinations are kept. Teams can contribute to the alerts repository to modify alerts and to add custom sources.\n\nThe repository provides an audit trail for changes made to the alerts as well as a quick way to revert erroneous changes made. On top of that, we have our repository configured to require peer review to help ensure new alerts have clear messages and reasonable settings. We added a Datadog syntax checker to the pre-commit check to shorten the alert development lifecycle by providing almost immediate feedback on malformed Datadog syntax.\n\nIn our infrastructure, commits to the alerts repository create a new build artifact containing the alert definitions and custom code. A deployment ships the artifact to an instance and invokes Interferon to synchronize the latest definitions with Datadog. Interferon downloads the current definitions from Datadog then compares the existing definitions with the ones it generated. In order to cut down on the amount of traffic, it will only modify the definitions that have changed.\n\nInterferon is also scheduled to run every hour to pick up infrastructure changes in order to keep Datadog in sync when new hosts and roles are introduced.\n\nInterferon also has dry-run functionality which allow teams to determine what changes were going to be made. We encourage people to deploy their branch to an instance which runs Interferon under dry-run before deploying to production. The output from dry-run displays the delta between the branch and what is stored in Datadog.\n\nWe have been using Interferon in production since late 2014 and have been very satisfied with the impact it has made. Both interferon and a sample alert repository are open source and available on Github. If you are interested in using Interferon, you can start by cloning the example repository. The example repository contains a few example alert definitions as well as a few custom host sources. It is especially convenient if you are running on AWS because Interferon includes built-in AWS host sources.\n\nThere is no need to submit PRs if you want to define custom host sources, group sources, or definitions because you can place them into your own custom alerts repository for Interferon to find. For example, our internal alerts repository includes Airbnb-specific host sources like the names of our Resque queues along with who owns them and how full they should be before we start alerting about them. When writing custom sources, we recommend experimenting by writing static host sources then rewriting it to pull dynamic information as necessary.\n\nIf you have written interesting general-purpose host sources, group sources, or destinations, we welcome contributions back to the upstream project: https://github.com/airbnb/interferon.\n\nIgor Serebryany as the original author of Interferon and Willie Yao for ongoing maintenance."
    },
    {
        "url": "https://medium.com/airbnb-engineering/tracking-the-money-scaling-financial-reporting-at-airbnb-6d742b80f040",
        "title": "Tracking the Money \u2014 Scaling Financial Reporting at Airbnb",
        "text": "At Airbnb, the Payments team is responsible for everything related to moving money in Airbnb\u2019s global marketplace. We build technology that powers Airbnb\u2019s massive daily transaction volume to collect payments from guests and distribute payouts to hosts. Our goal is to make the payment experience on Airbnb delightful, magical, and intuitive.\n\nHistorically, the payments team\u2019s focus was to implement new features, currencies, and payment methods to make payments local in a global business. Our sphere has grown to include compliance (sales taxes, earnings taxes, licenses, and more) as well as reconciliation and financial accounting according to generally accepted accounting principles.\n\nCurrently, Airbnb\u2019s payment and financial accounting system is a complex ecosystem that transacts in 191 countries, with 70+ currencies and 20+ processors. Not only has Airbnb\u2019s transaction volume experienced exponential transaction growth every year, we have also rapidly increased features and products on our platform. Airbnb hopes to become a premier end-to-end travel service, not only helping people with accommodations but also trip experiences as well.\n\nThe challenge to maintain the existing financial accounting system to support new products as well as the increasing data volume has become a \u201cmission impossible\u201d sort of a task.\n\nAirbnb\u2019s Finance Infrastructure engineering team is responsible for delivering accurate, reliable, and comprehensive business/financial data to our stakeholders. In this blog post, we\u2019ll talk about how we manage to keep track of where all of our money is and how it moves in a scalable way in the face of exploding data size and complexity, as well as to support new Airbnb initiatives and payment products. We\u2019ll share the workflow of our deprecated finance system, illustrate its challenges and issues and then describe the new system that we built to replace it.\n\nBuilt in early 2012 and retired in late 2016, our previous financial system was a MySQL data pipeline. It was a parameterized MySQL ETL that ran nightly to provide financial reporting, and served us faithfully for the past few years. The workflow was as follows:\n\nThere were some advantages to this approach. Since we only relied on MySQL DB triggers and MySQL guarantees data accuracy, engineers had lots of flexibility to change the business logic and to move very fast. SQL based reports can be written very fast when the logic is simple.\n\nHowever, the SQL-based ETL approach was not scaling well:\n\nAs our transaction volume grew and our transformation logic became more complex, the nightly pipeline took more time. A relational database is hard to scale up. It is difficult to shard the data and difficult to leverage a distributed system to process a massive amount of data. Towards the end of its life, we were only able to run it every other day due to its runtime of over 24 hours.\n\nAs we grow, we needed to be able to cope with dramatically increasing data size and the frequent addition of new Airbnb products and payment channels. Thus, we had two goals for our new system:\n\nOur event based financial report is designed to:\n\nIt is powered by Apache Spark, stored on our HDFS cluster, and written in Scala. Spark is a fast and general engine for large-scale data processing, with implicit parallelism and fault-tolerance. We chose Scala as the language because we wanted the latest features of Spark, as well as the other benefits of the language, like types, closures, immutability, lazy evaluation, etc.\n\nThis is huge considering our previous language was SQL.\n\nOur new financial reporting system has a concept of different product types, of which reservations are only one. Each product type has its own set of platform and payment events, and a corresponding set of financial events. Thus we can address each product type individually and systematically build up to a holistic report.\n\nThe system can be thought of as many event handlers that calculate the accounting impact of different products at different points in their life cycle. Because Scala has a strong static type system, while providing full support for functional programming, it is easy to design and write handlers about different products and how to process them.\n\nBelow is a diagram of how the data flows through the system. Don\u2019t worry, we\u2019ll explain everything.\n\nPlatform events are events that provide information about product related changes, like reservations, reservation alterations, photography, cancellations, etc. These usually have some financial expectation associated with them, but it is not always the case. Each time a product is created or updated, we derive an event for that product. For example, when a reservation is booked, we emit a booking event for the reservation product type. A day after the reservation starts, we consider the reservation to have \u201cservices rendered\u201d. When the service is delivered to the customer (the guest in this case), we can then recognize revenue. These events are important because they have financial accounting implications, and we will talk about those more below.\n\nPayment events describe money movement. They can be events where real money moves in and out of Airbnb bank accounts. Payment events also describe stored value, like when someone buys and uses gift cards. There are other kinds of payment events where money may not actually move, but we still have to account for the lack of cash movement. This can be when someone sends someone else a gift card, and that person claims it. We consider those to be balance transfers, or virtual movement. An example of no money movement would be when a guest uses a coupon. The money from the coupon is funded from the marketing budget, but no money has actually moved accounts \u2014 we just need to account for it somewhere. Money in must equal money out. Because coupons are on the guest side and don\u2019t impact the original host payout amount and likewise the other host side operations, we need to take these into account so the money equation balances.\n\nThese events are currently generated from examining the aforementioned accounting audit rows for changes. If the change has an accounting impact, then a platform or payment event is generated. This system was designed as a central place for all the data to pass through from different systems. The financial reporting system processes these platform and payment events with event handlers, and produces accounting events that describe the accounting impact of those events.\n\nAccounting events are generated by event handlers that build them from the payment and platform events. We introduced this layer of abstraction to represent the relationship between the different platform and payment events for each product, as well as the accounting logic. Sometimes, as you\u2019ll see below, a single platform event can generate more than one accounting event, because it has multiple accounting impacts at different times. These events basically keep track of what happened by assigning a unique identifier, the product type and the product id, to a set of activity. For us, we consider the product type and id to be the smallest accounting unit that we operate on.\n\nFrom accounting events, we generate the subledger, which is the basis for all of our financial accounting. Each entry is a detailed accounting record that includes information about the time a transaction occurred (payment or reservation booking), the amount, the currency, the direction of the monetary impact (credit or debit) and the account that it impacts.\n\nThe subledger is generated using double entry accounting. Double entry accounting allows us to be sure that everything is accounted for properly in the system. This means no money appears or disappears without a source.\n\nEven though each product type may behave differently, we\u2019ve found a generic life cycle that all product types share. Let\u2019s walk through how a reservation would look in this framework.\n\nAn event happens that introduces some accounting liability, and a contract is created. No money has moved at this point, but expectations for future money flow are set up here. The accounting events at this point describe the contract that has been created.\n\nAn event\u2019s money flow occurs. This can happen anytime after the contract has been created. The accounting events here describe the direction and for what liability the amount is fulfilling.\n\nThe event happens and so the contracted service is fulfilled.\n\nSometimes, there are alterations on a product or a payment. Examples would be a guest adding dates to a reservation inducing a reservation price change. To properly account for the price differences from an alteration, we \u201cunbook\u201d and \u201crebook\u201d the reservation entirely at the time of alteration.\n\nNow that we\u2019ve built the subledgers from the platform and payment events, and their handlers, we can easily query for the financial impact to different accounts that any event generates.\n\nAn example of how revenue would be queried from the subledger is as follows:\n\nOur financial reporting pipeline scales both on a product basis, and on a runtime basis. We can easily support new products on the financial engineering side because we\u2019ve built a framework around the right abstractions, instead of tying it too closely with one specific product\u2019s life cycle. We can also scale horizontally. This is much better than being limited by an Amazon RDS instance, no matter how beefy it may be. Our nightly runtime is 4\u20135 hours and has not been growing too much as of March 2017.\n\nBefore, when our Finance team needed comprehensive reports, we pulled the data separately for all the different reports and our finance team combined them at their discretion. Because we built all the reports separately, it could be very difficult to tell which change in which report caused unexpected deviances. This was not scalable when the company wanted to change the product or add new products more frequently. Now when there\u2019s an issue, we investigate data from a single source of truth, significantly simplifying the troubleshooting process.\n\nGoing from declarative programming to functional programming has been a powerful paradigm shift for us to think about financial processing and accounting. We can now think of this system as a straightforward actor/handler system rather than getting mired in complicated SQL-join logic.\n\nOriginally, the MySQL ETL was scheduled via a crontab, and was dependent on data arriving via a different pipeline. Instead of taking upwards of a day to complete, the nightly run takes around 4\u20135 hours to complete. We no longer have to babysit a legacy system that quite frequently encounters snags caused by upstream changes, taking hours of developer time each week to resolve.\n\nThis is perhaps the important part of the picture. Because our financial processing and reporting is no longer in SQL, we are now also able to write extensive suites of unit tests against specific handlers. Together with integration tests and smoke tests, we can easily identify regressions and other errors. Smoke tests are rules we expect our data to follow and when rule violations occur, they are logged and addressed. This gives us a high degree of confidence in the quality of our data and lets us quickly vet new changes and roll them out. We have built an extensive (and ever expanding) test suite of real transactions, in which we check how we expect them to look in the system individually as well as in aggregate. This test framework is critical when we have time sensitive requests from our various partners in Finance and Legal, as well as from our audit partners, and need to be confident in our reporting.\n\nIn the future, we will be moving towards an entirely event-based system. The financial reporting system will consume events emitted from other systems. Stay tuned to read about that in a future blog post. This will help us with even greater financial integrity and a richer vocabulary with which we can express different products and payment flows.\n\nIn the end, what we want most at Airbnb is to have is complete, accurate and extensible financial reporting for all of our current and future products at Airbnb. We believe that we have designed the financial reporting system to be a strong foundation of all financial processing at Airbnb. Because of the clean decoupling of business logic and accounting logic, this system is product agnostic, extensible, and future-proof, which gives us confidence it will serve us well for many years to come. This is just the start of our back office financial systems at Airbnb.\n\nIf you enjoyed reading this and thought this was an interesting challenge, the payments team is always looking for talented people to join the team, whether you are a software engineer or a data scientist.\n\nPlease stay tuned for more on the Payments ecosystem at Airbnb!\n\nMany thanks to Sarah Hagstrom, Lou Kosak, Shawn Yan, Brian Wey, Jiangming Yang, and Ian Logan for reading through many drafts and helping me to write this post."
    },
    {
        "url": "https://medium.com/airbnb-engineering/moving-airbnb-search-to-react-b85b815e166c",
        "title": "Moving Airbnb Search to React \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "In early 2015, the Airbnb Engineering team decided to embrace React as its canonical front-end view framework. We\u2019ve since built a significant amount of tooling around React to make it as pleasant an environment to develop in as possible, and we\u2019ve contributed a lot of these tools back to the open source community. \n\n \n\nUnfortunately, because our search page was written using a largely unmaintained framework (Twitter\u2019s Flight) that wasn\u2019t used anywhere else on our site, it couldn\u2019t benefit from these investments in tooling and the accumulation of institutional knowledge around using React. The search page code also lacked a comprehensive suite of tests. As such, it was often much more difficult (read: slower) to work in the search page codebase compared with other codebases of similar size.\n\n \n\nIn early 2016, we decided to start refactoring the search page into React. The simple UX of the search page belies its complex implementation; often, there are numerous external teams running experiments on the page, not to mention the various locale- and market-specific customizations. This complexity, paired with the thin test coverage, meant that any substantial refactor would likely cause some regressions in behavior. Since the search page is at the top of the guest funnel, we needed to be absolutely certain that regressions would be minimized, to reduce any negative impact to our core business. In this blog post, we talk about how we were able to use experiments to confidently launch the refactored search page.\n\nExperiment Reporting Framework (ERF) is an in-house tool developed by our Data Tools team which simplifies the tasks of experiment setup, data analysis, and results visualization. It lets us perform split-tests and analyze the impact of each treatment according to various metrics, including ones that tie in to our core business. We also have the ability to segment these metrics by dimensions like locale, country, browser, and platform; this proved to be invaluable in narrowing down regressions, as we\u2019ll talk about later. ERF is used in virtually every product and feature launch at Airbnb.\n\nThe idea here was to use ERF to split-test between the original code and the refactored React code. If the refactored code contained a regression, our hypothesis was that it would have a meaningful impact on key metrics, which we would see on the experiment dashboard. In determining the scope of each experiment, we tried to strike a balance between running fewer experiments (since the data collection phase often took some time), and keeping each experiment change-set as small as possible (to make it easier for us to isolate regressions).\n\nLike in React, Flight encapsulates behaviors into components and establishes a component hierarchy, and the component hierarchy introduced by Flight largely overlapped with what we imagined it to be in React. This made the process of refactoring easier; our approach was thus to reimplement a Flight component in React and split-test between the two using ERF.\n\n \n\nUntil the Flight components were completely replaced with React components, Flight was also responsible for managing all the data flows, so we also had to build an interoperability layer between Flight and React; this took the form of higher-order component (HOC) wrappers around each refactored component that translated Flight events to React prop and state changes and vice versa.\n\n \n\nWith that scaffolding and shimming complete, we could move on to refactoring the components themselves and testing them with experiments.\n\nA component that was targeted for refactoring was the listing card, which we make liberal use of on the search page.\n\nListing cards are one of the most complex components on the search page; there are many behaviors obscured behind many code paths, so it wasn\u2019t surprising that a refactor would fail to port some behaviors over. Sure enough, when we ran the experiment comparing the original listing card to its refactored counterpart, we saw a dip in views of the listing page from the search page. Segmenting it by platform revealed an interesting pattern \u2014 that the dip was isolated to the iPhone and Android platforms:\n\nOur listing cards are responsive components \u2014 a listing card on a small (read: mobile) breakpoint will behave slightly differently from a listing card on a large (read: desktop) breakpoint. The old listing card\u2019s behavior on small breakpoints opens a new tab to the listing page when clicked or tapped.\n\nDigging into the code after ERF surfaced the issue, we discovered that the refactored React listing card didn\u2019t implement this new tab behavior for small breakpoints; it opened the listing page in the same tab. Re-running the experiment with that fix showed a marked improvement in the metrics:\n\nThe regressions specific to the mobile platforms have disappeared; those numbers are now neutral. As additional remediation, we wrote regression tests to make sure we won\u2019t break that tab-opening behavior in the future.\n\nWe used this validation methodology throughout the rest of our refactoring, and were able to uncover several other regressions which, in aggregate, would have had a substantially negative effect on our core business.\n\nThere are several caveats to using experiments to validate a refactoring. There\u2019s the possibility that the effect from regressions may not be detected because the relevant metrics haven\u2019t yet been developed. In our case, we were reasonably confident that the coverage from our suite of metrics was sufficient for our purposes; the suite has seen contributions from numerous product teams over the course of several years, instrumenting things as diverse as page load performance and support tickets created.\n\nAnother caveat is that this validation strategy is generally more effective on pages that have high throughput \u2014 it takes less time to collect enough data to achieve significance. As the search page is at the top of our guest funnel (and thus receives a significant amount of traffic), it worked fine for us.\n\nThere\u2019s also a lag time between launching the experiment and collecting enough data to be able to make an informed decision \u2014 we needed to carefully pipeline work so that we weren\u2019t sitting idle while we were waiting for data to come in.\n\nUsing experiments to validate refactored code turned out to be an invaluable approach for surfacing issues, which allowed us to fix them and confidently and incrementally launch the refactor without impacting the core business. It\u2019s a validation strategy that we can adopt for future refactorings of critical high-throughput user-facing flows."
    },
    {
        "url": "https://medium.com/airbnb-engineering/superset-scaling-data-access-and-visual-insights-at-airbnb-3ce3e9b88a7f",
        "title": "Superset: Scaling Data Access and Visual Insights at Airbnb",
        "text": "At Airbnb, one of our fundamental beliefs is that data access should be democratized to empower every employee in order to make data informed decisions. We believe that grounding decisions with quantitative insights from data, together with qualitative insights (e.g. in-person user research) result in the best possible business decisions. This applies to all parts of the organization, whether it is about deciding to launch a new product feature or analyzing how to provide the best possible employee experience.\n\nOne of the challenges that accompanies data democracy is enabling data access to users at various levels of data literacy. A number of our users are deeply skilled at writing SQL, particularly those in our Data Science, Engineering and Business Operations teams. The ability to write SQL provides tremendous flexibility in accessing data from our Data Warehouse in Hadoop. However, in our vision to empower every user of data, SQL is often times too high a barrier. Secondly, once users have accessed the data, they are faced with the challenge of exploring and discovering insights. Our solution to both of these challenges is the development of Superset.\n\nSuperset is a data exploration and visualization platform designed to be visual, intuitive and interactive. It consists of two primary interfaces:\n\nThe combination of these two interfaces enables users to consume data in a variety of ways. Users can directly visualize data from tables stored a variety of databases including Presto, Hive, Impala, Spark SQL, MySQL, Postgres, Oracle, Redshift, and SQL Server. Connectivity with Druid extends the capabilities of Superset to visualize billions of rows of data thanks to its in-memory, column-oriented distributed architecture. With the addition of a SQL IDE, it provides power users with the ability to compose SQL queries to restructure or reduce the size of your data or union data across tables. Additionally, users can immediately visualize their query results using Superset\u2019s Visualize flow.\n\nWe first launched Superset\u2019s data exploration interface in March 2016 as a way for users to perform fast and intuitive \u201cslicing and dicing\u201d against any dataset. Since then, we have added a number of major new features including:\n\nToday we\u2019re announcing the introduction of \u201cSQL Lab\u201d, the new SQL IDE in Superset. Integrating SQL Lab into Superset is advantageous because it connects the flow from arbitrary SQL to data visualization, dashboarding and knowledge sharing. Integrating both the SQL IDE and the Data Exploration interfaces together had the additional benefit of managing authentication, roles and permissions in a single tool. SQL Lab as a part of Superset enables us to provide backend query support to all the databases that Superset supports (with the exception of Druid which is not SQL based).\n\nSQL Lab packs a number of powerful features including (for the full list, see the Superset documentation):\n\nComputationally intensive, long running queries are common in the \u201cpetabyte era\u201d of data, and SQL Lab is designed to provide a nice workflow for this use case. For deployments that have an asynchronous backend available, SQL Lab will automatically default to running queries asynchronously to support large queries. Additionally, with the Create Table As (CTAS) feature, SQL Lab allows users to store query results in a newly created table. With this table, users can then query and visualize data off of the summary table that was just created.\n\nSQL Lab also makes it easy to manage access for any internal database to a set of employees. Administrators can add a new database to Superset using a simple flow while subsequently granting permissions to users through roles. Users can be granted per-database-connection access, as well as per-table access. In the cases where per-table access applies, Superset introspects the query and identifies the table referenced in the SQL.\n\nSuperset had humble beginnings starting out as a hackathon data visualization project, however now it is a full-fledged open source project. Since we launched Superset in March 2016, it has grown into one of the most popular open source data visualization apps, with over 10,000 stars and 100 contributors on GitHub. New features and bug fixes are being added weekly by both Airbnb engineers and community contributors. With the addition of SQL Lab, we are confident that data users will find even more usefulness from the project. We\u2019re excited to see the project grow and improve over time. Some of the features in the near term roadmap include:\n\nAirbnb loves open source, and the Superset team does all of their work in the open. Come join our community on Github! Or if you are really excited about our vision and want to join the team, we\u2019re hiring software engineers to revolutionize the future of data visualization."
    },
    {
        "url": "https://medium.com/airbnb-engineering/getting-to-swift-3-at-airbnb-79a257d2b656",
        "title": "Getting to Swift 3 at Airbnb \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Airbnb has been using Swift since the language\u2019s inception. We\u2019ve seen many benefits from using this modern, safe, community-driven language.\n\nUntil recently, a large part of our codebase had been in Swift 2. We\u2019ve just finished our migration to Swift 3, right in time for the release of Xcode that drops Swift 2 support.\n\nWe want to share with the community our approach to this migration, the effect Swift 3 has had on our app, and some technical insights we gained along the way.\n\nWe have dozens of modules and several 3rd-party libraries written in Swift, comprising thousands of files and hundreds of thousands of lines of code. As if the size of this Swift codebase weren\u2019t enough of a challenge, the fact that Swift 2 and Swift 3 modules cannot import each other further complicated the migration process. Even correct Swift 3 code that imports Swift 2 libraries will not compile. This incompatibility made it difficult to parallelize code conversion.\n\nTo make sure we could incrementally convert and validate our code, we began by creating a dependency graph which topologically sorted our 36 Swift modules. Our upgrade plan was as follows:\n\nFrom speaking with other companies who had already completed the migration, we learned that freezing development was a common strategy. We wanted to avoid a code freeze if at all possible, even if it meant some added difficulty for those doing the migration. Since the conversion work would not be easily parallelizable, an all-hands-on-deck approach would be inefficient. Also, since it was difficult to estimate how long the conversion would take, we wanted to ensure that we could continue to ship new releases during the migration.\n\nWe had three people working on the migration. Two people focused on the code conversion, and the third focused on coordinating, communicating with the team, and benchmarking.\n\nIn the end, including the preparation work, our actual project timeline looked like this:\n\nWhile we were excited about Swift 3\u2019s new language features, we also wanted to understand how the update would affect our end users and overall developer experience. We closely monitored Swift 3\u2019s impact on release IPA size and debug build time, since these have been our two largest Swift pain points so far. Unfortunately, after experimenting with different optimization settings, Swift 3 still scored marginally worse on both metrics.\n\nAfter migrating to Swift 3, we saw a 2.2MB increase in our release IPA. A bit of digging revealed that this was almost entirely due to increases in the size of Swift\u2019s libraries (the size of our own binaries barely changed). Here are a few examples we found of increases in uncompressed binary size:\n\nGiven the enhancements in Swift 3\u2019s libraries like Foundation, this change is understandable. Although, when the much-anticipated stable Swift ABI lands, applications should no longer have to suffer size increases to benefit from these enhancements.\n\nOur debug build time was 4.6% slower after the migration, adding 16 seconds to what was previously 6 minutes.\n\nWe tried to compare per-function compile times between Swift 2 and Swift 3, but were unable to draw concrete conclusions since the profiles were so different. However, we did find a function whose compile time had ballooned to 12 seconds due to the migration. Fortunately, we were able to massage it back down, but it illustrated to us the importance of checking converted code for outliers like this. Tools like Build Time Analyzer for Xcode can help, or you can just set the appropriate Swift compiler flags and parse the resulting build logs.\n\nUnfortunately, the migration work isn\u2019t finished even after your code compiles in Swift 3. The Xcode code conversion tool doesn\u2019t guarantee identical runtime behavior. Moreover, as we\u2019ll discuss later, the code conversion still involves manual work and there are some gotchas. This, unfortunately, can mean regressions. Since our unit test coverage didn\u2019t give us sufficient confidence, we had to spend extra QA cycles on the newly migrated app.\n\nThe first QA pass through the newly migrated app yielded dozens of fairly obvious issues. The vast majority of issues were resolved quickly (in a matter of hours) by the 3-person team responsible for the migration, primarily through the application of a couple of the techniques discussed later in this doc. After this initial elimination of the low-hanging, highly visible regressions, the iOS team at large was left with 15 potential regressions \u2014 3 of which were crashes \u2014 that required investigation before the next app version release.\n\nWe started by creating a new branch from . As mentioned earlier, we tackled the code conversion module by module, starting with leaf modules and working our way up the dependency tree. Wherever possible, we worked on converting different modules in parallel. When we couldn\u2019t, we sat together, calling out what we were working on so as to avoid collisions.\n\nFor each module, the process was roughly the following:\n\nWhen manually updating code, we held to the philosophy \u201cdo the most literal code conversion.\u201d This meant that we didn\u2019t aim to improve the safety of our code during the conversion. We did this for two reasons. First, since the team was actively developing in Swift 2, the process was a race against time. Second, we wanted to minimize the ever-present risk of introducing regressions.\n\nFortunately, we undertook this project during a period of time when work was slower due to the holidays. This meant that we could safely go a number of days without rebasing onto without falling too far behind. Whenever we did rebase, we used to keep as much of as possible while defaulting to code in to resolve conflicts.\n\nOnce was caught up with , we knew we\u2019d need about a day to sort through a number of issues before we\u2019d be comfortable merging it. With an iOS team our size, though, is a moving target. So, to complete the Swift 3 migration we strongly encouraged the entire team (minus the ones doing the migration) to really, truly take a Saturday off work \ud83d\ude04.\n\nOne of the most common issues we encountered where Xcode did not automatically suggest a fix has to do with bridging block parameters between Objective-C and Swift. Consider this method declaration in an Objective-C header:\n\nIn Swift 2.3, the generated interface was:\n\nIn Swift 3, the generated interface is:\n\nA number of things have changed, but most importantly the parameter in has changed from an implicitly unwrapped optional to an optional. This can break its usage within the blocks.\n\nWe decided that the most \u201cliteral\u201d translation into Swift 3 (without touching Objective-C code) would be to declare at the top of the block a variable that has the same name as the parameter but is implicitly unwrapped:\n\nDoing this, rather than actually unwrapping the parameter when it\u2019s used, is the least likely to break semantics elsewhere in the block. In the above example, subsequent statements like and would continue to work as expected.\n\nAnother common (and related) issue has to do with how Swift 3 infers the type of a variable to which an implicitly unwrapped optional is assigned. Consider the following:\n\nIn Swift 2.3, was inferred to be of type . In Swift 3, it\u2019s of type .\n\nFor reasons outlined with the block parameters, the most literal solution is to declare your variable to be an implicitly unwrapped optional type:\n\nThis particular issue appeared more often than expected because bridged Objective-C initializers return implicitly unwrapped optionals.\n\nOccasionally during our code conversion work, the compiler would grind to a halt for many minutes.\n\nOur project is home to some functions that require a lot of complex type inference. Under normal conditions these take a trivial amount of time to compile. But when they contain compilation errors, it can send the compiler into a tailspin.\n\nWhen our progress was blocked by this type of problem, we used the Build Time Analyzer for Xcode to help us discover where the bottleneck was. Then we could focus our efforts on that function and unblock our happy cycle of converting code, rebuilding, and converting more code.\n\nOptional protocol methods are easy to accidentally miss during a Swift 3 conversion.\n\nConsider this method on :\n\nSuppose your class implements and declares the following method:\n\nCan you spot the differences? It\u2019s tough. But they\u2019re there. And your class will compile just fine without updating the definition\u2019s signature since it\u2019s an optional protocol method.\n\nFortunately, there are compiler warnings to help you in some of these cases, but not all. It\u2019s important to go through any types implementing protocols with optional methods \u2014 like most UIKit delegate and data source protocols \u2014 and verify their correctness. Searches for text like \u201c \u201d (note the first parameter label, a sure sign of lingering Swift 2) can help you find offenders in your codebase.\n\nProtocols may have default method implementations provided through protocol extensions. If a protocol\u2019s method signature has changed between Swift 2 and Swift 3, it\u2019s important to verify that it\u2019s been changed everywhere. The compiler will happily compile if either the protocol extension\u2019s implementation or your type\u2019s implementation is correct, but successful compilation is no guarantee that both implementations are correct.\n\nIn Swift 3, naming convention dictates that enum cases be . The Xcode code conversion tool will automatically make appropriate changes to any existing enums. It skips enums, however, whose raw value type is . There is a good reason for this \u2014 it\u2019s possible to initialize one of these enums with a matching an enum case name. If you change the enum case name you risk breaking initialization somewhere. You may be tempted to \u201cfinish the job\u201d by lower-casing some enum cases yourself, but only do so if you have the utmost confidence that it won\u2019t break -based initialization somewhere.\n\nLike most apps, we have some dependencies on 3rd-party libraries. The migration required updating any libraries written in Swift. This should hopefully seem obvious, but it\u2019s worth mentioning: read release notes very carefully, especially if your dependency has undergone a major version change (which is likely when changing language versions). It helped us discover some non-obvious API changes that the compiler would not have been able to help us with.\n\nWhew! Our branch is in Swift 3, and no new development is taking place in Swift 2. All migration-related work is done, right?\n\nWell, not quite. As mentioned earlier, during the code conversion process we were only making the most \u201cliteral\u201d conversion between Swift 2 and Swift 3 code. This means that we weren\u2019t always taking advantage of Swift 3\u2019s added safety or its new conventions.\n\nOn an ongoing basis, we\u2019ll be looking out for a number of potential improvements.\n\nBy default, the Xcode code conversion tool converts access control modifiers to , and ones to . This represents a \u201cliteral\u201d conversion which guarantees that the code will continue to work as before. However, it also bypasses an opportunity for the developer to consider whether the new and behaviors are actually better tools for the job. A next step is to revisit instances of literal access control conversion and check whether we can make use of Swift 3\u2019s increased expressiveness to provide more fine-grained control.\n\nWhen manually converting code (when the Xcode conversion tool didn\u2019t suffice, or when we were rebasing) we often took a \u201cliteral\u201d approach to changing method names so that call sites would continue to be correct. Take the following Swift 2.3 method signature, for instance:\n\nIn the interest of making the smallest, quickest change that would get our code compiling again in Swift 3, we would convert this to:\n\nA more \u201cSwift 3\u201d way of writing this, however, would be:\n\nA next step is finding instances where took the naming shortcut and updating method signatures to better follow Swift 3 conventions.\n\nAs shown earlier, the way we dealt with newly-optional (in Swift 3) Objective-C block parameters was by assigning them right away to implicitly unwrapped optional variables, which obviates the need to update much of the code within the block. However, what we should be doing in our block is appropriately handling the possibility of the parameter being .\n\nIn an effort to wrap up the code conversion process quickly, we ended up ignoring a fair number of compiler warnings that didn\u2019t strike us as especially urgent. Going forward, we\u2019ll have to be conscious of getting our warning count back down.\n\nAs Airbnb was an early and eager adopter of Swift, we accumulated lots of Swift code. The prospect of migrating to Swift 3 seemed daunting at first, and it wasn\u2019t clear how we were going proceed or how it would affect our app. If you haven\u2019t yet decided to convert your code to Swift 3, we hope our experience has helped demystify the process a bit.\n\nFinally, if you\u2019re interested in using the latest mobile technologies like Swift 3 to help people belong anywhere, we\u2019re hiring!"
    },
    {
        "url": "https://medium.com/airbnb-engineering/introducing-lottie-4ff4a0afac0e",
        "title": "Introducing Lottie \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Our new open-source tool makes adding animation to native apps a snap. In the past, building complex animations for Android, iOS, and React Native apps was a difficult and lengthy process. You either had to add bulky image files for each screen size or write a thousand lines of brittle, hard-to-maintain code. Because of this, most apps weren\u2019t using animation \u2014 despite it being a powerful tool for communicating ideas and creating compelling user experiences. One year ago, we set out to change that. Today, we\u2019re happy to introduce our solution. Lottie is an iOS, Android, and React Native library that renders After Effects animations in real time, and allows native apps to use animations as easily as they use static assets. Lottie uses animation data exported as JSON files from an open-source After Effects extension called Bodymovin. The extension is bundled with a JavaScript player that can render the animations on the web. Since February of 2015, Bodymovin\u2019s creator, Hernan Torrisi, has built a solid foundation by adding features and improvements to the plugin on a monthly basis. Our team (Brandon Withrow on iOS, Gabriel Peal on Android, Leland Richardson on React Native, and I on experience design) began our journey by building on top of Torrisi\u2019s phenomenal work.\n\nLottie allows engineers to build richer animations without the painstaking overhead of re-writing them. Nick Butcher\u2019s jump through animation, Bartek Lipinski\u2019s hamburger menu, and Miroslaw Stanek\u2019s Twitter heart demonstrate just how difficult and time consuming it can be to re-create animations from scratch. With Lottie, digging through frameworks for reference, guessing durations, manually creating B\u00e9zier curves, and re-making animations with nothing more than a GIF for reference will be a thing of the past. Now engineers can use exactly what the designer intended, exactly how it was made. To demonstrate that, we\u2019ve recreated their animations and provided After Effects and JSON files of each in our sample app. Our goal is to support as many After Effects features as we possibly can, to allow for a lot more than simple icon animations. We\u2019ve created a handful of other examples to show the library\u2019s flexibility, richness, and deep feature set. In the sample app, there are also source files for a variety of different kinds of animations, including basic line art, character-based animations, and dynamic logo animations with multiple angles and cuts.\n\nAirbnb is a global company that supports millions of guests and hosts, so having a flexible animation format that is playable on multiple platforms was extremely important to us. There are libraries similar to Lottie, such as Marcus Eckert\u2019s Squall and Facebook\u2019s Keyframes, but our goals are slightly different. Facebook picked a small set of After Effects features to support, since they were focusing mainly on reactions, but we want to support as many as possible. As for Squall, designers at Airbnb use it in combination with Lottie because it has an amazing After Effects preview app that\u2019s become a necessary part of our workflow. However, it only supports iOS and our engineering teams needed a cross-platform solution. Lottie also has several features built into its API to make it more versatile and efficient. It supports loading JSON files over the network, which is useful for A/B testing. It also has an optional caching mechanism, so frequently used animations, such as a wish-list heart, can load a cached copy each time. Lottie animations can be driven by gestures using the animated progress feature, and animation speed can be manipulated by changing a simple value. iOS even supports adding additional native UI to an animation at runtime, which can be used for complex animated transitions. In addition to all of the After Effects features and API additions we\u2019ve worked on so far, we have many ideas for the future. These include mapping views to Lottie animations, controlling view transitions with Lottie, support for Battle Axe\u2019s RubberHose, gradient, type, and image support. The hardest part is picking which features to tackle next.\n\nReleasing something as open source is more than just putting it out there for the public to use. It\u2019s a bridge that connects people and creates community. As we got closer to releasing Lottie to designers and engineers via GitHub, we wanted to be sure to connect with the animation folks as well. We were inspired by the communities that 9 Squares, Motion Corpse, and Animography have created. All three have brought together people from around the world, who otherwise would never have worked together, to collaborate on public animation projects. These projects take months of work and a lot of organization and wrangling by the respective teams, but they undoubtedly provide immense value for the animation community as a whole. Motion Corpse and Animography publicly share the After Effects source files as well, which provide tons of insights on how people work. Following their collaborative lead, we reached out to all three teams to contribute animations to our sample app. We\u2019ve included an animation from Motion Corpse created by J.R. Canest, one of Al Boardman\u2019s squares from the 9 Squares project, and an animated keyboard using Animography\u2019s Mobilo animated typeface, which features work from more than two dozen artists. We\u2019re hoping that the merging of these animation communities with the powerful engineering community will spark something special.\n\nWe\u2019d love to hear how you\u2019re using Lottie \u2014 no matter if you\u2019re a designer, animator, or engineer. Feel free to reach out to us directly at lottie@airbnb.com with your thoughts, feedback, and insights. We\u2019re excited to see what the community around the world will do when they begin to use Lottie in ways we never imagined."
    },
    {
        "url": "https://medium.com/airbnb-engineering/streamalert-real-time-data-analysis-and-alerting-e8619e3e5043",
        "title": "StreamAlert: Real-time Data Analysis and Alerting \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Today we are incredibly excited to announce the open source release of StreamAlert, a real-time data analysis framework with point-in-time alerting. StreamAlert is unique in that it\u2019s serverless, scalable to TB\u2019s/hour, infrastructure deployment is automated and it\u2019s secure by default.\n\nIn this blog post, we\u2019ll cover why we built it, additional benefits, supported use-cases, how it works and more!\n\nAirbnb needed a product that empowered both engineers and administrators to ingest, analyze, and alert on data in real-time from their respective environments.\n\nAs we reasoned about our use cases and explored available options, we codified our requirements:\n\nWe were unable to find a product that fit these requirements, so we built our own. Since one of our requirements necessitated the product be environment agnostic, it naturally lended itself to being an open source project.\n\nAs partially outlined above, StreamAlert has some unique benefits:\n\nThe graphic below denotes some example data sets that StreamAlert can analyze:\n\nStreamAlert aims to be as agnostic as possible in order to support the widest range of data analysis and alerting use-cases.\n\nFrom a data perspective, StreamAlert supports file formats such as JSON, CSV, Key-Value, and Syslog formats.\n\nIf you\u2019re an AWS customer, gzipped versions of these data formats are supported in S3 buckets. As a result, StreamAlert supports CloudTrail, AWS Config and S3 Server Access Logs out of the box.\n\nIf you\u2019re not an AWS customer, StreamAlert can support data such as:\n\nIt should be noted that StreamAlert is not intended for analytics, metrics or time series use-cases. There are many great open source and commercial offerings in this space, including but not limited to Prometheus, DataDog and NewRelic.\n\nRules encompass data analysis and alerting logic and are written in Python. Here\u2019s an example that alerts on the use of sudo in a PCI environment:\n\nYou have the flexibility to perform simple or complex data analysis. Some of the notable features of this approach are:\n\nAs outlined above, StreamAlert comes with a flexible alerting framework that can integrate with new or existing case/incident management tools. StreamAlert enables your Rules to send alerts to one or many outputs.\n\nOut of the box, StreamAlert supports S3, PagerDuty and Slack. It can also be extended to support any API. Outputs will be more modular in the near future to better support additional outputs and public contributions.\n\nAdhering to the secure by default principle, all API credentials are encrypted and decrypted using AWS Key Management Service (KMS).\n\nIf this looks overwhelming, don\u2019t worry \u2014 recall that infrastructure deployment is automated via Terraform, ensuring that you don\u2019t have to manage, patch or harden any new servers!\n\nThe idea of crowdsourcing your alerts isn\u2019t new. Slack does this and the blog speaks at length to the benefits. In the near future, StreamAlert will support this use-case, allowing you to decentralize your triage efforts, getting alerts to those with the most context. We\u2019re aiming for Q1/Q2'17.\n\nIn the near future, StreamAlert will support comparing logs against traditional indicators of compromise (IOCs), which can range from thousands to millions in volume. This will be built in a way that\u2019s provider agnostic, allowing you to use ThreatStream, ThreatExchange, or whatever your heart desires. We\u2019re aiming for Q1/Q2'17.\n\nStreamAlert will also support receiving data via an HTTP endpoint. This is for service providers or appliances that only support HTTP endpoints for logging. We\u2019re aiming for Q2'17\n\nFor historical searching, StreamAlert will use AWS Athena, a serverless, interactive query service that uses Presto to query data in S3. This will allow you to analyze data using SQL for both ad-hoc and scheduled queries. We\u2019re aiming for Q3/Q4'17.\n\nOpen source has allowed us as a community, to both share, collaborate, and iterate on common needs and goals. Now with the ability to represent infrastructure as code, this goal can be further realized with reduced costs for both development and deployment.\n\nWe hope StreamAlert serves as an example of this, making deployment simple, repeatable and safe so that anyone can use it easily.\n\nFinally, if you want to get started using or contributing to StreamAlert, please visit https://github.com/airbnb/streamalert"
    },
    {
        "url": "https://medium.com/airbnb-engineering/nebula-as-a-storage-platform-to-build-airbnbs-search-backends-ecc577b05f06",
        "title": "Nebula as a Storage Platform to Build Airbnb\u2019s Search Backends",
        "text": "Last year Airbnb grew to a point that a scalable and distributed storage system was required to store data for some applications. For example, personalization data for search grew larger than what a single machine can hold. While we could rebuild just the personalization service to scale up, we foresaw other services to have similar requirements and decided to build a common platform to simplify such tasks for other service owners.\n\nBesides the normal request/response pattern, many services have different needs such as performing periodic bulk operations to synchronize with the \u201csource-of-truth\u201d (e.g., an MySQL DB), bootstrapping a newly derived data source (e.g., a new search feature), consuming incremental updates from data stream (in our case Kafka), providing analytical interactions on the data, and serving the data with very low latency for site-facing traffic. As the company continues to grow, we will have more applications accumulating more and more data. Such data can provide huge value to our products if we can dig out useful signals and provide feedback to the application.\n\nWe started with personalized search. It needs to keep rapidly growing history of user behavior. It requires real-time user actions to be recorded and available immediately to help personalize search results (and improve other products). A data snapshot needs to be provided so that other applications can use it (e.g. for analytics or validation). It needs periodic compactions to aggregate and potentially truncate the older history, plus bulk load a new batch of features (computed offline) back to the system.\n\nThese requirements are common across many applications in the company. Therefore we decided to build a general storage platform to support such requirements, and to help service owners focus on their specific business logic. We aim to satisfy the following concrete requirements with this system:\n\nNote that we define \u201cbulk operations\u201d as the operations to snapshot and compact the full repository, to replace the existing snapshot with a new snapshot for serving, to merge a complete new signal into the repository, to replace an existing signals with a new set of data, and any other operations on the full data set. Nebula is the platform we built to fulfill these requirements at the storage level.\n\nNebula is a schema-less, versioned data store service with both real-time random data access and offline batch data management. It is an independent service with a dynamic data store for delta data (updates in a recent period of time), and a static data store for snapshot data which supports bulk operations. We chose to use DynamoDB for the dynamic data store (mainly because it has very low latency reads, and the operational cost is minimal since it is hosted by AWS), and HFileService (a scalable static storage built in-house at Airbnb which can partition and serve pre-computed HFile-formatted data from local disks) to serve static snapshots.\n\nNebula provides an unified API for the underlying physical storages. The API is provided to the application as a generic key-value store API with the delta and static data being merged internally so each application does not need to develop their own mechanism to handle real-time and batch data separately. Therefore, it has the flexibility to migrate to a different physical storage without changing the API and applications built on top of it.\n\nNebula uses a versioned tabular schema for storage abstraction, similarly to BigTable and HBase. A versioned tabular storage makes it more convenient for service owners to define their data model in comparison to raw key-value stores. Versions can also be used to do conflict resolution and track the time series data when needed. There is no limitation on how many versions an application can store for each row and column.\n\nNebula supports atomic operations at <row, column, version> level. Concurrent writes to the same <row, column> have different versions so they can be sorted appropriately. Each column have their own versioning and all writes are appended directly to individual columns (sorted by version). The user gets random access to retrieve one or more versions for a given <row, column>. Requests against multiple columns and/or rows can also be combined into a single multi-request.\n\nUsing personalization data as an example, the data model looks like this:\n\nEach row represents data associated with a user, each column represents a type of user interaction (a.k.a. user events) like previous searches, and each version is the value of the user event at the event timestamp. In production we have many user events, and each event column might accumulate a huge amount of events with different timestamps.\n\nTo serve a search request, the search backend looks up the required events data for the given user, and uses the personalized data in ranking models to decide ordered listings shown to the user. Since this is on the search request path, we have very strict requirements on latency and availability.\n\nThe data can be updated in per-cell basis by incremental data stream (containing individual user events) and in bulk mode by offline pipeline (compacting the history and/or bootstrapping/merging/replacing a new column).\n\nNebula runs an offline pipeline for each repository to snapshot the delta store, merge with previous snapshots, and load new snapshots back for serving. Since such jobs are executed separately from the online service, it has minimal impact on site-facing traffic.\n\nThe pipeline is highly configurable based on requirements. For example, each application can define their own policy on how to merge old and new data (e.g., new data overwrites old data, aggregates them by versions, throws away old data, etc.), how to compact history (e.g., keeps N versions, remove data older than certain period of time etc.), how to schedule the pipeline execution, and so on.\n\nNebula also provides well-defined interfaces for users to define their customized data and load into the system automatically. The user can put their data at a shared location, update some Nebula configurations, and the pipeline will pick up and merge the data into the system for serving.\n\nAn application owner can also hook their customized logic into the pipeline to fulfil their special requirements on the data snapshot. Their logic will be executed on the latest snapshot data, therefore, it can be done in a very efficient way.\n\nIn the personalized search case, we utilize the offline pipeline for the following:\n\nAll personalization data are versioned, so whenever there is a data problem, Nebula can rollback to previous good snapshot and discard any bad data before a version (timestamp). The logic of rollback belongs to the application, but the bulk interface in Nebula make it very easy to implement.\n\nHere is the overall architecture of the system (shown below) and a few highlighted design choices.\n\nA nebula read query looks up two data stores. While the delta data store holds only the latest data, the snapshot store holds full snapshots. Both stores serve read-queries, but only the dynamic store accepts write requests. The snapshot store is updated by switching the underlying snapshots. Co-ordination of data stores is handled by Zookeeper.\n\nWe choose DynamoDB because of the low-latency requirement, but it is possible to switch to a different physical storage (such as HBase) for different requirements. To be an underlying storage for Nebula, a physical storage only has to support a primary key and a sorted secondary key. Since they are different implementations of the same interfaces, switching the physical storage is transparent to any applications built on top of the system (and service users).\n\nWe do not plan to build yet another physical storage, so relying on DynamoDB as the low-level storage layer has allowed us to very quickly build a robust system.\n\nThe streaming input data are written to the dynamic store; it allows random updates and supports very high QPS for updating. The read latency from DynamoDB is low so we can satisfy the overall latency requirements with a median of 10 milliseconds. One optimization we did to keep DynamoDB table sizes manageable is to partition data into new tables per day. Therefore, each of our tables occupies only a few DynamoDB shards and provides good QPS guarantees.\n\nThe most recent snapshot is served in a dedicated HFileService cluster, from which Nebula composes a live view of the data together with dynamic updates.\n\nHFileService serves the static HFiles (format of the snapshot) from local disks directly, which can provide very low-latency and high throughput. Furthermore, the data loading process has minimal impact on read traffic, so the offline data merge operation does not influence real-time access of the data.\n\nHFileService partitions the data through a dynamic sharding mechanism, so it is horizontally scalable along with the total data size. Since it is static data, the replication policy is very easy and can be adjusted based on traffic growth.\n\nNebula supports both random online data access and bulk operations. The bulk operations do not affect online access. The figure below illustrates Nebula\u2019s offline architecture.\n\nThe delta store periodically dumps data as batch updates to a distributed file system (Amazon S3). Upon completion of the dumping, an offline Spark job starts to merge all historical data together with the batch updates. We often have other offline generated data, such as machine learning features, that we want to bulk upload into the system. This is also done at the merge stage. A new snapshot is created by combining batch updates, historical data, and the customized offline data. We add sanity checks in the process to prevent bad data from being imported into our system.\n\nThe newly created snapshot stays on S3 for the next round of merging. It also gets loaded into our historical data store. Throughout the whole dump-merge-load process, real time store continues to serve the delta data dumped into S3, and it drops them only after the new snapshot is successfully loaded into the historical data store. This guarantees the read queries always get a complete data by combining real-time and historical store.\n\nThe full snapshot on S3 is used for other offline data analytics.\n\nBeyond random access and batch processing through snapshots, Nebula also provides an update stream to provide instant access to data changes for the application. This is made possible by DynamoDB supporting update streams through the streams API. A separate component consumes the streams in a Kinesis consumer and publish it into specific Kafka streams, so any service interested can subscribe to it.\n\nAfter launching Nebula, we also rebuilt Airbnb\u2019s Search Indexing using Nebula. Let\u2019s first talk about why it needed rebuilding.\n\nSince Airbnb largely uses Rails/MySQL for the front-end, Search Indexing listened (and still does) to changes on Database tables, maintained a cache of the current search index documents, and updated the search instances with the new document if anything changed. There was uncertain performance due to features being loaded by polling loaders and periodic syncs with source-of-truth for data consistency. New Search machines could bootstrap their index by slowly streaming from the cache.\n\nThe following requirements were decided for improving the system:\n\nThe Nebula system was a perfect fit as we could do all of the above with its features. A versioned tabular storage meant that we could have auditable index documents, and support for bulk jobs meant we could generate the index offline (and merge listing features in) and directly deploy to Search. Because the index is built and deployed based on snapshots, we could quickly roll back in case of bad index data. The generated index could be used by new search instances to startup quickly (by just downloading the index).\n\nThe figure above shows the search indexing architecture with Nebula. The data snapshot is generated daily as a part of the offline data merge. The index builder job operates on this snapshot to build the sharded index which is then deployed to search periodically like an ordinary binary deploy. The system uses common features provided by Nebula, and only needs to implement customized logic related to search indexing.\n\nWe have built a couple of services on top of Nebula including the search indexing pipeline we just talked about, the personalization infrastructure, and Airbnb\u2019s pricing service data repository. It is serving multiple TBs of data for each application, with median latency ~10ms. We want to encourage other teams to build more applications served by Nebula.\n\nWe also plan to integrate the system deeply with our data warehouse, i.e., storing historical snapshots into Hive, sharing more data stream consuming logic, etc. The goal is to make the data available and consistent for analytics and making the system easier to manage and interact, such that it becomes much easier for developers to build their applications.\n\nThere are many people that made this work possible. We\u2019d like to thank Alex Guziel for his significant contributions to this project, also thank Jun He, Liyin Tang, Jingwei Lu for their generous help, and many people across Search, Application Infrastructure, Data Infrastructure, Product Infrastructure and other teams who helped in many ways."
    },
    {
        "url": "https://medium.com/airbnb-engineering/how-we-deliver-insights-to-hosts-7d836520a38",
        "title": "How we deliver insights to hosts \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "We observe a large amount of data from what people are searching for and booking on Airbnb. Behind these data, there are a lot of useful insights that can help hosts better manage their listings. At the same time, it can be overwhelming for hosts to process this information if they are not shared in efficient ways.\n\nFor example, one of the most common questions we hear from hosts is: How do I pick the right price? Setting a price can be hard without reliable information about other listings in your area, travel trends, and the interest people have in the amenities you offer. In the past couple years, we launched pricing tips and Smart Pricing that help hosts to get personalized, daily pricing recommendations right in the calendar. To do this, we developed a mathematical model that\u2019s learning how likely a guest is to book a specific listing, on specific dates, at a range of different prices. It uses different types of information, including your listing type, its location, your current price, your availability, and how far off each available date is.\n\nAs our models have gotten better at understanding the market, we have been able to learn more. And now we can share the insights that we learn with our hosts. For instance, if there is a propensity for guests to look for long term stays in an area, we can recommend setting long term discounts.\n\nAn insight is a campaign that guides hosts to become more successful at pricing. To come up with these insights, we decided that each insight has to be:\n\nTo serve personalized, targeted and actionable insights to various places with which hosts interact from the host dashboard to the host calendar page, we built an extensible backend service to serve insights to multiple channels. The service is a realtime system that ingests data from a set of offline and online data sources to generate personalized insights, rank their effectiveness for different listings and contexts, and deliver insights at the right place and right time. This service is called Narad. The name Narad comes from Indian tradition \u2014 Narad used to be a divine sage and a traveling story-teller who carried news and enlightening wisdom.\n\nInsights share many similar components. For example, different variations of insights encouraging hosts to set a weekly discount can target hosts with no weekly discount set or hosts who are only getting few long term bookings. Those insights would also share the same personalized payload, which would be the suggested amount for the weekly discount. With this in mind, the representation of insights is designed so that shared components can be easily plug-and-playable through the json config.\n\nAn insight consists of the following:\n\nBelow is an example of how the backend representation of an insight (on the right) can be translated into a personalized insight shown to the host (on the left). This weekly discount insight is delivered only to the host calendar page and targets hosts with no weekly discount set. And it contains two pieces of personalized information: the potential booking increase for long term bookings, and the suggested weekly discount value.\n\nWhenever a host takes an action on insights, the backend system tracks those events through Redis. Types of actions that the system tracks include impression, conversion, skip and dismiss. In order to keep insights fresh, it uses these interaction data to implement fatigue rules. The fatigue rules can be enforced through a set of dimensions such as actions, durations and counts. For example, one fatigue rule can allow max of 3 impressions in the past week for encouraging hosts to set the weekly discount. Another fatigue rule can limit a particular insight to appear after the host dismisses the insight. These configurable rules ensure that the system delivers the relevant content to the hosts with an enjoyable experience.\n\nThese interaction data are also used on ranking those insights. The system is responsible for delivering the most relevant and impactful insights to the host. The first iteration of ranking defines the total value of each insight through a set of terms. The first term is the weight which refers to the inherent impact of the insight. For example, an insight with an action that is applicable to the listing as a whole is inherently more valuable than an insight that only affects a couple available nights that the listing offers. The second term is the historical conversion rate of the particular insight. Some insights might carry high impact but draw less attention from hosts. Other insights might get a lot of conversions but not be inherently impactful. The first term and the second term keep this balance. The last term is the repetition penalty that discounts the total value of the insight if the same insight was ranked as the top insight last time. This helps providing some variance on the top position so that the same insight doesn\u2019t appear at the top over and over even though it is the best insight in order to keep hosts more engaged.\n\nWe have seen very positive results with the introduction of insights on the host calendar page. Hosts are engaging with the insights and are becoming more successful by taking actions on them. Overall we see a lift of 2% in bookings on Airbnb. Besides this, it has also given us a platform to communicate with hosts and introduce new insights through narad very quickly. Insights have a high acceptance rate with 1 out of 5 insights being acted upon by hosts.\n\nHere are some future works:\n\nAll of these would not have been possible without the work of Jerry Su (Frontend Engineer), Luca Beltrami (Product Manager), Clara Lam (Designer), Holly Hetherington (Content Strategist) and many others. If any of these roles sound interesting and exciting to you, let us know! We are hiring engineers, data scientists and designers on the pricing team at Airbnb."
    },
    {
        "url": "https://medium.com/airbnb-engineering/epoxy-airbnbs-view-architecture-on-android-c3e1af150394",
        "title": "Epoxy: Airbnb\u2019s View Architecture on Android \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Android\u2019s RecyclerView is a powerful tool for displaying lists of items, but its usage is cluttered with boilerplate and configuration. A common requirement for our team is to display lists with complexities such as multiple view types, pagination, tablet support, and item animations. We found ourselves duplicating the same configuration patterns over and over. We developed Epoxy to mitigate this trend, and to simplify the creation of list based views for both static and dynamically-loaded content.\n\nEpoxy takes a composable approach to building lists. Each item in the list is represented by a model that defines the item\u2019s layout, id, and span. The model also handles binding data to the item\u2019s view, and releasing the view\u2019s resources when it is recycled. These models are added to Epoxy\u2019s adapter in the order you want them displayed, and the adapter handles the intricacies of displaying them for you.\n\nLet\u2019s look at a practical example of how this works. Here is a view of the Airbnb app showing search results for neighborhoods in a city.\n\nBreaking this view down we have:\n\nBeyond that there are a few other views that are sometimes shown such as:\n\nThis gives us eight unique view types, and we need to use a RecyclerView to combine them all so that the whole page is scrollable and presented in a consistent interface.\n\nSetting up a RecyclerView adapter with this many view types would normally be messy. We would have a complicated class specifying view type ids, item counts, span counts, view holders, click listeners, and on and on.\n\nWith Epoxy\u2019s compositional approach our adapter can instead focus on specifying what items to show, and the details of displaying them are delegated to the models.\n\nThis roughly looks like the following:\n\nOur bindSearchData() method accepts an object that contains all of the information we need to build the view. It is idempotent, called whenever something may have changed, and it rebuilds the model state to reflect the new search data. In the last line we tell Epoxy to compute a diff between the new models and the old models, and it notifies the RecyclerView of the exact changes, if any.\n\nThis is similar to how React approaches user interfaces in javascript. The code only has to describe what should be shown, and the adapter takes care of the details of how to display it. We don\u2019t need to explicitly define any information such as item ids, counts, or view holders. Furthermore, we are freed of all responsibility of notifying what changed.\n\nThis lends itself to a nice architecture where an activity loads data from various sources such as databases, caches, or network requests. It stores this state in an object that is passed to the adapter, and the adapter builds its models to reflect the current state. Whenever the state object is changed, whether due to user input or newly loaded data, the new state is passed to the adapter and the models are updated again. Click listeners can be set on models to call back to the activity whenever something changed.\n\nThis approach separates responsibilities cleanly. Models can easily be swapped in or out as designs change or as new features are added. Complexity is kept low because of the compositional approach and the abstraction provided by the adapter.\n\nNormally performance might suffer due to frequent adapter item changes. However, Epoxy adds a diffing algorithm to detect changes in models and only update views that actually changed.\n\nAn additional complexity of a normal adapter is tracking item changes. Items may be added, removed, updated or moved, and the adapter must be notified of each of these changes to function properly. Done correctly, these notify calls allow the RecyclerView to only redraw the views that changed, as well as animate the changes. However, managing this manually in an already complex adapter can be difficult.\n\nEpoxy solves this problem for you by using a diffing algorithm on your models. Any time you change your model setup Epoxy finds the differences and reports the change set to the RecyclerView. This simplifies your adapter code, provides item change animations for free, and improves performance by only rebinding views when necessary.\n\nThis diffing algorithm relies on each model implementing hashCode so it can detect when a model has changed. Epoxy provides an annotation processor so your model can simply annotate the fields that should be considered in the model\u2019s state. A generated subclass implements the proper hashCode method for you, as well as getter and setter methods for each field.\n\nContinuing with our example from above, our header model would look something like this:\n\nA HeaderModel_ class is generated with the `setCity` method we need, and we use an instance of that class when adding a header to our models list. The header view is then only updated when the City object changes. This assumes that the City object also implements a proper hashCode method to define its state.\n\nYou\u2019ll also notice that the model implements getDefaultLayout() to return a layout resource. This resource is used to inflate the view that is passed to the model\u2019s bind method, where the data is actually set on the view. Additionally, the layout is used as the item\u2019s view type id in the adapter.\n\nIn order to function correctly Epoxy enables stable ids in the RecyclerView by default. This makes diffing possible, as well as enables item animations and saved state. Each model is responsible for defining its id, and we manually set an id on dynamically generated models. For example, each neighborhood carousel model is assigned the id given by the neighborhood object in the network request.\n\nStatic views like our header are trickier. There is no id inherently associated with it, so we have to make one up. Epoxy eases this by automatically generating an id for every new model created. The id is guaranteed to be unique among all other generated model ids for the life of the app process, and negative ids are used to avoid collision with the ids you may set manually.\n\nThe only catch here is we must use the same model for the life of the adapter so that the id stays constant. For our header model (and other static views) this means we declare it as a field, make it final, and initialize it inline. Then we add it to the models list as needed and update its data as normal. The id is then unique with no extra work from us.\n\nEpoxy also adds support for saving the state of views in a list, something which RecyclerView lacks by default. For example, the carousels in our search design above can be swiped through horizontally, and for a good user experience we want to save this carousel scroll position. If the user scrolls down through the results and then back up they should see a carousel in the same state they left it. Similarly, if they rotate their phone or switch apps and come back we should present the same state despite the activity being recreated.\n\nWith a normal RecyclerView adapter this would take considerable overhead to achieve. Epoxy, however, has generic support for saving the view state of any model. It does this by leveraging stable ids to associate the view\u2019s parceled state with the model id.\n\nTo use it, simply add the following:\n\nTo your model and Epoxy will save its state when it is goes off screen and restore when it comes back. By default this is set to false so that memory and scrolling performance aren\u2019t affected by saving the state of unnecessary views.\n\nRecycler views are often only used when showing dynamic content that is loaded from a remote source like a network request or database. Otherwise it is simpler to specify a scroll view with your components in XML. With Epoxy though, you can have the benefits of the RecyclerView without much more work than a ScrollView. We do this with our listing details page pictured below.\n\nIt would be simplest to build this with a ScrollView. However, we used a RecyclerView with Epoxy to give us faster page load times as well as easy animations.\n\nThis performance edge is crucial to us since this page is loaded often as users explore search results. Clicking a search result animates the listing image in a shared element transition to this details page. Making this animation smooth is important to an enjoyable search experience, but it requires us to have a listing details view that loads very quickly.\n\nLet\u2019s look at the views on this screen in detail and how they affect performance. First, the photo at the top is actually a horizontal RecyclerView so people can swipe through the listing\u2019s images. In the middle we have a static map view showing where the listing is, and at the bottom we have another RecyclerView showing similar listings in the area. Interspersed between those are quite a few text rows and smaller images that describe the listing.\n\nAltogether this gives us a fairly complicated view hierarchy with many bitmaps. This makes measure and layout passes take longer, and also requires more memory to load the images.\n\nAdditionally, we load data from a variety of sources \u2014 databases, in-memory caches, and multiple network requests \u2014 to power this page. This is great for showing the user immediate data, but results in extra time spent updating views if not handled properly.\n\nBetween the large view hierarchy, numerous bitmaps, and multiple view refreshes we have good reason to be worried about performance. Thankfully Epoxy lets us deliver a great user experience despite those concerns for three reasons:\n\nWe\u2019re happy to now share Epoxy as an open source library, and welcome contributions from developers interested in improving it with us. We are actively developing Epoxy to improve its annotation processor, diffing algorithm, and general utility. We hope other developers can find new uses for the library, and help us evolve it into an even better tool."
    },
    {
        "url": "https://medium.com/airbnb-engineering/interning-with-the-airbnb-px-sx-teams-5f43b7b0a8a6",
        "title": "Interning with the Airbnb PX & SX Teams \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "My name is Anna Matlin and I\u2019m a rising senior at Princeton University studying Operations Research Financial Engineering. I spent 12 weeks as a data science intern at Airbnb, working on the Product Excellence (PX) Team, which is responsible for the Help Center, the Resolution Center, and any features that empower users to help themselves on the platform. I also worked closely with Service Excellence (SX), which builds tools to maximize the productivity of Customer Experience (CX) specialists.\n\nI had an amazing experience at Airbnb. As an intern, I was assigned a challenging project and supported fully along the way by team members on PX, SX, and CX. Over the course of my internship, I designed and implemented a new taxonomy for CX data, creating new ways for PX and SX to understand the \u201clife of a ticket\u201d. The data I introduced will enable cost savings on the order of magnitude of tens of millions of dollars.\n\nWhile many of the teams at Airbnb are found across many companies \u2014 for example, Payments \u2014 PX & SX operate on a less familiar plane. Moreover, since only a fraction of users experience issues on Airbnb, it is easy to forget that Airbnb has a Help Center, a Resolution Center, and thousands of CX specialists working around the clock to serve the Airbnb community. Therefore, before I jump into the details of my project, it will be valuable to create some context about the high impact work that PX & SX are performing every day.\n\nI quickly learned that the sphere of PX and SX is especially challenging because it is so human. The realm of Payments, for example, is incredibly complex and intricate because of the sheer volume of transactions, currency exchange, bank routing, and more. But PX and SX are tasked with understanding human issues of every shape and form, at scale: the first time host who can\u2019t quite set her calendar properly 3 months in advance, the loyal user who arrived to a poorly cleaned listing and can\u2019t get in contact with the host, or the well-intentioned guest seeking a short term alteration after a car accident. When SX tools enable a CX specialist to save the day, or PX recommendation algorithms empower a user to find answers seamlessly, they enhance the user experience and the feeling of belonging that drives the entire Airbnb platform.\n\nIn addition to serving the mission of the company, PX & SX generate enormous cost savings because the teams reduce the number of users contacting CX, as well as time spent solving each case. Before PX & SX were formed, Airbnb was simply hiring and training more CX specialists, which was not a scalable solution.\n\nMy project for the summer was to \u201cintroduce structure\u201d into the vast realm of CX ticket data. On one end, PX maintains data about the help users search for before contacting CX. On the other, SX maintains data about ticket routing and cost metrics. However, the actual process by which a CX specialist solves a ticket, the how, is recorded in the format of unstructured text notes peppered with CX shorthand such as GCI (guest called in). As a result, data scientists on both teams are limited in the kind of analysis they can perform: PX can\u2019t predict what kind of resources would be most useful to a help-seeking user; SX can\u2019t identify particular CX actions that correspond to higher cost metrics.\n\nThe open ended nature of this task, when combined with the domain knowledge learning curve of PX & SX, made for an extremely challenging project. For the first few weeks, I shadowed CX specialists to observe their workflows, studied internal resources for CX as if I were a new CX specialist myself, and dove into existing text notes data. Before introducing structure into CX ticket data, I needed to develop an understanding of if, and then how, human actions mapped to the current unstructured data.\n\nTherefore, my first few weeks as a data science intern were highly unusual; I felt more like a PM, shuttling between teams from meeting to meeting and designing mocks of possible taxonomies for the data. I found that \u201cout of the box\u201d methods like clustering and simple regex parsing in Python were useful to get a sense of different themes in the text notes. Through this combination of qualitative fieldwork and quantitative analysis, I broke down the space of all actions CX specialists were describing in their notes into a finite set of categories and corresponding subcategories.\n\nThe next step was to translate the taxonomy I had designed into real data that would be accessible to PX and SX for analysis. I wrote the code for a directed acyclic graph (DAG) in Airflow, Airbnb\u2019s open source platform, to schedule and monitor data pipelines. Airflow jobs are written using a combination of Python and HQL scripts. Since I had limited time to build the DAG, which I hoped would be extended and maintained after I left, I was extremely careful about the data I chose to include. The DAG I wrote covered as many of the categories of the new taxonomy as possible, with just enough new data in each category to showcase potential for PX & SX insights.\n\nAfter I built the DAG, the next step was to make a case for continuation of the DAG as a long term project. The data would be valuable to both teams: for PX, understanding concrete actions CX specialists use to solve routine, simple tickets presents opportunities for automating help, in turn freeing CX agents to work on more difficult, impactful tickets; for SX, the same data could be used toward workforce management, routing optimization, and feature roadmaps for internal CX tools. In order to prove the value of the information, I needed a simple example of how the DAG could drive cost-saving insights.\n\nI dove into the new data and performed an analysis of ticket routing, identifying a small group of tickets that were routed inefficiently. Digging deeper, I identified trends among the CX actions for tickets in this group that could explain why the tickets were routed a certain way. These trends shed light on possible causes of the inefficient routing, enabling CX to step in and eliminate the problem. I then estimated the potential cost savings to be $1M annually. With this case study for the value of the data, as well as a detailed roadmap for expansion of the DAG, I was able to transfer ownership of the DAG to SX for the long term.\n\nMy three months at Airbnb were a fantastic learning experience, both technically and non-technically. As an intern, I worked on a high impact project that will be continued and expanded after I leave. I had the opportunity to visit the Portland office to meet team members on SX and CX, who were a wonderful and welcoming group. In general, I was constantly in awe of how thoughtful, kind, and supportive everyone was on PX, SX, and CX, in addition to being incredibly productive and hardworking.\n\nIt\u2019s difficult to express concisely what\u2019s so special about Airbnb\u2019s culture, but I\u2019ll do my best. On a high level, it\u2019s a result of the mission: to create a world in which anyone can belong anywhere. And then there are the little things, like fresh squeezed orange juice in the Eatrium every morning and the 1:7 dog-to-employee ratio. As cheesy as it sounds, I was truly excited to come into work every morning because of the boundless optimism, inclusivity, and can-do attitude of the people around me. Looking back on my summer, I\u2019m grateful not just for the chance to learn new skills but also for the opportunity to engage with the extraordinary Airbnb community."
    },
    {
        "url": "https://medium.com/airbnb-engineering/academia-to-data-science-99e68f36485e",
        "title": "Academia to Data Science \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "When you\u2019re in graduate school, it seems like the only career option available is to remain in the ivory tower. And it\u2019s reasonable to see why \u2014 your advisor and peers are very likely to encourage you to follow their chosen career path. Indeed, the selection bias is strong amongst those who surround you. And when you are a professor, you believe that the only job where you can expand the knowledge base, teach, and mentor others, is within the academic setting.\n\nHowever, taking a quick glance at the Ph.D. labor market shows that the number of doctorate-holders we produce annually exceeds the number of positions available. Also, Silicon Valley (and the tech industry in general) has been an appealing destination for many former academics and those with a research bent, but the leap from academia to industry is not an easy one. Michael Li documents the mindset shift required in one of his recent blog posts, and frames this shift within the context of delivering business-impactful results quickly (in industry) compared to delivering perfect results (in academia).\n\nWhile we agree with this basic trade-off, at Airbnb we feel that the mindset shift is slightly more nuanced. In this post, we first discuss the skills (both hard and soft) we look for in candidates hoping to transition from academia to Airbnb, followed by specific pieces of advice for those looking to move into the fast-paced startup world of Silicon Valley.\n\nData Science is very much an overloaded term these days for all things data-related at technology and startup companies. It sits at the intersection of Mathematics/Statistics, business domain knowledge, and \u2018hacking\u2019. Data Scientists are asked to extract insights from data to drive a company\u2019s metrics. At Airbnb this can mean munging data to inform which experiment to launch next or building a machine learning model to optimize our user experience. At Airbnb, when considering candidates coming out of advanced courses at graduate school, in addition to technical attributes and alignment with our core values, we think about 4 attributes:\n\nWith your advanced level of education, we expect that you are likely top of your field and very successful at everything you have touched so far academically. However, academic success and experience does not necessarily translate to industry success. We hope candidates are level headed and mindful of what they do not yet know about the business. Airbnb has a strong culture of mentorship and personal growth \u2014 getting here is not the end of the journey, there is still a lot to learn. We look for people who are eager to learn more and have an open mind to expand their skill set outside of their area of expertise.\n\nWe expect that a PhD candidate has learnt the art of self management. More than anything, graduate school should teach a student how to direct and prioritize their own learning. A senior researcher typically learns how to see a dead end approaching earlier, and quickly pivots their energy into a likely more fruitful direction. Research in a competitive field also provides opportunity to challenge peers and push back on assumptions. In Data Science at Airbnb we expect this to translate into not accepting the status quo but pushing the boundaries of our assumptions.\n\nSometimes we see senior academics underperform in their communication. Airbnb is a very collaborative environment with a Data Scientist typically working with other Data Scientists but also with Engineers, Designers, Product Managers, and non-technical people. Being good with data is important, but at Airbnb we need the insights to be well translated to all audiences \u2014 from a Data Scientist on your team to the CEO \u2014 otherwise the recommendations might not have the impact they merit. In both written and verbal communication, articulation of insights, methods, and assumptions must be crisp, convincing, and sympathetic to the audience.\n\nIt can take years to publish an article in academia but in industry the turnaround is much faster. That\u2019s not to say the quality is poorer, it\u2019s just that at Airbnb we expect to get a first version of a data product out as soon as possible and then continue to iterate where potential improvement is likely. Throughout our Data Science interview process at Airbnb we are looking for entrepreneurial spirit and candidates that can get past needing a perfect solution before shipping a data product or sharing an insight.\n\nAn academic\u2019s personal experience when transitioning away from the ivory tower will vary significantly based on the field they are coming from. Given the tighter coupling in subject matter between industry and academia these days (especially within fields like computer science and applied economics), the lag between a research idea and integration into an end product is often on the order of months now as opposed to years. Thus, those who make the leap often possess the \u201chard skills\u201d required for the job, such as strong programming and scientific computing knowledge. However, it is often harder to learn the softer skills and adapt one\u2019s mindset towards industry. We break down the mindset shift into 4 broad areas:\n\nAcademic research does an excellent job of abstracting the core problem from the messy details that often surround that problem in the real world. Sometimes we are provided sets of training data that are nicely curated and cleaned, and evaluation is performed on a well-documented and benchmarked test set that many others have evaluated. Some thought may go into additional data cleaning, but much of the information contained in the data, especially the training labels, are a given. Other times we may collect our own data but from a tightly controlled field or laboratory experiment where we can minimize data contamination.\n\nUnfortunately, this is not the case in industry. A lot more thought and creativity has to go into how to set up the problem in the first place. Are the labels that we extract or derive from logged data the signals that we need to solve our problem? Are there any bugs in the instrumentation? Are we even logging the information we need? Data Scientists need to understand the problem domain deeply and use that information to transform both the problem and the data to be able to produce something meaningful. Getting things to work in these fast-moving environments often requires as much, if not more creativity, as an abstract research problem.\n\nFollowing up on that note, in academia we are often focused on a fairly different regime of the problem. Given quality training and test data a priori, we are often tasked with investigating novel, state-of-the-art solutions to improve performance from an already strong level to something even stronger. On the flip side, the task in industry is to deploy a model where none has ever existed before. There is nothing to compare against, and translating intrinsic evaluation metrics (e.g., AUC) to business impact is challenging at best and perilous at worst.\n\nGiven these realities, it\u2019s advisable in these situations to deploy a model that isn\u2019t perfect, but is \u201c80% of the way there\u201d. Optimizing for the remaining 20% of performance oftentimes reflects not only a lack of prioritization (\u201cpremature optimization\u201d) but may also simply be not possible given the disconnect between intrinsic and extrinsic evaluation metrics.\n\nWhile Michael mentioned in his post that it\u2019s more important to deliver bottom-line impact than disseminate knowledge, at Airbnb we feel that the ultimate goal is to achieve a balance between these two extrema. Many of us were motivated to enter academia in the first place because we genuinely enjoy producing and disseminating knowledge, and within an industry setting there is significant value in effectively spreading these nuggets of knowledge and not have people reinvent the wheel. To this end, Airbnb has built the knowledge repository, which we recently open-sourced. The repository is our in-house peer-reviewed publications forum, and Data Scientists are encouraged to participate as actively as they can. We also have weekly seminars where Data Scientists or other leaders in the field can present their work, and mentorship is encouraged throughout the company.\n\nA successful researcher is one who knows not only the solutions to difficult problems, but also the right questions to ask. Asking the right questions requires a proactive attitude \u2014 instead of expecting to be handed a problem to work on, you need to identify the opportunities and craft a direction of inquiry accordingly. Proactive inquiry is a skill that is well-developed in academia, and is one of the reasons Airbnb encourages top academics to apply.\n\nTo rehash a well-known quote: \u201cit doesn\u2019t make sense to hire smart people and tell them what to do; we hire smart people so they can tell us what to do\u201d. We hire people from all kinds of academic backgrounds and qualifications precisely because these different vantage points provide a unique array of tools to tackle the kinds of interesting and challenging problems we face at Airbnb.\n\nIf you are interested in learning more about Data Science opportunities at Airbnb then you can visit our careers page here."
    },
    {
        "url": "https://medium.com/airbnb-engineering/unlocking-horizontal-scalability-in-our-web-serving-tier-d907449cdbcf",
        "title": "Unlocking Horizontal Scalability in Our Web Serving Tier",
        "text": "Airbnb\u2019s web application is powered by Ruby on Rails and Java. The web-facing application is a monolithic Rails application, which runs on many web servers. Web requests are handled by the Rails application and it talks to several different Java services, for instance the search service or listing pricing service. In Airbnb\u2019s technology stack, MySQL databases play the critical role of storing core business data. We partition databases by application for ease of capacity planning. For example, users\u2019 messaging threads and listings calendar management are separate from the core booking flow and they should be managed in their own databases. In 2015, we did a couple of database operations in the spirit of partitioning core databases by functionality (see this engineering blog post for how we did it). As the site traffic grew at an amazing rate every year, the infrastructure team responded by horizontally scaling the application servers tier for compute capacity and vertically partitioning databases for database headroom. Through the summer peak season of 2015, this 2-tier architecture had been working pretty nicely. However, one notable resource issue with MySQL databases had been the increasing number of database connections from application servers.\n\nWe use AWS\u2019s Relational Database Service (RDS) to run our MySQL instances. RDS uses the community edition of MySQL server, which employs a one-thread-per-connection model of connection management. The threads model in MySQL server is likely to hit the famous C10K problem. The C10K problem is that there is an upper bound in the number of connections that MySQL server can accept and serve without dramatically increasing the number of threads running, which severely degrades MySQL server performance. In MySQL, the Threads_running counter measures the number of queries that are executing concurrently. However, due to limited InnoDB storage engine thread concurrency, a spike in this metric really means client queries are piling up in MySQL database. When it happens, MySQL query latency increases, requests queue up across the entire stack and error rate spikes.\n\nIn production, we have had several severe database incidents that manifested in a large spike in active running MySQL server threads. The graphs above illustrate the Threads_running spike and the simultaneous applications error rate spike. While the root cause could be attributed to a poorly written query or underlying storage system outage, it usually would take a long time for MySQL database servers to recover. Often time, engineers had to resort to manually killing connections as a stabilization trick.\n\nAs bad as the threads running spike was, the most pressing problem was not even fire fighting database incidents when they brought site downtime. It was the database connections limitation. Application servers that run the web-facing application had direct connections to core RDS databases. MySQL server allocates thread stack and other resources for each client connection. Although thread stack size could be tuned to allow handling more client connections, it is a limited resource. A large number of threads would cause scheduling and context switch issue too. When a RDS MySQL server hits the resource limitation, clients will have a problem in creating connections. The database connection limitation puts a cap on the application server capacity that we can have to handle growing traffic. At the end of summer 2015, with the foresight of hitting a scaling bottleneck for 2016 summer traffic, the engineers on the infrastructure scalability team started to look into viable solutions.\n\nTo be fair, MySQL has a dynamic thread pool feature, however, it is only available in MySQL enterprise edition. Percona server for MySQL and MariaDB have similar offering too. Since Airbnb uses AWS MySQL RDS, we do not have access to MySQL thread pool feature. We had to look at an external proxy to address the connections limitation. We investigated several different open source technologies, and we chose MariaDB MaxScale. MariaDB MaxScale is a MySQL database proxy that supports intelligent query routing in between client applications and a set of backend MySQL servers. However, MaxScale did not solve the connection limitation problem because it would require establishing one backend MySQL server connection for each client connection. Since what we were looking for was connection pooling, we decided to fork MariaDB MaxScale and implement that ourselves.\n\nIn Airbnb MaxScale, connection pooling was implemented by multiplexing N client connections over M connections to a backend MySQL server. After a client connection request completes successful authentication with a backend MySQL server, the proxy severs the link between the backend connection and client connection and parks it in a connection pool of the backend server. The server connection pool size is configurable, and is typically a small number. Since we forked MariaDB MaxScale 1.3 developer branch, we were able to leverage the persistent connections feature to implement the server connection pool. When receiving a query on a client connection, MaxScale picks a backend connection in the pool, links it with the client connection, and forwards the query to the backend MySQL server. MaxScale understands the transaction context of a client session and therefore it knows to keep the linked backend connection until transaction commits. The link must be kept and used for forwarding the query result back to the client.\n\nOne challenge in this connection pooling implementation is knowing when to unlink and return the backend connection to the pool. A query response consists of one or more MySQL packets. Because MaxScale keeps one-to-one link between a client connection and a backend connection, it just forwards response packets as they come. In connection pooling mode, unlinking a backend connection prematurely would cause the client to wait indefinitely for the complete set of MySQL packets. For correct MySQL query response forwarding, we implemented MySQL packets by following the MySQL client server protocol for COM_QUERY_RESPONSE. That way, Airbnb MaxScale does not unlink a backend connection until it has seen complete MySQL packets of a query response. Aside from forwarding response, it allows us to measure query response size for monitoring.\n\nThe typical server connection pool size is configured to 10 in production. With many instances of Airbnb MaxScale proxy servers, the number of database connections on a MySQL server is several hundreds. In a normal situation, only a small portion of connections are active in use. When an underlying storage outage happens or a bad expensive query hits the fan, query execution becomes slow and it will be noticeable that the server connection pool runs out on each MaxScale proxy server instance. We take the symptom as a signal that the backend MySQL server may run into the concurrent threads running spike problem, and proactively throttle client requests by killing client connections. In production, request throttling has been proven very useful in preventing database incidents due to transient storage system outage.\n\nMaxScale uses an embedded MySQL parser for query routing. It actually builds a parse tree for every MySQL query in its query classifier module. We leveraged the query parse tree for bad query blocklisting in Airbnb MaxScale. The motivation for this feature was to protect us from Ruby VM heap memory corruption. Memory corruption can cause MySQL query statements generated by Rails ActiveRecord to become corrupted in such a way that its conditional predicate can be completely removed. This blog post has a detailed explanation of the nasty Ruby heap corruption problem.\n\nThe MySQL parse tree makes it easy to inspect the predicate list of a MySQL query. The query blocklisting feature leverages the MySQL parse tree to look for existence of malformed predicate conditions in update and delete statements and reject such statements. For more protective coverage, we block MySQL update and delete statements without any predicate condition as well. We deployed Airbnb MaxScale with query blocklist feature and it has protected us from at least one instance of scary corrupted query that could have caused damage to one of our core database tables.\n\nMariaDB MaxScale supports a multiple worker threads model. For practical reasons, we chose to use a single worker thread in an Airbnb MaxScale proxy server, and we deploy many instances to achieve concurrency. At Airbnb, we use SmartStack for service discovery. We deploy a cluster of Airbnb MaxScale servers as a database proxy service for each core production MySQL database. Applications discover and connect to the database proxy service instead of the MySQL database. With the Airbnb MaxScale database proxy service in between application servers and MySQL servers, the web application tier can be scaled horizontally per capacity demand. As a new tier in the core architecture, a database proxy service can be scaled horizontally as well, by launching new Airbnb MaxScale proxy server instances.\n\nThe diagram illustrates the 3-tier architecture in which different database proxy services are deployed in front of different core MySQL databases.\n\nThe Airbnb MaxScale database proxy introduces an additional network hop. The computation in the MaxScale proxy is lightweight in that it does request routing and response forwarding only. The connection pooling feature that we added in Airbnb MaxScale is straightforward and does not add overhead. The concern of possible latency impact by the extra network hop led us to implement availability zone aware request routing in SmartStack. SmartStack used to route requests to backend servers in random availability zones (AZ). The AZ aware routing allows application servers to send requests to database proxy servers that are in the same availability zone. We did extensive stress testing using our database workload replay framework and we found that the latency concern was really minimal, if not negligible. By the time we were ready to launch, we had high confidence in Airbnb MaxScale.\n\nEarlier in 2016, we deployed Airbnb MaxScale into production. In a smooth operation, we switched the monolithic Rails application connections from direct connections to MySQL servers to the database proxy service tier. MaxScale is MySQL protocol compliant. Switching connections to Airbnb MaxScale didn\u2019t require any client application changes. The following graph shows a drastic drop in the number of database connections on one core database when we performed the operation.\n\nCorrespondingly, the connections from the application shifted to the new Airbnb MaxScale database proxy service instead. At the time when we deployed the new database proxy, some of our MySQL databases were at the verge of database connection limitation. So, it made it into production right at the critical time.\n\nWith the connection pooling database proxy, we were able to scale the application server tier with the addition of more servers without an increase in MySQL server threads. We went into the summer of 2016 with peace in mind that we would have the service capacity required for handling yet another record traffic at the peak of the summer.\n\nToday, we have more than 15 Airbnb MaxScale database proxy services in production, each for a different core MySQL database and with different provisioned capacity. Many hundreds MaxScale server instances are churning requests in between web application servers and MySQL databases. The automatic request throttling has effectively worked as a back pressure mechanism, and it essentially replaces engineers killing connections manually. It has been running reliably and has not had one instance of crash in production.\n\nAt Airbnb engineering we believe in open source strongly. So far, we have been having great production experience with Airbnb MaxScale. We\u2019d like to share it with the community and therefore we announce that we have open sourced Airbnb MaxScale. The complete source and documentation can be found on github. Please try it out, and feel free to share your comments and pull requests with us. If you find working on this kind of problems interesting, we are hiring talented infra engineers, and please come work with us."
    },
    {
        "url": "https://medium.com/airbnb-engineering/announcing-codepath-partnership-89afb8e198f7",
        "title": "Announcing Codepath partnership \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "We are excited to announce a new partnership with Codepath \u2014 an organization that provides accelerated mobile engineering classes for professional developers and designers \u2014 to offer our first all-expenses paid Android and iOS mobile development course to 50 external engineers and Airbnb Connect participants.\n\nAirbnb\u2019s engineering team has been involved in some of the most exciting developments in the hospitality industry. With more than two and a half million listings and a host community that spans 191 countries around the world, Airbnb is truly a global company. We have some incredibly exciting projects in the works and our mobile team will continue to expand and grow as we scale the Airbnb platform.\n\nFinding mobile developers in today\u2019s competitive market is a tough challenge. We want to get to the heart of the issue so we\u2019ve partnered with Codepath to help train the next generation of mobile engineers. Through our partnership, we\u2019ll be able to not only increase the size of the mobile developer market but also open up a broader, more diverse pool of qualified engineers to hire from, which is something we care deeply about at Airbnb. Codepath is known for finding the highest quality engineers through a gender and color blind selection process.\n\nStarting in October, the eight week course will meet twice a week and will include lectures, labs, app assignments, and group projects that will enable students to develop their skills in Android or iOS development. The course will provide students with the relevant skills for today\u2019s workforce, and particularly, for Airbnb. Students will interact with Airbnb engineers throughout the course, and we\u2019ll offer top performers the opportunity to interview for a full-time role within Airbnb.\n\nWe have first-hand experience with Codepath\u2019s success; our ongoing partnership to onboard internal developers to the mobile ecosystem has been tremendous. Our internal onboarding program is a four week, full time accelerated course that teaches the fundamentals of mobile development and architecture. With this new knowledge, our developers can contribute to our world-class iOS and Android applications, helping people discover and explore destinations around the world.\n\nIf you\u2019re a software engineer with one or more years of work experience who wants to learn how to develop on the mobile platform, you can start the application process here: http://www.codepath.com/courses/airbnb"
    },
    {
        "url": "https://medium.com/airbnb-engineering/introducing-the-airbnb-imessage-app-806f48d303a8",
        "title": "Introducing the Airbnb iMessage App \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "If you plan a trip today using our mobile app and iMessage, your experience will most likely look like this. You find a beautiful place to stay:\n\nToday we are incredibly excited to introduce a brand new Airbnb iMessage app for iOS 10! We believe it will greatly enhance the way you plan your trip with friends and family.\n\nA key decision in this process was determining which listings to present. Conveniently, Airbnb already has two great ways to organize listings: Wish Lists and recently viewed listings. We leveraged existing resources to build these into the iMessage app.\n\nWe started out looking to define the grand vision of planning your Airbnb trip within Messages \u2014 what would that look like on this new platform? Over the course of just a few weeks before the iOS 10 release, our team rapidly prototyped on a variety of ideas. We started with countless drawings and mockups of possible features, and as we became more familiar with the possibilities and limitations of the Messages Framework, a product began to form. Once we had something we could use, a group iMessage thread between engineers, designers, and product managers allowed us to quickly discover the features we wanted \u2014 a rather meta approach. At this point, we had an iMessage app where people could share homes between friends and view key details about the accommodation.\n\nAs you can see, we\u2019ve completely lost the appeal of the original home after sharing it, since it\u2019s now just plain text. The transition from Airbnb to iMessage is lacking visual richness, in a time where an appealing presentation of data is of great value. Even though we have no control over how the listing is presented in the conversation thread, it directly affects how guests perceive our app, and the Airbnb experience as a whole.\n\nBy using a convenient tab-based UI, you can switch between both while remaining in the collapsed app view. When it came to presenting listing information, we knew most guests only needed to see a few key features to help shape their decision of which home to book. The new app presents price, photos, location, host information, and house details, all in a concise user interface, so that collaboration remains the focus.\n\nSpeaking of collaboration, an iMessage app wouldn\u2019t be complete without an additional social feature: voting. When sharing a message with friends, each collaborator can vote on homes they like and see who else has voted on them. This gives the trip organizer a bird\u2019s-eye indication of the most popular listing, greatly simplifying the decision of which one to book.\n\nApple is fully committed to privacy, and so are we. It\u2019s something that everyone wants, but it also raises certain engineering challenges. The main implication as applied to iMessage apps is the inability to acquire any contact information of conversation participants at all. We could only get the participants\u2019 UUIDs, which can then be used as placeholder labels, that iOS replaces with user names inside the conversation thread. However, this is not ideal for us as we would like to know a little bit more information about participants to be able to access their recently viewed listings and Wish Lists, among other things.\n\nIt turns out we can solve the problem by requiring everyone to be logged in to the main Airbnb app. By creating a shared App Group, we can use the keychain to share data between the main app and its iMessage counterpart. Once they are logged in, we can fetch all the required information in the extension and use it to personalize the UI, and make network calls.\n\nA small note on testing. When we started using the shared keychain to personalize the experience, we noticed we could no longer use the iOS Simulator to test group conversations. This is because, even though you can impersonate the current participant either as the Simulator default Kate Bell or John Appleseed, they still use the same instance of the keychain and therefore are considered to be the same runtime user. To work around this limitation, we had to always test on device.\n\nHuman race conditions happen when multiple users interact with the same message at the same time. Consider the following example: Jie shares a listing with Michelle and Noah. They both open the interactive message, vote, and send a reply back. If Michelle\u2019s message arrives last, it will overwrite Noah\u2019s message. Additionally, as all the messages from the same session are collapsed into a single bubble, one has no way of accessing previous replies, resulting in data loss.\n\nWhy does this happen? Well, the Messages Framework conveniently lets you attach a URL to an MSMessage instance. This enables us to add URL parameters and share state in a single message session among all participants. If multiple replies are composed in parallel, only one of them will land in the conversation. Hence, every other message will be overwritten and information in them will unfortunately be lost.\n\nFrom a participant\u2019s perspective, the message can be in 3 states \u2014 unopened, opened in edit mode, and staged (i.e. waiting for them to hit \u201cSend\u201d). We don\u2019t have to do anything about the first case, but we can be smart about the other two.\n\nWhen you interact with the message by tapping on its bubble in the conversation thread, it becomes a selected message in the active conversation. MSMessagesAppViewController requests the transition to the expanded presentation style by invoking the method on its delegate:\n\nAt this point, the iMessage app displays an expanded view of the currently selected message. In this state, a new message for the same session may arrive, likely containing updated data in the URL. MSMessagesAppViewController then notifies the app by calling:\n\nThis updates the current presentation with the new incoming data. This approach solves the problem of overriding valid data by keeping the current message up to date with incoming replies before it is sent.\n\nThe third use case arises when the message is staged in the input box and is ready to be sent. While the message is staged, a new message may arrive which will again trigger the didReceiveMessage callback. The recommended approach would be to read the URL of the incoming message, merge its data with the current message\u2019s URL, then re-stage the current message. Re-staging is achieved by simply inserting the message into the current conversation thread a second time; the insert method on MSConversation will replace the old message in the input box.\n\nWhile the two approaches above will greatly assist in keeping data safe, they don\u2019t mitigate the case when two or more messages are transmitted at the same time. It is extremely hard to prevent data loss here, since they are now outside of the message\u2019s lifecycle.\n\nUsing server-side resources to persist data is the most reliable way to maintain data integrity, and is also the suggested approach by Apple. We started out with URLs and eventually transitioned to server-side resources. Message URLs are treated like immutable tokens and so once created, don\u2019t change during the entire session. This makes them inherently great to be used as unique IDs of their counterpart server-side resource, on which we save message state. All clients write data to the same resource, and every time a message is opened, they re-fetch the data and update the UI.\n\nThere is a delicate balance to maintain when persisting message state on the server. As previously mentioned, MSConversation allows new messages to be inserted into the input field using the method:\n\nIt does not, however, send the message. A participant must explicitly send the message to actually commit that action. In our case, they need to do that to send a vote.\n\nApple doesn\u2019t expect data to be persisted on the server until the message is actually staged and sent. However, this poses the risk of data loss as there\u2019s no guarantee a network request to store data would be successful. After some brainstorming, we decided it would be more reliable to persist state data before the message is staged and sent, making sure that remote data resources are always up to date.\n\nThose not on iOS 10 will unfortunately not be able to enjoy this rich messaging experience. When the iMessage app sends a message, unsupported devices will receive two separate messages \u2014 an image and a URL. We still try to provide all participants with the best Airbnb experience possible, by forming URLs in such a way that, when opened in the browser, we know they\u2019re part of an iMessage conversation. For this specific scenario we display listing details as usual, but additionally also allow voting on it!\n\nConsequently, voting data is made available back to the users in the iOS 10 conversation thread. By generating push notifications server-side to notify iOS 10 users that new data is available, we maintain the real-time nature of the conversation flow for everyone.\n\nBefore leaving you to try out the app yourself, we\u2019d like to address one more challenge we faced: app discoverability. Hopefully by reading this article, you now are fully aware of the new capabilities we are introducing to the Airbnb app, but not everyone will be! We want to make it extremely easy for everyone to see that this new iMessage app is available. However, there are no APIs to easily deep link from the parent app into the iMessage settings screen to enable the corresponding app extension. One has to first open Messages, tap to open the app drawer, go to the iMessage App Store, select the \u201cManage\u201d tab, then enable the app.\n\nWe attempted to mitigate this lack of discoverability by introducing an educational prompt right after sharing a listing \u2014 the most common action in the main app that relates to sending messages. Our hope is that this promotes the iMessage app right when it is needed the most!"
    },
    {
        "url": "https://medium.com/airbnb-engineering/scaling-airbnbs-payment-platform-43ebfc99b324",
        "title": "Scaling Airbnb\u2019s Payment Platform \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "When people think about Airbnb, they usually think of beautiful travel destinations, unique homes, and living like a local. Most people don\u2019t think of Airbnb as a Payments company. However, similar to the manner in which PayPal was critical to eBay\u2019s success as the first global online marketplace, payments at Airbnb is critical to Airbnb\u2019s growth as a global platform for travel.\n\nIn this post we\u2019ll explore the origins of Payments at Airbnb and the reasons why we\u2019ve built such a large in-house payments operation. We\u2019ll also take a look at some of the technology that has helped Airbnb scale to 191 countries, over 70 currencies, and a network of over two dozen payment routes.\n\nIn 2008, when Airbnb started out, guests and hosts used the platform to find each other and not much else. Many of the trust features that Airbnb provides today, such as real identities, messaging between guests and hosts, and professional photography didn\u2019t exist. Payments were handled offline directly between guest and host. As you can imagine, this didn\u2019t always go smoothly.\n\nCase in point: in early 2008, co-founder Brian Chesky traveled to Austin to attend SXSW, and naturally stayed in an Airbnb. When he arrived at his listing, Brian didn\u2019t have any cash on him but he assured his host that he\u2019d stop by an ATM that day while attending the event. However, in the excitement of SXSW he forgot about his plan, and returned home empty handed. The next morning, his host asked about the money and Brian had to again promise (sheepishly) to withdraw cash during the day. That evening though, it had once again slipped his mind. Things were starting to get pretty awkward between the two of them.\n\nWhile Brian did eventually manage to get him the money, he realized there had to be a better way. Back in San Francisco at Airbnb HQ (aka the living room of his apartment), Brian declared that Airbnb had to start handling payments itself. This would eliminate the inconvenience and awkwardness of an in-person cash exchange and would also allow Airbnb to support both the guest and the host in case anything went wrong. With that, the beginning of Airbnb\u2019s payments infrastructure was born.\n\nAs the sole engineer at the time, Brian\u2019s technical co-founder, Nate, was tasked with making payments happen on the platform. At the time, there weren\u2019t many off-the-shelf solutions for handling bi-directional transfers, so he implemented the most straightforward solution available: guests paid via PayPal and hosts received a paper check in the mail.\n\nThe platform grew, and users began asking for an alternative to PayPal so Nate reached out to processors to set up partnerships that would enable Airbnb to accept credit cards. Additionally, Brian got tired of writing checks by hand, so Nate added support for automated bank transfers.\n\nAs usage spread outside the US, the limitations of this system became clear. Guests in Europe looking to book listings in the US would be charged in dollars and ended up paying an additional conversion fee. In some countries such as Germany, credit cards weren\u2019t commonplace, and German guests couldn\u2019t pay with their most trusted payment method. For hosts outside the US, receiving US dollars in a PayPal account wasn\u2019t always convenient (or even possible), and US bank transfers didn\u2019t work. This led the founders to their next big insight around payments \u2014 to be a truly global travel company, they had to build a global payments operation.\n\nA good test came in the form of Brazil. Payment support in Brazil is extremely fragmented due to the presence of numerous competing payment networks, and most Brazilian credit cards do not work internationally due to government regulations. Many companies opt to skip support for local cards in the name of simplicity, but with our goal of creating belonging on the platform this definitely wasn\u2019t an option. We wanted to make sure to add support for local credit cards as part of our expansion in Brazil.\n\nIn addition, cash-based mechanisms are very popular for handling online payments in Brazil (reference: How Airbnb Made Its Payments System More Accessible To Brazilians For The Rio Olympics). In order to support cash-based payment, we ended up introducing a new payment method in the form of Boletos. In this model, a guest can opt to book a reservation using a Boleto, which is an invoice for the amount owed to Airbnb. The guest can take the Boleto to their local bank or convenience store and pay the amount owed in cash or from his/her bank account directly. Once payment is confirmed by Airbnb, the reservation proceeds from a pending payment status into a confirmed state. Introducing Boletos to our platform not only presented us with an opportunity to support new users in Brazil, but also allowed us to develop a general-purpose pattern for asynchronous payments and booking which is now leveraged to enable new payment methods in many other countries.\n\nAs we expanded internationally, local needs like this kept coming up and our early infrastructure wasn\u2019t always well-suited to the task of adding new processors and flows. Next, we\u2019ll take a look at the history of our payments infrastructure and how it has evolved over time to meet a growing set of business needs.\n\nIn the beginning, payment integrations were introduced in an ad hoc fashion with little thought for code reuse. Since there was no real framework for integrating processors, each new integration had to be built from scratch which was extremely time consuming. Additionally, each integration added to the complexity of the system and introduced the potential for new bugs and issues.\n\nBecause we were using Rails, we relied on simple ActiveRecord models to access the database and its data; thus our core payment objects were mutable by default. Due to the way our low-level data model was exposed, we lacked a clear mechanism to track these changes from within our application and came to rely on DB-level triggers on our core payment tables to capture CREATE/ UPDATE/ DELETE actions in separate audit trail tables. Downstream, we had to reverse-engineer these audit entries to infer what happened in the production system and use elaborate SQL queries to process this data and generate the various financial reports that we needed.\n\nThis rudimentary system worked well for a while, but over time the money moving through our platform grew dramatically. The first real growing pains we encountered came from our reporting pipeline. Over time, it became increasingly painful to maintain those sprawling SQL queries, and the sheer volume of records began pushing the limits of our reporting database. New reporting requirements and product changes became increasingly expensive to support, and the overall runtime of our reporting process grew to many hours.\n\nLet\u2019s review the biggest challenges we were facing at this point:\n\nTo address these challenges, we introduced three new standalone systems, which represent the core components of a next-generation payments platform.\n\nHistorically, Airbnb Payments has handled the basic use case very well. The guest books and pays, the reservation is confirmed, and the host is paid out \u2014 everyone is happy. Over time, as we started adding more complexity to the transactions, these simple assumptions broke down. For example, when a reservation was altered, recalculating the price, fees, and taxes quickly became overwhelmingly complicated. We had difficulty associating additional financial actions with a reservation, such as resolution payments, security deposits, professional photography costs, and coupons. The addition of taxes, such as VAT in Europe, further complicated things.\n\nOur new billing interface solves for all of these issues by taking each Airbnb product and treating each item (reservation nights, fees, taxes, etc) as discrete items. It supports a unified checkout flow, removes product details, and replaces them with abstract data models and attributes, such as pricing and taxes. By abstracting the interface for processing each product, we can easily integrate new products on our platform.\n\nThe checkout flow for various products becomes very similar: first we check product inventory, then we take the buyer through a standard checkout flow, and finally, we execute a fulfillment process. The final checkout result is represented using our normalized data model with elements such as fees, taxes, and payables all broken out. This data model serves as the source of truth for any subsequent changes to the purchase. Behind the scenes, the billing interface generates a series of platform events in a consistent schema which will be distributed to all interested consumers via our Kafka message bus.\n\nThis new billing interface allows us to easily create, edit, and remove products on the Airbnb platform. We\u2019re optimistic that with this investment in our infrastructure, we\u2019ll continue to increase trust in the Airbnb platform by increasing the transparency of each component of the transaction.\n\nIn order to address the idiosyncrasies of cash payments, we developed clear reservation statuses as well as a timer so both parties knew who needs to take action and how much time was remaining for that stage. We also implemented a general-purpose pattern for asynchronous payments and bookings, which is now being leveraged to enable new payment methods in many other countries. We designed our payments gateway to provide a unified API that supports bi-directional transactions across all our processors. All payment transactions pass through this API layer and are recorded using an append-only data model. This system produces payment events with a separate well-defined schema, which are again distributed via Kafka. By using this unified API, it allows us to reuse payment models as we add new local payment and payout methods around the world.\n\nBy investing in our infrastructure to handle different payment transaction patterns, we can increase global support for many different payment methods. Because transactional behaviors vary greatly based on regions, this will considerably help our mission to enable both our guests and hosts to exchange money with ease.\n\nOur financial data pipeline system has been the biggest evolution in our payments ecosystem. In the last year, we built our second generation financial data processing system. It is a Spark + Scala based financial reporting application. It is built around the concept of discrete financial events with well-defined schemas that map to the booking, payment, and supplementary events that occur on our platform. This serves as the complete replacement for our SQL-based reporting system. This new system has proven to be easier to debug, maintain, and extend than the legacy system, and is designed to scale horizontally.\n\nThe financial system tracks financial accounts (such as receivables, payables, revenues, and taxes), and how funds are transferred in between these accounts. It does not depend on product details such as how reservations block a calendar, how users are notified about activity, or any other business logic. The financial pipeline receives platform events from the Billing Interface and payment events from the Payment Gateway, then converts these events into accounting information. We then construct unified reporting logic based on discrete events and filter it into different accounts, such as receivables, liabilities, revenue, etc. The financial data pipeline is also constructed around the principle of double-entry bookkeeping, which means money must be added somewhere and removed from somewhere else. In other words, money can\u2019t be created out of thin air.\n\nThis new financial data pipeline now produces all of the data required for financial reporting to regulators, local governments, and even Airbnb\u2019s investors. In a future blog post, we\u2019ll dive a little deeper into this topic.\n\nWith these services complete, we have a solid foundation for our next-generation payments processing system. Looking further out, we can begin to build on top of this foundation by adding sophistication to our stack, leveraging new data flows, employing machine learning techniques, and overall, making the system more flexible and intelligent. Some of the things we\u2019re planning:\n\nPayments at Airbnb has come a long way since that first awkward interaction between Brian and his host. As Airbnb has expanded around the world to 191 countries, the decision made back then to own payments and keep it as a core part of the platform experience continues to be the right one. Without that ownership, we wouldn\u2019t be able to add the trust, belonging, and legitimacy that is required for a marketplace like Airbnb to work for so many millions of people.\n\nIf this sounds like an interesting challenge to you \u2014 and you\u2019ve made it all the way to the end of this post \u2014 we\u2019re hiring engineers, data scientists, and counselors."
    },
    {
        "url": "https://medium.com/airbnb-engineering/using-googlesheets-and-mailr-packages-in-r-to-automate-reporting-c09579e0377f",
        "title": "Using googlesheets and mailR packages in R to automate reporting",
        "text": "Most companies make extensive use of spreadsheets to store, work and share data internally.\n\nOne important use is for reporting purposes. If you are an analyst, you are often tasked with reporting sales, service level, or marketing campaign results on a monthly, weekly or even daily basis. Most decision-makers do not pull their own data and are used to seeing and working on this data in spreadsheets. For instance, at Airbnb, I share a weekly summary of our performance in a spreadsheet for my business stakeholders to be able to make data-driven decisions.\n\nFor years, Excel has been the main software that allowed people to share internal data. But using Excel for reporting has its drawbacks:\n\nWith its Google docs suite, Google has solved point 3) by giving anyone the ability to create online collaborative spreadsheets for teams to work on simultaneously, for colleagues to give comments and communicate with one another in one place without having to email different mark-ups back and forth.\n\nThis post is about providing solutions to points 1) and 2) to help you reduce the amount of time you spend on reporting.\n\nR allows for easy automation of the reporting process. This means that the analyst\u2019s task of pulling data, making some computations, uploading it in a nice spreadsheets and emailing it to relevant business stakeholders can be completely automated in a single script. And because with the same script and the same dataset, you will confidently obtain the same reproducible results you will not have to worry about making errors anymore.\n\nFirst, one of the most important feature of R is that it allows for reproducible results: with the same script and the same dataset, you will confidently obtain the same results and not have to worry about making errors. At Airbnb, for example, our Data Science team built an R package which allows employees to collaborate and share reproducible R code. More info about how Airbnb scales data science by using R packages here.\n\nSo whether you are currently running a daily reports showing sales or service level, this post can help you improve your productivity.\n\nBefore we get started, and assuming you have already installed R and Rstudio on your computer, let\u2019s install everything we need:\n\nIf you have never installed this package on your computer, install it by typing in the console:\n\nYou can now load the googlesheets package by typing:\n\nYou are now ready to work on your google spreadsheets in R!\n\nBefore installing mailR, you will need to install another package called rJava. In Rstudio:\n\nFor the purpose of this post, we will use this spreadsheet.\n\nTo Authorize googlesheets to view and manage your files, you first need to type:\n\nYou will be directed to a web browser, asked to sign in to your Google account, and to grant googlesheets permission to operate on your behalf with Google Sheets and Google Drive.\n\nNow, let\u2019s assume that you have been asked to create a daily report in a google spreadsheet. The first thing you need to do is to get some data. This may be via the web, via an API or a database. At Airbnb, our internal R package called Rbnb allows anyone in the company to quickly access our database without leaving the friendly environment of Rstudio.\n\nFor the purpose of this post we will be using the mtcars dataset and we will be pretending that the report you need to update daily shows the top 10 cars with the highest mpg.\n\nWe can select the top 10 cars in the following way:\n\nYou now have some data to upload to your spreadsheet. To do so you will first load your spreadsheet with either the gs_title() or the gs_url() function. It is probably a good practice to use gs_url(\u201curl_of_your_spreadsheet\u201d) in case the name of your spreadsheet changes over time.\n\nYou can then use gs_edit_cells() and specify the worksheet you want to update with the ws argument:\n\nDone! You can now run your script every single day and it will update your sheet faster than loading your browser!\n\nOnce your report is updated, you might want to send an email to the people looking at your dashboard. If you are running this script daily, opening your gmail and drafting an email seems like a waste of time.\n\nFirst go to this page to add a specific app password. This will generate a random password that you can just use for mailR, allowing you to save it in a script without revealing your actual credentials. You can also revoke this password at any time from your Google accounts settings page. Be careful, however \u2014 if this password is compromised, it can be used to fully access your account from anywhere! Consequently, make sure you don\u2019t share a script with your password in it with anyone else.\n\nThen add these lines at the end of your script and observe the magic:\n\nThere is one major limitation with the googlesheets package is that it relies on a Google API which is apparently slow (see this). As a result, uploading large datasets to your spreadsheet will not work in one chunk.\n\nOne fix I have found and been using is to batch my data and load it in the spreadsheet in chunks.\n\nYou can use this code to chunk your data:"
    },
    {
        "url": "https://medium.com/airbnb-engineering/how-the-world-planned-a-trip-to-rio-on-airbnb-232d42979a89",
        "title": "Rio August 2016 \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/airbnb-engineering/one-step-forward-in-data-protection-8071e2258d16",
        "title": "One Step Forward in Data Protection \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Encrypting sensitive data is an important security measure to help protect our users\u2019 data that presents unique technical challenges around key storage and management. We built an encryption service that we call Cipher to address those technical challenges and enable engineers at Airbnb to encrypt data easily and consistently across our infrastructure. Our goal with Cipher was to provide an easy to use, language agnostic interface to any service that has encryption requirements.\n\nCipher abstracts away all of the complexities that come with encryption, like algorithms, key bootstrapping, key distribution and rotation, access control, monitoring, etc. Service owners can focus on their service and rely on Cipher to take care of encryption. Cipher is only responsible for computation, but not storage of ciphertext. The encryption keys used in Cipher never leave the service, which is fundamentally different from many other encryption solutions (e.g. using native encryption libraries directly), where the key materials often coexist with the sensitive data.\n\nThere are a number of technical decisions that come with designing a secure encryption service. In the course of building Cipher, we have made a couple of important design choices that may benefit others who are looking to build something similar. We\u2019ll cover each of these in detail below:\n\nThere are several benefits to isolating computation from storage. (1) Simplicity: The Cipher service can be optimized to provide high availability and low latency without an additional hop to the storage layer for reading and writing encrypted data. As a result, additional complexity and maintenance requirements aren\u2019t drawn into the Cipher service. Cipher is designed to do one thing and do it really well; (2) Lower Security Risk: Since Cipher doesn\u2019t own storage, obtaining and decrypting a large set of data becomes more difficult. The attacker would have to compromise both the service that owns the data and the Cipher service in order to decrypt the data; (3) Flexibility: Leaving the storage decisions to the teams who own the data ensures that data treatment can be individualized. The encrypted data, along with other business data, is owned by different services. Some business data has different retention policies, some have strong requirements on transactional integrity, etc.\n\nCipher only grants access to authorized clients, and therefore it needs to authenticate each client for each request. Cipher leverages mutual TLS using x509 certificates because (1) It provides transport security so the data is encrypted over the wire; (2) It allows us to identify who the caller is via the client certificate; (3) TLS is widely adopted in industry and supported by many HTTP clients, which makes integration, testing and maintenance easier; (4) TLS performs reasonably well in terms of reliability and latency.\n\nWe built a customized authorization model in Cipher over TLS to enforce which client is authorized to perform which action on which data. All the data being encrypted are classified as resources, e.g. \u201cRESOURCE_FOO\u201d, \u201cRESOURCE_BAR\u201d etc. Cipher applies different encryption keys for different resources, and each key is rotated periodically. When a client wants to encrypt a new type of data, it pre-registers a new resource in Cipher with an appropriate access policy. The resource name is provided along with the data in both encryption and decryption API calls, so Cipher knows which keys to use, and whether the client is authorized to perform the requested action. The authorization model follows the \u201cno more, no less\u201d principle, i.e. the access is granted only by specification. For example, you can configure a resource to only be encrypted by client \u2018foo\u2019, but decryption can only occur on client \u2018bar\u2019, using the following code snippets in Cipher:\n\nAirbnb runs most services on Amazon Web Services (AWS), so we were well positioned to leverage Amazon\u2019s Key Management Service (KMS) instead of using a customized Hardware Security Module (HSM). As this document details, AWS KMS uses Hardware Security Module (HSMs) to protect the security of the keys. On provisioning a new resource, Cipher creates a random secret, which is versioned and rotated periodically in an automatic manner. The secrets are used to encrypt and decrypt the client resources, and they never leave Cipher. The secrets are encrypted by a versioned master key and stored in a database for persistence. Instead of relying on some security hardware to protect the master keys, we encrypt the master keys using the AWS KMS in multiple regions for availability and disaster recovery. The AWS KMS makes our system much simpler and easier for operation and maintenance.\n\nAll the encryption and decryption actions go through Cipher. As a result, it\u2019s easy to lock down to minimize the risk of exposure. Since Cipher handles all encryption and decryption requests, we can easily audit the usage of keys and log who/what/where/when/why for each resource accessed.\n\nThe architecture of Cipher is pretty straightforward, as illustrated below. Cipher bootstraps the resource encryption keys by reading the encrypted copies from a dedicated database. It then decrypts those keys using AWS\u2019 KMS and stores them in volatile memory.\n\nAs you can see above, Cipher receives traffic from different clients over mutual TLS, extracts the identity from the client certificate for each individual call, checks the authorization model and proceeds only if the client is permitted to perform the requested action.\n\nAs previously mentioned, Cipher hides all the complexities in terms of computation, access control, bootstrapping, algorithm/key evolvement and secret rotation by providing simplified client libraries for languages that are supported in Airbnb.\n\nEfficiently encrypting data presents unique technical challenges. But investing in encryption can help defend against an attacker who is trying to exfiltrate large amounts of user data. Cipher attempts to make hardened encryption universally available to all engineering teams at Airbnb, so that we can more easily protect our sensitive data."
    },
    {
        "url": "https://medium.com/airbnb-engineering/how-airbnb-manages-to-monitor-customer-issues-at-scale-b883301ca461",
        "title": "How Airbnb manages to monitor customer issues at scale",
        "text": "How do you discover new, growing problems from your customers right now? For a typical person, this seems like a pretty straightforward question that every company should be able to answer. However, we found that it wasn\u2019t so simple. Of course, we had the data of customer issues stored in our database but we didn\u2019t have a great way to answer the question. We didn\u2019t have a way to bubble up trending problems that were starting and we lacked the ability to see this in real time. Most of the trends we discovered were found on an ad-hoc basis which meant that by the time we discovered a trend, it was too late to act on fixing it. So with that in mind, we began to devise a solution. To provide some background, since Airbnb began, we\u2019ve handled 80 million guest arrivals and we\u2019re quickly growing. With our rapid growth, our engineering team is finding ways to tackle the new and challenging problems that arise. A large part of Airbnb\u2019s operation relies on having customer service agents to handle the high volume of incoming questions from our hosts and guests \u2014 check out this previous post by Emre Ozdemir. One of our challenges is to understand this large volume of tickets and detect trends or unexpected problems as they occur in real time. We need a way to monitor and alert whenever we see an increase in folks calling us about a certain issue.\n\nWe built a web-based service that computes trends in all our tickets and visually displays the top trends at any given time in history. It considers several different attributes about a ticket such as issue type, browser version, user country, subject line, source, and more. With this data, it analyzes a time series of each attribute across all tickets and ranks them using an algorithm to detect spikes or trends. We actually run two different algorithms side-by-side to make improvements while still having a previous baseline to compare against. How we made it work The infrastructure required to make this scale involved a few different pieces. First we had the data store where the tickets were going to be stored and queried quickly for our data crunching. We ran Elasticsearch to handle this. Next we needed to run a web service that could take in requests of new ticket data. This become our Node.js app that served our web app and processing of incoming tickets. We also ran a separate job instance that was constantly computing ticket trends. It would query the Elasticsearch instance for ticket data and store the results into our Redis instance. Redis maintained a cache of ticket trend results for our Node.js app to render on the web. The entire front-end was built in React, making it easy to develop a rich UI to display the data. We decided to stream all tickets into an Elasticsearch cluster in real-time, as they are created. This data is then consumed by our batch jobs that run on regular intervals to compute ticket trends. Having the tickets stored in Elasticsearch makes it easy to scale and perform aggregate queries on our data set. The tickets\u2019 fields were indexed depending on the type of data stored.\n\nThe role of our job worker is to query our data store and compute a trend score for each set of tickets. We did this on a separate instance from our web workers because we didn\u2019t want long processing to delay the latency of serving incoming API requests of new ticket data. The resulting trend data is then sent to Redis, which allows for our other web instances to fetch that data. Trend detection was core to making this all work. We start by running a multi-search query into Elasticsearch to get a time-series with ticket count for every ticket attribute that we wish to consider for trends, ending at the time we wished to consider. We then apply our scoring model to each time-series, sort the results, and return all trending attributes above a minimum threshold. These results become the trends for that time period. The scoring model\u2019s job is to adjust for periodicity (e.g. daily fluctuations in counts), remove noise and smooth the graph, and then decide if there is a spike. In order to smooth and adjust for periodic trends, we transform the graph into the frequency domain using a Fourier transform, find the peak frequency, and discard all other frequencies except for those in a close band to the peak. This produces a smoothed graph (shown in input-ifft) of the periodic component of our ticket counts. By subtracting this from our original graph, we can get an estimate of how much our graph deviates from its expected value. The final score is determined by looking at a few factors in the final adjusted graph, like a change in max and total ticket volume over time. After computing all the scores for each ticket field for each hour, we then sorted the stored top scores into Redis for consumption by the frontend.\n\nOnce our computed data is in Redis, it became very fast for us to display it. We added the ability to look at past trends for any point of time and also to look at the most trending items over a longer range of time. Our dashboard has already had an impact in detecting issues quickly since it has been released. As an example, we noticed a spike in users reporting they could not see their listing in search. New users often have these sorts of issues when first starting on the platform, so individually our customer service agents didn\u2019t think much of it. Additionally, since it wasn\u2019t a full failure where no listings were being returned (but rather a subtle edge case), it wasn\u2019t immediate to the engineering team that there was an issue. However, because we saw this was a ticket spike, we were confident that an issue existed and needed fixing. Our engineers were quick to fix the problem, and after a while we observed the corresponding drop in tickets from our tool, assuring us that the issue was actually fixed. Without this dashboard, this incident could have lasted days or even weeks, causing higher strain for our agents and user frustration.\n\nWe\u2019ve been using this dashboard at Airbnb for over six months and have uncovered things that would have been much harder to discover otherwise. We\u2019ve been able to catch many spikes, ranging from subtle bugs to small problems with the potential to get big. Our ticket dashboard doesn\u2019t replace our existing monitoring systems for outages and system errors, but is used as the catch-all of issues. This new ticket monitoring system has proven to be an invaluable tool that we believe every large company should have. It has reduced our users\u2019 frustration by prioritizing the most pressing issues to fix immediately. We estimate that this ticket dashboard has been able to reduce our overall ticket volume by 3%. Sometimes it is crazy to see how a two-person hackathon project ends up saving Airbnb customers a ton of time and get things back on track quickly. If you have any questions or comments, leave them in the space below."
    },
    {
        "url": "https://medium.com/airbnb-engineering/internship-highlight-review-upgrades-2820c9cea56a",
        "title": "Internship Highlight: Review Upgrades \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Hello! We are 4 engineering interns who joined Airbnb during summer of 2015 to work on the Search Experience team. The Search team is responsible for everything related to finding and booking a home on Airbnb. Over the course of the summer we have worked on a variety of projects for the team, but our final project was focused on improving how guests digest and interact with reviews. It has been an incredible opportunity, and we\u2019ve helped work on the designs, defined OKRs, and been given significant implementation tasks.\n\nThe four of us are: Maya Ebsworth (UPenn, B.S. Computer Science 2016), Keziah Plattner (Stanford, B.S./M.S. Computer Science 2016), Iain Nash (USC, B.S. Computer Science 2016), and Nicholas Moschopoulos (UC Berkeley, B.S. Computer Science 2016).\n\nReviews set the foundation of trust on Airbnb. Our data scientists have done research suggesting that reviews are among the most important features guests look at when deciding whether or not to stay at a certain home. Guests rely on reviews to make decisions and set expectations, but for homes with many reviews it becomes unmanageable to read through all those reviews and find the relevant information (our most popular home currently has 700 reviews). Because of this, reviews are not as helpful to guests as they could be. We believed that making reviews more accessible would help guests decide which home to book.\n\nWe identified that there were several ways to make reviews more useful:\n\nFor listings with a large number of reviews, guests need to manually sift through page by page in order to find what they are looking for. For example, a traveler going to San Francisco may want to see whether it was easy to find parking near the home by reading reviews that mention \u201cparking\u201d. This becomes more and more difficult as the the number of reviews is increasing exponentially. Our solution was to allow guests to search through all reviews for a given home.\n\nWe use Elasticsearch for the review search backend, returning relevant review ids to our main Rails application. The Rails application fetches the full review data and associated objects required to render the reviews. We implemented a backend service to index and return results from Elasticsearch. The initial indexing step is done by loading the reviews database export from S3 and running a batch import to populate the Elasticsearch database.\n\nIn order to support updates, we needed the service to receive updates every time a review is inserted, deleted, and updated. Elasticsearch is designed to be near real-time search, so in order to make a review visible in the search service, all we have to do is add it to the index whenever a database update is made available. We used two internal services to support real-time updates: a pub-sub service for statically-typed event messages and a service that produces these events to be consumed. We set up the pipeline for the reviews search service to receive these updates, and updated Elasticsearch as needed.\n\nThe aim of this experiment was to allow guests to see a short snippet of helpful reviews for a given home. In order to simplify the project, we decided to create a preliminary filter to identify sentences that were of high enough quality to display to guests. The initial approach was to filter using keywords that frequently appear in guest reviews. The set of keywords can easily be updated in the future.\n\nWe then applied a sentence scoring function on the filtered sentences to rank results. The sentence scoring function is where a lot of the magic is, and where a lot of the experimentation took place. We tried a variety of techniques including a sum of TF-IDF weights on the words in the sentence, a count of unigrams in the top 500 unigrams over the whole review corpus, and even just looking at the sentence length. We\u2019ll be experimenting with launching variations of review highlights as the Search team improves clarity on the listing detail page.\n\nIt\u2019s been awesome experience working on this project. I was given a lot of freedom to decide what to try out, and how to determine what works best. It also gave me, as an intern, exposure to and responsibility for a project end-to-end, starting with defining the scope and working through the architecture. Throughout the summer I was given a lot of help and mentorship, but it definitely let me see what it would be like to work as a full-time employee. Thanks Lu!\n\nFor our third experiment, we wanted to see if we could help guests identify helpful reviews, and improve the sort order of reviews. We thought adding the ability to vote for reviews would allow guests identify reviews that they found helpful, and displaying the number of votes would allow other users to see what has helped other guests in the past. And once enough votes are collected, this would allow us to sort reviews based on their helpfulness value.\n\nThe backend for this feature is implemented with a general backend service framework developed in-house at Airbnb. This framework abstracts away boilerplate logic and encapsulates service logic in lightweight, reusable components called operators. The operators we implemented connect with the MySQL database to insert, delete and update votes on a review. In addition, the counts and user ids associated with a review are cached for efficient retrieval. Since we are loading upvotes for many reviews, we used MySQL+ Memcached for caching the results and improving performance.\n\nThe backend was implemented with generic voting capability, to allow for other types of voting in the future (downvoting, or categorizing a review as funny, etc). This will allow the backend to be very easily extended if the Search team decides to experiment with the voting capabilities in the future. Review helpfulness will be incorporated as signals into other relevance systems as Airbnb continues to personalize the guest experience."
    },
    {
        "url": "https://medium.com/airbnb-engineering/announcing-openair-2016-93e57dc56e49",
        "title": "Announcing OpenAir 2016 \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "We are excited to announce that Airbnb will be hosting our annual tech conference, OpenAir on June 8th at CityView Metreon from 9:00am \u2014 5:00 pm.\n\nLast year we focused on scaling human connection, this year we are expanding on that concept. OpenAir 2016 is about Getting Personal. As we continue to build products that are put to use in the real world and online, we need to consider how we understand people, what they want, how to connect them, and their impact on society.\n\nWe\u2019ll be discussing issues such as identity, matching, trust and safety, machine learning, reputation and much more.\n\nThis year we\u2019ll have an incredibly robust set of speakers and panels to discuss these topics, including:\n\nAttendees will get access to technical talks, hands-on sessions and thought-provoking discussions to help tackle some of the biggest challenges and projects in technology and society today. Throughout the day there will be time to network with local engineers, designers, data scientists, academics, take part in interactive sessions, drop in for lightning talks, and meet the speakers.\n\nRegistration is $50 and all proceeds, less fees, from registration fees will be donated to CODE2040. CODE2040 is a nonprofit organization that creates programs that increase the representation of Blacks and Latino/a in the innovation economy. CODE2040 believes the tech sector, communities of color, and the country as a whole will be stronger if talent from all backgrounds is included in the creation of the companies, programs, and products of tomorrow."
    },
    {
        "url": "https://medium.com/airbnb-engineering/apple-tv-authentication-a156937ea211",
        "title": "Apple TV Authentication \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "The new Apple TV opens up a world of possibilities for developers and consumers alike. But, when it comes to text entry, it still leaves much to be desired. So when we set out to design Airbnb for Apple TV, it was clear that requiring a person to enter their email address and password was not the delightful experience we wanted to provide. Moreover, Airbnb allows users to authenticate via third-party services like Facebook and Google. As such, even though entering usernames and passwords is a bit easier with voice entry in tvOS 9.2, such a strategy would only address a subset of our user base.\n\nWhen someone already has the Airbnb mobile app installed, there are clever ways that we could implement authentication with no typing at all (see Voucher, which facilitates \u201ckeyless\u201d authentication in the presence of a sibling iOS application). However, since we can\u2019t depend on all of our Apple TV users having the Airbnb mobile app handy (let alone an up-to-date version) we decided that we first needed to design a general authentication strategy that would work for any Airbnb user. By creating this robust foundation, we would be well-positioned to invest in additional authentication strategies that would provide more seamless experiences for specific subsets of our user base.\n\nWhen a person wants to log in to the Airbnb Apple TV app, the app provides them with a short code that they enter in their browser at www.airbnb.com/appletv. Once the code has been entered, the Airbnb app is able to retrieve an authentication token from our API which it can use to make subsequent API requests.\n\nThis authentication strategy can be broken down into three phases. First, the Apple TV app initializes an authentication request and is given secrets by the server. Next, the person is shown the simple secret and uses it to approve the request by means of an authenticated web session. Finally, the Apple TV app exchanges its secrets for an authorization token usable in future requests. I\u2019ll cover each stage in detail, including code examples for a simple Sinatra-based implementation.\n\nOur first stage is responsible for initializing our authentication attempt. We start from a place of zero trust in the client. The app makes a request to our authentication API requesting new authentication codes. It receives back a freshly generated nonce and short code.\n\nThe nonce is the true secret here; it can be thought of as a password. It is critical to use a cryptographically secure pseudo random number generator (CSPRNG) for this purpose. Many common RNGs are susceptible to attack [PDF], and if the RNG is compromised the security of the entire system breaks down. We utilize SecureRandom.uuid from the Ruby standard library for this purpose.\n\nFor the short code we prioritize ease of use. Our character set is uppercase alphanumeric with any easily confused symbols removed. This leaves us with a total of twenty six characters. A six character short code then has 26\u2076, or 308915776, possible values. Because these codes have a lifespan on the order of seconds to minutes and don\u2019t provide any direct pathway to account takeover even if guessed, this provides sufficient randomness for our purposes. Again, we use SecureRandom as our number generator as a guard against predictable random numbers.\n\nOn the server side we use an expiring data store (specifically, Redis) to store data related to the authentication attempt. Our data is keyed by the short code, and the payload includes only a hash of the nonce, generated using bcrypt. It is important to avoid storing plaintext secrets so that if a data store is compromised it doesn\u2019t automatically compromise all accounts.\n\nOur second stage of authentication is responsible for linking the authentication attempt to an active account. The short code generated in stage 1 is displayed to the person and they are prompted to go to an activation URL (www.airbnb.com/appletv) to complete the process.\n\nThe web-based activation page requires an authenticated session in order to link a short code to an account. If the person is not logged in they will be pushed to a login or sign up flow. They will be returned to the activation page after they have authenticated. The activation page contains a simple form prompting for the short code. Once it\u2019s submitted a success message is displayed and the user interaction is done.\n\nIn this system the short code is the equivalent of a username. On the server we look up the stored short code in Redis and update the payload to include the user\u2019s ID, establishing a link to the account. This endpoint is accessible only to clients who have already established a trusted identity, so we can effectively apply rate limiting to it to prevent abusive attempts to guess short codes.\n\nAfter the Apple TV app receives the secrets in stage 1 it begins polling the server to see if they have been authorized. Each polling request includes both the short code and the nonce. The polling frequency is provided by the server so that load may be throttled if necessary. Each polling request is used to refresh the lifetime of the generated authentication tokens. Because the server controls both the polling interval and the key expiry, it can ensure that the expiry is sufficiently distant to prevent timeouts while the person is still engaged with the process.\n\nOn the server each request leads to a lookup of the short code in Redis. If the hash of the nonce matches the value in the payload then we check for an associated ID.\n\nIf there is no ID the expiry is reset and nothing else occurs. If we find an ID then we delete the short code from Redis and generate a session for the associated user. At this point, the person has successfully logged in. The Apple TV may now interact with our API just like any other authenticated client.\n\nAt Airbnb we aim to design software that surprises and delights. Building a login flow that doesn\u2019t require entering a password is one of the little ways that we try to get out of the way and let you get on your way. It\u2019s also something we can share with others facing this common challenge. Check out an example implementation of this workflow at https://github.com/airbnb/apple-tv-auth."
    },
    {
        "url": "https://medium.com/airbnb-engineering/introducing-syslog-to-aws-kinesis-via-osquery-da4fc19de5ce",
        "title": "Introducing Syslog to AWS Kinesis via Osquery \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "At Airbnb, we are committed to protecting our community. This blog is an extension of that effort, as we will be providing periodic updates about investments we are making into our security program and the broader community.\n\nFor our opening security-related post, we\u2019re happy to announce we\u2019ve open sourced osquery tables that allow enterprises to collect and query syslog data for both OS X and Linux hosts! This allows you to capture privileged actions (sudo), lateral movement (sshd), errors impacting system availability/reliability and more. In addition to these tables, we\u2019ve open sourced osquery plugins that allow you to send query results to Amazon\u2019s Kinesis Streams and Firehose offerings. Read below for more information!\n\nWe wanted an agent for OS X & Linux that supports:\n\nosquery was the first option we looked at.\n\nFor those unfamiliar with osquery, it\u2019s an open source tool that exposes the operating system as a performant relational database. You write SQL-based queries against tables that represent system attributes, such as users, processes, devices, network connections, etc.\n\nOut of the box, osquery supports all but one of our use-cases: Syslog collection for OS X & Linux.\n\nSince osquery is an open source project, we built syslog tables and contributed back to the community!\n\nBefore we jump into the details, it\u2019s important to note that we only used safe, supported operating system API\u2019s. You won\u2019t find any kernel hacks or shell\u2019ing-out like you\u2019ll commonly see in security vendor products. You can see the code for yourself here and here.\n\nOur first contribution is an osquery table that allows you to surface, collect and query OS X ASL syslog data without any additional configuration.\n\nIn the hypothetical example below, we are querying for privileged actions performed on a given host. The query results show an attacker modifying/etc/hosts, escalating their privileges to root and loading a key-logger:\n\nBesides security-related queries, you can also query ASL to surface errors that are impacting system availability or reliability:\n\nIn our case, the ASL table informed us that our VPN client was flooding syslog with debug information, a bug that was addressed in a later build.\n\nThe ASL table definition is here and additional usage and configuration details can be found here.\n\nOur second contribution is an osquery table that allows you to surface, collect and query Linux syslog data.\n\nUsing osquery\u2019s event framework, we ingest logs forwarded from rsyslog over a named pipe, maintaining appropriate permissions for data integrity. We then make those logs available for consumption through our new table. This is compatible with any Linux distribution supported by osquery and rsyslog (Ubuntu 12/14, CentOS, RHEL, \u2026).\n\nWith relative ease you now have greater visibility into your infrastructure:\n\nAdditional usage and configuration details about the Linux syslog table can be found here. This feature is merged in osquery\u2019s master branch and is expected to ship with osquery v1.7.4.\n\nBy default, OS X doesn\u2019t send all of it\u2019s logs to the Apple System Log (ASL). For example, here are two logs that are not sent to ASL that capture information about application and package installation:\n\nWe toiled for hours, trying to capture this data via /etc/asl.conf and/etc/syslog.conf. We Google\u2019d, we read the man page, nothing. We almost came to the conclusion these paths were hardcoded as a result of some references in /System/Library/PrivateFrameworks/*.\n\nWe stumbled upon success when we carefully re-read the man page:\n\nTherefore, we can get these logs flowing into ASL by appending the storeaction into the following files and by restarting syslogd:\n\nThere are unfortunate use-cases where an application may not (or cannot) log to syslog. As a result, we will be developing an osquery table for OS X and Linux that can consume any log-file and present it in a schema like so:{path, line, time, message}.\n\nIn addition to the syslog tables, we have released osquery plugins that allow for any query-results to be sent to Amazon Kinesis Streams & Kinesis Firehose. This feature is merged into osquery\u2019s master branch and is expected to ship with osquery v1.7.4. These plugins use the AWS C++ SDK to avoid the need of deploying the Amazon Kinesis Agent.\n\nKinesis Streams & Kinesis Firehose give us flexibility in how we both process and store our logs. Auto-scaling features and interoperability with services like S3 (~$0.0300 per GB) make it cost effective and low-maintenance.\n\nIn an upcoming blog post, Airbnb Security Engineer Jack Naglieri will detail our Kinesis Streams & Kinesis Firehose use-cases and infrastructure. Stay tuned!\n\nThis engineering effort was a labor of love by Zach Wasserman. We want to thank Teddy Reed and Mike Arpaia for their code reviews and help.\n\nIt was a lot of fun contributing to this open source initiative and we encourage others to do the same! One fantastic way to give back is by sharing your syslog-related queries to the community via query pack contributions.\n\nWe hope this blog post and our open source contributions encourage others to explore this logging architecture with us."
    },
    {
        "url": "https://medium.com/airbnb-engineering/turbocharged-javascript-refactoring-with-codemods-b0cae8b326b9",
        "title": "Turbocharged JavaScript Refactoring with Codemods \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "It is fun to plant and harvest new crops in my garden, but I\u2019ll eventually wake up to a mess if I don\u2019t regularly weed. While each weed isn\u2019t a problem by itself, they combine forces to choke the system. Working in a weed-free garden is a productive pleasure. Codebases are like this too.\n\nI also don\u2019t enjoy weeding, so I forget to and end up in trouble. Thankfully, in coding, we have great tools like ESLint and SCSS-Lint to ensure that we weed as we go. However, if we find large chunks of legacy code that deserve attention, we can be overwhelmed by the thought of manually tweaking a million spaces here and a gazillion dangling commas there.\n\nMillions of lines of JavaScript have been checked into source control at Airbnb over the past 8 years; meanwhile, frontend Web development has evolved dramatically. Features, frameworks, and even JavaScript itself are moving targets \u2014 although following a good style guide from the get-go will help minimize this kind of pain, it is easy to end up with a codebase that no longer follows current \u201cbest practices\u201d. Each small inconsistency is like a little weed, waiting to be plucked to make room for something beneficial and pave the way for a more productive team. Look at the shape our garden was in:\n\nI\u2019m obsessed with making the team faster and know that consistent code and information from linters tightens up feedback loops and reduces communication overhead. We recently started a weeding project to prepare a lot of old JavaScript to follow our style guide and enable our linters in more places. Doing all of the work by hand is pretty tedious and time-consuming, so we looked for tools to automate some of it. Although `eslint \u2014fix` is a great starting point, it is currently limited in what it can fix. They\u2019ve recently started accepting pull requests for auto fixing any rule, and efforts are underway to implement a Concrete Syntax Tree (CST) for JavaScript, but it will take some time before that is well-integrated. Thankfully, we found Facebook\u2019s jscodeshift, which is a toolkit for codemods (codemods are tools that assist large-scale, partially automatable codebase refactoring). If a codebase is a garden, jscodeshift is a robo gardener.\n\nThis tool parses JavaScript into an Abstract Syntax Tree (AST), applies transformations, and writes out the modified JavaScript while matching local coding style. The transformations themselves are written in JavaScript, which makes this tool pretty accessible for our team. Finding or inventing the transformations we need accelerates mundane refactoring, which helps our team focus on more meaningful work.\n\nAfter running a few codemods, our garden looks a little better:\n\nSince most codemods take less than a minute with thousands of files, I find that codemodding is a great side-task while I wait for something on my main thread (e.g. code review). This helps maximize my productivity while making progress on bigger or more important projects.\n\nThe main challenges I\u2019ve run into while working on large-scale refactorings usually revolve around the 4 C\u2019s: Communication, Correctness, Code reviews, and (merge) Conflicts. I\u2019ve used some of the following tactics to help minimize these challenges.\n\nNot all codemods produce the exact result I want in every case, so it is important to review and tweak the changes. I\u2019ve found the following commands to be useful after running a codemod:\n\nSmall commits and pull requests are best, and codemods are no exception. I usually work on one codemod at a time to ease reviewing and resolving merge conflicts. I also often commit the codemod result by itself, followed by a manual cleanup commit if necessary. This makes resolving merge conflicts while rebasing my branch easier, since I can usually\n\nand run the codemod on that file again without messing with my manual changes.\n\nSometimes, the codemod produces a very large diff. For these cases, I\u2019ve found that it is helpful to use separate commits or pull requests for slices of the codebase based on path or filename. For instance, one commit fixes *.js files and another commit fixes *.jsx files. This makes reviewing easier and smooths out merge conflict resolution. Thanks to adherence to the Unix philosophy, running codemods on different slices is as straightforward as adjusting a call to `find`:\n\nTo avoid stepping on others\u2019 toes, it has worked well for me to push codemod commits to review early on Friday, and then rebase and merge early on Monday before most people have started working again. This gives folks a chance to wrap up whatever they were working on before the weekend without your codemod getting in their way.\n\nAlthough this tool is pretty young, there are a number of useful codemods already available to use. Here are some that we\u2019ve had success with so far.\n\nThese codemods were useful and relatively painless to apply, giving us quick wins.\n\njs-codemod/no-vars: conservatively convert `var` to `const` or `let`\n\njs-codemod/object-shorthand: transform object literals to use ES6 shorthand for properties and methods\n\nThese codemods either produced bigger diffs, which were more challenging to merge and avoid conflicts, or they required more follow-up tweaking to ensure that the code still looked good.\n\nThis codemod avoids the transformation when mixins are in play, and it does a good job of making other necessary conversions like how `propTypes`, default props, and initial state are defined, and binding callbacks in the constructor.\n\nBecause this moves lots of large chunks of code around, git won\u2019t automatically resolve most merge conflicts. I found it best to communicate well before running this transformation, and run it at a time that is least likely to step on toes (e.g. over a weekend). When I ran into conflicts while rebasing this one,\n\nand running the codemod again was the best approach.\n\nSince we had a lot of string concatenation, and this codemod transforms as much of it as it can to template literals, I ended up with a number of cases that ended up being less readable. I\u2019ve listed this codemod under the \u201cheavyweight\u201d section simply because it touched so many files and required a lot of manual choosing and tweaking to get the best results.\n\nWhether you want to write your own codemods, or just learn about what is possible, here are some useful resources:\n\nUsing available codemods and a few that we wrote and contributed back, we quickly made big improvements to old code. I have modified 40,000 lines via codemod with little effort, which brings a lot of old code into better compliance with our ES6 style guide. Our garden is in much better shape, and we are ready for happier and more productive harvests going forward.\n\nRunning codemods that are already available only scratches the surface \u2014 true power is unlocked when you pick up a keyboard and start writing your own. Codemods are great for modifications ranging from style refactorings to support breaking API changes, so let your imagination run wild. These techniques are well worth investing in and may save you and the people who use your projects a lot of time and effort."
    },
    {
        "url": "https://medium.com/airbnb-engineering/reair-easy-to-use-tools-for-migrating-and-replicating-petabyte-scale-data-warehouses-5153f8a433da",
        "title": "ReAir: Easy-to-Use Tools for Migrating and Replicating Petabyte-Scale Data Warehouses",
        "text": "Airbnb\u2019s data infrastructure has been an essential part of our strategy to continuously improve our products. Our Hive data warehouse grew exponentially from 350TB in the middle of 2013 to 11PB by the end of 2015. As the company grew, the demands on the reliability of the warehouse grew as well, and we sought to migrate our warehouse to new architectures. We found that existing migration tools either had issues with a large data warehouse or had significant operational overhead, so we developed ReAir to save time and effort when replicating data at this scale. In this blog post, we\u2019ll cover details about how ReAir works and how it can be used to easily replicate petabyte-scale data warehouses.\n\nInitially, all of our data was in a single HDFS / Hive warehouse. The single namespace provided a simple mental model and was easy to manage. However, mixing production with ad hoc workloads adversely affected reliability. Consequently, we aimed to split our warehouse into two \u2014 one for critical production jobs and another for ad hoc queries. In making this split, we had to handle two challenges \u2014 how can we easily migrate such a large warehouse, and after the split, how do we keep datasets in sync? We\u2019ve created ReAir to address these challenges and are open sourcing the tool for the community.\n\nReAir is useful for replicating data warehouses based on the Hive metastore. We\u2019ve made these tools to be scalable to clusters that are petabytes in size. Using ReAir with your cluster is simple as well \u2014 it connects to the Hive metastore Thrift service and copies data using map-reduce jobs. Because these are the only requirements, ReAir can work across a variety of different Hive and Hadoop versions and can operate largely standalone \u2014 only a deployment of Hadoop and Hive is required (+MySQL DB for incremental replication). Since ReAir handles both data and metadata, tables and partitions are ready to query as soon as the process completes.\n\nWithout ReAir, the typical solution for migrating the warehouse involves launching DistCp and managing metadata manually though database operations. Such an approach is labor intensive and error prone. Some of the errors can introduce inconsistencies that are difficult to resolve. Also, this process has issues with copying constantly changing warehouse directories. Other replication solutions require specific Hive versions \u2014 a requirement that\u2019s hard to meet if migrating from an older deployment.\n\nReAir includes 2 different replication tools: batch and incremental. The batch replication tool allows you to copy a specific list of tables at once, which is ideal for a cluster migration. In contrast, the incremental replication tool allows you to track changes that occur in the warehouse and replicate the objects as they are generated or modified. The incremental mode is ideal for keeping datasets in sync between clusters as it start copying changes within seconds of object creation. More information on the two modes is presented in the following sections.\n\nThe batch replication mode is typically used to copy the entire data warehouse. The speed and throughput of the copy depend on the number and the throughput of the reducers, but here at Airbnb, we\u2019ve been able to copy 2.2PB using 500 reducers in 24 hours.\n\nStarting the batch replication process is simple: the user runs a shell command that launches a series of map-reduce (MR) jobs. The execution contract is to replicate entities (Hive tables and partitions) from the source warehouse to the destination warehouse. The batch replication process is bandwidth efficient when run multiple times in succession \u2014 it detects files that match between the source and the destination and only copies the differences. Likewise, the metastore is only updated when needed. This idempotent behavior ensures that the tool is easy to run without wasted work.\n\nThere are several challenges for batch replication. One of the significant ones is that, in a production data warehouse, the sizes of the entities are not uniform, but the replication latency should not depend on the largest one. For example, common tables can have <100 partitions, while the largest tables can have over 100,000. To keep latency in check, it\u2019s necessary to evenly distribute the replication work.\n\nTo solve the load balancing issues, batch replication runs as a series of MR jobs. The two most expensive operations in batch replication are files copies and metadata updates, so those steps are distributed via a shuffle phase. The three jobs generate the list of files to copy, execute the file copy, and lastly, execute the metadata update. Each of these jobs generates logging data as files on HDFS, so it\u2019s easy to examine what happened after the job finishes.\n\nIn the first MR job, entity identifiers are read from HDFS and shuffled evenly to reducers. The reducers run various checks on the entities and produce a mapping from entity to the HDFS directory that needs to be copied. The second MR job goes through the list of directories generated by the first job and creates a list of files in those directories. The file names are shuffled to reducers based on hash of file path. Once shuffled, the reducers execute the copy. The third MR job handles the commit logic for the Hive metastore. Since the list of entities was already evenly distributed in the first MR job, the third MR job can be map-only.\n\nThe three-stage MR plan scales and balances load well: copying 2.2PB with 1 million entities takes about 24 hours. An update run after 20 TB has changed takes just an hour. During stage 1 and stage 3, we observed that the bottleneck is the Hive metastore MySQL database, while in the file copy of stage 2, the bottleneck is available network bandwidth.\n\nFor our migration, we also had to write a custom file copy job to handle our HDFS data. While tools like DistCp are commonly used, we found several issues during testing:\n\nTo solve these problems, we developed a series of two MR jobs to handle general HDFS copies. The first job builds a list of splits using a heuristic, level-based directory traversal and multiple threads. Once there are enough directories, mappers traverse those directories to generate the file list for copying. The file names are shuffled to reducers to determine whether a copy is necessary. The second MR job reads the list of files to copy and distributes the copy work via a shuffle.\n\nWith two clusters in place, we had a need to share data between the two clusters. For example, the daily log data was aggregated in the production cluster, but ad hoc users would need the data as well. The batch replication job mentioned above was a great tool for migrations, but it could take an hour to figure out the differences between two warehouses of our size and execute an update. Since users of the ad hoc cluster want data as soon as it\u2019s ready on the production cluster, it made sense to build an even faster method for landing the new content. There were some efforts in open source to handle this, but due to the dependencies on Hive versions, the need to copy S3 backed partitions, and the desire for a standalone solution that follows the same logic as batch replication, we developed the incremental replication tool to keep our production cluster in sync.\n\nAs the name suggests, the incremental replication tool involves recording changes to entities and copying over changes as soon as they occur. To record the changes on the source (i.e. production) cluster, we used the hooks mechanism in Hive to write an entry to a MySQL DB whenever a query succeeded. In this way, we could track all the changes that occurred in the production cluster. For changes made directly on HDFS or through systems like Spark, a metadata update (e.g. with a TOUCH command) can be made to trigger replication.\n\nOnce we had the changes recorded in the DB, we needed a way replicate those changes to the ad hoc cluster. This execution was implemented through a standalone Java service. The process reads entries from the change log and then convert the entries into a set of actions. For example, a successful create table query on the source cluster would translate to a \u201ccopy table\u201d action on the destination. The Java process would use make appropriate metastore calls and launch MR jobs to execute the actions.\n\nSince the changes on the source cluster are serialized in the log, it would be straightforward to execute the changes in the same order on the destination cluster. However, in practice, this would be too slow as copying a single table or partition can take seconds to minutes. Instead, the changes must be replicated in parallel with multiple threads. To facilitate concurrency, the actions are arranged into a DAG depending on the concurrency restrictions. Usually, these restrictions result from multiple actions on a single table. For instance, table creation should be done before copying partitions. By executing actions in parallel, the lag in replication is kept to a minimum.\n\nUsing incremental replication, it\u2019s possible to replicate changes quickly and reliably. Replicating data can be used to keep two warehouses in sync for disaster recovery \u2014 if one cluster is down, the other can be used to perform essential functions. Compared to batch replication, incremental replication is more efficient where the data warehouse is large, but the amount of data that changes is relatively small. Currently, at Airbnb, less than 2TB of data changes on the production cluster on a daily basis. Since this is a small part of the warehouse, incremental replication makes sense.\n\nBy using both batch and incremental replication, we were able to quickly migrate to a two-cluster setup that realized our goals for higher isolation and reliability. We hope that these tools will be useful to the community as well!"
    },
    {
        "url": "https://medium.com/airbnb-engineering/caravel-airbnb-s-data-exploration-platform-15a72aa610e5",
        "title": "Superset: Airbnb\u2019s data exploration platform \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "At Airbnb, we love data, and we like to think that analytics belongs everywhere. For us to be data-driven, we need data to be fluid, fast flowing, and crystal clear.\n\nAs a vector for data exploration, discovery, and collaborative analytics, we have built and are now open sourcing, a data exploration and dashboarding platform named Superset. Superset allows data exploration through rich visualizations while performing fast and intuitive \u201cslicing and dicing\u201d against just about any dataset.\n\nData explorers can easily travel through multi-dimensional datasets while creating and sharing \u201cslices\u201d, and assemble them in interactive dashboards.\n\nIt takes very little time, maybe 10 to 30 seconds of delays, to break someone\u2019s cognitive flow. Superset keeps your thinking loop spinning by providing a fluid query interface and enforces fast query times. Slicing, dicing, drilling down, and pivoting across visualizations allow users to explore multi-dimensional data spaces effectively.\n\nThe codeless approach to data navigation allows everyone on board, democratizing access to data. On one side of the spectrum, users that are less technical find an easy interface to query data. On the other end of that spectrum, advanced users enjoy gaining velocity and the ease of sharing the content they create.\n\nData scientists, engineers and other data wizards can still use Tableau, R, Jupyter, Airpal, Excel, and other means to interact with data, but Superset is gaining mind share internally as a frictionless and intuitive vehicle for sharing data and ideas.\n\nSuperset should work just as well in your environment as it does in ours. The query layer was written using SQLAlchemy, a SQL toolkit that allows authoring queries that can be translated to most SQL dialects out there.\n\nBeyond the SQL world, Superset is designed to harness the power of Druid.io. Druid is an open source, fast, column-oriented, realtime, distributed data store. Coupling the two together accelerates analysis cycles by taking delays out of the equation.\n\nSuperset allows you to manage a thin layer to enrich your datasets\u2019 metadata. This simple layer defines how your dataset is exposed to the user and is composed of:\n\nWe\u2019ve made taking Superset for a test drive very easy. After the simple installation process, you\u2019ll get Superset loaded with a nice set of dashboards, charts, and datasets that you can explore and interact with. The next logical step is to connect to your local databases and start visualizing them."
    },
    {
        "url": "https://medium.com/airbnb-engineering/using-r-packages-and-education-to-scale-data-science-at-airbnb-906faa58e12d",
        "title": "Using R packages and education to scale Data Science at Airbnb",
        "text": "One of my favorite things about being a data scientist at Airbnb is collaborating with a diverse team to solve important real-world problems. We are diverse not only in terms of gender, but also in educational backgrounds and work experiences. Our team includes graduates from Mathematics and Statistics programs, PhDs in fields from Education to Computational Genomics, veterans of the tech and finance worlds, as well as former professional poker players and military veterans. This diversity of training and experience is a tremendous asset to our team\u2019s ability to think creatively and to understand our users, but it presents challenges to collaboration and knowledge sharing. New team members arrive at Airbnb proficient in different programming languages, including R, Python, Matlab, Stata, SAS, and SPSS. To scale collaboration and unify our data science brand, we rely on tooling, education, and infrastructure. In this post, we focus on the lessons we have learned building R tools and teaching R at Airbnb. Most of these lessons also generalize to Python.\n\nOur approach has two main pillars: package building and education. We build packages to develop collaborative solutions to common problems, to standardize the visual presentation of our work, and to avoid reinventing the wheel. The goals of our educational efforts are to give all data scientists exposure to R and to the specific packages we use, and to provide opportunities for further learning to those who wish to deepen their skills.\n\nIn small data science teams, individual contributors often write single functions, scripts, or templates to optimize their workflows. As the team grows, different people develop their own tools to solve similar problems. This leads to three main challenges: (i) duplication of work within the team, both in writing the tools and reviewing code, (ii) lack of transparency about how tools are written and lack of documentation, often resulting in bugs or incorrect usage, (iii) difficulty sharing new developments with other users, slowing down productivity.\n\nR packages shared through Github Enterprise address these three challenges, which makes them a great solution for our needs. Specifically, (i) multiple people can collaborate simultaneously in order to improve the tools and fix bugs, (ii) contributions are peer reviewed, and (iii) new versions can be deployed to all users as needed. Packages are the basic units of reproducible R code. They can include functions, documentation, data, tests, add-ins, vignettes, and R markdown templates. I started working on our first internal R package, called Rbnb, nearly two years ago. It was initially launched with only a couple of functions. The package now includes more than 60 functions, has several active developers, and is actively used by members of our Engineering, Data Science, Analytics, and User Experience teams. As of today, our internal Knowledge Repo has nearly 500 R Markdown research reports using the Rbnb package.\n\nThe package is developed in an internal Github Enterprise repository. There, users can submit issues and suggest enhancements. As new code is submitted in a branch, it is peer reviewed by our Rbnb developers group. Once the changes are approved and documented, they are merged into the master codebase as a new version of the package. Team members can then install the newest release of Rbnb directly from Github using devtools. We are currently working on adding lintr checking for both style and syntax, and test coverage with testthat.\n\nThe package has four main components: (i) a consistent API to move data between different places in our data infrastructure, (ii) branded visualization themes, scales, and geoms for ggplot2, (iii) R Markdown templates for different types of reports, and (iv) custom functions to optimize different parts of our workflow.\n\nThe most used functions in Rbnb allow us to move aggregated or filtered data from a Hadoop or SQL environment into R, where visualization and in-memory analysis can happen more naturally. Before Rbnb, getting data from Presto into R in order to run a model required multiple steps. Data scientists would have to authenticate with their cluster credentials, open an SSH tunnel, enter host, port, schema, and catalog information for Presto, download a csv file, load that file into R, and only then run the desired models. Now, all of this can be done by piping two functions, as Rbnb takes care of all of the implementation details under the hood, while working with other well-maintained packages like RPresto. Similarly, getting data from R and moving it to Amazon S3 can be done with only one line of code. Data scientists no longer have to save a csv file from R, set up multi-factor authentication with our API keys, configure AWS, and run a bash command to move the csv into remote storage. More importantly, all functions follow a similar specification (i.e., place_action(origin, destination)).\n\nIf our data infrastructure changes \u2014 for instance, if a cluster moves or our Amazon S3 authentication details change \u2014 we can change our implementation of Rbnb without changing our functions\u2019 interface.\n\nThe package has also helped us brand our work across Airbnb through the use of consistent styles for data visualizations \u2014 see these posts by Bar Ifrach and Lisa Qian for examples. We have built custom themes, scales, and geoms for ggplot2, CSS templates for htmlwidgets and Shiny, and custom R Markdown templates for different types of reports. These features override R defaults with fonts and colors that are consistent with the Airbnb brand.\n\nThe Rbnb package also has dozens of functions that we have created to automate common tasks such as imputing missing values, computing year-over-year trends, performing common data aggregations, and repeating patterns that we use to analyze our experiments. Adding a new function to the package might take some time, but this initial investment pays off in the long run. By using the same R package we develop a common language, visualization style, and foundation of peer-reviewed code as our building blocks.\n\nIt does not matter how many tools you build if people do not know how to use them. After a period of rapid growth, we started organizing monthly week-long data bootcamps for new hires and current team members. They include 3-hour R workshops, and optional mentorship in a bootcamp project coded in R and written in R Markdown.\n\nThe bootcamp R class focuses on the Rbnb package and on common R packages used to reshape and manipulate data frames (tidyr and dplyr), visualize data (ggplot2), and write dynamic reports (R Markdown). We give participants study guides and materials a few days before our class. During class, we walk through a structured tutorial using our own data, including challenges that we commonly face on the job as working examples.\n\nThis approach allows users who are not familiar with R to start coding within a few hours, without having to worry about the intricacies of more advanced programing. We also introduce users to our internal style guide and to many useful R packages, such as formattable, diagrammeR, and broom. Finally, we give them directions on how to find help and online resources.\n\nAfter the bootcamp, we encourage users to continue learning. We sponsor individual memberships to DataCamp and help team members organize study groups around self-paced and interactive online courses. We also pair new hires with experienced peers who serve as mentors. These mentors walk new team members through their first contributions as data scientists. We have an internal Slack channel in which users can pose any questions related to R, and organize regular office hours in which experienced developers can help with more complex coding challenges. Our team members organize learning lunches and classes on topics such as SparkR, R object systems, and package development. Most recently, four team members attended a Master R Developer Workshop organized by RStudio, and shared what they learned with the team afterwards.\n\nMembers of our Data Science team are also encouraged to contribute code to Rbnb. The process of going through a comprehensive code review allows users to develop new skills that are valuable to future projects. In addition, they feel ownership of an important internal tool and see how their contributions can benefit their peers\u2019 work. We guide new contributors on best practices, function documentation, testing, and style.\n\nWe also engage with the broader R community outside Airbnb. We sponsor conferences like the upcoming rOpenSci Unconf, contribute to open source projects (e.g., ggtech, ggradar), and give talks at meetings such as the Shiny Developer Conference and UseR Conference. We have been fortunate to have influential R developers visit our headquarters in San Francisco last year, including Hadley Wickham and Ramnath Vaidyanathan.\n\nIn addition to tools and education, we also invest in strong data infrastructure. Our Shiny apps have had nearly 100k page views since our server was first started three years ago. We recently started supporting a new RStudio Server and SparkR cluster. We have a single Chef recipe with R packages and version control across all of the machines in our clusters, allowing for rapid updates and large-scale deployment.\n\nPowerful R tools, continuous education, engagement with the R community, and strong data infrastructure have helped our Data Science team scale. Since we started this initiative nearly two years ago, we have watched team members who had never before opened R transform into strong R developers who now teach R to our new hires. The foundation we have built allows us to hire a wide range of data scientists, sharing a growth mindset and excitement to learn new skills. This approach has helped us build a diverse team that brings new insights and perspectives to our work.\n\nThe creation of the Rbnb R package has inspired our Python developers to release an internal Python package for data scientists, called Airpy. Our developers collaborate so that the packages have a similar interface and set of functions. We encourage team members to contribute code to both Rbnb and Airpy, and we work together to develop more effective education resources and tools to empower our team. Today, many members of our team are proficient in both R and Python, and are able to review and write reliable code in both languages. In a recent survey with 66 members of our team, we found that 80% of our data scientists and analysts rated themselves as closer to \u201cExpert\u201d than \u201cBeginner\u201d in using R for data analysis, even though only 64% of them use R as their primary data analysis language. Similarly, 47% of the team members rated themselves as closer to \u201cExpert\u201d than \u201cBeginner\u201d in using Python for data analysis, though only 31% use it as their primary data analysis tool. The remaining 5% said they used both languages around equally. We focus on building a balanced team with strong developers using both languages, and have no preference or bias for either in our hiring process. This is yet another way through which diversity of skills, experiences, and backgrounds, have helped increase the impact of our team.\n\nThanks to Jenny Bryan, Mine Cetinkaya-Rundel, Scott Chamberlain, Garrett Grolemund, Amelia McNamara, Hilary Parker, Karthik Ram, Hadley Wickham, and to the Airbnb Engineering and Data Science teams for comments on an earlier version of this post."
    },
    {
        "url": "https://medium.com/airbnb-engineering/building-for-trust-503e9872bbbb",
        "title": "Building for Trust \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "That was the intro to a talk Joe Gebbia, one of Airbnb\u2019s co-founders, recently gave at TED. You can watch it here to find out how the story ends, but (spoiler alert) the theme centers on trust \u2014 one of the most important challenges we face at Airbnb.\n\nDesigning for trust is a well understood topic across the hospitality industry, but our efforts to democratize hospitality mean we have to rely on trust in an even more dramatic way. Not long ago our friends and families thought we were crazy for believing that someone would let a complete stranger stay in their home. That feeling stemmed from the fact that most of us were raised to fear strangers.\n\n\u201cStranger danger\u201d is a natural human defense mechanism; overcoming it requires a leap of faith for both guests and hosts. But that\u2019s a leap we can actively support by understanding what trust is, how it works, and how to build products that support it.\n\nHow best to support trust \u2014 particularly between groups of people who may not have the opportunity to interact with each other on a daily basis \u2014 is a core research topic for our data science and experience research teams. In preparation for Joe\u2019s talk, we reflected on how we think about trust, and we pulled together insights from a variety of past projects. The goal of this post is to share some of the thoughts and insights that didn\u2019t make it into the TED talk and to inspire more thinking about how to cultivate the fuel that helps the sharing economy run: trust.\n\nWhen Airbnb was just getting started, we were keenly aware of the need to build products that encourage trust. Convincing someone to try us for the first time would require some confidence that our platform helps to protect them, so we chose to take on a series of complex problems.\n\nWe began with the assumption that people are fundamentally good and, with the right tools in place, we could help overcome the stranger-danger bias. To do so, we needed to remove anonymity, giving guests and hosts an identity in our community. We built profile pages where they could upload pictures of themselves, write a description about who they are, link social media accounts, and highlight feedback from past trips. Over time we\u2019ve emphasized these identity pages more and more. Profile pictures, for example, are now mandatory \u2014 because they are heavily relied upon. In nearly 50% of trips, guests visit a host\u2019s profile at least once, and 68% of the visits occur in the planning phase that comes before booking. When people are new to Airbnb these profiles are especially useful: compared to experienced guests, first time guests are 20% more likely to visit a host\u2019s profile before booking.\n\nIn addition to fostering identity, we knew we also needed defensive mechanisms that would help build confidence. So we chose to handle payments, a complicated technical challenge, but one that would enable us to better understand who was making a booking. This also put us in a position to design rules to help remove some uncertainty around payments. For example, we wait 24 hours until after a guest checks-in before releasing funds to the host to give both parties some time to notify us if something isn\u2019t right. And when something goes wrong there needs to be a way to reach us, so we built a customer support organization that now covers every timezone and many languages, 24/7.\n\nOne way we measure the effect of these efforts is through retention \u2014 the likelihood that a guest or host uses Airbnb more than once. This isn\u2019t a direct measure of trust, but the more people come to trust Airbnb, the more likely they may be to continue using our service, so there\u2019s likely a correlation between the two. Evaluating customer support through this lens makes a clear case for its value: if a guest has a negative experience, for example a host canceling their reservation right before their trip, their retention rate drops 26%; intervention by customer support almost entirely negates this loss \u2014 retention rebounds up from 26% to less than 6%.\n\nWe didn\u2019t get everything right at first, and we still don\u2019t, but we have improved. One thing we learned after a bad early experience is that we needed to do more to give hosts confidence that we\u2019d be there for them if anything goes wrong, so we rolled out our $1 million guarantee for eligible hosts. But each year, more and more people are giving this a try because we\u2019ve been able to build confidence that their experience is likely to be a good one. This isn\u2019t the same as trusting the person they will stay with, but it\u2019s an important first step: if trust is the building that hosts and guests construct together, then confidence is the scaffold. Just like the scaffold on a building, our efforts to build confidence make it easier for the work of trust-building to happen, but they won\u2019t create trust. Only hosts and guests can do that.\n\nResearchers define trust in many ways, but one interesting definition comes from political scientist Russell Hardin. He argues that trust is really about \u201cencapsulated interest\u201d: if I trust you, I believe that you\u2019re going to look after me and my interests, that you\u2019re going to take my interests to heart and make decisions about them like I would.\n\nPeople who are open to trusting others aren\u2019t suckers \u2014 they usually need evidence that the odds are stacked in their favor when they choose to trust a stranger. Reviews thus form the raw material that we can collect and then surface to users on our platform. This is one of our most important data products; we refer to it as our reputation system.\n\nThe reputation system is an invaluable tool for the Airbnb community, and it\u2019s heavily used \u2014 more than 75% of trips are voluntarily reviewed. This is particularly interesting because reviews don\u2019t benefit the individuals who leave them; they benefit future guests and hosts, validating members of the Airbnb community and helping compatible guests and hosts find each other. Having any reputation at all is a strong determinant of a host\u2019s ability to get a booking \u2014 a host without reviews is about four times less likely to get a booking than a host that has at least one.\n\nOur reputation system helps guide guests and hosts toward positive experiences and it also helps overcome stereotypes and biases that unconsciously affect our decisions. We know biases exist in society, and one of the strongest biases we have in life is that we tend to trust others who are similar to us \u2014 sociologists call this homophily. As strong a social force as homophily is, it turns out reputation information can help counteract it. In a recent collaborative study with a team of social psychologists at Stanford, we found evidence of homophily among Airbnb travelers, but we also found that having enough positive reviews can help to counteract homophily, meaning in effect that high reputation can overcome high similarity. (Publication forthcoming)\n\nGiven the importance of reputation, we\u2019re always looking for ways to increase the quantity and quality of reviews. Several years ago, one member of our team observed that reviews can be biased upward due to fears of retaliation and bad experiences were less likely to be reviewed at all. So we experimented with a \u2018double blind\u2019 process where guest and host reviews would be revealed only after both had been submitted or after a 14-day waiting period, whichever came first. The result was a 7% increase in review rates and a 2% increase in negative reviews. These may not sound like big numbers, but the results are compounding over time \u2014 it was a simple tweak that has improved the travel experiences of millions of people since.\n\nOnce trust takes root, powerful community effects begin to emerge and long-standing barriers can begin to fall. First and foremost, people from different cultures become more connected. On New Year\u2019s Eve last year, for example, over a million guests, hailing from almost every country on earth, spent the night with hosts in over 150 countries. Rather than staying with other tourists, they stayed with locals, creating an opportunity for cross-cultural connection that can break down barriers and increase understanding.\n\nThis is visualized in the graphic below, which shows how countries are being connected via Airbnb trips. Countries on the vertical axis are where people are traveling from, and countries on the horizontal axis are where people are traveling to. The associated link will take you to an interactive visualization where you can see trends in connections relative to different measures of distance.\n\nAirbnb experiences are overwhelmingly positive, which creates a natural incentive to continue hosting. Hosts\u2019 acceptance rates rise as they gain more experience hosting. And we see evidence of their relishing the cross-cultural opportunities Airbnb provides: guests from a different country than the host gain a 6% edge in acceptance rates.\n\nHosting also produces more practical benefits. About half of our hosts report that the financial boost they receive through Airbnb helps them stay in their homes and pay for regular household expenses like rent and groceries; depending on the market, 5\u201320% of hosts report that this added source of earnings has helped them avoid foreclosure or eviction. The remaining earnings pads long-term savings and emergency funds, which helps them weather financial shocks in the future, or is used for vacations, which provides similar economic benefits to other markets.\n\nAs communities become more trusting, they also become more durable; they can serve as a source of strength when times are tough. In 2012, after Hurricane Sandy hit the east coast, one of our hosts in New York asked for help changing her listing\u2019s price to $0 in order to provide shelter to neighbors in need. Our engineers worked around the clock to build this and other features that would enable our community to respond to natural disasters which resulted in over 1,400 additional hosts making spaces available to those affected by the hurricane. Since then, we have evolved these tools into an international disaster response program, allowing our community to support those impacted by disasters \u2014 as well as the relief workers supporting the response \u2014 in cities around the world. In the last year we have responded to disasters and crises including the Nepal earthquake, the Syrian refugee crisis, the Paris attacks and most recently Tropical Storm Winston in Figi.\n\nJoe, Brian, and Nate realized from the beginning how crucial trust was going to be for Airbnb. These are just some of the stories that have followed years of effort to build confidence in our platform and facilitate one-on-one trust. While the results are quite positive, we still have a long way to go.\n\nOne ongoing challenge for us is to concretely measure trust. We regularly ask hosts and guests about their travel experiences, their relationships with each other, and their perceptions of the Airbnb community overall. But none of those are perfect proxies for trust, and they don\u2019t scale well. The standard mechanisms researchers often use to measure trust are cumbersome, and we can\u2019t reliably infer trust from behavioral data. But we\u2019re working to build this measurement capacity so we can continue to carefully design and optimize for trust over time.\n\nOur motivation to understand trust doesn\u2019t end with the need to build great products \u2014 it\u2019s also about understanding what communities full of trust can do. In one night last year, 1.2m guests stayed with 300,000 hosts. Each of those encounters is an opportunity to break down barriers and form new relationships that further strengthen communities over time."
    },
    {
        "url": "https://medium.com/airbnb-engineering/enzyme-javascript-testing-utilities-for-react-a417e5e5090f",
        "title": "Enzyme: JavaScript Testing utilities for React \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Testing code is crucial for the maintainability of a complex code base, but it is just as important that tests are easy to write, maintain, and understand. Frontend code is no exception to this rule, and behaviors that live in your UI should be tested as well.\n\nAlmost three months ago, Airbnb open sourced Enzyme, a JavaScript library for testing React components. Since then, the reception has been extremely positive, currently with over 3,000 stars, and over 50 contributors, 45 of which are outside of Airbnb.\n\nHistorically, testing UI has been hard to accomplish for a variety of reasons, but using React removes a lot of these hurdles. We hope that enzyme does a good job removing the remaining ones!\n\nAll new UI features for Airbnb.com are now implemented using React, which structures an application\u2019s UI into a set of reusable \u201cComponents\u201d. Components are a way to declare the way a UI should be rendered through an idempotent render function that is a pure function of application state.\n\nPure functions (and thus React components) are much easier to test because they simply return a description for what UI of the component should look like, given some application state, rather than actually mutating the UI and having side-effects. This \u201cdescription\u201d is known as a \u201cVirtual DOM\u201d and is a tree-like data structure.\n\nMaking assertions on the state of a React render tree can include a lot of boilerplate code and is hard to read, which detracts from the value of the test. Moreover, directly asserting on the resulting tree can strongly couple your tests to implementation details that end up making your tests extremely fragile.\n\nEnzyme makes asking questions about the rendered output of your React components easy and intuitive by providing a fluent interface around rendered React components.\n\nI gave a lightning talk about Enzyme and testing react components at the React Conference last week. If you haven\u2019t seen it yet, check it out!\n\nTo see how Enzyme works, let\u2019s take a look at an example: The \u201cTo Do\u201d list.\n\nSay we have these two components:\n\nThis is a relatively simple example, but let\u2019s see what kind of assertions we can make.\n\nEnzyme exports three different \u201cmodes\u201d to render and test components, shallow, mount, and render. Shallow is the recommended mode to start with since it does a better job of isolating your tests to just a single component. If shallow doesn\u2019t work for your use case (for example, if you are relying on the presence of a real DOM), mount or render likely will.\n\n*Note: This example will use a combination of mocha and chai expect, but neither are required to use enzyme.\n\nWe can also test the ToDoList component:\n\nAnd then we\u2019re done!\n\nIf you\u2019ve tried to test React components before, you might be aware that React provides testing utilities to achieve some of the same goals.\n\nEnzyme uses several of the utilities provided by React to build its API, but provides a much different interface and several more methods to reduce boilerplate and reduce the coupling between your tests and your implementation.\n\nThe problems that Enzyme addresses are by no means specific to Airbnb. Hopefully, by open sourcing Enzyme many others will find testing React components easier and more approachable. We have a number of features planned for future development, and welcome contributions from the community. You can fork Enzyme or open feature requests in the Github repository."
    },
    {
        "url": "https://medium.com/airbnb-engineering/scaling-knowledge-at-airbnb-875d73eff091",
        "title": "Scaling Knowledge at Airbnb \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "A responsibility for the Data team at Airbnb is to scale the ability to make decisions using data. We democratize data access to empower all employees to make data-informed decisions, give everybody the ability to use experiments to correctly measure the impact of their decisions, and turn insights on user preferences into data products that improve the experience of using Airbnb. Recently, we\u2019ve started to tackle a different type of problem. As an organization grows, how do we make sure that an insight uncovered by one person effectively transfers beyond the targeted recipient? Internally, we call this scaling knowledge.\n\nWhen our team consisted of just a handful of people, it was easy to share and discover research findings and techniques. But as our team has grown, issues that were once minor have become more significant. Take the case of Jennifer, a new data scientist looking to expand on work produced by a colleague on the topic of host rejections. Here\u2019s what we\u2019d see happening:\n\nBased on conversations with other companies, this experience is all too common. As an organization grows, the cost of transmitting knowledge across teams and across time increases. An inefficient and anarchic research environment raises this cost, slowing down analysis and the speed of decision making. Thus, a more streamlined solution can expedite the rate at which decisions are made and keep the company nimble atop a growing base of knowledge.\n\nAs we saw this problematic workflow play out over and over, we realized that we could do better. As a team, we got together and decided on five key tenets for what we wanted in our DS research:\n\nWith these tenets in mind, we surveyed the existing set of tools that had solved these problems in isolation. We noticed that R Markdowns and iPython notebooks solved the issue of reproducibility by marrying code and results. Github provided a framework for a review process, but wasn\u2019t well adapted to content outside of code and writing, such as images. Discoverability was usually based on folder organization, but other sites such as Quora were structuring many-to-one topic inheritance with tags. Learning was based on whatever code had been committed online, or via personal relationships.\n\nTogether, we combined these ideas into one system. Our solution combines a process around contributing and reviewing work, with a tool to present and distribute it. Internally, we call it the Knowledge Repo.\n\nAt the core there is a Git repository, to which we commit our work. Posts are written in Jupyter notebooks, Rmarkdown files, or in plain Markdown, but all files (including query files and other scripts) are committed. Every file starts with a small amount of structured meta-data, including author(s), tags, and a TLDR. A Python script validates the content and transforms the post into plain text with Markdown syntax. We use GitHub\u2019s pull request system for the review process. Finally, there is a Flask web-app that renders the Repo\u2019s contents as an internal blog, organized by time, topic, or contents.\n\nOn top of these tools, we have a process focused on making sure all research is high quality and consumable. Unlike engineering code, low quality research doesn\u2019t create metric drops or crash reports. Instead, low quality research manifests as an environment of knowledge cacophony, where teams only read and trust research that they themselves created.\n\nTo prevent this from happening, our process combines the code review of engineering with the peer review of academia, wrapped in tools to make it all go at startup speed. As in code reviews, we check for code correctness and best practices and tools. As in peer reviews, we check for methodological improvements, connections with preexisting work, and precision in expository claims. We typically don\u2019t aim for a research post to cover every corner of investigation, but instead prefer quick iterations that are correct and transparent about their limitations. Our tooling includes internal R and Python libraries to maintain on-brand, aesthetic consistency, functions to integrate with our data warehouse, and file processing to fit R and Python notebook files to GitHub pull requests.\n\nTogether, this provides great functionality around our knowledge tenets:\n\nThe Knowledge Repo houses a large variety of content. The bulk of the work consists of deep-dives aimed at answering non-trivial questions, but examinations of experiment results not covered by our experiment reporter are also common. Other posts are written purely to broaden the base of people who do data analysis, including writeups of a new methodology, example of a tool or package, and tutorials on SQL and Spark. Our public data blog posts also now live in the Knowledge Repo, including this one. In general, the heuristic is: if it could potentially be useful to someone in the future, post it!\n\nThe Knowledge Repo is still a work in progress. The small team that is working on it continuously addresses feature requests. We are working towards adoption by all teams in the company that do research, such as Qualitative Researchers who don\u2019t use GitHub. To that end, we are testing an internally built review process on an in-browser Markdown editing app. Another possible feature is a moderator for proposing and upvoting research topics. We are also looking into making it easier to fork existing posts and continue working on them as new posts.\n\nOne issue we are working on is the synthesis of the work that is done by individuals. Now that we have an archive for our knowledge threads, we have an opportunity to weave them into a shared understanding of Airbnb\u2019s ecosystem. We put a high premium on employees owning their own strategic impact, and want to enable everyone to develop a broad and deep understanding of any side of the business. We are currently iterating on the best way to keep content current and structured while the company is rapidly evolving and team structures changing.\n\nEspecially in the nascent field of Data Science, different data teams are likely reinventing the wheel regularly. In sharing our process we hope to inspire other organizations to work towards addressing the kinds of problems we are trying to solve, and share their learnings so we can collaboratively produce best practices."
    },
    {
        "url": "https://medium.com/airbnb-engineering/data-infrastructure-at-airbnb-8adfb34f169c",
        "title": "Data Infrastructure at Airbnb \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "This year we undertook a significant migration to go from a poorly architected set of clusters called \u201cPinky and Brain\u201d and onto the \u201cGold and Silver\u201d system described above. To set some context for scale, two years ago we moved from Amazon EMR onto a set of EC2 instances running HDFS with 300 terabytes of data. Today, we have two separate HDFS clusters with 11 petabytes of data and we also store multiple petabytes of data in S3 on top of that. With that background, here were the major problem areas and what we did to resolve them:\n\nSome early Airbnb engineers had a keen interest in a piece of infrastructure called Mesos, which sets out to deploy a single configuration across many servers. We built a single cluster of c3.8xlarge machines in AWS each backed by 3TB of EBS and ran all Hadoop, Hive, Presto, Chronos, and Marathon on Mesos.\n\nTo be clear, many companies use Mesos to great effect and implement novel solutions to administer large sets of machines running important infrastructure. But, our small team decided running a more standard, ubiquitous deployment would reduce the time we spent on operations and debugging.\n\nResolution: the answer was simply to move to a \u201cstandard\u201d stack. We were happy to learn from the hundreds, or potentially thousands, of other companies that operate large clusters and not try to invent a novel solution to a problem that is not ours to solve.\n\nBy storing all our HDFS data in mounted EBS (elastic block storage) volumes, we were sending lots of data over the public Amazon EC2 network in order to run queries. Hadoop was built for commodity hardware and expects local reads and writes on spinning disks, so this was a design mismatch.\n\nFurther to the remote nature of reads and writes, we had incorrectly chosen to split our data storage across three separate availability zones, within a single region, in AWS. Furthermore, each availability zone was designated as its own \u201crack\u201d so the 3 replicas were stored on different racks, therefore remote reads and writes were happening constantly. This was again a design flaw that led to slow data transfers and remote copies happening anytime a machine was lost or a block corrupted.\n\nResolution: having dedicated instances using local storage, and running in a single availability zone without EBS fixed these issues.\n\nLooking at our workload, we found that there were distinct requirements for our architectural components. Our Hive/Hadoop/HDFS machines required tons of storage, but didn\u2019t need much RAM or CPU. Presto and Spark were thirsty for memory and processing power, but didn\u2019t need much storage. Running c3.8xlarge instances backed by 3TB EBS was proving to be very expensive as storage was our limiting factor.\n\nResolution: Once we migrated off the Mesos architecture, we were able to choose different machine types to run the various clusters, for example using r3.8xlarge instances to run Spark. Amazon happened to be releasing their new generation of \u201cD-series\u201d instances at the time we were evaluating a shift, which made the transition even more desirable from a cost perspective. Moving from 3TB of remote storage per node on the c3.8xlarge machines to to 48TB of local storage on d2.8xlarge machines was very appealing and will save us millions of dollars over the next three years.\n\nWe had been running a federated HDFS cluster with Pinky and Brain where the data was held in shared physical block pools but the sets of mappers and reducers were isolated per logical cluster. This led to an awesome end user experience where any piece of data could be accessed by either Pinky queries or Brain queries, but unfortunately we found that federation was not widely supported and was considered experimental and unreliable by experts.\n\nResolution: moving to a fully distinct set of HDFS nodes, and not running federation, gave us the true isolation of clusters at the machine level, which also provided better disaster recovery coverage.\n\nOne of the most serious problems of having a unique infrastructure system was being forced to create custom monitoring and alerting for the cluster. Hadoop, Hive, and HDFS are complicated systems, prone to bugs across their many knobs and dials. Trying to anticipate all failure states and set reasonable pager thresholds proved quite challenging and it also felt that we were solving a solved problem.\n\nResolution: we signed a support contract with Cloudera to gain from their expertise in architecting and operating these large systems, and most importantly to reduce our maintenance burden by using the Cloudera Manager tool. Linking it into our Chef recipes has greatly reduced our monitoring and alerting burden and we are happy to report that we spend very little time on system maintenance and alerting.\n\nAfter evaluating all the errors and inefficiencies with our old cluster setup, we set out to systematically resolve these problems. It was a long process to migrate petabytes of data and hundreds of user jobs without disrupting service for our colleagues; we will author a new post on that subject alone and release some of our tooling to the open source community.\n\nNow that the migration is complete, we have drastically reduced the number of incidents and outages in our platform. It is not difficult to imagine the number of bugs and the number of issues we dealt with while running our custom stack on immature platforms, but the system is now well-worn and largely stable. The other benefit is that when we hire new engineers to join the team, onboarding is streamlined because the systems are familiar to what other companies have adopted.\n\nLastly, because we had the chance to architect things fresh in the new Gold and Silver setup, we had an opportunity to spin up all new instances and add IAM roles to manage security in a sensible fashion. This has meant a much more sane access control layer on top of the cluster, and integrates with how we manage all our machines.\n\nThe fact that we were able to dramatically cut costs and at the same time increase performance was awesome. Here are a few stats:"
    },
    {
        "url": "https://medium.com/airbnb-engineering/beginning-with-ourselves-48c5ed46a703",
        "title": "Beginning with Ourselves \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "In a recent post, we offered some insights into how we scaled Airbnb\u2019s data science team in the context of hyper-growth. We aspired to build a team that was creative and impactful, and we wanted to develop a lasting, positive culture. Much of that depends on the points articulated in that previous post, however there is another part of the story that deserves its own post \u2014 on a topic that has been receiving national attention: diversity.\n\nFor us, this challenge came into focus a year ago. We\u2019d had a successful year of hiring in terms of volume, but realized that in our push for growth we were not being as mindful of culture and diversity as we wanted to be. For example, only 10% of our new data scientists were women, which meant that we were both out of sync with our community of guests and hosts, and that the existing female data scientists at Airbnb were quickly becoming outnumbered. This was far from intentional, but that was exactly the problem \u2014 our hiring efforts did not emphasize a gender balanced team.\n\nThere are, of course, many ways to think about team balance; gender is just one dimension that stood out to us. And there are known structural issues that form a headwind against progress in achieving gender balance (source). So, in a hyper-growth environment where you\u2019re under pressure to build your team, it is easy to recruit and hire a larger proportion of male data scientists.\n\nBut this was not the team we wanted to build. Homogeneity brings a narrower range of ideas and gathers momentum toward a vicious cycle, in which it becomes harder to attract and retain talent within a minority group as it becomes increasingly underrepresented. If Airbnb aspires to build a world where people can belong anywhere, we needed to begin with our team.\n\nWe worried that some form of unconscious bias had infiltrated our interviews, leading to lower conversion rates for women. But before diving into a solution, we decided to treat this like any problem we work on \u2014 begin with research, identify an opportunity, experiment with a solution, and iterate.\n\nOver the year since, the results have been dramatic: 47% of hires were women, doubling the overall ratio of female data scientists on our team from 15% to 30%. The effect this has had on our culture is clear \u2014 in a recent internal survey, our team was found to have the highest average employee satisfaction in the company. In addition, 100% of women on our team indicated that they expect to still be here a year from now and felt like they belonged at Airbnb.\n\nOur work is by no means done. There\u2019s still more to learn and other dimensions of diversity to improve, but we feel good enough about our progress to share some insights. We hope that teams at other companies can adopt similar approaches and build a more balanced industry of data scientists.\n\nWhen we analyze the experience of a guest or host on Airbnb, we break it into two parts: the top-of-funnel (are there enough guests looking for places to stay and enough hosts with available rooms) and conversion (did we find the right match and did it result in a booking). Analyzing recruiting experiences is quite similar.\n\nAnd, like any project, our first task was to clean our data. We used the EEOC reporting in Greenhouse (our recruiting tool) to better understand the diversity of our applicants, doing our own internal audit of data quality as well. One issue we faced is that while Greenhouse collects diversity data on applicants who apply directly through the Airbnb jobs page, it does not collect information on the demographics of referrals (candidates who were recommended for the job by current Airbnb employees), which represent a large fraction of hires. Then we combined this with data from an internal audit of our teams history and from Workday, our HR tool, in order to compare the composition of applicants to the composition of our team.\n\nWhen we dug in, we found that historically about 30% of our applicants \u2014 the top of the funnel \u2014 had been women. This told us that there were opportunities for improvement on both fronts. Our proportion of female applicants was twice that of employees, so there was clearly room for improvement in our hiring process \u2014 the conversion portion. However, there wasn\u2019t male/female parity in our applicant pool so this could also prove a meaningful lever.\n\nIn addition, we wanted to ensure that our efforts to diversify our data science team didn\u2019t end with us. Making changes to the top of the funnel \u2014 to how many women want to and feel qualified to apply for data science jobs \u2014 could help us do that. Our end goal is to create a world where there is diversity across the entire data science field, not just at Airbnb.\n\nWe decided that the best way to achieve these goals would be to look beyond our own applicants to inspire and support women in the broader field. One observation was that while there were a multitude of meetups for women who code, and many great communities of women in engineering, we hadn\u2019t seen the same proliferation of events for women in data science.\n\nWe decided to create a series of lightning talks featuring women in data, under the umbrella of the broader Airbnb \u201cTaking Flight\u201d initiative. The goals were twofold: to showcase the many contributions of women in the field, and to create a forum for celebrating the contributions of women to data science. At the same time, we wanted to highlight diversity on multiple dimensions. For each lightning talk, we created a panel of women from many different racial and ethnic backgrounds, practicing different types of data science. The talks were open to anyone who supported women in data science.\n\nWe came up with the title \u201cSmall Talks, Big Data\u201d and started with an event in November 2014 where we served food and created a space and time for mingling. The event sold out, with over 100 RSVPs. Afterward we ran a survey to see what our attendees thought we could improve in subsequent events and turned \u201cSmall Talks, Big Data\u201d into a series, all of which have continued to sell out. Given this level of interest, several of the women on our team volunteered to write blog posts about their accomplishments (for example, Lisa\u2019s analysis of NPS and Ariana\u2019s overview of machine learning) in order to circulate their stories beyond San Francisco, and to give talks and interviews (for example, Get to know Data Science Panelist Elena Grewal). Many applicants to our team have cited these talks and posts as inspirations to consider working at Airbnb.\n\nIn parallel to these large community events we put together smaller get-together for senior women in the field to meet, support one another, and share best practices. We hosted an initial dinner at Airbnb and were amazed at what wonderful conversations and friendships were sparked by the event. This group has continued to meet informally, with women from other companies taking the lead on hosting events at their companies, further exposing this group to the opportunities in the field.\n\nAlongside our efforts to broaden our applicant pool, we scrutinized our approach to interviewing. As with any conversion funnel, we broke our process down into discrete steps, allowing us to isolate where the drop-off was occurring.\n\nThere are essentially three stages to interviewing for a data science role at Airbnb: a take-home challenge used to assess technicality and attention to detail; an onsite presentation demonstrating communication and analytical rigor; and a set of 1:1 conversations with future colleagues where we evaluate compatibility with our culture and fit for the role itself. Conversion in the third step was relatively equal, but quite different in steps one and two.\n\nWe wanted to keep unconscious bias from affecting our grading of take-home challenges, either relating to reviewers being swayed by the name and background of the candidate (via access to their resume) or to subjective views of what constitutes success. To combat this, we removed access to candidate names[1] and implemented a binary scoring system for the challenge, tracking whether candidates did or did not do certain tasks, in an effort to make ratings clearer and more objective. We provided graders with a detailed description of what to look for and how to score, and trained them on past challenges before allowing them to grade candidates in flight. The same challenge would circulate through multiple graders to ensure consistency.\n\nOur hypothesis for the onsite presentation was that we had created an environment that catered more to men. Often, a candidate would be escorted into a room where there would be a panel of mostly male data scientists who would scrutinize their approach to solving the onsite challenge. The most common critique of unsuccessful candidates was that they were \u2018too junior\u2019, stemming from poor communication or a lack of confidence. Our assumption was that this perception was skewed by the fact that they were either nervous or intimidated by the presentation atmosphere we had created.\n\nA few simple changes materially improved this experience. We made it a point to ensure women made up at least half of the interview panel for female candidates. We also began scheduling an informal coffee chat for the candidate and a member of the panel before the presentation, so they would have a familiar face in the room (we did this for both male and female candidates and both said they appreciated this change). And, in our roundup discussions following the presentation, we would focus the conversation on objective traits of the presentation rather than subjective interpretations of overall success.\n\nTaken together, these efforts had a dramatic effect on conversion rates. While our top-of-funnel initiatives increased the relative volume of female candidates, our interviewing initiatives helped create an environment in which female candidates would be just as likely to succeed as any male candidate. Furthermore, these changes to our process didn\u2019t just help with diversity; they improved the candidate experience and effectiveness of hiring data scientists in general.\n\nThe steps we took over the last year grew the gender balance on our team from 15% to 30%, which has made our team stronger and our work more impactful. How?\n\nFirst, it makes us smarter (source) by allowing for divergent voices, opinions, and ideas to emerge. As Airbnb scales, it has access to more data and increasingly relies upon the data science team\u2019s creativity and sophistication for making strategic decisions about our future. If we were to maintain a homogenous team, we would continue to rely upon the same approaches to the challenges we face: investing in the diversity of data scientists is an investment in the diversity of perspectives and ideas that will help us jump from local to global maxima. Airbnb is a global company and people from a multitude of backgrounds use Airbnb. We can be smarter about how we understand that data when our team better reflects the different backgrounds of our guests and hosts.\n\nSecond, a diverse team allows us to better connect our insights with the company. The impact of a data science team is dependent upon its ability to influence the adoption of its recommendations. It is common for new members of the field to assume that statistical significance speaks for itself; however, colleagues in other fields tend to assume the statistical voodoo of a data scientist\u2019s work is valid and instead focus on the way their ideas are conveyed. Our impact is therefore limited by our ability to connect with our colleagues and convince them of the potential our recommendations hold. Indeed, the pairing of personalities between data scientists and partners is often more impactful than the pairing of skillsets, especially at the leadership level. Increasing diversity is an investment in our ability to influence a broader set of our company\u2019s leadership.\n\nFinally, and perhaps most importantly, increasing our team\u2019s diversity has improved our culture. The women on the data science team feel that they belong and that their careers can grow at Airbnb. As a result, they are more likely to stay with the company and are more invested in helping to build this team, referring people in their networks for open roles. We are not done, but we have reversed course from a vicious to virtuous cycle. Additionally, the results aren\u2019t just restricted to women \u2014 the culture of the team as a whole has improved significantly over past years; in our annual internal survey, the data science team scores the highest in employee satisfaction across the company.\n\nOf course, gender is only one dimension of diversity that we aim to balance within the team. In 2015 it was our starting point. As we look to 2016 and beyond, we will use this playbook to enhance diversity in other respects, and we expect this will strengthen our team, our culture, and our company."
    },
    {
        "url": "https://medium.com/airbnb-engineering/growth-at-scale-getting-to-product-sharing-fit-ccb4b501ecf",
        "title": "Growth at Scale: Getting to product + sharing fit \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Travelers like to talk about Airbnb.\n\nSome of this conversation occurs outside of Airbnb\u2019s purview (like an in-person conversation or a Snap of an Airbnb listing), but other times it happens on airbnb.com, the iOS or Android apps, or on a site linking to Airbnb. In the latter case, we (Airbnb) have an opportunity to influence how the conversation transpires.\n\nIn this post, I will share a few features and optimizations that the Growth Team launched in 2015 to aid travelers in their conversation about Airbnb.\n\nThe first code I shipped at Airbnb was adding a Twitter Card for the referral page. Being a Twitter nerd, I knew how nice it was to share links that are backed by Twitter Cards in order to give viewers more context (and hopefully increase the click-through rate). This is the current experience of sharing an Airbnb referral code on Twitter (via https://www.airbnb.com/invite):\n\nBy adding the above Twitter Card, we reassured viewers about the referral link\u2019s authenticity and surrounded the tweet with details which may have been left out of the user-customized message."
    },
    {
        "url": "https://medium.com/airbnb-engineering/airbnb-product-and-engineering-teams-now-landing-in-portland-3a0bf4bf58fe",
        "title": "Airbnb Product and Engineering Teams Now Landing in Portland",
        "text": "Airbnb is on a mission to help people belong anywhere in the world. To reach this goal, we are building an exceptional product for our guests and hosts, with a fast-growing team of some of the smartest people in the industry. That\u2019s why today, we are excited to announce that we are expanding our presence to include a product and engineering team at our award-winning office in Portland, Oregon.\n\nThis is the first time we\u2019ve had engineers, product managers, designers, usability researchers, and data scientists outside of San Francisco, and we wanted to take a really thoughtful approach to expanding our team. For the last few months, we\u2019ve had an engineering landing team who have laid the groundwork for some of the cool projects the team is going to work on this year, as well as scoped out the best places to get caffeinated. Additionally, we\u2019ve gotten to know a lot of folks in Portland through our hosting the HackPDX Winter Hackathon in December, and we are excited to announce that in 2016 we will be sponsoring Django Girls Portland. Our new product team in Portland is going to build a world-class platform and set of tools for our global customer experience organization. To learn more about some of the projects the team has tackled so far, check out this blog post from Emre, one of the engineers on the team.\n\nIf you are an engineer, designer, data scientist, or product manager and you\u2019re as excited about this as we are, join us."
    },
    {
        "url": "https://medium.com/airbnb-engineering/how-well-does-nps-predict-rebooking-9c84641a79a7",
        "title": "How well does NPS predict rebooking? \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Data scientists at Airbnb collect and use data to optimize products, identify problem areas, and inform business decisions. For most guests, however, the defining moments of the \u201cAirbnb experience\u201d happen in the real world \u2014 when they are traveling to their listing, being greeted by their host, settling into the listing, and exploring the destination. These are the moments that make or break the Airbnb experience, no matter how great we make our website. The purpose of this post is to show how we can use data to understand the quality of the trip experience, and in particular how the \u2018Net promoter score\u2019 adds value.\n\nCurrently, the best information we can gather about the offline experience is from the review that guests complete on Airbnb.com after their trip ends. The review, which is optional, asks for textual feedback and rating scores from 1\u20135 for the overall experience as well as subcategories: Accuracy, Cleanliness, Checkin, Communication, Location, and Value. Starting at the end of 2013, we added one more question to our review form, the NPS question.\n\nNPS, or the \u201cNet Promoter Score\u201d, is a widely used customer loyalty metric introduced by Fred Reicheld in 2003 . We ask guests \u201cHow likely are you to recommend Airbnb to a friend?\u201d \u2014 a question called \u201clikelihood to recommend\u201d or LTR. Guests who respond with a 9 or 10 are labeled as \u201cpromoters\u201d, or loyal enthusiasts, while guests who respond with a score of 0 to 6 are \u201cdetractors\u201d, or unhappy customers. Those who leave a 7 or 8 are considered to be \u201cpassives\u201d. Our company\u2019s NPS (Net Promoter Score) is then calculated by subtracting the percent of \u201cdetractors\u201d from the percent of \u201cpromoters\u201d, and is a number that ranges from -100 (worst case scenario: all responses are detractors) to +100 (best case scenario: all responses are promoters).\n\nBy measuring customer loyalty as opposed to satisfaction with a single stay, NPS surveys aim to be a more effective methodology to determine the likelihood that the customer will return to book again, spread the word to their friends, and resist market pressure to defect to a competitor. In this blog post, we look to our data to find out if this is actually the case. We find that higher NPS does in general correspond to more referrals and rebookings. But we find that controlling for other factors, it does not significantly improve our ability to predict if a guest will book on Airbnb again in the next year. Therefore, the business impact of increasing NPS scores may be less than what we would estimate from a naive analysis.\n\nWe will refer to a single person\u2019s response to the NPS question as their LTR (likelihood to recommend) score. While NPS ranges from -100 to +100, LTR is an integer that ranges from 0 to 10. In this study, we look at all guests with trips that ended between January 15, 2014 and April 1, 2014. If a guest took more than one trip within that time frame, only the first trip is considered. We then try to predict if the guest will make another booking with Airbnb, up to one year after the end of the first trip.\n\nOne thing to note is that leaving a review after a trip is optional, as are the various components of the review itself. A small fraction of guests do not leave a review or leave a review but choose not to respond to the NPS question. While NPS is typically calculated only from responders, in this analysis we include non-responders by factoring in both guests who do not a leave a review as well as those who leave a review but choose not to answer the NPS question.\n\nTo assess the predictive power of LTR, we control for other parameters that are correlated with rebooking. These include:\n\nWe acknowledge that our approach may have the following shortcomings:\n\nDespite these shortcomings, we hope that this study will provide a data informed way to think about the value NPS brings to our understanding of the offline experience.\n\nOur data covers more than 600,000 guests. Our data shows that out of guests who submitted a review, two-thirds of guests were NPS promoters. More than half gave an LTR of 10. Of the 600,000 guests in our data set, only 2% were detractors.\n\nWhile the overall review score for a trip is aimed at assessing the quality of the trip, the NPS question serves to gauge customer loyalty. We look at how correlated these two variables are by looking at the distributions of LTR scores broken down by overall review score. Although the LTR and overall review rating are correlated, they do provide some differences in information. For example, of the small number of guests who had a disappointing experience and left a 1-star review, 26% were actually promoters of Airbnb, indicating that they were still very positive about the company.\n\nKeeping in mind that a very small fraction of our travelers are NPS detractors and that LTR is heavily correlated to the overall review score, we investigate how LTR correlates to rebooking rates and referral rates.\n\nWe count a guest as a referrer if they referred at least one friend via our referral system in the 12 months after trip end. We see that out of guests who responded to the NPS question, higher LTR corresponds to a higher rebook rate and a higher referral rate.\n\nWithout controlling for other variables, someone with a LTR of 10 is 13% more likely to rebook and 4% more likely to submit a referral in the next 12 months than someone who is a detractor (0\u20136). Interestingly, we note that the increase in rebooking rates for responders is nearly linear with LTR (we did not have enough data to differentiate between people who gave responses between 0\u20136). These results imply that for Airbnb, collapsing people who respond with a 9 versus a 10 into one \u201cpromoter\u201d bucket results in loss of information. We also note that guests who did not leave a review behave the same as detractors. In fact, they are slightly less likely to rebook and submit a referral than guests with LTR of 0\u20136. However, guests who submitted a review but did not answer the NPS question (labeled as \u201cno_nps\u201d) behave similar to promoters. These results indicate that when measuring NPS, it is important to keep track of response rate as well.\n\nNext, we look at how other factors might influence rebooking rates. For instance, we find just from our 10 weeks of data that rebooking rates are seasonal. This is likely because more off season travelers tend to be loyal customers and frequent travelers.\n\nWe see that guests who had shorter trips are more likely to rebook. This could be because some guests will use Airbnb mostly for longer stays and they just aren\u2019t as likely to take another one of those in the next year.\n\nWe also see that the rebooking rate has kind of a parabolic relationship to the price per night of the listing. Guests who stayed in very expensive listings are less likely to rebook, but guests who stayed in very cheap listings are also unlikely to rebook.\n\nIn addition to the Overall star rating and the LTR score, guests can choose to respond to the following subcategories in their review, all of which are on a 1\u20135 scale:\n\nIn this section we will investigate the power of review ratings to predict whether or not a guest will take another trip on Airbnb in the 12 months after trip end. We will also study which subcategories are most predictive of rebooking.\n\nTo do this, we compare a series of nested logistic regression models. We start off with a base model, whose dependent variables include only the non-review characteristics of the trip that we mentioned in the above section:\n\nThen, we build a series of models adding one of the review categories to this base model:\n\nWe compare the quality of each of the models `f1` to `f8` against that of the nested model `f0` by comparing the Akaike information criterion (AIC) of the fits. AIC trades off between the goodness of the fit of the model and the number of parameters, thus discouraging overfitting.\n\nIf we were just to include one review category, LTR and overall score are pretty much tied for first place. Adding any one of the subcategories also improves the model, but not as much as we were to include overall score or LTR.\n\nNext, we adjust our base model to include LTR and repeat the process to see what is the second review category we could add.\n\nGiven LTR, the next subcategory that will improve our model the most is the overall review score. Adding a second review category to the model only marginally improves the fit of the model (note the difference is scale of the two graphs).\n\nWe repeat this process, incrementally adding review categories to the model until the models are not statistically significant anymore. We are left with the following set of review categories:\n\nThese findings show that because the review categories are strongly correlated with one another, once we have the LTR and the overall score, we only need three of the six subcategories to optimize our model. Adding more subcategories will add more degrees of freedom without significantly improving the predictive accuracy of the model.\n\nFinally we tested the predictive accuracies of our models:\n\nUsing only a guest\u2019s LTR at the end of trip, we can accurately predict if they will rebook again in the next 12 months 56% of the time. Given just basic information we know about the guest, host and trip, we improve this predictive accuracy to 63.5%. Adding review categories (not including LTR), we add an additional 0.1% improvement. Given all this, adding LTR to the model only improves the predictive accuracy by another 0.002%.\n\nPost trip reviews (including LTR) only marginally improves our ability to predict whether or not a guest rebooks 12 months after checkout. Controlling for trip and guest characteristics, review star ratings only improve our predictive accuracy by ~0.1%. Out of all the review subcategories, LTR is the most useful in predicting rebooking, but it only adds 0.002% increase in predictive accuracy if we control for other review categories. This is because LTR and review scores are highly correlated.\n\nReviews serve purposes other than to predict rebooking. They enable trust in the platform, help hosts build their reputation, and can also be used for host quality enforcement. We found that guests with higher LTR are more likely to refer someone through our referral program. They could also be more likely to refer through word of mouth. Detractors could actually detract potential people from joining the platform. These additional ways in which NPS could be connected to business performance are not explored here. But given the extremely low number of detractors and passives and the marginal power post trip LTR has in predicting rebooking, we should be cautious putting excessive weight on guest NPS."
    },
    {
        "url": "https://medium.com/airbnb-engineering/confidence-splitting-criterions-can-improve-precision-and-recall-in-random-forest-classifiers-ad2d4ba696a4",
        "title": "Confidence Splitting Criterions Can Improve Precision And Recall in Random Forest Classifiers",
        "text": "The Trust and Safety Team maintains a number of models for predicting and detecting fraudulent online and offline behaviour. A common challenge we face is attaining high confidence in the identification of fraudulent actions. Both in terms of classifying a fraudulent action as a fraudulent action (recall) and not classifying a good action as a fraudulent action (precision).\n\nA classification model we often use is a Random Forest Classifier (RFC). However, by adjusting the logic of this algorithm slightly, so that we look for high confidence regions of classification, we can significantly improve the recall and precision of the classifier\u2019s predictions. To do this we introduce a new splitting criterion (explained below) and show experimentally that it can enable more accurate fraud detection.\n\nA RFC is a collection of randomly grown \u2018Decision Trees\u2019. A decision tree is a method for partitioning a multi-dimensional space into regions of similar behaviour. In the context of fraud detection, identifying events as \u20180\u2019 for non-fraud and \u20181\u2019 for fraud, a decision tree is binary and tries to find regions in the signal space that are mainly 0s or mainly 1s. Then, when we see a new event, we can look at which region it belongs to and decide if it is a 0s region or a 1s region.\n\nTypically, a Decision Tree is grown by starting with the whole space, and iteratively dividing it into smaller and smaller regions until a region only contains 0s or only contains 1s. Each final uniform region is called a \u2018leaf\u2019. The method by which a parent region is partitioned into two child regions is often referred to as the \u2018Splitting Criterion\u2019. Each candidate partition is evaluated and the partition which optimises the splitting criterion is used to divide the region. The parent region that gets divided is called a \u2018node\u2019.\n\nAfter these tweaks to the algorithm we find an insignificant change to the runtime of the Scikit-Learn routines. The Python code with the new criterion looks something like this:\n\nFor more details on the Machine Learning model building process at Airbnb you can read previous posts such as Designing Machine Learning Models: A Tale of Precision and Recall and How Airbnb uses machine learning to detect host preferences. And for details on our architecture for detecting risk you can read more at Architecting a Machine Learning System for Risk.\n\nTo test the improvements the Confidence splitting criterion can provide, we use the same dataset we used in the previous post Overcoming Missing Values In A Random Forest Classifier, namely the adult dataset from the UCI Machine Learning Repository. As before the goal is predict whether the income level of the adult is greater than or less than $50k per annum using the 14 features provided.\n\nWe tried 6 different combinations of [C0,C1] against the baseline RFC with Gini Impurity and looked at the changes in the Precision-Recall curves. As always we holdout a training set and evaluate on the unused test set. We build a RFC of 1000 trees in each of the 7 scenarios.\n\nObserve that C0=0.5 (yellow and blue lines) offers very little improvement over the baseline RFC, modest absolute recall improvements of 5% at the 95% precision level. However, for C0=0.9(green and purple lines) we see a steady increase in recall from at precision levels of 45% and upwards. At 80% precision and above, C0=0.9 improves recall by an absolute amount of 10%, riing to 13% at 95% precision level. There is little variation between C1=0.9(green line) and C1=0.99 (purple line) for C0=0.9 although [C0,C1]=[0.9,0.9] (green line) does seem to be superior. For C0=0.9 (pale blue and pink lines), the improvement is not so impressive or consistent.\n\nIt would be useful to extend the analysis to compare the new splitting criterion against optimising existing hyper-parameters. In the Scikit-Learn implementation of RFCs we could experiment with min_samples_split or min_samples_leaf to overcome the scaling problem. We could also test different values of class_weight to capture the asymmetry introduced by non-equal C0 and C1.\n\nMore work can be done on the implementation of this methodology and there is still some outstanding analytical investigation on how the confidence thresholds Cj tie to the improvements in recall or precision. Note however that the methodology does already generalise to non binary classifiers, i.e. where j=0,1,2,3,\u2026. It could be useful to implement this new criterion into the Apache Spark RandomForest library also.\n\nFor the dataset examined, the new splitting criterion seems to be able to better identify regions of higher density of 0s or 1s. Moreover, by taking into account the size of the partition and the probability of such a distribution of observations under the null hypothesis, we can better detect 1s. In the context of Trust and Safety, this translates into being able to more accurately detect fraudulent actions.\n\nThe business implications of moving the Receiver Operating Characteristic outwards (equivalently moving the Precision-Recall curve outwards) have been discussed in a previous post. As described in the \u2018Efficiency Implications\u2019 section of Overcoming Missing Values In A Random Forest Classifier post, even decimal percentage point savings in recall or precision can lead to enormous dollar savings in fraud mitigation and efficiency respectively."
    },
    {
        "url": "https://medium.com/airbnb-engineering/how-we-partitioned-airbnb-s-main-database-in-two-weeks-55f7e006ff21",
        "title": "How We Partitioned Airbnb\u2019s Main Database in Two Weeks",
        "text": "Heading into the 2015 summer travel season, the infrastructure team at Airbnb was hard at work scaling our databases to handle the expected record summer traffic. One particularly impactful project aimed to partition certain tables by application function onto their own database, which typically would require a significant engineering investment in the form of application layer changes, data migration, and robust testing to guarantee data consistency with minimal downtime. In an attempt to save weeks of engineering time, one of our brilliant engineers proposed the intriguing idea of leveraging MySQL replication to do the hard part of guaranteeing data consistency. (This idea is independently listed an explicit use cases of Amazon RDS\u2019s \u201cRead Replica Promotion\u201d functionality.) By tolerating a brief and limited downtime during the database promotion, we were able to perform this operation without writing a single line of bookkeeping or migration code. In this blog post, we will share some of our work and what we learned in the process.\n\nWe tend to agree with our friends at Asana and Percona that horizontal sharding is bitter medicine, and so we prefer vertical partitions by application function for spreading load and isolating failures. For instance, we have dedicated databases, each running on its own dedicated RDS instance, that map one-to-one to our independent Java and Rails services. However for historical reasons, much of our core application data still live in the original database from when Airbnb was a single monolithic Rails app.\n\nUsing a client side query profiler that we built in-house (it\u2019s client side due to the limitations of RDS) to analyze our database access pattern, we discovered that Airbnb\u2019s message inbox feature, which allows guests and hosts to communicate, accounted for nearly 1/3 of the writes on our main database. Furthermore, this write pattern grows linearly with traffic, so partitioning it out would be a particularly big win for the stability of our main database. Since it is an independent application function, we were also confident that all cross-table joins and transactions could be eliminated, so we began prioritizing this project.\n\nIn examining our options for this project, two realities influenced our decision making. First, the last time we partitioned a database was three years ago in 2012, so pursuing this operation at our current size was a new challenge for us and we were open to minimizing engineering complexity at the expense of planned downtime. Second, as we entered 2015 with around 130 software engineers, our teams were spread across a large surface area of products\u2013ranging from personalized search, customer service tools, trust and safety, global payments, to reliable mobile apps that assume limited connectivity\u2013leaving only a small fraction of engineering dedicated to infrastructure. With these considerations in mind, we opted to make use of MySQL replication in order to minimize the engineering complexity and investment needed.\n\nThe decision to use MySQL\u2019s built-in replication to migrate the data for us meant that we no longer had to build the most challenging pieces to guarantee data consistency ourselves as replication was a proven quantity. We run MySQL on Amazon RDS, so creating new read replicas and promoting a replica to a standalone master is easy. Our setup resembled the following:\n\nWe created a new replica (message-master) from our main master database that would serve as the new independent master after its promotion. We then attached a second-tier replica (message-replica) that would serve as the message-master\u2019s replica. The catch is that the promotion process can take several minutes or longer to complete, during which time we have to intentionally fail writes to the relevant tables to maintain data consistency. Given that a site-wide downtime from an overwhelmed database would be much more costly than a localized and controlled message inbox downtime, the team was willing to make this tradeoff to cut weeks of development time. It is worth mentioning that for those who run their own database, replication filters could be used to avoid replicating unrelated tables and potentially reduce the promotion period.\n\nMoving message inbox tables to a new database could render existing queries with cross-table joins invalid after the migration. Because a database promotion cannot be reverted, the success of this operation depended on our ability to identify all such cases and deprecate them or replace them with in-app joins. Fortunately, our internal query analyzer allowed us to easily identify such queries for most of our main services, and we were able to revoke relevant database permission grants for the remaining services to gain full coverage. One of the architectural tenets that we are working towards at Airbnb is that services should own their own data, which would have greatly simplified the work here. While technically straightforward, this was the most time consuming phase of the project as it required a well-communicated cross-team effort.\n\nNext, we have a very extensive data pipeline that powers both offline data analytics and downstream production services. So the next step in the preplanning was to move all of our relevant pipelines to consume the data exports of message-replica to ensure that we consume the newest data after the promotion. One side effect of our migration plan was that the new database would have the same name as our existing database (not to be confused with the name of our RDS instances, e.g. message-master and message-replica) even though the data will diverge after the promotion. However, this actually allowed us to keep our naming convention consistent in our data pipelines, so we opted not to pursue a database rename.\n\nLastly, because our main Airbnb Rails app held exclusive write access to these tables, we were able to swap all relevant service traffic to the new message database replica to reduce the complexity of the main operation.\n\nOnce all the preplanning work was done, the actual operation was performed as follows:\n\nShould the op have failed, we would have reverted the database host entries in Zookeeper and the message inbox functionality would have been restored almost immediately. However, we would have lost any writes that made it to the now-independent message databases. Theoretically it would be possible to backfill to restore the lost messages, but it would be a nontrivial endeavor and confusing for our users. Thus, we robustly tested each of the above steps before pursing the op.\n\nEnd-to-end, this project took about two weeks to complete and incurred just under 7 1/2 minutes of message inbox downtime and reduced the size of our main database by 20%. Most significantly, this project brought us significant database stability gains by reducing the write queries on our main master database by 33%. These offloaded queries were projected to grow by another 50% in coming months, which would certainly have overwhelmed our main database, so this project bought us valuable time to pursue longer-term database stability and scalability investments.\n\nAccording to the RDS documentation:\n\nWe generally have Multi-AZ deployment enabled on all master instances of RDS to take full advantage of RDS\u2019s high availability and failover support. During this project, we observed that given a sufficiently heavy database load, the latency experienced during an RDS snapshot even with Multi-AZ deployment can be significant enough to create a backlog of our queries and bring down our database. We were always aware that snapshots lead to increased latency, but prior to this project we had not been aware of the possibility of full downtime from nonlinear increases in latency relative to database load.\n\nThis is significant given that RDS snapshots is a core RDS functionality that we depend on for daily automated backups. Previous unbeknownst to us, as the load on our main database increases, so did the likelihood of RDS snapshots causing site instability. Thus in pursuing this project, we realized that it had been more urgent than we initially anticipated.\n\nAcknowledgements: Xinyao Hu led the project while I wrote the initial plan with guidance from Ben Hughes and Sonic Wang. Brian Morearty and Eric Levine helped refactor the code to eliminate cross-table joins. The Production Infrastructure team enjoyed a fun afternoon running the operation.\n\nCheckout more past projects from the Production Infrastructure team:"
    },
    {
        "url": "https://medium.com/airbnb-engineering/unboxing-the-random-forest-classifier-the-threshold-distributions-22ea2bb58ea6",
        "title": "Unboxing the Random Forest Classifier: The Threshold Distributions",
        "text": "In the Trust and Safety team at Airbnb, we use the random forest classifier in many of our risk mitigation models. Despite our successes with it, the ensemble of trees along with the random selection of features at each node makes it difficult to succinctly describe how features are being split. In this post, we propose a method to aggregate and summarize those split values by generating weighted threshold distributions.\n\nDespite its versatility and out-of-box performances, the random forest classifier is often referred to as a black box model. It is easy to see why some might be inclined to think so. First, the optimal decision split at each node is only drawn from a random subset of the feature set. And to make matters more obscure, the model generates an ensemble of trees using bootstrap samples of the training set. All these just means that a feature might split at different nodes of the same tree and possibly with different split values and this could be repeated in multiple trees.\n\nWith all this randomness being thrown around, a certainty still remains. After training a random forest, we know exactly every detail of the forest. For each node, we know what feature is used to split, with what threshold value, and with what efficiency. All the details are there, the challenge is knowing how to piece them together to built an accurate and informative description of the allegedly black box.\n\nOne common way to describe a trained random forest in terms of the features is to rank them by their importances based on their splitting efficiencies. Although this method manages to quantify the contribution of impurity decreases from each of the features, it does not shed light on how the model makes decisions from them. We propose in this post one way to concisely describe the inner decisions of the forest by mining node-by-node the entire forest and present the weighted distributions (by split efficiencies and sample sizes) of the thresholds for each feature.\n\nFor purpose of illustrating this method, we resort to the publicly available Online News Popularity Data Set from the UCI Machine Learning Repository. The dataset contains 39,797 observations of Mashable articles each with 58 features. The positive labels for this dataset are defined as whether the number of shares for a particular article is greater or equal to 1,400. The features are all numerical and ranges from simple statistics like number of words in the content to more complex ones like the closeness to a particular LDA-derived topic.\n\nAfter training the random forest, we crawl through the entire forest and extract the following information from each non-terminal (or non-leaf) node:\n\nwhich can be collected into a table like the following:\n\nA common table like the above will very likely contain the same feature multiple times across different trees and even within the same tree. It might be tempting at this point to just collect all the thresholds for a particular feature and pile them up in a histogram. This, however, would not be fair since nodes where the optimal feature-threshold tuples are found from a handful of observations should not have the same weight as those found from thousands of observations. Hence, we define, for a splitting node [latex]i[/latex], the sample size [latex]N_i[/latex] as the number of observations that reached the splitting node i during the training phase of the random forest.\n\nwhere the filled circles are the positively labeled observations whereas the unfilled circles are the negatively labeled ones. In this example, although both nodes split on the same feature, their reasons to do so is quite different. In the left splitting node, the majority of the positive observations end up on the less-or-equal-to branch whereas in the right splitting node, the majority of the positive observations end up in on the greater-than branch. In other words, the left splitting node views the positively labeled observations to be more likely to have smaller feature X values whereas the right splitting node views the positively labeled observations to be more likely to have larger feature X values. This difference is accounted using the is_greater_than_threshold (or chirality) flag, where a 1 is true (or greater than) and a 0 is false (or less or equal to).\n\nAfter training the classifier model, we crawl through the entire forest and collect all the information specified in the previous section. This information empowers us to describe which thresholds dominates the splittings for, say num_hrefs (number of links in the article):\n\nIn the above plot we see that there are two distributions. The orange one corresponds to the nodes where the chirality is greater_than and the red-ish (or rausch) one where the chirality is less-than-equal-to. These weighted distributions for num_hrefs indicate that whenever the feature num_hrefs is used to decide whether an article is a popular one (+1,400 shares) the dominant description is greater than ~15 links, which is illustrated by the spiking bin of the greater-than distribution near 15. Another interesting illustration of this method is on the global_rate_positive_words and the global_rate_negative_words, which are defined as the proportion of, respectively, positive and negative words in the content of the article. The former one is depicted as follows:\n\nwhere, as far as the model is concerned, popular articles tend to be dominated by larger global_rate_positive_words where the cutoff is dominated by 0.03. However, a more interesting distribution is that for the global_rate_negative_words:\n\nwhere the distributions indicate use of negative words greater than ~0.01 words per content size adds a healthy dose of popularity whereas too much of it, say, more than ~0.02 will make the model predict a lower popularity. This is inferred from the spike of the greater-than distribution spiking at ~0.01 whereas the less-than-equal-to distribution spikes at ~0.02.\n\nIn TnS we are eager to make our models more transparent. This post only deals with one very specific way to inspect the inner workings of a trained model. Other possible approaches can be by asking the following questions:"
    },
    {
        "url": "https://medium.com/airbnb-engineering/mastering-the-tvos-focus-engine-f8a13b371083",
        "title": "Mastering the tvOS Focus Engine \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "The tvOS interaction paradigm presents a unique challenge to developers and designers alike. The new Apple TV pairs a trackpad-like remote with a UI lacking a traditional cursor. As a result, \u201cfocus\u201d is the only means by which an app can provide visual feedback to its user(s) as they navigate.\n\nYou can think of the focus engine as the bridgekeeper between users and your shiny new tvOS application. Though this bridgekeeper doesn\u2019t expect you to know the airspeed velocity of an unladen swallow, befriending the focus engine is an essential step towards building an app that feels native to this platform.\n\nAny seasoned iOS engineer will feel at home in UIKit on tvOS \u2014 but don\u2019t let the platforms\u2019 notable similarities seduce you into believing they are the same. Apple has made it easy to port your iOS app to tvOS. But if you don\u2019t consider how your application will interact with the focus engine from the outset, you\u2019ll find yourself fighting an uphill battle as you approach the finish line.\n\nUsers navigate a tvOS application by moving focus between items onscreen. When an item is focused, its appearance is adjusted to stand out from the appearance of other items onscreen. Focus effects are the crux of what makes tvOS communal. Focus effects provide visual feedback not only to whomever is quarterbacking the remote, but also to any onlookers who are following along. They\u2019re what separate this native TV experience from AirPlay-ing your iPad to the big screen.\n\nOnly views can receive focus and only one view may be in focus at a time. Consider these buttons:\n\nButton C is currently in focus. Swiping left on the remote will focus button B. Swiping right will focus button D. Swiping left or right more aggressively will focus button A or button E, respectively. It\u2019s worth noting that even though a more aggressive left swipe will result in button A ultimately gaining focus, button B will instantaneously gain (and then lose) focus in the process.\n\nWhether a particular view is focusable is determined by a new instance method added UIView.\n\nApple has audited its public frameworks and provided sensible implementations for canBecomeFocused(). Only the following classes shipped with UIKit are focusable:\n\nUICollectionViewCell and UITableViewCell are exceptions. Whether a cell is focusable is determined by the UICollectionView or UITableView delegate:\n\nThough not focusable itself, UIImageView is also a special case with the addition of the adjustsImageWhenAncestorFocused property. When enabled, an UIImageView instance will display a focused effect whenever an ancestor receives focus. As the system provides no default focus effect for UICollectionView cells, this is an easy way to breathe life into image-based collection views. You will see this technique used extensively by Apple throughout the system UI and builtin applications.\n\nYou can ask any view whether it\u2019s currently in focus.\n\nYou can also ask the screen for the currently focused view (if it exists).\n\nIn tvOS, all classes that participate in the focus system conform to the UIFocusEnvironment protocol. Each focus environment corresponds to a particular branch of the view hierarchy, meaning that focus environments can be nested.\n\nThe UIFocusEnvironment API allows for a two-way dialogue between developers and the focus engine regarding how focus should be updated within a particular branch of the view hierarchy. UIView, UIViewController, and UIPresentationController conform to UIFocusEnvironment out of the box.\n\nOverriding shouldUpdateFocusInContext(_:) provides an opportunity to vet the proposed focus update before it\u2019s applied. UICollectionView and UITableView delegates provide NSIndexPath-based versions of this API where the provided context contains the previously and next focused index paths rather than the views themselves.\n\nHere\u2019s a toy example showing how to use the UICollectionView delegate method to disable focus updates within a collection view when a cell has been selected.\n\nOverriding didUpdateFocusInContext(_:withAnimationCoordinator:) provides an opportunity to take action in response to a focus update and participate in the associated animation. Given two adjacent buttons, here\u2019s how one could horizontally center whichever one is currently in focus.\n\nWhen a focus update cycle occurs, each method is invoked on the view receiving focus, the view losing focus, and all parent focus environments of these two views.\n\nThe focus engine will automatically initiate focus updates at appropriate times like app launch or when the currently focused view is removed from the view hierarchy. Developers can also request focus updates, but any requests must be issued through the focus engine. Since only the focus engine can update focus, it\u2019s here that the focus engine most literally takes on the role of bridgekeeper.\n\nUIFocusEnvironment\u2019s setNeedsFocusUpdate() and updateFocusIfNeeded() interact with the focus engine in much the same manner as setNeedsLayout() and layoutIfNeeded() interact with the layout engine. Invoking setNeedsFocusUpdate() will make a note of your request and return immediately. The focus engine will recompute focus during the next update cycle. Invoking updateFocusIfNeeded() forces a focus update immediately.\n\nWhen an update cycle begins, the focus engine queries the initiating focus environment for its preferredFocusedView. If this view is non-nil and focusable, the focus engine will attempt to give that view focus by issuing the aforementioned notification events through the focus responder chain.\n\nLet\u2019s look at a few examples with a particularly useless AnimalsViewController that I\u2019ve created. In each example, dogButton initially has focus.\n\nThe focus engine will only honor requests that are issued by a focus environment that currently contains focus. As a result, even though catButton\u2019s preferredFocusedView is itself, catButton\u2019s request to update focus is ignored.\n\nWhen animalsViewController requests a focus update, however, focus will move from dogButton to catButton since the currently focused view (dogButton) is within the branch of the view hierarchy governed by the animalsViewController focus environment.\n\nWhen two focus environments request a focus update simultaneously, the focus engine will defer to the parent environment.\n\nWith this focus engine primer in hand, you will be well on your way to taking full advantage of this new platform. The focus system and API is familiar and well-engineered, so it\u2019s easy for it to become an afterthought as you bring your application to the TV. But the more you work on this platform the more clear it will become that harnessing the full power of the focus engine will mark the difference between an iOS port and a native tvOS experience."
    },
    {
        "url": "https://medium.com/airbnb-engineering/how-it-ticks-building-the-airbnb-apple-watch-app-663288dc52e8",
        "title": "How It Ticks: Building the Airbnb Apple Watch App \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Airbnb wants to be present on all the devices that our hosts and guests use every day so that we can bring the experience to everyone no matter where they are. Earlier this summer, a small group of us set out to define what the Airbnb experience would be like on a brand new mobile platform: Apple Watch. We were intrigued by the benefits this inherently on-the-go wearable could possibly bring to the table.\n\nOur initial thought was perhaps the most natural one: \u201clet\u2019s port our iOS app to the Watch!\u201d. So we started by building a prototype Watch app that had the capability to browse listings in a given city, send and receive messages, as well as edit and access wish lists. Because Watch interfaces are built using Interface Builder in Xcode, this was a relatively quick, drag-and-drop process of laying out the UI elements on each screen. So, we were done, right?\n\nNot really. It quickly became evident that the prototype we had built was not the best-possible Airbnb experience that we could deliver. Browsing listings was difficult on the small screen, and many interactions like saving to a wish list were complicated to carry out. Also, we couldn\u2019t display all the details we needed for hosts and guests to make informed decisions, and trying to do so would inevitably result in information overload.\n\nOne of our Core Values here at Airbnb is Simplify. We were reminded to apply this lesson to the Watch app by Marco Arment\u2019s Redesigning Overcast\u2019s Apple Watch app, where he goes into great detail about flattening unnecessarily complicated navigation layers. We stopped trying to port the iOS app, and started from scratch. This time, we had one very specific goal in mind: deliver the best-possible messaging experience.\n\nThe Apple Watch is very much a secondary device; a second screen, if you will, of the iPhone. Naturally, it follows that our Watch app should be just that: a lightweight extension of the Airbnb iOS app. We needed to capitalize on a targeted and important part of the Airbnb experience that embodied the on-the-go nature of the Apple Watch, and messaging fit that description perfectly. The Airbnb Apple Watch app we\u2019re launching today finally began to take its form.\n\nWorking with WatchKit is an interesting juggle of trade-offs. Because watchOS 1 apps are inherently tethered to the main app on the iPhone, there are many things to take note when developing within the framework of the WatchKit extension. More than ever, iOS developers at all levels need to be cognizant of the effects of their code on performance and battery usage.\n\nA question that kept coming up is the following: should a non-trivial computation like a network request be performed on the WatchKit extension on the iPhone, or should the Watch wake the parent iOS app in the background and ask it to perform the computation?\n\nThe answer depends on the structure of the existing code. If complicated business logic is already baked into the iOS app, then the easiest option would be to invoke openParentApplication: on WKInterfaceController, so that code wouldn\u2019t need to be duplicated in the Watch app. This triggers handleWatchKitExtensionRequest: in the parent app, which returns a dictionary containing the result of the desired computation to the WatchKit extension. Note that because the result needs to be serialized and transferred via Bluetooth, all of the values it contains must conform to NSCoding.\n\nIn our case, because we already had a separate framework within the app, AirbnbAPI, that encapsulates the networking layer, we were able to simply import this into the WatchKit extension, and perform everything within it, without having to constantly wake the iOS app. This reduces the communication overhead between the extension and its parent, and results in cleaner code.\n\nWhen deciding who should be responsible for performing a computation, it is also important to keep in mind the complexity and duration of the computation. The more complex it is, the more the Watch should defer to the iPhone\u2019s processing power. Today on watchOS 1, the performance difference is generally not perceptible. However, on the upcoming watchOS 2, one of the biggest changes is that the WatchKit extension will be moved off the iPhone and onto the Watch. As a result, this distinction will become much clearer, because code in the extension is executed directly on the Watch. Now, would it be faster for the iPhone to carry out the request, then send the result back to the Watch through Bluetooth or for the Watch to perform it by itself and avoid the communication overhead altogether?\n\nThe simplicity and laser-focus of the Apple Watch as a platform make it both a joy and a challenge to develop for at the same time. The small screen size means that not a whole lot of content can live on each screen, so a great amount of care needs to exercised when designing for it. Before embarking on this journey, all Apple Watch developers and designers should ask themselves questions like:\n\nThe effort, time, and deliberation put into seemingly small decisions like these would pay back dividends at the later stages of development. We learned that the more thought we put into determining what really matters, the less time our hosts and guests would waste navigating the Watch app, and the more frustration they will avoid. One of the main yardsticks for an amazing Watch app is how quickly someone would be able to accomplish the desired task, from raising their wrist, to lowering it back down.\n\nThe Airbnb Apple Watch app was not designed to replace or mirror the main iOS app. It is a messaging hub \u2014 a means to help hosts respond to their guests more quickly, and for guests to get notified of important events. With rich, interactive notifications, hosts can accept a booking request right from their wrist:\n\nA guest can then respond to a host\u2019s welcome message the second they receive that notification on their Apple Watch, using the built-in Dictation feature:\n\nWe went one step further, and integrated the Watch app with iOS by adding a Settings-Watch.bundle to the main iOS app target in Xcode (Apple Watch Programming Guide), which enables hosts and guests to enter pre-recorded, quick responses in the Apple Watch settings app on their iPhone. These responses will then show up every time a \u201cReply\u201d button is tapped on the Watch, allowing for one-tap responses. We hope that this will help hosts who find themselves repeatedly typing in responses to common questions from their guests, like \u201cWhat\u2019s the wifi password?\u201c.\n\nAt Airbnb, we really believe that Every Frame Matters. Because the Watch is still a young platform, we felt that many people are still not fully clear what the apps on their Watch can and cannot do. We wanted to make sure our hosts and guests knew how the new Watch app can help them, so we built a first-time user experience for the Watch; something we haven\u2019t seen done before.\n\nThe designers on the Watch team, Britt Nelson, Salih Abdul-Karim, and Helen Tseng, produced a handful of beautifully illustrated and animated sequences that tell people what the Watch app does, and why we think it\u2019s a compelling addition to their Airbnb experience. The animated illustrations are intentionally bright and light-hearted to set a friendly and approachable tone for the app.\n\nWe\u2019re extremely excited to see the Airbnb Apple Watch app go live to our hosts and guests worldwide today. It was an incredibly fun challenge over the past few months, tackling a platform where common practices haven\u2019t really been laid out and boundaries haven\u2019t been fully explored.\n\nThe Watch app began as a pet project, and it slowly evolved into a first-class citizen of the Airbnb mobile experience, something the whole team is very proud of. Moving forward, we hope that the focus, discipline, and attention to detail we put into creating this app would not only inspire other developers to do the same, but also motivate ourselves as we continue to improve the Airbnb experience."
    },
    {
        "url": "https://medium.com/airbnb-engineering/life-of-a-ticket-903790ed5d53",
        "title": "Life of a Ticket \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "At the end of 2012, a cumulative total of over 4 million guests had stayed on Airbnb.\n\nThis was huge! It took us nearly four years to get our first million total guests \u2014 now over 2 million guests stay on Airbnb every month. With this growth comes the challenge of scaling marketplace operations which is the task that my team aims to improve.\n\nI work on a team called Internal Products which is comprised of two areas, Product Excellence and Service Excellence. We\u2019re tasked with building tools to improve the ability for our guests and hosts to help themselves, as well as increase the productivity of our Customer Experience specialists.\n\nAs Yishan Wong says, \u201cyour operating efficiency \u2026 [is] directly impacted by the ingenuity of your internal products.\u201d At Airbnb, we want to offer the best customer service in the world, so we take this very seriously and have an entire team dedicated to it.\n\nOur team gets to spend its time directly with its customers, the Customer Experience (CX) team, understanding their needs and fixing problems. Our team has a tight feedback loop with our CX specialists which results not only building tools that are tailored to their needs, but improving by iterating quickly.\n\nOne of the clear issues our team uncovered \u2014 was how to speed up processing a customer support ticket. The best way to share our solution is to show you our approach to the problem by examining the lifespan of a ticket.\n\nThere are 3 channels to get in touch with a specialist at Airbnb:\n\nAfter calling, starting a chat or sending the email, a ticket will be created with us. This ticket eventually gets assigned to an Airbnb CX specialist, where the guest/host and the specialist correspond. When the issue is resolved, the specialist solves the ticket, and the case is closed.\n\nOur team was put in charge of improving two areas while working on the ticket flow:\n\nLet\u2019s rewind to 2012 for a moment. At the time, when a person needed to contact us, they would simply select an issue on the contact page, fill out the form and submit. We aim to be accessible, but we want to make sure we prioritize the most urgent requests. In order to do this we needed to give certain people and issues priority, and reduce the priority of issues that could be handled by people themselves. We needed an application that would enable our content team to make the necessary changes, rather than needing engineers every time we had a product update or workflow change.\n\nWe set to turn our contact page into a troubleshooting wizard; something that could provide custom content depending on the issue you choose, as well as add robust tagging and routing in the event a ticket needed to be created.\n\nFor example, if someone contacts us about how to upload their profile picture, we could display some FAQs on how to upload their profile picture, provide a link to the upload page and if this still didn\u2019t help, give them the option to contact Airbnb.\n\nAnd you could go even deeper by displaying content based on the person\u2019s platform, whether they are a host or a guest, whether they have a reservation within the next 3 days, etc.\n\nOur team and I built the necessary tools for our content team so they can personalize each issue where we would ask the right question, display the right help content, put you in touch with a CX specialist. We call it the Contact Page Editor and it looks like this:\n\nThe left section of this screenshot shows what the Contact Page Editor looks like (admin side) whereas the right section shows the same content but on the contact page.\n\nThe backend relies on a simple decision tree that contains different types of nodes. There are three types of nodes:\n\nBy asking the right kind of question to our guest/host, each and every issue becomes more personalized, which helps speed up person/CX specialist interaction since there would be less back and forth.\n\nBy building a flow of custom and more personalized content on the contact page, we were able to help 15% more people in 2013.\n\nThe admin console is our Customer Experience tool tailored to our Customer Experience specialists needs. We have observed and analyzed the work being done by our specialists to design a tool that will not only automate tasks that a computer can do faster, but also surface the information that is needed only when it\u2019s needed.\n\nThe admin console is a Rails application with a very heavy front-end. We started the project with 2 engineers. The goal here was to build a tool to boost productivity in solving tickets coming from the contact page for our Customer Experience team.\n\nI was in charge of the front-end and at the time we were only using Backbone. Since the admin console is very dynamic, the content changes constantly, I decided to use knockout.js for data-binding, thus letting us not manipulate the DOM directly with a library like jQuery, but allowing the use of data-binding for DOM manipulation and so removing extra complexity by having only one single source of truth in the code. The code snippets below show an example of how a typical Backbone/Knockout component was built.\n\nOver time, we added more functionality to the admin console and it didn\u2019t scale very well. The pages got slower and the code harder to read. It was time to move away from Backbone/Knockout and look for other alternatives \u2026 and React came along. The application became much faster, more responsive and cleaner thanks to its Shadow DOM and the combination of the template and javascript in a single file. As for the engineers, they have a more enjoyable developer experience, less prone to have buggy code, a cleaner architecture (eq. less code duplication, readability, modularity) and can work faster because React makes your rethink how to write your code.\n\nThe code below shows the same Backbone/Knockout component from above written in ES6 syntax and built with React and flux.\n\nOur team was the first one to experiment with React. We used it in the admin console and in the Resolution Center. This successful experiment of React on the Resolution Center gave our front-end team the confidence to make React part of the front-end stack at Airbnb. We still have some Backbone code for routing, models, and collections in older apps, but we are moving away from it. New applications are being written with Alt for unidirectional data flow and react-router for routing.\n\nOur team is a great playground for any engineer who wants to experiment with new technologies, since we are building products for our friends and fellow employees. Due to the interplay of teams, our roadmap can be more flexible than other groups. One weekend in 2012, I decided to build an internal chat, called Tin Can. I built it with with node.js, websockets and redis. All new technologies that I had never used. I presented it to the team the following Monday and shipped it a week later after feedback. Tin Can is now used by all Customer Experience agents and processes 70k messages a day, improving our specialists productivity every day. We have since rewritten the code to scale the product but this is a great example of quickly an idea can become a product on our team.\n\nThese are only two of the many projects my team has been working on to improve our customers\u2019 experience. We help people get in touch with an Airbnb specialist when needed, but we also improve our specialists\u2019 ability to help our guests and hosts by building tools that are faster, more thoughtful, and more intuitive. All of which is done with fun and colored pants in the pony lounge."
    },
    {
        "url": "https://medium.com/airbnb-engineering/anomaly-detection-for-airbnb-s-payment-platform-e3b0ec513199",
        "title": "Anomaly Detection for Airbnb\u2019s Payment Platform \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "With hosts and guests around the globe, Airbnb aspires to provide a frictionless payments experience where our guests can pay in their local currency via a familiar payment method, and our hosts can receive money via convenient means in their preferred currency. For example, in Brazil the currency is Brazilian Real, and people are familiar with Boleto as a payment method. These are quite different than what we use in the US, and imagine this problem spread across the 190 countries Airbnb serves.\n\nIn order to achieve this, our Payments team has built a world-class payments platform that is secure and easy to use. The team\u2019s responsibilities include support of guest payments and host payouts, new payment experiences like gift cards, and assisting in financial reconciliation, to name a few.\n\nBecause Airbnb operates in 190 countries, we support a great number of currencies and processors. Most of the time, our system functions without incident, but we do encounter some hiccups where a certain currency cannot be processed or a certain payment gateway is inaccessible. In order to catch these interruptions as quickly as possible, the Data Science team has built an anomaly detection system that can identify problems in real time as they develop. This helps the product team detect issues and opportunities quickly while freeing up data scientists\u2019 time to work on A/B testing (new payment methods, new product launches), statistical analysis (impact of price on bookings, forecasting), and building machine learning models to personalize user experience.\n\nTo give you a look at the anomaly detection tool we built to detect outliers in payments dataset, in this blog I will use a few different sets of mock data to demonstrate how the model works. I will pretend to run an ecommerce shop in the summer of 2020 that sells three hackers items: Monitors, Keyboards, and Mouses. Also I have two suppliers: Lima and Hackberry.\n\nThe primary objective for the anomaly detection system is to find outliers in our time series dataset. Sometimes, a high level outlook would be sufficient, but most of the time we need to cut the data to decipher underlying trends. Let\u2019s consider the case below where we are monitoring Monitors\u2019 imports.\n\nThis overall numbers for Monitors look quite normal. We then take a look at the imports of Monitors by our two different suppliers: Lima and Hackberry.\n\nHere we can see, Lima, our major supplier for Monitors, was not delivering the expected amount on the 18th of August, 2020 for around 3 days. We automatically fell back on our secondary supplier, Hackberry, during this time. If we only look at the high level data, we wouldn\u2019t have detected the issue, but looking a few levels deeper provides us the information to target the right problem.\n\nAn intuitive model is to run a simple Ordinary Least Square regression with dummy variables indicating day of the week. The model takes the following form:\n\nwhere y is the amount we want to track, t is the time variable, I_day_i is the indicator variable that denotes if today is the ith day of the week, and e is the error term. This model is pretty simple, and it generally does a good job identifying the trend. However, there are several drawbacks:\n\nEven though we can observe the pattern of the metrics we want to track, and manually change the form of the model (i.e. we can add additional dummy variables when observing a strong monthly or yearly seasonality), the process is not scalable. An automated way to identify seasonality helps us to avoid our own bias and enables us to use this technique in data sets beyond payments.\n\nWhen building a model about a time series with both trend and seasonality, it\u2019s a common practice to build a model that takes the following form:\n\nwhere Y is the metric; S represents the seasonality; T represents the trend; e is the error term. For example, in our simple regression model, S is represented by the summation of the indicator functions, and T is represented by at+b.\n\nIn this section, we will develop new methods to detect trend and seasonality, incorporating the knowledge we gained from the previous section. We will use the sales of the two imaginary products, Keyboards and Mouses, to demonstrate how the model works. The two products\u2019 sales values are depicted in the following graph:\n\nAs shown above, Keyboards are the major product initiated in September 2016, and Mouses gets introduced in Aug 2017. We will model the seasonalities and trends, and try to find anomalies where the error terms are too far from the average.\n\nTo detect the seasonality, we will use Fast Fourier Transform (FFT). In simple linear regression model, we had assumed a weekly seasonality. As we can see above, there is no strong weekly pattern in Mouses, so blindly assuming such a pattern can hurt the model because of unnecessary dummy variables. In general, FFT is a good tool to detect seasonality if we have a good amount of historical data. It is very good at detecting seasonal patterns. After applying FFT to both time series, we get the following graph:\n\nwhere season_day is the period for the cosine wave of the transformation. In FFT, we usually only select periods with peak amplitudes to represent the seasonality and treat all other periods as noise. In this case, for Keyboards, we see two big peaks at 7 and 3.5 and another two smaller peaks at 45 and 60. For Mouses, we see a significant peak at 7 day, and some smaller peaks at 35, 60, and 80. The seasonalities of Keyboards and Mouses generated from the FFT are represented in the following graph:\n\nAs we can see, the amplitude of the seasonality for Keyboards grows with time, and it contains a major weekly seasonality, whereas the amplitude of Mouses shows a significant seasonality both in weekly trend and in a 40 day period.\n\nHere we will use the rolling median as a trend of a time series. The assumption here is that the growth is not very significant in a very short period of time. For example, for a certain day, we will use the previous 7-day rolling median as the trend level for that day. The advantage of using the median instead of the mean is having better stability in case of outliers. For example, if we have a sudden increase of 10x value for a day or two, that will not affect the trend if we look at median. However, it will affect our trend if we use mean. In this demonstration, we use 14 day median as our trend, shown in the graph below:\n\nAfter getting the seasonality and trend, we are going to evaluate the error term. We will use the error terms to determine if we have an anomaly in the time series dataset. When we combine the trend and the seasonality together, and subtract it from the original sales data, we get the error term. We plot the error term in the following graph:\n\nAs we can see, we have some spikes in the error term which represent anomalies in the time series data. Depending on how many false positives we can tolerate, we can choose how many standard deviations away from 0 we allow. Here we will use 4 standard deviations to get a reasonable amount or alerts.\n\nAs we see above, the alert system does a pretty good job identifying most of the spikes in the error term, which corresponds with some anomalies in the data. Note that some of the anomalies we have detected is not abnormal at all to human eyes, but they are actually real anomalies because of the seasonality pattern.\n\nOverall, based on our internal tests, this model performs well in identifying the anomalies while making minimal assumptions about the data.\n\nHopefully this blog post provides some insights on how to build a model to detect the anomalies. Most anomaly detection models involves modeling seasonality and trend. One key element of modeling is to make as few assumptions as possible. This will make the model general enough to suit lot more situations. However, if some assumptions greatly simplifies your modeling process, don\u2019t shy away from adopting those assumptions."
    },
    {
        "url": "https://medium.com/airbnb-engineering/how-marketplace-search-differs-from-traditional-search-c7bb533b6680",
        "title": "How Marketplace Search Differs from Traditional Search",
        "text": "A marketplace search is when you go to a specific website, such as Airbnb or Amazon, to hunt for a particular product, service or, in Airbnb\u2019s case, property. As Airbnb engineering manager Surabhi Gupta explained in her recent OpenAir 2015 talk, there are three main reasons why marketplace search is more challenging for engineers than traditional search (such as on Google or Bing):\n\nWith traditional search, a user types a keyword or phrase, reviews the results, then clicks on the most relevant result. With a marketplace search on Airbnb, other steps sometimes must happen before a user \u2018converts\u2019 (makes a booking).\n\nFor example, a user may identify a listing she likes but wants to ask the host questions. The host can respond to the questions and accept the reservation; answer the questions but reject the booking; or not get the chance to answer the questions, and the guest moves on to another listing.\n\nAirbnb\u2019s search ranking uses machine learning to try and predict a final booking outcome, in order to help the guest easily find the best listing. To accomplish this, the Airbnb search team modeled the five intermediate states it cares most about: Impression (the displayed search results); Clicks (did the user like a result enough to click on it?); Accept (the host accepts the guest\u2019s reservation); Reject (the dates don\u2019t work out); and Booking.\n\nEach state is given a score based on past user actions. By serving up search results influenced by how likely the user is to book the properties displayed, Airbnb has achieved \u201ca huge booking gain,\u201d Gupta said.\n\nBecause Airbnb properties are unique, users need a lot of context to decide whether a listing is right for them. Airbnb\u2019s engineering team has experimented with different ways of presenting information to users. For example, because location and price are the two most important requirements, Airbnb has given users information about neighborhood characteristics displayed on a city map as well as price histograms.\n\nAirbnb listings are \u201cperishable\u201d because they are unique properties whose availability comes and goes. To prevent users from reading about a property only to discover it\u2019s not available when they want it, Airbnb built a real-time information infrastructure using MySQL databases, a centralized index, and Ruby on Rails. (Jump to 15:53 in the video for more details.)\n\nGoing forward, Airbnb wants to make it easier for users to pick up exactly where they left off when returning to the site; add more personalization options; and obtain a deeper understanding of its listings in order to give guests \u201cthe best possible experience\u201d when matching them with hosts."
    },
    {
        "url": "https://medium.com/airbnb-engineering/at-airbnb-data-science-belongs-everywhere-917250c6beba",
        "title": "At Airbnb, Data Science Belongs Everywhere \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Five years ago, I joined Airbnb as its first data scientist.\n\nAt that time, the few people who\u2019d even heard of the company were still figuring out how to pronounce its name, and the roughly 7 person team (depending on whether you counted that guy on the couch, the intern, and the barista at our favorite coffee shop) was still operating out of the founders\u2019 apartment in SOMA. Put simply, it was pretty early stage.\n\nBringing me on was a forward-looking move on the part of our founders. This was just prior to the big data craze and the conventional wisdom that data can be a defining competitive advantage. Back then, it was a lot more common to build a data team later in a company\u2019s lifecycle. But they were eager to learn and evolve as fast as possible, and I was attracted to the company\u2019s culture and mission. So even though we were a very small-data shop at the time, I decided to get involved.\n\nThere\u2019s a romanticism in Silicon Valley about the early days of a startup: you move fast, make foundational decisions, and any good idea could become the next big thing. From my perspective, that was all true.\n\nBack then we knew so little about the business that any insight was groundbreaking; data infrastructure was fast, stable, and real-time (I was querying our production mysql database); the company was so small that everyone was in the loop about every decision; and the data team (me) was aligned around a singular set of metrics and methodologies.\n\nBut five years and 43,000% growth later, things have gotten a bit more complicated. I\u2019m happy to say that we\u2019re also more sophisticated in the way we leverage data, and there\u2019s now a lot more of it. The trick has been to manage scale in a way that brings together the magic of those early days with the growing needs of the present \u2014 a challenge that I know we aren\u2019t alone in facing.\n\nSo I thought it might be worth pairing our posts on specific problems we\u2019re solving with an overview of the higher-level issues data teams encounter as companies grow, and how we at Airbnb have responded. This will mostly center around how to connect data science with other business functions, but I\u2019ll break it into three concepts \u2014 how we characterize data science, how it\u2019s involved in decision-making, and how we\u2019ve scaled it to reach all sides of Airbnb. I won\u2019t say that our solutions are perfect, but we do work every day to retain the excitement, culture, and impact of the early days.\n\nThe foundation upon which a data science team rests is the culture and perception of data elsewhere in the organization, so defining how we think about data has been a prerequisite to ingraining data science in business functions.\n\nIn the past, data was often referenced in cold, numeric terms. It was construed purely as a measurement tool, which paints data scientists as Spock-like characters expected to have statistics memorized and available upon request. Interactions with us would therefore tend to come in the form of a request for a fact: how many listings do we have in Paris?What are the top 10 destinations in Italy?\n\nWhile answering questions and measuring things is certainly part of the job, at Airbnb we characterize data in a more human light: it\u2019s the voice of our customers. A datum is a record of an action or event, which in most cases reflects a decision made by a person. If you can recreate the sequence of events leading up to that decision, you can learn from it; it\u2019s an indirect way of the person telling you what they like and don\u2019t like \u2014 this property is more attractive than that one, I find these features useful but those.. not so much.\n\nThis sort of feedback can be a goldmine for decisions about community growth, product development, and resource prioritization. But only if you can decipher it. Thus, data science is an act of interpretation \u2014 we translate the customer\u2019s \u2018voice\u2019 into a language more suitable for decision-making.\n\nThis idea resonates at Airbnb because listening to guests and hosts is core to our culture. Since the early days, our team has met with community members to understand how to make our product better suit their needs. We still do this, but the scale of the community is now beyond the point where it\u2019s feasible to connect with everyone everywhere.\n\nSo, data has become an ally. We use statistics to understand individual experiences and aggregate those experiences to identify trends across the community; those trends inform decisions about where to drive the business.\n\nOver time, our colleagues on other teams have come to understand that the data team isn\u2019t a bunch of Vulcans, but rather that we represent the very human voices of our customers. This has paved the way for changes to the structure of data science at Airbnb.\n\nA good data scientist is therefore able to get in the mind of people who use our product and understand their needs. But if they\u2019re alone in a forest with no one to act on the insight they uncovered, what difference does it make?\n\nOur distinction between good and great is impact \u2014 using insights to influence decisions and ensuring that the decisions had the intended effect. While this may seem obvious, it doesn\u2019t happen naturally \u2014 when data scientists are pressed for time, they have a tendency to toss the results of an analysis \u2018over the wall\u2019 and then move on to the next problem. This isn\u2019t because they don\u2019t want to see the project through, but with so much energy invested into understanding the data, ensuring statistical methods are rigorous, and making sure results are interpreted correctly, the communication of their work can feel like a trivial afterthought.\n\nBut when decision-makers don\u2019t understand the ramifications of an insight, they don\u2019t act on it. When they don\u2019t act on it, the value of the insight is lost.\n\nThe solution, we think, is connecting data scientists as tightly as possible with decision-makers. In some cases, this happens naturally; for example when we develop data products (more on this in a future post). But there\u2019s also a strong belief in cross-functional collaboration at Airbnb, which brings up questions about how to structure the team within the broader organization.\n\nA lot has been written about the pros and cons of centralized and embedded data science teams, so I won\u2019t focus on that. But suffice to say we\u2019ve landed on a hybrid of the two.\n\nWe began with the centralized model, tempted by its offering of opportunities to learn from each other and stay aligned on metrics, methodologies, and knowledge of past work. While this was all true, we\u2019re ultimately in the business of decision-making, and found we couldn\u2019t do this successfully when silo\u2019d: partner teams didn\u2019t fully understand how to interact with us, and the data scientists on our team didn\u2019t have the full context of what they were meant to solve or how to make it actionable. Over time we became viewed as a resource and, as a result, our work became reactive \u2014 responding to requests for statistics rather than being able to think proactively about future opportunities.\n\nSo we made the decision to move from a fully-centralized arrangement to a hybrid centralized/embedded structure: we still follow the centralized model, in that we have a singular data science team where our careers unfold, but we have broken this into sub-teams that partner more directly with engineers, designers, product managers, marketers, and others. Doing so has accelerated the adoption of data throughout the company, and has elevated data scientists from reactive stats-gatherers to proactive partners. And by not fully shifting toward an embedded model we\u2019re able to maintain a vantage point over every piece of the business, allowing us to form a neural core that can help all sides of the company learn from one another.\n\nStructure is a big step toward empowering impactful data science, but it isn\u2019t the full story. Once situated within a team that can take action against an insight, the question becomes how and when to leverage the community\u2019s voice for business decisions.\n\nThrough our partnership with all sides of the company, we\u2019ve encountered many perspectives on how to integrate data into a project. Some people are naturally curious and like to begin by understanding the context of the problem they\u2019re facing. Others view data as a reflection of the past and therefore a weaker guide for planning; but these folks tend to focus more on measuring the impact of their gut-driven decisions.\n\nBoth perspectives are fair. Being completely data-driven can lead to optimizing toward a local maximum; finding a global maximum requires shocking the system from time to time. But they reflect different points where data can be leveraged in a project\u2019s lifecycle.\n\nOver time, we\u2019ve identified four stages of the decision-making process that benefit from different elements of data science:\n\nSometimes a step is fairly straightforward, for example if the context of the problem is obvious \u2014 the fact that we should build a mobile app doesn\u2019t necessitate a heavy synopsis upfront. But the more disciplined we\u2019ve become about following each step sequentially, the more impactful everyone at Airbnb has become. This makes sense because, ultimately, this process pushes us to solve problems relevant to the community in a way that addresses their needs.\n\nThe above model is great when data scientists have sufficient bandwidth. But the reality of a hypergrowth startup is that the scale and speed at which decisions need to be made will inevitably outpace the growth of the data science team.\n\nThis became especially clear in 2011 when Airbnb exploded internationally. Early in the year, we were still a small company based entirely in SF, meaning our army of three data scientists could effectively partner with everyone.\n\nSix months later, we opened over 10 international offices simultaneously, while also expanding our product, marketing, and customer support teams. Our ability to partner directly with every employee suddenly, and irrevocably, disappeared.\n\nJust as it became impossible to meet every new member of the community, it was now also impossible to meet and work with every employee. We needed to find a way to democratize our work, broadening from individual interactions, to empowering teams, the company, and even our community.\n\nDoing this successfully requires becoming more efficient and effective, mostly through investment in the technology surrounding data. Here are some examples of how we\u2019ve approached each level of scale:\n\nScaling a data science team to a company in hypergrowth isn\u2019t easy. But it is possible. Especially if everyone agrees that it\u2019s not just a nice part of the company, it\u2019s an essential part of the company.\n\nFive years in, we\u2019ve learned a lot. We\u2019ve improved how we leverage the data we collect; how we interact with decision-makers; and how we democratize this ability out to the company. But to what extent has all of this work been successful?\n\nMeasuring the impact of a data science team is ironically difficult, but one signal is that there\u2019s now a unanimous desire to consult data for decisions that need to be made by technical and non-technical people alike. Our team members are seen as partners in the decision-making process, not just reactive stats-gatherers.\n\nAnother is that our increasing ability to distill the causal impact of our work has helped us wrestle the train away from the monkey. This has been trickier than one might expect because Airbnb\u2019s ecosystem is complicated \u2014 a two-sided marketplace with network effects, strong seasonality, infrequent transactions, and long time horizons \u2014 but these challenges make the work more exciting. And as much as we\u2019ve accomplished over the last few years, I think we\u2019re still just scratching the surface of our potential.\n\nWe\u2019re at a point where our infrastructure is stable, our tools are sophisticated, and our warehouse is clean and reliable. We\u2019re ready to take on exciting new problems. On the immediate horizon we look forward to shifting from batch to realtime processing; developing a more robust anomaly detection system; deepening our understanding of network effects; and increasing our sophistication around matching and personalization.\n\nBut these ideas are just the beginning. Data is the (aggregated) voice of our customers. And wherever we go next\u2013wherever we belong next\u2013will be driven by those voices."
    },
    {
        "url": "https://medium.com/airbnb-engineering/recap-of-openair-d26da4562df5",
        "title": "Recap of OpenAir \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Three weeks ago we hosted OpenAir 2015, our second technology conference. We had an amazing turnout of bright minds from across the industry, more than doubling attendance from 2014.\n\nA new generation of companies are emerging whose customers aren\u2019t judging them by their apps and websites but on the experiences and content the products connect them with. With that in mind the theme for OpenAir 2015 was scaling human connection and we focused on online to offline and the better matching that enables it.\n\nThroughout the day we learned how Instagram helps their users discover new content that inspires them; how Stripe helps people transact across borders, how LinkedIn used data to power their social network, how Periscope came to life on Android, and of course, how Airbnb helps turns strangers into friends.\n\nBehind all of these challenges there are central concepts that we as a tech industry need to understand better \u2014 trust, personalization and the data that enables both.\n\nWith that in mind, Airbnb open-sourced two new tools for wrangling data. The first is called Airflow which is a sophisticated tool to programmatically author, schedule and monitor data pipelines. People in the industry will know this work as ETL engineering. The second was Aerosolve. Aerosolve is a machine learning package for Apache Spark. It\u2019s designed to combine high capacity to learn with an accessible workflow that encourages iteration and deep understanding of underlying patterns on a human level. Since we launched these tools they have gotten over 2000 stars on GitHub \u2014 we can\u2019t wait to see how people use and contribute to them.\n\nWe also announced a new tool for our hosts called Price Tips, which is powered by Aerosolve. Price Tips creates ongoing tips for our hosts on how to price their listing, not just for one day, but for each day of the year. This pricing is fully dynamic \u2014 it takes into account demand, location, travel trends, amenities, type of home and much more. There are hundreds of signals that go into the model to produce each price tip. We believe that better pricing will be a great way to further empower our hosts to meet their personal goals through hosting.\n\nFinally we closed out the opening keynote morning with the launch of our brand new Gift Cards website. Now anyone in the US can give their family, friends, colleagues, frenemies, whomever, the gift of travel on Airbnb. And for those lucky folks in the audience, we gave everyone a $100 gift card.\n\nWe will be following up with more videos from the event, so keep your eyes on this space."
    },
    {
        "url": "https://medium.com/airbnb-engineering/designing-machine-learning-models-7d0048249e69",
        "title": "Designing Machine Learning Models \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "At Airbnb, we are focused on creating a place where people can belong anywhere. Part of that sense of belonging comes from trust amongst our users and knowing that their safety is our utmost concern.\n\nWhile the vast majority of our community is made up of friendly and trustworthy hosts and guests, there exists a tiny group of users who try to take advantage of our site. These are very rare occurrences, but nevertheless, this is where the Trust and Safety team comes in.\n\nThe Trust and Safety team deals with any type of fraud that might happen on our platform. It is our main objective to try to protect our users and the company from various types of risks. An example risk is chargebacks \u2014 a problem that most ecommerce companies are familiar with. To reduce the number of fraudulent actions, the Data Scientists within the Trust and Safety team build various Machine Learning models to help identify the different types of risks. For more information on the architecture behind our models, please refer to a previous blog post on Architecting A Machine Learning System For Risk.\n\nIn this post, I give a brief overview of the thought process that comes with building a Machine Learning model. Of course every model is different, but hopefully it will give readers an insight on how we use data in a Machine Learning application to help protect our users, and the different approaches we use to improve our models. For this blog post, suppose we want to build a model to predict if certain fictional characters are evil*.\n\nThe most fundamental question in model building is determining what you would like the model to predict. I know this sounds silly, but often times, this question alone raises other deeper questions.\n\nEven a seemingly straightforward character classification model can raise many questions as we think more deeply about the kind of model to build. For example, what do we want this model to score: just newly introduced characters or all characters? If the former, how far into the introduction do we want to score the characters? If the latter, how often do we want to score these characters?\n\nA first thought might be to build a model that scores each character upon introduction. However, with such a model, we would not be able to track characters\u2019 scores over time. Furthermore, we could be missing out on potentially evil characters that might have \u201cgood\u201d characteristics at the time of introduction.\n\nWe could instead build a model that scores a character every time he/she appears in the plot. This would allow us to study the scores over time and detect anything unusual. But, given that there might not be any character development in every single appearance, this may not be the most practical route to pursue.\n\nAfter much consideration, we might decide on a model design that falls in between these two initial ideas i.e. build a model that scores each character each time something significant happened such as gathering of new allies, possessions of dragons, etc. This way, we would still be able to track the characters\u2019 scores over time without unnecessarily scoring those with no recent development.\n\nSince our objective is to analyze scores over time, our training data set needs to reflect characters\u2019 activities across a period of time. The resulting training data set will look similar to the following:\n\nThe periods associated with each character are not necessarily consecutive since we are only interested in days where there exist significant developments.\n\nIn this instance, Jarden has significant character developments on 3 different occasions and is constantly growing his army over time. Dineas has significant character developments on 5 different occasions and is responsible for 4 dragons mid-plot.\n\nOften with Machine Learning models, it is necessary to down-sample the number of observations. The sampling process itself can be quite straightforward i.e. once one has the desired training data set, one can do a row-based sampling on the population.\n\nHowever, because the model described herein is dealing with multiple periods per character, row-based sampling might result in scenarios where the occasions pertaining to a character get split between the data for model build and the validation data. The table below shows an example of such scenario:\n\nThis is not ideal because we are not getting a holistic picture of each character and those missing observations could be crucial to building a good model.\n\nFor this reason, we need to do character-based sampling. This will ensure that either all of the occasions pertaining to a character get included in the model build data, or none at all.\n\nThe same logic applies when it comes time to splitting our data into training and validation sets.\n\nFeature engineering is an integral part of Machine Learning, and a good understanding of the data helps generate ideas on the types of features to engineer for a better model. Examples of feature engineering include feature normalization and treatment of categorical features.\n\nFeature normalization is a way to standardize features that allows for more sensible comparisons. Let\u2019s take the table below as an example:\n\nBoth characters have 10,000 soldiers. However, Serion has been in power for 5 years, while Dineas has only been in power for 2 years. Comparing the absolute number of soldiers across these characters might not have been very useful. However, normalizing them with the characters\u2019 years in power could provide better insights and produce a more predictive feature.\n\nFeature engineering on categorical features probably deserves a separate blog post due to the many different ways to deal with them. In particular for missing values imputation, please take a look at a previous blog post on Overcoming Missing Values in a Random Forest Classifier.\n\nThe most common approach for transforming categorical features is vectorizing (also known as one-hot encoding). However, when dealing with many categorical features with many different levels, it is more practical to use conditional-probability coding (CP-coding).\n\nThe basic idea of CP-coding is to compute the probability of an event occurring given a categorical level. This method allows us to project all levels of a categorical feature into a single numerical variable.\n\nHowever, this type of transformation may result in noisy values for levels that are not represented well. In the example above, we only have one observation from the House of Tallight. As a result, the corresponding probability is either 0 or 1. To get around this issue and to reduce the noise in general, one can adjust how the probabilities are computed by taking into account the weighted average, the global probability, as well as introduce a smoothing hyperparameter.\n\nSo, which method is better? It depends on the number and levels of the categorical features. CP-coding is good because it reduces the dimensionality of the feature, but by doing so, we are sacrificing information on feature-to-feature interactions, which is something that vectorizing retains. Alternatively, we could integrate both methods i.e. combine the categorical features of interest, and then performing CP-coding on the interacted features.\n\nWhen it comes time to evaluate model performance, we need to be mindful about the proportion of good/evil characters. With our example model, the data is aggregated at [character*period] level (left table below).\n\nHowever, the model performance should be measured at character level (right table below).\n\nAs a result, the proportion of good/evil characters between the model build and model performance data is significantly different. It is crucial that one assigns proper weights when evaluating a model\u2019s precision and recall.\n\nAdditionally, because we would likely have down-sampled the number of observations, we need to rescale the model\u2019s precision and recall to account for the sampling process.\n\nThe two main performance metrics for model evaluation are Precision and Recall. In our example, precision is the proportion of evil characters the model is able to predict correctly. It measures the accuracy of the model at a given threshold. Recall, on the other hand, is the proportion of evil characters the model is able to detect. It measures how comprehensive the model is at identifying evil characters at a given threshold. This can be confusing, so I\u2019ve broken it down in the table below to illustrate the difference:\n\nIt is often helpful to classify the numbers into the 4 different bins:\n\nPrecision is measured by calculating: Out of the characters predicted to be evil, how many did the model identify correctly i.e. TP / (TP + FP)?\n\nRecall is measured by calculating: Out of all evil characters, how many are predicted by the model i.e. TP / (TP + FN)?\n\nObserve that even though the numerator is the same, the denominator is referring to different sub-populations.\n\nThere is always a trade-off between choosing high precision vs. high recall. Depending on the purpose of the model, one might choose higher precision over higher recall. However, for fraud prediction models, higher recall is generally preferred even if some precision is sacrificed.\n\nThere are many ways one can improve model\u2019s precision and recall. These include adding better features, optimizing pruning of trees and building a bigger forest to name a few. However, given how extensive this discussion can be, I will leave it for a separate blog post.\n\nHopefully, this blog post has given readers a glimpse of what building a Machine Learning model entails. Unfortunately, there is no one-size-fits-all solution for building a good model, but knowing the context of the data well is key because it translates into deriving more predictive features, and thus a better model.\n\nLastly, classifying characters as good or evil can be subjective, but labels are a really important part of machine learning and bad labeling usually results in a poor model. Happy Modeling!\n\n* This model assumes that each character is either born good or evil i.e. if they are born evil, then they are labeled as evil their entire lives. The model design will be completely different if we assume characters could cross labels mid-life."
    },
    {
        "url": "https://medium.com/airbnb-engineering/deeplinkdispatch-778bc2fd54b7",
        "title": "DeepLinkDispatch: a simple, annotation-based library for making deep link handling better on\u2026",
        "text": "Deep links provide a way to link to specific content on either a website or an application. These links are indexable and searchable, and can provide users direct access to much more relevant information than a typical home page or screen. In the mobile context, the links are URIs that link to specific locations in the application.\n\nAt Airbnb, we use these deep links frequently to link to listings, reservations, or search queries. For example, a typical deep link to a listing may look something like this:\n\nThis deep link directly bypasses the home screen in the application and opens listing information for the Mushroom Dome cabin. Other deep links lead to other non-content screens like sign up screens or informational screens on how the application works.\n\nAndroid supports deep links through declaration in the Manifest. You can add an intent filters which define a mapping between deep link schemas and Activities. Subsequently, any URI with the registered scheme, host, and path will open up that Activity in the app.\n\nWhile convenient for simple deep link usage, this traditional method becomes burdensome for more complicated applications. For example, you could use the intent filter to specify the path pattern, but it\u2019s somewhat limiting. You can\u2019t easily indicate the parameters that you would expect in the URI that you are filtering for. For complex deep links, you are likely to have to write a parsing mechanism to extract out the parameters, or worse, have such similar code distributed amongst many Activities.\n\nDeepLinkDispatch is designed to help developers handle deep links easily without having to write a lot of boilerplate code and allows you to supply more complicated parsing logic for deciding what to do with a deep link. You can simply annotate the Activity with a URI in a similar way to other libraries. Looking at the example deep link URI from above, you could annotate an activity like so, and declare an \u201cid\u201d parameter that you want the application to parse:\n\nAfter annotating a particular Activity with the deep link URI that the activity should handle, DeepLinkDispatch will route the deep link automatically and parse the parameters from the URI. You can then determine whether the intent was fired by a deep link and extract out the parameters declared in the annotation. Here\u2019s an example:\n\nAt its core, DeepLinkDispatch generates a simple Java class to act as a registry of what Activities are registered with which URIs, and what parameters should be extracted. DeepLinkDispatch also generates a shim Activity which tries to match any deep link with an entry in the registry\u2013 if it finds a match, it will extract the parameters and start the appropriate Activity with an Intent populated with the parameters.\n\nAdditionally, DeepLinkDispatch is intended to provide greater insight into deep link usage. Android by default does not give much insight into what deep links are being used nor what deep links are failing. DeepLinkDispatch provides callbacks in the Application class for any deep link call, either successful or unsuccessful, allowing developers to track and correct any problematic links firing at the application.\n\nAn example of such a callback would be:\n\nIn summary, use DeepLinkDispatch if you\u2019d like an easy way to manage deep links. Declaring deep links and parameters are simple with annotations, and it will handle the more complex parsing and routing to your Activities without a lot of extra code on your part. DeepLinkDispatch also gives you greater insight into how your deep links are used by providing simple callbacks on deep link events for you to tie into.\n\nFor more information take a look at our Github page here: https://github.com/airbnb/DeepLinkDispatch"
    },
    {
        "url": "https://medium.com/airbnb-engineering/aerosolve-machine-learning-for-humans-55efcf602665",
        "title": "Aerosolve: Machine learning for humans \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Have you ever wondered how Airbnb\u2019s price tips for hosts works?\n\nIn this dynamic pricing feature, we show hosts the probability of getting a booking (green for a higher chance, red for a lower chance), or predicted demand, and allow them to easily price their listings dynamically with a click of a button.\n\nMany features go into predicting the demand for a listing among them seasonality, unique features of a listing and price. These features interact in complex ways and can result in machine learning models that are difficult to interpret. So we went about building a package to produce machine learning models that facilitate interpretation and understanding. This is useful for us, developers, and also for our users; the interpretations map to explanations we provide to our hosts on why the demand they face may be higher or lower than they expect.\n\nWe have been operating on the belief that enabling humans to partner with a machine in a symbiotic way exceeds the capabilities of humans or machines alone.\n\nFrom the project\u2019s inception we have focused on improving the understanding of data\n\n sets by assisting people in interpreting complex data with easy to understand models. Instead of hiding meaning beneath many layers of model complexity, Aerosolve models expose data to the light of understanding.\n\nFor example, we are able to easily determine the negative correlation between the price of a listing in a market and the demand for the listing just by inspecting the image below. Rather than passing features through many deep hidden layers of non-linear transforms we make models very wide, with each variable or combinations of variables modeled explicitly using additive functions. This makes the model easy to interpret while still maintaining a lot of capacity to learn.\n\nThe red line encodes the general belief before looking at the data, or the prior. In this case we generally believe that the demand decreases with increasing price. We are able to inform the model of our prior beliefs in Aerosolve by adding them to a simple text configuration file during training. The black curve is the belief of the model after learning from billions of data points. It corrects any assumptions of the person working with the model with actual market data, while allowing human beings to feed back their initial beliefs about a variable.\n\nWe also took great care to model unique neighborhoods around the world by creating algorithms to automatically generate local neighborhoods based on where Airbnb listings are located. These differ from the hand made neighborhood polygons in two ways. Firstly, they are automatically generated so we are able to construct these quickly for new markets that just open up. Secondly, they are build in a hierarchical manner, so we are able to quickly accumulate statistics that are point like (e.g. listing views) or polygonal (e.g. search boxes) in a scalable way.\n\nThe hierarchy also lets us borrow statistical strength from parent neighborhoods as they fully contain the children neighborhoods. These Kd-tree constructed neighborhoods are not user visible but used to compute local features for the machine learning models. In the figure below, we demonstrate the ability of the Kd-tree structure to automatically create local neighborhoods. Notice the care we have taken in informing the algorithm that it should not cross large bodies of water. Even Treasure Island has a neighborhood of it\u2019s own. In order to not have sudden changes along a neighborhood boundary we take care to smooth the neighborhood information in a multi-scale manner. You can read more, and visually see, this kind of smoothing in the Image Impressionism demo of Aerosolve on Github.\n\nBecause every listing is unique in its own special way, we built image analysis algorithms into Aerosolve to account for the detail and loving care the hosts have put into decorating their homes. We trained the Aerosolve models on two kinds of training data. On the left we have trained the model on scores given by professional photographers and on the right the model was trained on organic bookings. The professional photographers tend to prefer pictures of ornate, brightly lit living rooms, while the guests seem to prefer warm colors and cozy bedrooms.\n\nWe take into account many other things in computing the demand, some of which include local events. For example in the image below we can detect increased demand for places to stay in Austin during the SXSW festival and could perhaps ask hosts of consider opening their homes during a high demand period.\n\nSome features, such as seasonal demand are naturally spiky. Other features, such as number of reviews, generally should not exhibit the same kind of spikiness. We smooth out these smoother features using cubic polynomial splines while preserving end point spikiness using Dirac delta functions. For example in the relationship between number of reviews and 3 stars (out of five), there is a big discontinuity between no reviews and one review.\n\nFinally, after all the feature transformations and smoothing, all this data is assembled into a pricing model with hundreds of thousands of interacting parameters to provide a dashboard for hosts to inform themselves on the probability of getting a booking at a given price.\n\nPlease check out Aerosolve on Github. There are some demos you can find on how to apply Aerosolve for your own modelling such as teaching the algorithm how to paint in the pointillism style of painting. There is also an income prediction demo based on US census data that you can check out as well."
    },
    {
        "url": "https://medium.com/airbnb-engineering/airflow-a-workflow-management-platform-46318b977fd8",
        "title": "Airflow: a workflow management platform \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "As people who work with data begin to automate their processes, they inevitably write batch jobs. These jobs need to run on a schedule, typically have a set of dependencies on other existing datasets, and have other jobs that depend on them. Throw a few data workers together for even a short amount of time and quickly you have a growing complex graph of computation batch jobs. Now if you consider a fast-paced, medium-sized data team for a few years on an evolving data infrastructure and you have a massively complex network of computation jobs on your hands. This complexity can become a significant burden for the data teams to manage, or even comprehend.\n\nThese networks of jobs are typically DAGs (directed acyclic graphs) and have the following properties:\n\nWorkflow management has become such a common need that most companies have multiple ways of creating and scheduling jobs internally. There\u2019s always the good old cron scheduler to get started, and many vendor packages ship with scheduling capabilities. The next step forward is to have scripts call other scripts, and that can work for a short period of time. Eventually simple frameworks emerge to solve problems like storing the status of jobs and dependencies.\n\nTypically these solutions grow reactively as a response to the increasing need to schedule individual jobs, and usually because current incarnation of the system doesn\u2019t allow for simple scaling. Also note that people who write data pipelines typically are not software engineers, and their mission and competencies are centered around processing and analyzing data, not building workflow management systems.\n\nConsidering that internally grown workflow management systems are often at least one generation behind the company\u2019s need, the friction around authoring, scheduling and troubleshooting jobs creates massive inefficiencies and frustrations that divert data workers off of their productive path.\n\nAfter reviewing the open source solutions, and leveraging Airbnb employees\u2019 insight about systems they had used in the past, we came to the conclusion that there wasn\u2019t anything in the market that met our current and future needs. We decided to build a modern system to solve this problem properly. As the project progressed in development, we realized that we had an amazing opportunity to give back to the open source community that we rely so heavily upon. Therefore, we have decided to open source the project under the Apache license.\n\nHere are some of the processes fueled by Airflow at Airbnb:\n\nMuch like English is the language of business, Python has firmly established itself as the language of data. Airflow is written in pythonesque Python from the ground up. The code base is extensible, documented, consistent, linted and has broad unit test coverage.\n\nPipeline authoring is also done in Python, which means dynamic pipeline generation from configuration files or any other source of metadata comes naturally. \u201cConfiguration as code\u201d is a principle we stand by for this purpose. While yaml or json job configuration would allow for any language to be used to generate Airflow pipelines, we felt that some fluidity gets lost in the translation. Being able to introspect code (ipython!, IDEs) subclass, meta-program and use import libraries to help write pipelines adds tremendous value. Note that it is still possible to author jobs in any language or markup, as long as you write Python that interprets these configurations.\n\nWhile you can get up and running with Airflow in just a few commands, the complete architecture has the following components:\n\nWhile Airflow comes fully loaded with ways to interact with commonly used systems like Hive, Presto, MySQL, HDFS, Postgres and S3, and allow you to trigger arbitrary scripts, the base modules have been designed to be extended very easily.\n\nHooks are defined as external systems abstraction and share a homogenous interface. Hooks use a centralized vault that abstracts host/port/login/password information and exposes methods to interact with these system.\n\nOperators leverage hooks to generate a certain type of task that become nodes in workflows when instantiated. All operators derive from BaseOperator and inherit a rich set of attributes and methods. There are 3 main types of operators:\n\nExecutors implement an interface that allow Airflow components (CLI, scheduler, web server) to run jobs jobs remotely. Airflow currently ships with a SequentialExecutor (for testing purposes), a threaded LocalExecutor, and a CeleryExecutor that leverages Celery, an excellent asynchronous task queue based on distributed message passing. We are also planning on sharing a YarnExecutor in the near future.\n\nWhile Airflow exposes a rich command line interface, the best way to monitor and interact with workflows is through the web user interface. You can easily visualize your pipelines dependencies, see how they progress, get easy access to logs, view the related code, trigger tasks, fix false positives/negatives, analyze where time is spent as well as getting a comprehensive view on at what time of the day different tasks usually finish. The UI is also a place where some administrative functions are exposed: managing connections, pools and pausing progress on specific DAGs."
    },
    {
        "url": "https://medium.com/airbnb-engineering/the-antidote-to-bureaucracy-is-good-judgment-1dd372e7387f",
        "title": "The Antidote to Bureaucracy is Good Judgment \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "This was originally posted on First Round Review.\n\nMike Curtis may not have the typical background of someone dedicated to vanquishing bureaucracy. AltaVista, AOL, Yahoo, Facebook \u2014 he\u2019s a veteran of some legendary Silicon Valley behemoths. Now VP of Engineering for Airbnb, he\u2019s at the helm of a small but rapidly growing team. With nearly two decades of innovation at tech giants under his belt, he\u2019s become an expert at chopping through red tape.\n\nAs it turns out, one simple lesson guides Curtis\u2019 approach to building effective teams: the antidote to unproductive bureaucracy is good old-fashioned judgment \u2014 having it, hiring for it, and creating conditions that allow people to exercise it. Armed with this truth, he\u2019s tackled the challenges of scaling a world-class engineering team at Airbnb, from taming the beast of expense reports to dramatically improving site stability. And he\u2019s done it by eliminating rules, not making them.\n\nAt First Round\u2019s recent CTO Summit, Curtis shared actionable tactics for what he calls \u201creplacing policy with principles\u201d that can guide fast, flexible growth and progress. Every startup looking to dodge a fate dictated by increasing structure and processes that inevitably slow you down can benefit from these tips.\n\nTo start, Curtis\u2019s definition of bureaucracy (when it comes to startups) isn\u2019t one you\u2019ll find in the dictionary:\n\nThe curious thing about organizations is that having more people somehow doesn\u2019t equal more output. \u201cAs size and complexity of an organization increases, productivity of individuals working in that organization tends to decrease,\u201d he says. As head count grows, so too does the policy-and-paperwork stuff that gets in the way of rapid iteration and scale.\n\nWhy is this the case? \u201cI think it comes down to human nature and the way we react to problems,\u201d Curtis says. Our natural response to any problem \u2014 from a downed server to a social gaffe \u2014 is to try to ensure that it doesn\u2019t happen again. In companies, more often that not, those solutions take the form of new policies. \u201cWhat happens when you create a new policy, of course, is that you have to fit it into all of your existing rules.\u201d And so begins a web of ever increasing complexity that\u2019s all about prevention. Soon, you start to hit safeguards no matter what it is you\u2019re trying to do.\n\nTo avoid this type of bureaucracy from the very beginning of your company, you should adopt two particular tactics: \u201cFirst, you have to build teams with good judgment, because you need to be able to put your trust in people,\u201d Curtis says. \u201cThen you shape that good judgment with strong principles.\u201d\n\nMinimizing rules that become roadblocks in your organization will only work if you\u2019ve built a team that will make good decisions in the absence of rigid structure. Your hiring process is where you can take the biggest strides toward preventing bureaucracy.\n\nNo company will ever achieve perfection, ever. So when things break, you want people who will be motivated by solving problems \u2014 those are the people who won\u2019t pause to place blame, and blame is wasteful. Even if you have a honed process for screening and interviewing candidates, it\u2019s worth revisiting how you test for culture fit to make sure this is part of it.\n\nToo many companies and engineering leaders are willing to compromise to maximize technical savvy. Do not do this. Curtis recommends allocating at least 45 minutes to an interview that is entirely about culture and character. Diversity of backgrounds and opinions is championed at Airbnb, so \u2018Culture fit\u2019 is about finding people who share the high-performance work ethic and belief in the company\u2019s mission. If people don\u2019t share your conviction in the company\u2019s success, they aren\u2019t a fit.\n\nAt Airbnb, Curtis found that these four moves truly extract the most value out of this type of interview:\n\nIdeally, your culture interview ensures that you\u2019re hiring a diverse set of people who share your beliefs and work ethic while introducing new ideas and perspectives. Once they\u2019re in the door, you have your next key opportunity to establish shared priorities. \u201cYour first week is the chance for you to set expectations with new engineers,\u201d Curtis says. He\u2019s found that adding a few key elements to the onboarding process pays off big down the road:\n\nYour questions to these team members can be very straightforward. Curtis suggests: \u201cHow are they ramping up in an unfamiliar code base?\u201d and \u201cWhat issues have they encountered and how have they reacted?\u201d\n\nShare the feedback you receive with the person in writing. It will be a valuable reference point for engineers as they ramp up. And if you hear any causes for concern, address them right away. Sit down with that person and clarify your expectations.\n\nWhen it comes to hiring and onboarding managers, though, there\u2019s another layer to consider. These are people who are going to actively shape your company\u2019s culture, personality and progress every day. At Airbnb, a commitment to helping managers make good decisions has manifested in an unusual policy:\n\nUnsurprisingly, this can make it more difficult to hire managers. But he\u2019s devised a four step process engineering leaders can use to find managers who will be best for their companies long-term:\n\nAt this point, through careful hiring and training, you\u2019ve built a team with good judgment. So how can you leverage that to streamline how you run your organization? \u201cNow you can start taking a more principled approach to how you govern the organization,\u201d Curtis says. To bring this point home, he provides several examples that succeeded at Airbnb:\n\nNEW PRINCIPLE: If you would think twice about spending this much from your own account, gut-check it with your manager.\n\n\u201cI can\u2019t tell you how much pain in my life has come from expense reports,\u201d Curtis says. Airbnb\u2019s old policy was a cumbersome one: Charges big and small required approval before they could be submitted. So Curtis tried replacing it with a principle, simple good judgment, using $500 as a rule of thumb for when to get a gut-check. The result? No increase in discretionary spending (but a whole lot of time saved).\n\nOLD POLICY: Engineers can\u2019t create new backend services without approval from managers.\n\nNEW PRINCIPLE: While working within a set of newly articulated architectural tenets \u2014 conceived by a group of senior technical leaders \u2014 engineers are free to develop backend services.\n\nHere\u2019s another case where policy was creating a huge amount of overhead. \u201cYou\u2019d have to go explain what you wanted to do to your manager, explain the rationale, get them to understand, and then get them to approve and move forward,\u201d Curtis says. So he tried something new: A group of senior engineers set up sessions to determine the architectural processes that mattered most to the organization, then articulated them in a series of architectural tenets. Guided by that document, engineers are now free to create new backend services. \u201cIt might even be okay to go outside of those architectural tenets, as long as you gut-check it with the team,\u201d Curtis says.\n\nThe process used here ends up being even more important than the result. \u201cIt wasn\u2019t me sending an email saying, \u2018Here\u2019s the rules by which you must create new services.\u2019 Instead, it was a group of peers coming together,\u201d Curtis says. \u201cThat created great social pressure within our team, which has worked incredibly well to keep us within the boundaries of what we think we should be developing with to solve our technology problems.\u201d\n\nA few years ago at Airbnb, pretty much none of the code being pushed to production was peer reviewed. The team was moving fast, but site stability was suffering. Curtis knew it was time to make peer reviews a priority \u2014 but how? \u201cThis was a decision point for me. I could have written up a big email and sent it out to the team and said, \u2018You must get your code reviewed before you push to production.\u2019 But instead we took a different approach.\u201d\n\nYour team\u2019s goals may be different, but the steps that Curtis used to effect this principled change can serve as a template for any paradigm shift:\n\nMake it possible. Before you establish a new priority, make sure it\u2019s feasible within your current systems. \u201cIt turned out that a lot of our tooling for code reviews was extremely cumbersome and painful, so it was taking too long for people to even get a code review if they wanted one,\u201d Curtis says. So he made sure that tooling was improved before rolling out this initiative. People can\u2019t do what you haven\u2019t made possible. If you don\u2019t take this into account, they\u2019ll be confused and resentful.\n\nCreate positive examples. Enlist a group of well-respected engineers to lead by example. In Airbnb\u2019s case, Curtis asked a handful of senior engineers to start requesting reviews. \u201cIt created a whole bunch of examples of great code reviews that we could draw from to set examples for the team.\u201d\n\nApply social pressure. All-hands meetings can be invaluable tools for advancing a culture-shifting agenda. That time together is already booked, so why not make it work for you? \u201cWe started highlighting one or two of the best code reviews from the week before,\u201d Curtis says. \u201cWe\u2019d have the person who got the review talk about why it was helpful for them and why this was useful.\u201d Your best spokesperson for a new principle a member of the team who\u2019s already bought in.\n\nAddress stragglers. If you don\u2019t get everyone on board on the first pass, don\u2019t take it personally. In fact, Curtis considers converting this crowd an important final step in the process. In the case of Airbnb\u2019s code reviews, he and his senior engineers talked to each holdout and learned what their concerns were. \u201cUsually the end of that conversation was just \u2018Give it a try for a couple of weeks, see how it goes, see if it works.\u2019 Most of them had a very positive experience and then were brought along.\u201d\n\nIn roughly two months, Curtis had made peer code reviews the overwhelming norm without establishing a single policy. \u201cThis is the power of positive reinforcement and social pressure to bring about cultural change in an organization. I didn\u2019t hand down any edicts, I didn\u2019t say \u2018It has to be done this way from now on,\u2019 I didn\u2019t put any formal policy in place,\u201d he says. In fact, code reviews still aren\u2019t enforced in any way; an engineer could still go straight to production anytime \u2014 but no one does it.\n\nAt the end of the day, though, Curtis is not advocating for the unilateral elimination of all company policies. Sometimes you need rules. \u201cA good example for us is when you\u2019re traveling overseas, there are very specific policies about what kind of data you can have access to and what kind you can\u2019t,\u201d Curtis says. When the health of your organization depends on something that can\u2019t be left open to interpretation, go ahead and make a rule \u2014 but do so sparingly.\n\nThe real trick is to recognize that a policy doesn\u2019t exist in a vacuum \u2014 it interacts with every policy that went before it \u2014 and adds to a collective mental and documented overhead that adds up the bigger you get. You want to minimize this over-head however possible, and the easiest way to do that is to trust your team, and clearly articulate your values."
    },
    {
        "url": "https://medium.com/airbnb-engineering/openair-is-back-for-2015-cb02b648a86d",
        "title": "OpenAir is back for 2015 \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "We are excited to announce that Airbnb will be hosting our annual tech conference, OpenAir. We\u2019ll be hosting it on June 4th at CityView At The Metreon from 9:00am \u2014 7:00 pm.\n\nOpenAir is the premier tech conference that focuses on creating engineering solutions to the challenges of matching. The brightest minds in the industry will come together to tackle such issues as search and discovery, trust, internationalization, mobile, and infrastructure.\n\nWe have representation from a broad swatch of companies speaking at OpenAir \u2014 Netflix, Stripe, Periscope, LinkedIn, Etsy, Pinterest, Lyft, HomeJoy, Watsi, Instagram, Facebook, and Google.org\n\nThis year we\u2019ll have more technical talks and we\u2019ll hear about Scaling from Instagram co-founder, Mikey Krieger, Innovation at Netflix from Carlos Gomez-Uribe, Reaching underserved communities from Watsi co-founder, Grace Garey, and Building Periscope from Sara Haider \u2014 among many others.\n\nAttendees will get access to technical talks, hands-on sessions and thought-provoking discussions to help you break through some of your own engineering challenges and projects. Throughout the day there will be time to network with local engineers, take part in interactive sessions, drop in for lightning talks, and meet the speakers.\n\nRegistration is $50 and all proceeds from registration fees will be donated to CODE2040.\n\nCODE2040 is a nonprofit organization that creates programs that increase the representation of Blacks and Latino/a in the innovation economy. CODE2040 believes the tech sector, communities of color, and the country as a whole will be stronger if talent from all backgrounds is included in the creation of the companies, programs, and products of tomorrow."
    },
    {
        "url": "https://medium.com/airbnb-engineering/behind-the-scenes-building-airbnb-s-first-native-tablet-app-608a9f50c2f4",
        "title": "Behind the Scenes: Building Airbnb\u2019s First Native Tablet App",
        "text": "At Airbnb, we\u2019re trying to create a world where people can connect with each other and belong anywhere. Whether you\u2019re traveling, planning a trip with friends, or lying on your couch window-shopping your next adventure, you\u2019re most likely using a mobile device to do your connecting, booking, or dreaming on Airbnb. Our tablet users have probably been surprised to learn that thus far, we have never had a native tablet app. Last summer, a small team decided to change that. We started exploring what Airbnb could become on tablet, and today, we\u2019re excited to share it with the world. It\u2019s a challenging feat to build an entirely new platform while simultaneously maintaining and shipping an ever-evolving phone app; and we\u2019ll tell you more about what went right, what went wrong, and how we ultimately made it happen.\n\nAfter the successful launch of our Brand Evolution last summer, we formed a small team to start laying the groundwork for tablet. In building the tablet app, we took many of the technical learnings from the rebrand. For instance, much like the rebrand, we wanted to build the app over the course of several releases and eventually release the official tablet app. A small team of designers and three engineers (two iOS, one Android) formed to start building the foundation of the app and exploring the tablet space. We knew we couldn\u2019t rewrite the entire phone app, and that if we ever wanted to ship, we\u2019d have to reuse some of the views already existing on the phone. We reviewed every screen of the phone and every feature to determine the engineering to design cost of rebuilding each screen. One thing we quickly realized was that our top-level navigation system, \u201cAirNav,\u201d wouldn\u2019t translate well to the tablet space. Instead, we\u2019d have to design and build something new.\n\nAt Airbnb, we strive to have a seamless experience across platforms and to maintain feature parity no matter the device or form factor. This meant that whatever navigation system we chose for tablet would also have to work on phone. In order to quickly find a solution while covering as much ground as possible, the team split up to prototype as many navigation systems as we could. One of our designers, Kyle Pickering, even went as far as teaching himself Swift so he could build functional prototypes. At the end of the week we had several prototypes in all different forms, fully functional (albeit hacky) prototypes built from our live code, functional prototypes built in Swift with baked data, and even some keynote and after-effects prototypes. We took these to our user research team to quickly get some real-world user feedback on the designs. A big part of the culture at Airbnb is to move quickly and run experiments along the way, rather than waiting until the end. With a pending phone release on the horizon, we decided to build and ship the Tab Nav on phone, wrapped behind an experiment we could roll out and test on. Since the majority of the mobile team was still hard at work on building new features, we had to build the new nav quickly and quietly, in a way that would allow the rest of the team to turn it on or off at runtime without restarting the app. We launched the new nav in November 2014, which gave us several months to collect data and iterate on the high-level information architecture while we built out the tablet app.\n\nFun fact: Up until the launch of tablet, both navigation systems were still active and could be turned on or off via experiment flag.\n\nOn iOS, MVC is the name of the game. We knew we were shipping a universal binary; we weren\u2019t going to split targets or release two apps. In terms of code architecture, we worried that shipping a universal app would cause a split in our app that would become unwieldy over time. It wouldn\u2019t take long for the codebase to become littered with split logic, copy-and-pasted code for experiments, and duplicate tracking calls. At the same time, we didn\u2019t want to have massive view controller classes that split functionality between platforms. This required us to rethink the MVC pattern that was previously tried and true. What we realized was that almost every model object in our data layer (Listings, Users, Wish Lists, etc.) have three UI representations: a table view cell, a collection view cell, and a view controller.\n\nEach of these representations would differ from tablet to phone, so instead of having branching logic in place everywhere these objects were used, we decided to ask the model how it preferred to be displayed. We built a view-model protocol that allows us to ask any model object for its \u201cdefault representation view controller.\u201d The model returns a fully allocated device-specific view controller to be displayed. At first, these view-model objects simply returned the phone view controller, but when we eventually started building the tablet version we simply had to change a single line of code for the tablet view controllers to be displayed app-wide. This reduced the amount of refactoring we had to do once we started building out view controllers and allowed us to focus on polishing the view controllers. Also, this kept all of our code splitting check centralized to a few classes. Next we started moving through the existing phone controllers and pulling all of our tracking and experiment logic into shared logic controllers that would be used for both the phone and tablet views. This allowed the team to continue working on the phone, by adding experiments and features that would automatically find their way onto the tablet app.\n\nBy January 2015, the tablet team was all-hands-on-deck, and design was in a stage where we could start building the tablet app. We had around two months to build out the app and about a month for final polish and bug fixes. Design had produced several fully working demo apps to prototype interactions and UI animations. In producing these code-driven demos, the design team was able to identify gotchas in the design long before engineering was ramped up, which made for an overall smooth development period. There were, however, a few issues that inevitably popped up. For several scrolling pages throughout the app, design called for a lightweight scroll snapping. The idea was to have scroll views always decelerate to perfectly frame the content. This is not a new or revolutionary idea, but on a large-scale tablet device we discovered that, more often than not, this interaction annoyed the user. One user described it as being like \u201ctrying to kick a soccer ball up a hill.\u201d Though the final results were visually pleasing, taking control from the user undermined the beauty of the design. Instead of cutting the feature completely, we decided to take a deeper look at the problem. Previously, we were using delegate callbacks which were fired when the user finished scrolling, and then adjusting the target content offset to the closest pre-computed snapping offset. We realized the problem with this system is that it doesn\u2019t take into account the intent of the user. If a user scrolls a view and slides their finger off the screen in a tossing manner, the system works great. If a user purposefully stops scrolling and then releases touch the scroll view snaps to the nearest point, creating the \u201cuphill soccer ball\u201d effect. We decided to disable scroll snapping on the fly once the velocity of the scroll dropped below a certain point, giving the user control of the scrolling experience. Achieving these small wins and being truly thoughtful around user intent helped elevate the app experience to a whole new level of delight and usability.\n\nAs we crossed the finish line and landed the project (mostly) on time, we took a little time to reflect what worked to complete such a massive project. We were reminded of the old Boy Scout mantra: \u201cAlways Be Prepared.\u201d Even though the entire team built the tablet app in just a few short months, it wouldn\u2019t have been possible without the foundation work that was silently laid throughout the year before. From designers learning to code and building prototypes, to shipping the tablet navigation system on phone months ahead of release, this prep work ensured that when it came time to officially move towards our goal, we were ready."
    },
    {
        "url": "https://medium.com/airbnb-engineering/meet-the-nerds-gary-wu-5cf4047f72dc",
        "title": "Meet The Engineers: Gary Wu \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Today we\u2019re introducing you to Gary Wu, a traveler, family man, who values simplicity above most other things.\n\nHow did you get started in Computer Science?\n\nI was a quiet kid, I loved math and electronics. It is not surprising that I fell in love with computers. As a teenager, I picked up Basic. At first, it is just curiosity, but soon I liked the ability to create something fun with little investment. Any new tip learned from a book could be applied immediately to my naive programs. Soon, I was showing off my projects to parents and friends, and they gave me feedback on how to make it better. The process of continuous innovation was exciting. After graduation and working for tech companies, I also found that this repeated cycle of learn => build => feedback is not so much different from my own teenage experience. More importantly, the quicker the cycle runs, the faster the products will innovate and people will grow.\n\nWhat was your path to Airbnb?\n\nI am very interested in the sharing economy and love travel. The Internet and mobile technology has completely revamped the ways that we communicate, shop, and collaborate. I believe the relationship between people and services will have a revolutionary change as well in the near future, and this may have a profound long term impact to the human society. I followed and heard of many impressive Airbnb stories, but I haven\u2019t considered changing job, as I was pretty happy with the previous company . Some friends approached me and shared a lot of internal stories, especially how Airbnb builds up teams based on its core values. This impressed me that Airbnb has a strong vision to completely change how everyone experiences the world. If Airbnb\u2019s vision comes true, the world will be very different from today. This convinced me to have a try and be a part of the journey. I am glad that I made this career decision.\n\nWhat\u2019s the most interesting technical challenge you\u2019ve worked on since joining?\n\nAfter joining Airbnb, I focused on prototyping several early stage product ideas with a potential to significantly expand our market in the future. There are two sets of challenges both from the products and the infrastructure.\n\nOn the product side, we want to maximize our learning with minimum efforts, and so we develop MVPs (Minimum Viable Product). Building MVPs is easy to say but hard to execute right, because if the product is not appreciated by the customers, it is difficult to tell whether it is because of no enough engineering efforts or because of the wrong idea. Likely, we may over-emphasize the engineering execution instead of reevaluating the ideas.\n\nOn the infrastructure side, there may also have many pitfalls. Building MVP may unfortunately introduce technical debt that could be hard to extend or scale in the long run, especially under a complex business flow. Furthermore, unlike other mature technical companies, which have abundant resources and are easily able to handle 5\u201310X sudden load increase, Airbnb infrastructure is still at an early stage and doesn\u2019t have enough cushion to deal with unusual resource usage pattern. Therefore, we have to be very thoughtful in developing MVPs, eliminate any possible system threats, and try to minimize any long term debt.\n\nIn summary, it is really a enjoyable and fast learning process. Airbnb has a plenty of these opportunities because there are so many areas to explore in the traveling space.\n\nWhat do you want to work on next?\n\nI would like to enable more micro-entrepreneurs to create services on top of Airbnb. We are grateful that many hosts leverage Airbnb to rent out their extra spaces for the travelers, and it is only a beginning of changing how we experience the world. To provide a magic experience for each traveller, there are a lot of opportunities for the local people to participate in, and Airbnb can be the prefect platform for them to contribute to the traveling industry.\n\nWhat is your favorite core value, and how do you live it?\n\nSimplify. I like a simplified way of thinking and doing things. First, simplification drives me to provide a simplified \u201cinterface\u201d to others. When doing a presentation, no matter it is just 5 mins or 30 mins, I push myself to only make one sentence takeaway for the audiences. When discussing a comprehensive system design topic, I tried to summarize my points in a couple of bullet points. When writing programs, I do my best to make the function names and execution flow are so intuitive that other engineers can pick it up easily. Second, simplification makes me stay focused. It is important to be very productive, but to me, it is more important of avoid doing irrelevant things. Finally, simplification can help achieving a better software quality in the long run. I like to challenge myself to avoid unnecessary complexity for marginal improvements, but seek for architecture simplicity for future extension with a potential 10X improvement.\n\nI spent a few days with my family living an amazing cabin inside the Sequoia national park early this year. The location of the cabin made me so close to the nature. We were surrounded by the great mountain views, hundred years old giant pine trees, and beautiful stony creeks. In addition, we totally lived in a pre-Interenet world, as there was no cable and the closest village to receive cellular signals was one hour driving distance away. We had a lot of fun with hiking, climbing, and photography. In the last day, an unexpected snow storm turned the entire mountain into white, and our cabin in the snow mountain was exactly my childhood fantasy. It was a magic experience that traditional hotel is probably impossible to offer."
    },
    {
        "url": "https://medium.com/airbnb-engineering/airmapview-a-view-abstraction-for-maps-on-android-4b7175a760ac",
        "title": "AirMapView: A View Abstraction for Maps on Android \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Many Android applications today require some form of an interactive map as part of their user interface. Google provides a native package and experience with Google Play Services to satisfy this need, but the question remains of how one creates interactive maps for devices without Play Services.\n\nIn some countries, the majority of devices are sold without Google Play Services. Device manufacturers who ship their devices without Play Services are continuing to gain popularity worldwide. In order for our application to provide a truly internationalized experience, we can\u2019t leave out a feature as critical as maps. And because we know other companies have this same issue, we\u2019ve created and open sourced AirMapView.\n\nAirMapView is a view abstraction that enables interactive maps for devices with and without Google Play Services. Devices that do have Google Play Services will use Google Maps V2, while devices without will use a web based implementation of Google Maps. This all comes as one single API that is designed after that of Google Maps V2 that most developers are used to.\n\nAirMapView will choose by default the best map provider available for the device. By default it will use native Google Maps V2 if available and fallback to a WebView solution if Google Play Services are not available. The API is designed to be completely transparent to the user so that developers can use the same APIs that are currently used for Google Maps to gain the fallback functionality.\n\nNative GoogleMap is implemented as a Fragment inside of the AirMapView providing the exact same functionality as using Google Maps V2 directly. Porting existing implementations from GoogleMap to AirMapView is as simple as replacing calls to GoogleMap with calls to AirMapView and implementing the correct callback classes for operations such as OnCameraChanged. The API is designed to be pluggable so developers can add their own providers for specific devices such as Amazon Maps for Amazon kindle fire devices.\n\nThe fallback webview map displays a Google Map inside of an Android WebView and uses the javascript bridge callbacks to allow dynamic interaction with the map. Due to it being a webview and not native code it isn\u2019t as performant as the native GoogleMap but it only performed slightly worse in experiments in the Airbnb app.\n\nUsing the Javascript Bridge we are able to implement the same API in the web map so no client code changes are required to support the web map once AirMapView has been implemented for native maps.\n\nThe web map allows setting a location, centering, adding markers, dragging, tapping on the map and other common operations that are currently supported in the GoogleMap.\n\nWe\u2019ve built AirMapView in such a way that allows us to easily add additional map providers in the future, such as Amazon Maps V2, Baidu, Mapbox, etc.\n\nFor more information take a look at our Github page here: https://github.com/airbnb/airmapview"
    },
    {
        "url": "https://medium.com/airbnb-engineering/how-airbnb-uses-machine-learning-to-detect-host-preferences-18ce07150fa3",
        "title": "How Airbnb uses Machine Learning to Detect Host Preferences",
        "text": "At Airbnb we seek to match people who are looking for accommodation \u2014 guests \u2014 with those looking to rent out their place \u2014 hosts. Guests reach out to hosts whose listings they wish to stay in, however a match succeeds only if the host also wants to accommodate the guest.\n\nI first heard about Airbnb in 2012 from a friend. He offered his nice apartment on the site when he traveled to see his family during our vacations from grad school. His main goal was to fit as many booked nights as possible into the 1\u20132 weeks when he was away. My friend would accept or reject requests depending on whether or not the request would help him to maximize his occupancy.\n\nAbout two years later, I joined Airbnb as a Data Scientist. I remembered my friend\u2019s behavior and was curious to discover what affects hosts\u2019 decisions to accept accommodation requests and how Airbnb could increase acceptances and matches on the platform.\n\nWhat started as a small research project resulted in the development of a machine learning model that learns our hosts\u2019 preferences for accommodation requests based on their past behavior. For each search query that a guest enters on Airbnb\u2019s search engine, our model computes the likelihood that relevant hosts will want to accommodate the guest\u2019s request. Then, we surface likely matches more prominently in the search results. In our A/B testing the model showed about a 3.75% increase in booking conversion, resulting in many more matches on Airbnb. In this blog post I outline the process that brought us to this model.\n\nI kicked off my research into hosts\u2019 acceptances by checking if other hosts maximized their occupancy like my friend. Every accommodation request falls in a sequence or in a window of available days in the calendar, such as on April 5\u201310 in the calendar shown below. The gray days surrounding the window are either blocked by the host or already booked. If accepted and booked, a request may leave the host with a sub-window before the check-in date (check-in gap \u2014 April 5\u20137) and/or a sub-window after the check-out (check-out gap \u2014 April 10).\n\nA host looking to have a high occupancy will try to avoid such gaps. Indeed, when I plotted hosts\u2019 tendency to accept over the sum of the check-in gap and the check-out gap (3+1= 4 in the example above), as in the next plot, I found the effect that I expected to see: hosts were more likely to accept requests that fit well in their calendar and minimize gap days.\n\nBut do all hosts try to maximize occupancy and prefer stays with short gaps? Perhaps some hosts are not interested in maximizing their occupancy and would rather host occasionally. And maybe hosts in big markets, like my friend, are different from hosts in smaller markets.\n\nIndeed, when I looked at listings from big and small markets separately, I found that they behaved quite differently. Hosts in big markets care a lot about their occupancy \u2014 a request with no gaps is almost 6% likelier to be accepted than one with 7 gap nights. For small markets I found the opposite effect; hosts prefer to have a small number of nights between requests. So, hosts in different markets have different preferences, but it seems likely that even within a market hosts may prefer different stays.\n\nA similar story revealed itself when I looked at hosts\u2019 tendency to accept based on other characteristics of the accommodation request. For example, on average Airbnb hosts prefer accommodation requests that are at least a week in advance over last minute requests. But perhaps some hosts prefer short notice?\n\nThe plot below looks at the dispersion of hosts\u2019 preferences for last minute stays (less than 7 days) versus far in advance stays (more than 7 days). Indeed, the dispersion in preferences reveals that some hosts like last minute stays better than far in advance stays \u2014 those in the bottom right \u2014 even though on average hosts prefer longer notice. I found similar dispersion in hosts\u2019 tendency to accept other trip characteristics like the number of guests, whether it is a weekend trip etc.\n\nAll these findings pointed to the same conclusion: if we could promote in our search results hosts who would be more likely to accept an accommodation request resulting from that search query, we would expect to see happier guests and hosts and more matches that turned into fun vacations (or productive business trips).\n\nIn other words, we could personalize our search results, but not in the way you might expect. Typically personalized search results promote results that would fit the unique preferences of the searcher \u2014 the guest. At a two-sided marketplace like Airbnb, we also wanted to personalize search by the preference of the hosts whose listings would appear in the search results.\n\nEncouraged by my findings, I joined forces with another data scientist and a software engineer to create a personalized search signal. We set out to associate hosts\u2019 prior acceptance and decline decisions by the following characteristics of the trip: check-in date, check-out date and number of guests. By adding host preferences to our existing ranking model capturing guest preferences, we hoped to enable more and better matches.\n\nAt first glance, this seems like a perfect case for collaborative filtering \u2014 we have users (hosts) and items (trips) and we want to understand the preference for those items by combining historical ratings (accept/decline) with statistical learning from similar hosts. However, the application does not fully fit in the collaborative filtering framework for two reasons.\n\nWith these points in mind, we decided to massage the problem into something resembling collaborative filtering. We used the multiplicity of responses for the same trip to reduce the noise coming from the latent factors in the guest-host interaction. To do so, we considered hosts\u2019 average response to a certain trip characteristic in isolation. Instead of looking at the combination of trip length, size of guest party, size of calendar gap and so on, we looked at each of these trip characteristics by itself.\n\nWith this coarser structure of preferences we were able to resolve some of the noise in our data as well as the potentially conflicting labels for the same trip. We used the mean acceptance rate for each trip characteristic as a proxy for preference. Still our data-set was relatively sparse. On average, for each trip characteristic we could not determine the preference for about 26% of hosts, because they never received an accommodation request that met those trip characteristics. As a method of imputation, we smoothed the preference using a weight function that, for each trip characteristic, averages the median preference of hosts in the region with the host\u2019s preference. The weight on the median preference is 1 when the host has no data points and goes to 0 monotonically the more data points the host has.\n\nUsing these newly defined preferences we created predictions for host acceptances using a L-2 regularized logistic regression. Essentially, we combine the preferences for different trip characteristics into a single prediction for the probability of acceptance. The weight the preference of each trip characteristic has on the acceptance decision is the coefficient that comes out of the logistic regression. To improve the prediction, we include a few more geographic and host specific features in the logistic regression.\n\nWe ran this model on segments of hosts on our cluster using a user-generated-function (UDF) on Hive. The UDF is written in Python; its inputs are accommodation requests, hosts\u2019 response to them and a few other host features. Depending on the flag passed to it, the UDF either builds the preferences for the different trip characteristics or trains the logistic regression model using scikit-learn.\n\nOur main off-line evaluation metric for the model was mean squared error (MSE), which is more appropriate in a setting when we care about the predicted probability more than about classification. In our off-line evaluation of the model we were able to get a 10% decrease in MSE over our previous model that captured host acceptance probability. This was a promising result. But, we still had to test the performance of the model live on our site.\n\nTo test the online performance of the model, we launched an experiment that used the predicted probability of host acceptance as a significant weight in our ranking algorithm that also includes many other features that capture guests\u2019 preferences. Every time a guest in the treatment group entered a search query, our model predicted the probability of acceptance for all relevant hosts and influenced the order in which listings were presented to the guest, ranking likelier matches higher.\n\nWe evaluated the experiment by looking at multiple metrics, but the most important one was the likelihood that a guest requesting accommodation would get a booking (booking conversion). We found a 3.75% lift in our booking conversion and a significant increase in the number of successful matches between guests and hosts.\n\nAfter concluding the initial experiment, we made a few more optimizations that improved conversion by approximately another 1% and then launched the experiment to 100% of users. This was an exciting outcome for our first full-fledged personalization search signal and a sizable contributor to our success.\n\nFirst, this project taught us that in a two sided marketplace personalization can be effective on the buyer as well as the seller side.\n\nSecond, the project taught us that sometimes you have to roll up your sleeves and build a machine learning model tailored for your own application. In this case, the application did not quite fit in the collaborative filtering and a multilevel model with host fixed-effect was too computationally demanding and not suited for a sparse data-set. While building our own model took more time, it was a fun learning experience.\n\nFinally, this project would not have succeeded without the fantastic work of Spencer de Mars and Lukasz Dziurzynski."
    },
    {
        "url": "https://medium.com/airbnb-engineering/overcoming-missing-values-in-a-random-forest-classifier-7b1fc1fc03ba",
        "title": "Overcoming Missing Values In A Random Forest Classifier",
        "text": "Airbnb is trying to build a world where people can belong anywhere and there are no strangers. This helps hosts feel comfortable opening their homes and guests be confident traveling around the globe to stay with people they have never met before.\n\nWhile almost all members of the Airbnb community interact in good faith, there is an ever shrinking group of bad actors that seek to take advantage of the platform for profit. This problem is not unique to Airbnb: social networks battle with attempts to spam or phish users for their details; ecommerce sites try to prevent the use of stolen credit cards. The Trust and Safety team at Airbnb works tirelessly to remove bad actors from the Airbnb community and to help make the platform a safer and trustworthy place to experience belonging.\n\nWe can train machine learning models to identify new bad actors (for more details see the previous blog post Architecting a Machine Learning System for Risk). One particular family of models we use is Random Forest Classifiers (RFCs). A RFC is a collection of trees, each independently grown using labeled and complete input training data. By complete we explicitly mean that there are no missing values i.e. NULL or NaN values. But in practice the data often can have (many) missing values. In particular, very predictive features do not always have values available so they must be imputed before a random forest can be trained.\n\nTypically, random forest methods/packages encourage two ways of handling missing values: a) drop data points with missing values (not recommended); b) fill in missing values with the median (for numerical values) or mode (for categorical values). While a) does not use all the available information by dropping data points, b) can sometimes brush too broad a stroke for data sets with many gaps and significant structure.\n\nThere are alternative techniques for dealing with missing values, but most of these are computationally expensive e.g. repeated iteration of random forest training to compute proximities. What we propose in this post is a one-step pre-computation method which normalises features to construct a distance metric for filling in missing values with the median of their k-nearest neighbors.\n\nAll of the features in our fraud prediction models fall into two types: a) numerical and b) categorical. Boolean features can be thought of as a special case of categorical features. Since we work in the business of fraud detection, our labels are binary: 0 if the data point is not fraud and 1 if the data point is fraud. Below are some feature transformations we wish to compare for missing value treatment.\n\nThe aim of the scaling transforms Fn and Fc is two fold. First of all to make the transformation invertible so no information is lost. Secondly, to uniformly distribute the data points with fraud in the interval [0,1] for each feature. If we think of data as points in N dimensional space where N is the number of features, then the distance in each dimension between two data points becomes comparable. By comparable we mean that a distance of 0.4 in the the first dimension contains twice as many fraud data points as a distance of 0.2 in the second dimension. This enables better construction of distance metrics to identify fraud.\n\nIn order to see the effect of the above feature transforms, we use the adult dataset from the UCI Machine Learning Repository and assess the performance of the model under different feature transformations and proportions of missing values. The dataset containts 32,561 rows and 14 features, of which 8 are categorical and the remaining 4 are numerical. The boolean labels correspond to whether the income level of the adult is greater than or less than $50k per annum. We divide the dataset into a training and test set in the ratio of 4 to 1 respectively.\n\nFor the first experiment we compare different models using the methodology:\n\nObserve that M and \u03bb are unspecified parameters \u2014 we will loop over different values of these during experimentation. The Performance of each model will be judged using Area Under Curve (AUC) scores which measures the area under the Receiver Operating Characteristic (ROC) graph (this is a plot of the true postive rate vs the false postitive rate). We will test the following nine models:\n\nFirst we consider how the models perform for different values of [latex]M[/latex] and [latex]\\lambda[/latex] with a fixed number of trees (100) in the RFC training process and fixed number of nearest neighbours (100) for NNMV imputation.\n\nThe graphs above display interesting patterns, some intuitive and some surprising:\n\nHaving observed the outperformance of model 8) over the other candidates, we next check how model 8) compares to the baseline as we vary i) the number of trees in the RFC training and ii) the number of nearest neigbours in the NNMV imputation. We take the fourth scenario above \u2014 where 60% of values are missing \u2014 and we chose \u03bb=0.5.\n\nThe left hand plot does not suggest the performance of model 8) improves with the number of nearest neighbours used in the NNMV imputation. However, there is a consistent pattern of improved performance as the number of trees increases, plateauing after about 100 trees. The right hand plot shows how much faster it is to train a RFC with the transformed feature set. This is to be expected as, instead of exploding categorical features to many binary features in the baseline model, we keep the number of features fixed in model 8).\n\nConsider the ROC curves for one of the scenarios above, say, where the number of trees is 100 and the number of nearest neighbors used is 100.\n\nThe improvement of the ROC curve suggests, for example, that holding recall fixed at 80%, say, the false positive rate falls from 26% to 24%. Suppose each day we are scoring 1 million events, 99% of which are non-fraud, each flagged event needs to be manually reviewed by a human, and each review takes 10 seconds. Then the aforementioned decrease in the false positive rate can save reviewing 1,000,000 x 0.99 x 0.02 = 19,800 events or 19,800 / (6 x 60) = 55 hours of reviewing per day! This is why even single digit or decimal digit improvements in the auc score of a RFC can have a dramatic effect on a department\u2019s efficiency."
    },
    {
        "url": "https://medium.com/airbnb-engineering/meet-the-nerds-barbara-raitz-4e1646c2fd4b",
        "title": "Meet The Engineers: Barbara Raitz \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Meet Barbara Raitz! In today\u2019s Q&A Barbara tells us about storytelling through code and how to be a magician.\n\nI grew up around computers and initially pursued a Computer Science degree for practical reasons. After working a few years in the industry, I discovered that I absolutely love it! There is beauty in well crafted code and elegant architectures. I love diving in deep, trying to understand the `story` of complex code, and then providing cleaner, simpler, and more powerful solutions.\n\nIf I could say anything to those considering this career path, I want to shout out that it is an amazing field! Though it can be initially daunting, there is such a rich, vast, nearly limitless set of opportunities with this skill set. I love quoting a colleague who said: \u201cDo you want to watch the magic show, or do you want to become the magician?\u201d I feel so lucky to have found a home where I can have such impact, while having so much fun.\n\nI first heard of Airbnb on a podcast from The Commonwealth Club. I very quickly identified with the whole premise of Airbnb, but didn\u2019t consider it much further until a recruiter reached out and contacted me. Because of that initial spark, I decided to come visit the office and meet the team, and was undeniably affected by the positive energy and excitement of the place! Even though it was an introductory visit, I knew I was changed, and every subsequent visit re-enforced that.\n\nI joined Airbnb because it is a technical playground with many opportunities to dig in and make a difference; because the amazing people and culture absolutely blew me away; and because I felt a strong connection to the Airbnb mission. I am particularly drawn to the concept of unique, authentic, local travel that benefits the community, and, that in this ever virtual world, we are bringing people together again.\n\nI would like to share two!\n\nInitially, I paired up with Spike Brehm to work on Rendr, a library that can re-use and render javascript code on either the client or server: original, powerful, fast \u2014 isomorphic javascript! This was a fascinating, ground-breaking, and richly challenging and rewarding project, and I am proud of our results!\n\nFrom there, I moved to a completely different project that needed me most. I have spent much of my efforts diving deep into tangled legacy code and gradually moving it towards a cleaner, tested, decoupled service-oriented architecture. This process of refactoring mission critical code and data structures while still \u201cin-flight\u201d requires absolute attention to detail, small steps, and patience. Specific to the Calendar, it is rewarding to see powerful new features, such as seasonal availability rules, shine through.\n\nConsistent with the mission of Simplify, there are other product areas that would greatly benefit from a deep-dive and refactor. Each has it\u2019s own challenges, and is thus richly interesting. I will likely continue supporting opportunities that deliver the biggest impact in terms of decoupling code and processes, increasing stability and maintainability, and delivering powerful new capabilities and features.\n\nIn terms of learning something new, I have my eye on React.js \u2014 I instinctively like it and want to try it out myself sometime. I have also wanted to explore and experience writing a mobile client application. Maybe in my free time =)\n\nI am drawn to all the core values, and absolutely love that our company truly promotes and lives them! Though hard to choose a favorite, I suppose that I best embody \u201cChampion the Mission\u201d. In terms of my technical role, I passionately pursue real architecture changes that I feel will have the most impact to our fast-growing team, product, and company. And then I follow through with careful, persistent, patient hard work. In terms of something bigger, I truly love how Airbnb promotes \u201cbelonging anywhere\u201d, in being gracious and welcoming hosts, and being your individual unique self. I love how Airbnb is beneficial to local communities and real individuals. I love how this form of travel encourages people to go outside their normal bounds to perhaps discover something new. These actually map to other core values, and I promote and champion them as well!\n\nFor New Year\u2019s 2013, I wanted to go somewhere special with my boyfriend and we landed here, the experience was absolutely amazing! The space is truly beautiful, from the stunning landscaping and views, to the fabulous and uniquely crafted house itself. For us, it was magical. It was beautiful, relaxing, and creatively stimulating \u2014 it allowed us to both relax and go into hyper-creative mode, imagining all sorts of creative, crazy, inspiring futures \u2014 colorful ideas that are still with us today. But as any well-travelled person could tell you, the most meaningful memories are tied to the people and kindred spirits you meet along the way. Jeanne, our host, is one of the most gracious, welcoming people I\u2019ve met. She went out of her way to make our stay uniquely special, arranging for a special boat on New Years eve to go to the center of the lake to watch ALL the fireworks. But truly, she opened her heart to us, and we shared real life stories, goals, and aspirations. It\u2019s rare and beautiful when, unexpectedly, your life and perspective changes because you went somewhere or crossed paths and made friends with someone new. This was one of those memorable experiences.\n\nSide note, this listing won Airbnb\u2019s Most Unique Listing award in 2014."
    },
    {
        "url": "https://medium.com/airbnb-engineering/meet-the-nerds-phillippe-siclait-add964110f0",
        "title": "Meet the Engineers: Phillippe Siclait \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "In this Q&A we meet Phillippe Siclait. Phillippe has been with Airbnb for over 2 years and has worked on five teams since joining \u2014 clearly he likes to travel both personally and professionally!\n\nHow did you get started in Computer Science?\n\nI started programming to make video games. When I was in elementary school, I found a book at a school book fair called \u201cLearn to Program Basic\u201d. I had the vague sense that programming was a thing you needed to do to make games, so I picked it up, went through all the exercises, and got hooked. In the proceeding years, I continued to learn from all the resources I could find online. This led to doing programming competitions through my school and the development of an interest in graphics programming. By the end of high school I was pretty set on studying either CS or Economics.\n\nWhat was your path to Airbnb?\n\nIt turns out that I didn\u2019t actually end up majoring in CS. I received a BS in Economics mostly focused on game theory and econometrics while simultaneously working in the Computer Graphics group in the Computer Science and Artificial Intelligence Lab. I decided prior to my final year of school that management consulting would help me learn how to run a business, and after an internship at the Boston Consulting Group, I accepted a full-time offer there. I learned a lot at BCG and got to do a fair amount of travel, both for work and for fun, but in the end realized that I wanted to come back to the technical side. With a couple friends, I packed my bags and moved across the country to explore what the Bay had to offer. After a few months of working independently on mobile and web projects, a friend of mine brought me over to Airbnb for a Tech Talk. It was after meeting many people, hearing about the vision of the company, and thinking about how much I wanted to see this idea spread, that I knew that I had to be here.\n\nWhat\u2019s the most interesting technical challenge you\u2019ve worked on since joining?\n\nSince joining Airbnb I\u2019ve worked on many teams. I started working on Search (frontend, ranking, infrastructure, evaluation tools, and more), worked a bit on web security, and then worked on our (at the time newly formed) Discovery team. One of the problems for Discovery is recommending places and listings to our guests. I found it fascinating to think through how you could determine which locations an individual would likely be interested in and I worked with my teammates to implement the data pipelines and serving architecture for providing the recommendations to many parts of our product. It was a complex problem partly because the decisions we make while traveling are very personal and multi-faceted.\n\nWhat do you want to work on next?\n\nI\u2019m currently working on a team that focuses on engaging people to host on Airbnb. We\u2019re an incredibly experiment driven team and we have plans to change many parts of the product to make starting to host easier. We have a small, multidisciplinary team of designers, data scientists, a product manager and engineers who are all focused on this problem. I\u2019m excited to see us increase the rate at which we are testing new product changes.\n\nWhat is your favorite core value, and how do you live it?\n\nSimplify. I try to simplify any code I write. Code is meant to be read by people and as a result, the simplest code is often the best code. And all of us at Airbnb live it in the product we develop. We are building a product for our guests and hosts, and a simple product leads to a better end experience.\n\nWhat\u2019s your favorite Airbnb experience?\n\nMy favorite Airbnb experience may have actually been over a one night stay in Manila earlier this year. The host had a beautiful house with wonderful, vibrant wood throughout, and he was hosting other guests who were visiting from the UK. He invited us out to dinner that night and it was fascinating to get to know these people who had lives so different from my own. Our host was an American expat who had long been in the Philippines and the other guests were retirees who traveled the world, exploring new places and teaching motorcycle racing. The dinner was wonderful and we spent several hours afterwards chatting in his kitchen about our lives and what we had all seen of the world. In the morning our host organized our transportation back to the airport and made sure we knew that we were welcome the next time we were back in Manila."
    },
    {
        "url": "https://medium.com/airbnb-engineering/airpal-a-web-based-query-execution-tool-for-data-analysis-33c43265ed1f",
        "title": "Airpal: a Web UI for PrestoDB \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Airbnb is pleased to announce the launch of Airpal, a web-based query execution tool that leverages Facebook\u2019s PrestoDB to facilitate data analysis.\n\nPeople who spend time using SQL for exploration and investigation know that the workflow is not always smooth. Remembering how a query was written, copying and pasting from the command line, and running multiple terminal windows can slow down analysis and be frustrating. Additionally, when diverse teams are using SQL for analytics, the learning curve can be steep for beginners, so good UI tools can help drive adoption and promote knowledge sharing.\n\nAt Airbnb, we launched Airpal internally about a year ago and now more than 1/3 of all employees have issued a query through the tool. This is an astounding statistic and it shows how integral Presto is to our company\u2019s data infrastructure.\n\nWe currently hold about one and a half petabytes of data as Hive managed tables in HDFS, and the relatively small data size of our important \u201ccore_data\u201d tables allows us to use Presto as the default query engine for analysis. When running ad hoc queries and iterating on the steps of an analysis, Presto is much snappier and more responsive than traditional map reduce jobs. The biggest benefit to adding Presto to our infrastructure stack, though, is that we don\u2019t have to add additional complexity to allow \u201cinteractive\u201d querying. Because we are querying against our one, central Hive warehouse, we can keep a \u201csingle source of truth\u201d with no large scale copies to a separate storage/query layer. Additionally, the fact that we don\u2019t need change data storage type from RC format to see the speed improvements, makes Presto a great choice for our infrastructure.\n\nWe are excited to share this tool with the open source community and we hope that it can provide similar utility for others.\n\nKeeping with the spirit of Presto, we have tried to make it simple to install Airpal by providing a local storage option for people who would like to test it out without any overhead or cost. For more detailed information, visit the GitHub page here: https://github.com/airbnb/airpal\n\nA few of the notable technology features in Airpal are:\n\nFinally, we would be remiss if we did not mention the awesome direction that Facebook provided as the original developers of Hive and the pioneers of building UI tools to facilitate easy access to big data. We stood on the shoulders of giants to make this tool and we appreciate the influence and input that the data infrastructure and data tools teams at Facebook were able to provide.\n\nIf you\u2019re interested in helping build a world class suite of data tools, check out this open job position: https://www.airbnb.com/jobs/departments/position/48112"
    },
    {
        "url": "https://medium.com/airbnb-engineering/meet-the-nerds-daniel-loreto-543b3ad7a643",
        "title": "Meet the Engineers: Daniel Loreto \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Today we meet Daniel Loreto, an engineering manager here who is an entrepreneur at heart. Daniel tells us about his love for high risk/high reward projects and his new life moving from NYC to SF for his role at Airbnb.\n\nWhen I was a kid my dad bought our first personal computer for the house, an IBM PC. It was state-of-the-art at the time: a monochrome monitor, no hard drive, a 5 1/2 inch floppy drive, and whopping 64 kb of RAM. I would use it to play games like paratrooper and snake. One day I was poking around the floppy disk that had the game of snake and I found a file with what looked like gibberish english. I asked my dad about it and he explained it was actually the source code for the game written in BASIC.\n\nMy kid-mind was blown. I knew people could write novels and that they could compose music \u2026 but games? computer programs? You could actually write something interactive and have it do things? I was hooked ever since.\n\nI lived in six different countries growing up and speak three different languages. I\u2019ve love to travel and getting to know other cultures. Most of my professional career was in New York, but I was looking to move to California and join a company where I could have a big impact and could connect with the mission. I talked to a few friends at Airbnb and I knew I had found a match. As a first project I also had the opportunity to work on a team directly with Joe Gebbia, one of the founders. It was a very unique opportunity to work with a founder on challenging new directions for the company.\n\nIn my current project we\u2019re working on new features that changes some of the fundamental assumptions of the original design of Airbnb\u2019s architecture. As a result my team and I have had to very quickly familiarize ourselves with the overall architecture, and then dig deep into some pretty core and complex systems like payments and search to make our project possible.\n\nI\u2019m always looking for potentially risky but high-impact projects. I love working on new product directions that expand what\u2019s possible for Airbnb today and I\u2019m always pushing my team to work on big ideas that can really move the needle if they\u2019re successful. We have a few ideas that we want to work on next that fit that description \u2026. but I would be giving it all away if I revealed them now. Stay tuned!\n\nMy favorite core value is, without a doubt, Cereal Entrepreneur. To me it embodies the idea of being creative, scrappy, bold and resourceful in order to accomplish your goals. Don\u2019t be afraid of big audacious goals even with a small team. Is there something blocking you from accomplishing that goal? Don\u2019t be afraid to tackle it, even if it\u2019s outside of your area of expertise. Make things happen. Drive towards a big vision. Shoot for the moon. Don\u2019t stop."
    },
    {
        "url": "https://medium.com/airbnb-engineering/large-scale-payments-systems-and-ruby-on-rails-bfe5b89f6f4",
        "title": "Large Scale Payments Systems and Ruby on Rails \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "I\u2019ve been writing code for fun and for work at both small startups and large companies for the past 30 years.\n\nOver the years I\u2019ve become fascinated with the payments industry, and I have spent the past decade working in payments related companies. Before coming to work on Payments at Airbnb, I worked for PayPal, for a few startups and then was part of the NFC Wallet project at Google.\n\nThe payments space is interesting because it combines a strong need for hard core computer science with an industry that is built on an aging foundation. The basic way payments work around the world has essentially stayed the same even as technologies such as location-awareness and strong encryption have become ubiquitous. The industry is ripe for change but extremely resistant to it. It is a fun technical problem and a tough business problem.\n\nOver the years I\u2019ve seen payments systems written in a variety of languages. I\u2019ve noticed that the way payments work translates to requirements that affect the language and framework selection trade offs. For example, payments applications typically require strong transactional integrity, a robust audit trail and very predictable failure behavior.\n\nRuby on Rails is well known for its quick iteration cycle and the plethora of magical tools that speed up development and simplify prototyping. Those benefits are focused on improving the development process, but in some cases make maintenance of production system more difficult. In terms of the way payments work, some of its shortcomings mean trouble.\n\nThe ActiveRecord pattern favors large, monolithic model classes that contain database access logic and business logic. Testing these classes is difficult. It is not easy to describe their dependencies above and beyond the database table they depend on, and it it is therefore hard to write good, comprehensive unit tests for them. This makes it difficult to reason about their behavior. E.g.: Can you really guarantee that this 3000 line model that touches our transaction table does not contain off-by-one-cent errors if it doesn\u2019t have really good unit tests?\n\nMaintaining a log of \u2018who did what when\u2019 for every change involving money or user information is an important part of any payments system. ActiveRecord makes it trivially easy to make database changes, making it hard to ensure that every part of the code that mutates data actually records an audit trail. Furthermore, in some cases (e.g. with mass updates such as update_all), there is no way to enforce that every database change creates an audit trail. The only way to programmatically capture all data mutation is via database triggers.\n\nIt is usually a good idea to explicitly check for expected conditions and parameter values at the start of public interface methods. For example, if a parameter can\u2019t be nil, raising an exception if nil is passed. This translates into very loud failure scenarios, but ultimately ensures that problems bubble up and get fixed, and makes bugs easier to track down.\n\nRuby\u2019s weak typing makes it more difficult to enforce these rules. In strongly typed languages, the compiler complains if you pass in an int where a string is expected.\n\nFurthermore, Ruby has lots of cases where nil is used as a sentinel to signify \u2018no value returned\u2019. This is dangerous because nil is also the default value of an uninitialized variable. This makes it difficult to identify bugs where a variable name is mistyped or otherwise not properly initialized.\n\nTaking into account those issues, payments might seem more suited for a strongly typed language where the database access model is more restrictive. Yet, at Airbnb we use Rails for our payments stack. Instead of ditching Rails for its drawbacks, we\u2019ve come up with solutions for its main issues, making it work well for payments. This enables us to continue to enjoy the many benefits it provides.\n\nA key requirement for proper audit trails is reduced access of code to the raw database data. To achieve this we\u2019re starting to use a layer on top of ActiveRecord we call ProtectedAccess which controls changes to the underlying table and ensures an audit trail is written. It also greatly reduces the ActiveRecord surface area by not exposing all of its data mutation methods by default, and thus forces developers to reason about what methods they call, making it more likely that unwanted data mutations are caught during code reviews.\n\nProtectedAccess is designed to replace an existing ActiveRecord::Base class. It exposes some of the same interfaces, but actually hides most of the ways to mutate an object. The idea is to disallow most mutations to a Model that can\u2019t easily be audit-trailed, and for those that can, create a shim that transparently creates the audit trail.\n\nFor example, suppose you have a Payment model that has an amount field. By virtue of extending ActiveRecord::Base, it exposes a setter amount=. Any piece of code can call:\n\nwhich would create an un-audited modification of the payment record.\n\nIn comparison, ProtectedAccess creates a shim between the surface area, which still contains amount= and save, and the ActiveRecord model. When save is called it instead creates and saves a new version of the record instead of overwriting the existing version. There is no way for anyone to call the actual model that has access to the Payments table because it does not exist, save for a private member of the class Payment < ProtectedAccess object.\n\nWe use a declarative framework for parameter validation that enables service objects (as well as methods) to check that parameters are of the correct type and value range, ensure mandatory parameters are present and also ensure that no unexpected parameters are passed in. It is loosely inspired by Guava\u2019s Preconditions class.\n\nAn example might be an implementation of the validate method of a service object:\n\nThis is not an attempt to make ruby strongly typed (though that might not be such a bad idea\u2026), but rather a way to explicitly declare and enforce the dependencies of an object or a method. It is also not dealing with instantiation, as some dependency injection frameworks do. But it can make it clearer to someone reading a piece of code what its dependencies are. It also causes faster failures. Without validation a bug would cause an unexpected value to propagate through the code until (hopefully) something blew up or (more likely) a weird result was displayed to a user with no indication as to the source. With validation an exception is thrown, a process crashes, and developers get alerted to the problem. Arguably, it is better to tell the user something went wrong rather than to present something that might be completely off.\n\nFreezing constants is another related method we use, above and beyond the warnings the ruby interpreter issues when modifying a constant.\n\nParameter validation is very generic, and can be used with any Ruby application. We\u2019re currently evaluating when the right time might be to open source it.\n\nInstead of placing business logic in models, we keep models lean and focused only on data retrieval and storage. All business logic goes in service objects, which are single use business logic objects that are initialized with a clearly defined set of parameters, and then perform an action on them. They tend to be smaller in size and very focused on executing a particular task. Because they have a well defined set of parameters, they are much easier to test, and can be tested primarily using mocking (as opposed to creating test database entries or having test network services).\n\nAs an example, suppose you have some code that makes a call to some external gateway for completing a transaction. It might look like\n\nA service object might look like:\n\nCalling this service object would be done like this:\n\nThe service object framework will do some pre flight logging and then call perform, following it with post flight logging.\n\nNote that validation, logging and other administrativia are now segregated to their own methods, and the perform method can focus on pure business logic. It can make lots of assumptions on the parameters, as they have been strongly validated. The interesting part of the code is also more easily accessible for someone trying to figure out what it is doing.\n\n Because the o.validate must declare every parameter that can be passed to the service object, it is really clear what it depends on. This is a form of code-documentation that does not go out of date.\n\nTo make code easier to understand, we use rubocop and cane in some of our repos. These tools enforce a common style on our code (rubocop) and ensure code is not overly complex (cane).\n\nRubocop is great for helping teach newcomers the \u2018lay of the land\u2019, and also keep everyone in sync as to what is the expected style of code. Cane\u2019s ability to measure ABC code size metric is very useful in forcing the creation of small, easy to read and understand methods. We\u2019ve occasionally seen cases where the cane threshold we chose (15 ABC score) was too restrictive, but generally it has pushed us to remain clear and concise in our code.\n\nTake for example the following condition:\n\nThis has the ABC score of 12.\n\nCompare it with the equivalent:\n\nwhich has an ABC score of 8.\n\nCane does not propose those changes, it simply alerts you that there is too much complexity. It is up to the team to establish rules of thumb on how to reduce complexity when that happens.\n\nThe practice of DRY-ing things up is used quite heavily in the way of gems or mixins.\n\nGems are typically used to extract commonly used functionality that needs to be shared among more than one internal app. Some of our gems are also open source. Aside from those open source examples, we also have internal gems for securely sending credit card information to our credit card processor, for keeping currency information consistent among our various applications and for argument validation.\n\nIn cases where functionality needs to be reused within the same application, we sometimes use module mixins \u2014 putting functionality in a module and then including it where it is needed.\n\nA common scaling strategy in a MySQL environment is to use database replicas for reads, thus reducing the traffic on the master MySQL. This has the disadvantage that your code might sometimes get stale data. For payments, we mostly disable replica reads, trading off load on the main database for data integrity. E.g, it is not acceptable for a new transaction you make to not include the results of the previous one.\n\nOver the past several years we have learned a lot about building and maintaining large scale payments systems. Some of what we have learned is already implemented in our code base and is part of our daily best practices. Other learnings are still fairly new, and the frameworks they rely on still under construction. As always, our system continues to evolve and we\u2019re constantly looking for ways to make it better and more robust.\n\nIf you\u2019re interested in taking a more active part in our journey towards a better, more scalable and robust system, I\u2019d love to hear from you \u2014 michel.weksler@airbnb.com"
    },
    {
        "url": "https://medium.com/airbnb-engineering/mapping-the-world-a631a96a3b3a",
        "title": "Mapping the World \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "This is all about \u201cAT-AT,\u201d a new internal tool we built to understand locations and their relationships to other locations. It looks like this:\n\nWhich is true, so what\u2019s all this about?\n\nThey say, \u201cthe world is already mapped! There\u2019s Google Maps, Foursquare, Yelp, Garmin, National Geographic, Zheng He, Isabella Bird, Lewis & Clark & Sacagawea, Leif Erikson, the Planet Earth series narrated by David Attenborough\u2026\u201d\n\nPeople are always surprised when I tell them about Zack Walker, Airbnb\u2019s in-house cartographer , who works on mapping the world.\n\nAt Airbnb, the question we want to answer is how do you understand a place without ever having been there?\n\nHow do you capture why your friend said you wouldn\u2019t like Fisherman\u2019s Wharf, but you should still go there to get a double double from In N Out?\n\nHow do you know where in the world will make for your perfect trip?\n\nWe currently tackle this problem in a number of ways:\n\nThese products combine to help Airbnb travelers discover all of the wonderful places that make up a city, not just downtown areas where traditional accommodations are normally found.\n\nLocation is a crucial building block for a number of our engineering efforts. Our team needs to help the Airbnb traveler mitigate their number one concern when finding a place to stay: location. On the flipside we want to highlight the million-plus unique listings opened up by Airbnb hosts around the world.\n\nNeighborhoods can only take you so far when trying to match you with the perfect place to stay.\n\nThere are culturally significant areas or regions that aren\u2019t drawn on maps. They mostly live in the minds of the local community that calls those areas home. These areas might be a unique part of town along a single road, cutting through 3 neighborhoods. Or they might be a way to describe something in common among multiple neighborhoods.\n\nFinding and mapping these places takes a heroic effort on Zack Walker\u2019s part. He researches historical and current data, talks with folks from the community, and answers emails from hosts to build a clear image of how locals understand their place in the world.\n\nSo when you\u2019re looking for a place to stay in \u201cwine country\u201d in Northern California we should be showing you Napa and Sonoma and helping you understand the differences and similarities between the two counties.\n\nWhile we were very excited to make this happen, this wasn\u2019t possible with our current architecture and existing internal tools. We needed something bigger.\n\nA small team consisting of Christopher Lin, Alex Blackstock, Daniel Loreto, and myself set out to build the tool to help Zack Walker\u2019s pursuit of mapping the world.\n\nA note on the project code name: Zack wouldn\u2019t let us call it \u201cThe Walker System\u201d, so Christopher Lin did the next best thing and used a Star Wars reference (the Imperial Walker or All-Terrain Armored Transport, commonly called \u201cAT-AT\u201d).\n\nAT-AT\u2019s tech stack looks like this:\n\nOur first step was generalizing the existing system so it could handle various geographical polygons (continents, countries, market areas, special regions, towns) and not just cities and neighborhoods."
    },
    {
        "url": "https://medium.com/airbnb-engineering/meet-the-nerds-eric-levine-5862bf8b7ac1",
        "title": "Meet the Engineers: Eric Levine \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Howdy, Eric Levine! Eric is an engineering manager on our Trust and Safety team. Eric tells us about keeping bad guys at bay and going for swims in a Turkish bay (okay, just a pool, but sounded better to say bay).\n\nMy path toward computer science started when my brother and mentor Matthew came home from university and decided that he wanted to teach his kid brother how to write some code. He taught me the basics and helped me pick out a book to continue my development. I just ran with it from there, and by the time I finished high school I was running an online game with about a dozen regular users.\n\nMy path to Airbnb was a long one, starting in 2008. I received an email from a recruiter who worked at YouTube at the time who asked if I was interested in an internship. That internship definitely changed the course of my professional career in a very positive way. Fast forward four years, and that same recruiter from my YouTube days, reached out to me about a position at Airbnb. Due to the previous encounter, I knew I could trust her, and Airbnb seemed pretty amazing from the outside. I was working at Google at the time and decided that I didn\u2019t think the skills I was accruing were as portable as I was hoping they\u2019d be. You don\u2019t take a lot of Google\u2019s infrastructure with you when you leave and I wanted to be more versatile in my skill set. Airbnb was the perfect fit and it has been one of the best decisions I\u2019ve made.\n\nThe most interesting technical challenge that I\u2019ve worked on since joining has definitely been our machine learning-based risk detection systems. My colleagues Naseem Hakim and Aaron Keys wrote up a great blog post describing some of the systems that I\u2019ve worked on, and the next iterations are definitely taking it to the next level.\n\nWe\u2019re hoping to start thinking more about Verified ID and how we can do even more with that. While it\u2019s been a great start, we can start to apply our learnings from other areas and try to apply it to identity to better understand the users of the platform. Further, we have a ton of awesome work planned to improve our systems to further help protect the community.\n\nMy favorite core value is Embrace the Adventure. The way I interpret this core value is a recognition that perfection is an impossible ideal and that one should try to \u201croll with the punches\u201d when faced with undefined situations. We try to apply that in our engineering culture by encouraging people to fix things that are broken and to really take ownership of our product. It\u2019s this core value and the way we apply it that makes Airbnb so unique.\n\nMy favorite Airbnb experience was with a couple in a Kirazli, a small village in Turkey. The space itself was absolutely stunning, beautifully designed and the perfect fit for our stay. Every morning I\u2019d wake up to the village\u2019s morning call to prayer. After lazily awakening, I\u2019d go for a quick swim before the hosts would treat me to a traditional Turkish breakfast. The hosts would then take me for a walk through the hills to pick fresh figs off the trees. The whole experience was just extraordinarily pleasant and inspiring."
    },
    {
        "url": "https://medium.com/airbnb-engineering/meet-the-nerds-carol-leung-590a6a6d7fdd",
        "title": "Meet the Engineers: Carol Leung \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Welcome to our second installment in our series that peeks behind the curtain of Airbnb\u2019s engineering team. Today, say hello to Carol Leung, an Engineer on our mobile team working on our Android app. Carol shares with us her experiences from founding her own photography start-up in Hong Kong to making the Airbnb mobile app seamless to use.\n\nMy high school started offering a computer science class in my sophomore year. I was curious and took the intro and follow-up CS classes. I was fascinated by how applicable CS is to so many things in our daily life. I was inspired to build a program that simplified the process of matching students with their interests for our annual \u201cDiversity Day.\u201d The experience of being able to build a solution to a real problem set me on the path to become a software engineer.\n\nI love travel and immersing myself in different cultures. I have lived and worked as an engineer in different parts of the world: building algorithmic trading systems at Goldman Sachs Tokyo, co-founding my own digital photography software startup in Hong Kong, leading a mobile development team at XtremeLabs (now Pivotal) in Toronto, and building an iOS and Android app from the ground up for a small social network startup after I moved back to San Francisco. My love of travel (and software development) lead me to Airbnb. I love that I can use my passion for traveling to help shape a product that enriches other people\u2019s journeys.\n\nGuests have traditionally felt more comfortable browsing and searching for listings on the web than on mobile. It\u2019s my mission to change that perception by creating an outstanding guest experience on mobile. The challenge is creating a seamless, cohesive user experience across a variety of complex features in the app: Wish Lists, optimized search and filtering, embedded maps in the listing feed, listing navigation flows in map search. My teammates and I strive to hide that complexity from users and make the experience intuitive and \u201cjust work.\u201d\n\nI think we can go beyond helping guests find their ideal place. We can empower them to plan, build and experience the trips of their dreams. I want to build a mobile experience that would allow guests to enjoy a delightful journey from when they first start thinking about their trip. Searching, planning, and booking should be frictionless, delightful, and personalizable. We want users to be able to easily collect and compare listings they browse across different devices, make collaboration with friends and families on group trips fun, and surface recommendations based on users\u2019 previous searches and location to inspire more meaningful trips.\n\n\u201cEvery frame matters!\u201d As a mobile engineer, I look at every UI element on every screen with a critical lens. I constantly ask myself: is this iconography clear? Does this animation feel natural? Are the widgets user-friendly? Does this navigation and transition make sense? Does this screen provide enough context while being pleasant to use? These questions often lead to great discussions with my teammates: engineers, designers, and product managers. \u201cEvery frame matters,\u201d is also a starting point when we create technical designs and review code. This detailed-oriented attitude allows us to constantly improve code quality and structure of the app. It also yields world class products.\n\nWhile I\u2019ve enjoyed Airbnb in large cities like New York, Barcelona and Tokyo, my favorite was a lovely, cozy listing in Sonoma with my boyfriend and his family. The host thoughtfully wrote down great recommendations of local restaurants and vineyards on their cute, colorful kitchen chalkboard, and left us a fruit basket and a complimentary bottle of wine as welcome gift. I still remember the delicious smell from their spacious, beautiful garden growing fresh tomatoes and herbs. We had a blast relaxing and playing croquet and horseshoes in their backyard at night, a perfect way to end our wine country trip."
    },
    {
        "url": "https://medium.com/airbnb-engineering/maintaining-quality-at-scale-a3b0ffa03ef9",
        "title": "Maintaining Quality at Scale \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "When I joined Airbnb in the fall of 2012 things were a little chaotic to say the least. At some point in the preceding year the growth of the company had kicked into high gear, or hyper-growth, as some call it. Along with the increase in site traffic and transaction volume, the engineering team also expanded very rapidly. When I arrived we were a team of about 40, compared to 16 one year before (and over 130 today). Looking back now, I think we were in the midst of a make-or-break period in Airbnb\u2019s history.\n\nGrowing a website and engineering team as quickly as Airbnb presents both technical and cultural challenges, and in many cases those challenges are intertwined. In the old days, we were struggling with many issues, but broadly those issues were related to scaling a monolithic Ruby on Rails application (which wasn\u2019t built with scaling in mind) and scaling the engineering team in a way that facilitated the former.\n\nToday, I think it\u2019s fair to say that we\u2019ve overcome the existential challenges we faced back then. This is not to say we don\u2019t have numerous new challenges and hard work ahead of us, but we survived the kick into hyper-growth without tearing apart. There are a lot of different things we can look back on, but in this post I\u2019d like to focus on how we have transitioned to writing code in a more maintainable (and thus scalable way).\n\nI work on the Payments Team at Airbnb. For us, code quality, robustness, and maintainability are of the utmost importance. With the amount of transactions flowing through our system these days, we literally can\u2019t afford even small mistakes. Over the past year, we\u2019ve put a lot of work into figuring out how our team can continue to iterate quickly while still producing quality code and avoiding technical debt. The three things that I think have been most helpful at this level are forming and adhering to specific code style and conventions, practicing a comprehensive and collaborative peer review process, and testing.\n\nCode conformity relates both to syntactic/textual style and to applying conventions and best practices when writing code. It has been said many times before, but if you have a codebase where you can open up a file and tell which person on your team wrote it, that\u2019s a very bad sign. Formulating a standard way of writing code and getting everyone on board has been highly beneficial for us.\n\nComputer code is generally consumed by two classes of entities, machines and humans. Machines generally don\u2019t care about what your code looks like (assuming it compiles), but humans on your team probably do. Having a wide variety of styles and approaches to similar problems in your codebase will create unnecessary cognitive load for everyone, and eat up a lot of time as people attempt to grok new terrain. On the other hand, if everyone on the team is writing code in a more-or-less identical manner, everyone on the team should be able to grok, debug, and/or maintain one another\u2019s code with close to the same level of effort (once they get through the initial ramp-up). Put another way, it\u2019s important to favor the human reader, not the machine, when crafting your code, and it\u2019s even more important to be consistent. This is not to say you should chose an O(N) solution over an O(1) solution for readability or simplicity\u2019s sake, but there are many steps you can take to facilitate your fellow humans through code conformity.\n\nWe\u2019ve achieved better code conformity in two ways. The first is the use of style guides (of which we\u2019ve created several of note). Our most detailed guides cover JavaScript and Ruby, however we also have internal guides for RSpec, API design, service design, etc. Although these guides grew from our specific experience, some of them are now public and utilized by other individuals and organizations (who then often contribute back). Depending on the type of application you\u2019re working on, they may or may not be useful for you, but if you work with JavaScript or Ruby, I\u2019d encourage you to check them out, if for no other reason than inspiration.\n\nSome style rules we\u2019ve settled on are completely arbitrary. Take for example indentation characters (tabs vs. spaces), or which line the opening brace goes on. It\u2019s hard to argue for one over the other in cases like these. The important thing is settling on something and then having everyone stick to it. On the other hand, some seemingly simple rules can have vast ramifications.\n\nConsider line length. We like to keep lines short and sweet. Shorter lines are not only more friendly to other engineers\u2019 eyes (and editors), but they also tend to result in cleaner, simpler statements (especially when paired with descriptive variable names). Methods that consist of a series of short, simple statements are easier for other team members to comprehend and modify. Diffs (in version control tools such as Git) also end up showing more clearly what behavior was changed between commits. And, since we\u2019re also writing testable code (see \u201cTesting\u201d below) which promotes small, concise methods, you start to develop a very clean, modular, and easy to understand codebase.\n\nConsider this example:\n\nImagine we wanted to tweak the behavior of this line by switching from downcase to upcase. The diff would end up looking something like this:\n\nIt\u2019s hard to tell which part changed without parsing the line in your head. Had the code been written more like this:\n\nThe diff would also be much cleaner, showing exactly what behavior changed:\n\nThe funny thing is that those of us who started promoting this faced a good deal of pushback in the beginning. Like testing, however, we were persistent and we\u2019ve gotten to a place where this has become second nature. To see some extreme examples of this mentality, I suggest checking out the Joint Strike Fighter C++ coding guide or the JPL C Coding guide. Obviously these standards are overkill for most consumer web applications, but always keep in mind what type of application and goals you have when settling on rules. It\u2019s about picking the right balance.\n\nAs mentioned before, we\u2019ve also begun to create style guides for higher-level concepts such as API and service design. Although prescribing how to build things like this can be a bit more of a gray area, it\u2019s been incredibly useful to consolidate the collective knowledge of the team into single resources that are easy to digest. When coupled with a peer review process and a collective insistence on conforming, we\u2019ve eliminated a lot of pain points and needlessly repeating conversations.\n\nAnother way we have been able to achieve more consistent code is by having a comprehensive and collaborative code review process. Code reviews come with many benefits. The obvious case that comes to mind is a colleague catching a horrible bug before it goes out and brings the site down. But there are many more subtle benefits. Over the past year or so, we\u2019ve instituted code reviews for every change and we like to see at least one other person to sign off on each diff before it goes out.\n\nHaving just one other person sign off, however, is the bare minimum. We encourage everyone who has context to get involved, and most non-trivial pull requests have at least one or two substantive conversations pop up. Sometimes these can get pretty deep and some of us end up learning something we didn\u2019t know about (credit card processing, database internals, and cryptography come to mind). If there are big insights we are also sure to record those in the relevant guide/documentation. Most importantly, there is no authority in code reviews based on seniority or title. Everyone\u2019s input is valid and yet best practices always tend to prevail and new team members tend to learn quickly.\n\nIt\u2019s important to remember that just because a style guide exists doesn\u2019t mean that it will always be followed (or even read) or that it addresses every case. Peer reviews are what really help people get on the same page. They are also an effective way to learn and teach the art of programming itself, which can\u2019t be as easily captured in pre-written material. In addition, having an environment where people learn a lot on the job helps keep them engaged and boosts the quality of their work.\n\nAs an illustration of something that might not belong in a style guide because it falls more into the wisdom category, take this example.\n\nWe used to run into a lot of examples like the first one in our codebase. While they both output the same thing, the first example is far more expensive, and at scale could actually cause a catastrophe. Adding select to the User.active proxy object means that Rails will go to MySQL, fetch every active user, instantiate them, put them into an array, then iterate through that array and select the users with country equals \u2018US\u2019. All this just to get the count.\n\nIn the second example, we start with the same proxy object, Users.active, but then we use where to filter on that object. The first line does not trigger any DB queries! On the next line, when we ask for the count instead, Rails knows to just do a SELECT COUNT(*) query, and not bother fetching any rows or instantiating any models.\n\nKnowing the language and framework well is very important, along with spreading that knowledge to everyone involved. We have been diligent at Airbnb about fixing these types of common mistakes and promoting the best practice of pushing work to the DB when it makes sense. Take another example. How much did we pay out today?\n\nThe first example could be worse, we do at least narrow the scope to a single day, but it still causes a large number of payout objects to get fetched and instantiated, just to sum a numeric field. In the second example, we have MySQL do the summing during the lookup and just return the number we care about. This comes at essentially no extra cost to MySQL yet we avoid both unnecessary network traffic and a potentially vast amount of processing in Ruby.\n\nSo one benefit to having a healthy review process is to formulate and teach conventions and best practices that will benefit everyone. These seemingly trivial examples make our site more responsive (and can even prevent it from going down). Things like this also might have the side effect of striking up conversations about database internals or financial reporting. The thing about this kind of knowledge is that it\u2019s shared and it would be hard to consolidate it into a single resource (and expect everyone to read that resource). It gets introduced by new team members, or veterans who have been bitten, and then passed down through the generations. It can only live on, however, if there is an active, healthy dialog surrounding the production of code. Personally, I feel as though the rate at which I\u2019m learning in this environment is much greater than in previous positions I\u2019ve held where code reviews and quality weren\u2019t taken as seriously.\n\nI\u2019m not going to go too deep into testing in this post because my colleague Lou recently published an excellent post of his own on this blog. What I will say though, and I don\u2019t think I\u2019m saying anything new, is that I believe testing is extremely important to writing quality, maintainable code. However, it\u2019s not just about raw coverage. It\u2019s about creating culture in which testing becomes second nature to everyone on the team. When writing tests becomes second nature, writing testable code becomes second nature. At this point you really start to see dividends.\n\nSo concluding this little overview, I\u2019ll summarize the three main topics again. Code conformity. On a professional team of any size, it\u2019s important for everyone to write code the same way. This helps spread good practices and makes maintaining other peoples\u2019 code much easier. Style guides are a great starting point. Even seemingly simple style choices can impact code quality, robustness, and ease of refactoring. Active, healthy review process. Above maybe all else, it\u2019s important to foster an active and healthy review process. At least one person besides the author should look at each diff, and suggestions should be gladly given and welcomed by team members of any level. This helps spread wisdom. It helps newer team members learn and develop good habits. It helps more senior members teach and stay honest. Ultimately it can keep your application from failing spectacularly. Testing. It\u2019s important to have a strong testing ethic, and not to compromise or take shortcuts. The more we test, the more second nature it becomes, and we might eventually even learn to love it. It doesn\u2019t have to start by decree, but can grow organically in a few spots until it begins sprouting up everywhere."
    },
    {
        "url": "https://medium.com/airbnb-engineering/meet-the-nerds-scott-raymond-bc9c7e74a7f6",
        "title": "Meet the Engineers: Scott Raymond \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "We\u2019re starting up (or restarting depending on how you look at it) a series that looks at life inside the Airbnb engineering team and introduces you to some of our engineers. In this first installment, Engineering Manager, Scott Raymond talks about his early days in computer science, transitioning from founding Gowalla to working on the mobile apps at Airbnb, and of course his favorite Airbnb experience.\n\nMy first rudimentary childhood programming was in BASIC, on a Commodore VIC-20, a hand-me-down from my uncle. The only storage was on cassette tape. A little later, I got access to a Mac SE/30 with HyperCard, and things really got rolling, making games and graphics programs. Eventually we got a modem, which allowed me to connect with the wider world of programming. In college, I actually studied Linguistics instead of comp sci \u2014 and every once in a while, I get the chance to apply that training in my work.\n\nIn 2007, I co-founded a company called Gowalla, whose mission was to to get people out exploring the world. The app was like a digital passport that you could fill with stamps \u2014 unique illustrations from thousands of places around the world. We were acquired by Facebook. After some time, I got to know a few folks here at Airbnb, and became enchanted by the product. It\u2019s been really cool to pick up on some of the themes from Gowalla, especially connecting people with unique places.\n\nWe are constantly looking for ways to improve the core experience (like performance and stability), running experiments to make the marketplace efficient, and building new features to make travel better. The hard part is doing all three at once. We spent a lot of time this year working on tools and processes that allow us to iterate really quickly, without letting quality slip. For one example, see my co-worker Zane\u2019s recent post about our project to redesign Airbnb for iOS.\n\nRight now, we\u2019re thinking a lot about how people move between devices. They might use their tablet to browse listings, and their computer to book, and their phone to communicate with their host. I think we can make the seams in that experience much smoother. If we\u2019re successful, the logistics of travel fade into the background, so that people can focus on the joy of travelling.\n\nEasy: \u201cBe a Host\u201d. I actually think about this all the time, even when planning how to code a feature in our app. There are all kinds of things that can go wrong with technology, especially when you\u2019re travelling \u2014 you might be on a flaky network, run out of space on your phone, whatever. Like a great Airbnb host, our app can try to anticipate needs and deal with snafus graciously.\n\nI recently took a great trip with my wife and 3-year-old daughter to Paris and Amsterdam. Our listing in Amsterdam was this delightful place on a canal, with the steepest staircase I\u2019ve ever seen. We woke up in the morning and the street outside had transformed into a massive bustling Saturday market."
    },
    {
        "url": "https://medium.com/airbnb-engineering/taking-flight-women-in-tech-series-c92d73d96bd1",
        "title": "Taking Flight \u2014 Women in Tech Series \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "This past June we hosted our first Taking Flight event. The evening centered around women in engineering and the presentations and discussions were wide ranging, looking at everything from \u201cInfrastructure, demystified\u201d to \u201cHow to build your network\u201d. The highlight of the night was the breakout sessions where people were able to ask each other questions and engage in a small group.\n\nWe left inspired by the camaraderie that was built from the gathering. We decided to make Taking Flight a more regular series and we are excited to share our additional installments. Last night, we hosted \u201cSmall Talks, Big Data\u201d which featured lightening talks from women in Data Science (including our very own Lisa Qian).\n\nOur next event will be held on Monday, November 17 with Niniane Wang, CEO of Evertoon (former CTO, Minted) and Varsha Rao, Head of Global Operations, Airbnb. They both will be discussing their experiences within the tech industry. We hope you can join us!"
    },
    {
        "url": "https://medium.com/airbnb-engineering/interning-with-the-airbnb-payments-team-af8cbe51eae0",
        "title": "Interning with the Airbnb Payments Team \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "My name is Rasmus Rygaard, and I\u2019m a graduate student at Stanford University about to wrap up my masters in computer science. I have spent the last 12 weeks interning as a software engineer at Airbnb for the second summer in a row. This year, I joined the Payments team to help build out our global payments platform. It has been a fantastic experience, and I have been amazed by the kind of responsibility I have gotten as an intern. During my internship, I sped up our financial pipeline by more than an order of magnitude, isolated and upgraded the code that lets guests pay with credit cards, and set up payouts through local bank transfers that will save our hosts millions of dollars in fees.\n\nMy main project for the summer was improving our financial pipeline. Airbnb is fortunate to have plenty of graphs that go up and to the right, but when those graphs track application performance, we would rather have them stay flat. To help our Finance team understand our business, the Payments team built a service to generate and serve key business metrics. That service, however, was not scaling with our business. The Finance team depends on having fresh data available at all times, but our infrastructure was struggling to deliver results fast enough. My task was clear yet open-ended: Speed up the pipeline.\n\nI considered several approaches to speeding up the pipeline, but I ended up adapting our existing MySQL queries to be runnable in Presto. Presto, is a distributed in-memory query engine built at Facebook, is heavily used for ad-hoc analytics queries at Airbnb. Presto\u2019s in-memory computation provides significantly better performance than Hive, and for a task that required significant financial number-crunching, it seemed like the obvious choice. In addition to translating the queries, I built a pipeline that will help us seamlessly run complex queries in Presto even for data that lives in MySQL. The system is written in Ruby, which lets us leverage Presto\u2019s computational power from our existing Rails application. As a result, the functionality that we have moved to our new infrastructure now runs 10 to 15 times faster.\n\nMy second project was to reorganize the way we process credit cards when you book a trip on Airbnb. I upgraded the JavaScript code that handles checkout logic so our system no longer needs to be exposed to sensitive credit card information. Now, we exchange the credit card information for a token in the browser when a user books a listing. This token works as a replacement for the credit card information with our credit card processing partner so we can complete the transaction exactly like before.\n\nThis was a high-impact project that had to be treated like surgery: We were replacing code on our perhaps most important page, and our guests should not experience any bumps along the way, nor should we cause any downtime. In addition to upgrading the code, I extracted this functionality into a separate module that we now share between our Rails and Node.js apps for web and mobile web. This required decomposing the existing code, but as a side-effect, we now have much more modularized code around capturing and processing transactions that we can test more aggressively.\n\nWherever possible, we want to pay our hosts in their local currency. A convenient way of doing that is through bank transfers that are faster and less expensive than international wire transfers. My final project for the summer was to work with a third party provider to start supporting local bank transfers in a number of markets where the existing methods for payouts are slow or expensive. I was the point person for our engineering side of the integration, working on the technical integration with our provider and on product decisions with our own Global Payments team.\n\nWe rely on several partners to support our platform, so the experience of partnering with another provider was not new to the team. I, however, had never worked this closely on a third party integration before. Still, my team trusted me to see through a project that would end up delivering payouts to hosts in countries around the world. Although work that requires syncing with an external company can be difficult to plan, the experience of balancing interests, requirements, and timelines between us and our partner gave me real world experience that no other class project or internship could provide. Our hosts in these markets can now look forward to saving millions of dollars in processing fees that would otherwise be charged by the providers of the existing payout methods.\n\nMy 12 weeks at Airbnb have been an amazing learning experience. As an intern, I have worked on projects that had significant impact on our hosts, guests, and core business. I have seen the global scale of our payments platform and how a small team of talented engineers make sure that money flows safely from guests to hosts on Airbnb. I\u2019ve been blown away by the visible, high-impact projects that the company trusts interns with, and it\u2019s clear that interns are treated just like full-time employees."
    },
    {
        "url": "https://medium.com/airbnb-engineering/inside-the-airbnb-ios-brand-evolution-33b4fa994833",
        "title": "Inside the Airbnb iOS Brand Evolution \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Earlier this year we embarked on a six month project to refresh the mobile app, to coincide with the launch of our updated brand. The Airbnb product teams continued to ship new features for our mobile apps even during the months it took to implement the new brand. Logistically, this is the equivalent of repainting a plane while it is in flight \u2014 without any of the passengers knowing it. When it landed, we also wanted it to be a surprise.\n\nWhen we embarked upon the brand evolution we chose to use small and nimble engineering teams to prevent blocking each other. When the spec for a project is changing it is easy to have too many cooks in the kitchen as engineers begin to move faster than the shifting designs. Until the last month the team consisted of one engineer on each mobile application and three on the website. Part of the logic behind this decision was also an acknowledgement that design is an iterative process: no PSD file survives first contact with the user. A small engineering team is better able to adapt, with fewer places for communication to breakdown.\n\nThe design team had already been hard at work for months creating the new brand, but they brought the engineers into the room early to start helping to scope the project. At this point, we were considering:\n\nEven thinking within the confines of reskinning the application using a small team, it became apparent that there were some quick usability wins to be had. For example, we changed the background color of our side-drawer (Airnav) to be dark, which greatly improved readability. For contrast, here\u2019s the before and after shots of our navigation system:\n\nWe made the tabs on the Reservation screen more obviously buttons, which improved the ability of users to discover functionality. It was tempting to say \u201cyes\u201d to every great idea dreamed up in these design/engineering meetings, but the constraint of small engineering teams meant that we were hyper aware of our limited resources.\n\nWe had two options for how to structure our workflow. We could have constrained the branding code to a long-lived Git branch, or we could have kept merging to master and split the XCode targets. We decided to pursue the latter approach for a number of reasons. Most notably, we decided that even though there was only one engineer responsible for the brand evolution itself, team members would be responsible for ensuring that new features were compatible with the brand evolution.\n\nAfter duplicating our target and swapping out the app icon, we now were faced with a new set of challenges. First, we needed to make sure no assets were leaked by accidental inclusion in the distribution target. We meticulously examined every .ipa which was built for leaks, and even went so far to use code words as symbol names in the code to prevent leakage through reverse engineering the binary.\n\nNext, we needed to decide how to branch the code. On the surface this was simple: we just added a preprocessor macro to the new target called STYLE_PRETZEL. Then, we could use #if defined(STYLE_PRETZEL) in our code and rest assured that anything in the block would not be compiled into the existing deployment target. There were definitely drawbacks to this method, such as:\n\nIn some ways, these restrictions actually helped us: it forced us to think about the minimum viable code change necessary to accomplish the task. In other words: if you\u2019re writing lots of code, you\u2019ve probably moved beyond reskinning.\n\nOur internal styling library, Nitrogen, allowed us to swap out colors, fonts, and basic component (\u201ctoolkit\u201d) styles. This wholesale replacement was then followed by multiple progressive refinement passes across all the screens. We liked to think of it like the progressive rendering of images downloaded over a slow internet connection:\n\nAfter the toolkit came the \u201cfirst-cut\u201d, at which point the app definitely looked new, but was still nowhere near Airbnb\u2019s standard of design perfection; we essentially eyeballed the changes:\n\nNext was \u201credlining.\u201d The production team took the PSDs provided and used software to call out the exact fonts, colors, margins, etc.:\n\nWith nested views, subclassed controllers, and other pitfalls it can sometimes be hard to know that the design is actually implemented to-spec. To check our work, we used Reveal to compare the correct values to the actual implementation:\n\nWith about a month left until the big unveiling, we deleted the old XCode target and removed the preprocessor macro and #if defined statements, locking us into deploying the next version of the app with the brand evolution. Not only were all mobile team members now working with the reskinned app, but it was also distributed to employees of the company to start testing.\n\nOne of our mantras at Airbnb is that \u201cevery frame matters,\u201d and now was the time to prove it. Designers began going through the app with a fine-tooth comb, and engineers\u2019 queues filled with bug reports. It cannot be stressed enough how important these last few weeks were. This is the point at which we were moving text by one pixel here and one pixel there, yet all these pixels added up to the difference between a good app and a clean app.\n\nThe biggest oversight we made was to not think about localization earlier. Airbnb is an exceptionally global brand, and it is paramount that the app look stunning no matter what language it is shown in. This speaks to a greater need to \u201cdesign for the dynamic.\u201d We as engineers need to do a better job communicating with the design team about how they imagine the implementation changing. Not just for different sizes of text, but for animations, screen sizes, etc.\n\nThe early decision we made to split the XCode targets rather than creating a Git branch proved to be a good one. The result was that the rest of the team was at least superficially familiar with the new code, and when we called for All Hands On Deck there was minimum friction.\n\nOverall, the process went exceptionally smoothly. Teams across the company continued to work on their own projects until just a few weeks before launch. On the big day, all of the teams pressed the \u201claunch\u201d button at the same time, and the world saw the new brand across all channels."
    },
    {
        "url": "https://medium.com/airbnb-engineering/architecting-a-machine-learning-system-for-risk-941abbba5a60",
        "title": "Architecting a Machine Learning System for Risk \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "At Airbnb, we want to build the world\u2019s most trusted community. Guests trust Airbnb to connect them with world-class hosts for unique and memorable travel experiences. Airbnb hosts trust that guests will treat their home with the same care and respect that they would their own. The Airbnb review system helps users find community members who earn this trust through positive interactions with others, and the ecosystem as a whole prospers.\n\nThe overwhelming majority of web users act in good faith, but unfortunately, there exists a small number of bad actors who attempt to profit by defrauding websites and their communities. The trust and safety team at Airbnb works across many disciplines to help protect our users from these bad actors, ideally before they have the opportunity to impart negativity on the community.\n\nThere are many different kinds of risk that online businesses may have to protect against, with varying exposure depending on the particular business. For example, email providers devote significant resources to protecting users from spam, whereas payments companies deal more with credit card chargebacks.\n\nWe can mitigate the potential for bad actors to carry out different types of attacks in different ways.\n\nMany risks can be mitigated through user-facing changes to the product that require additional verification from the user. For example, requiring email confirmation, or implementing 2FA to combat account takeovers, as many banks have done.\n\nScripted attacks are often associated with a noticeable increase in some measurable metric over a short period of time. For example, a sudden 1000% increase in reservations in a particular city could be a result of excellent marketing, or fraud.\n\nFraudulent actors often exhibit repetitive patterns. As we recognize these patterns, we can apply heuristics to predict when they are about to occur again, and help stop them. For complex, evolving fraud vectors, heuristics eventually become too complicated and therefore unwieldy. In such cases, we turn to machine learning, which will be the focus of this blog post.\n\nFor a more detailed look at other aspects of online risk management, check out Ohad Samet\u2019s great ebook.\n\nDifferent risk vectors can require different architectures. For example, some risk vectors are not time critical, but require computationally intensive techniques to detect. An offline architecture is best suited for this kind of detection. For the purposes of this post, we are focusing on risks requiring realtime or near-realtime action. From a broad perspective, a machine-learning pipeline for these kinds of risk must balance two important goals:\n\nThese may seem like competing goals, since optimizing for realtime calculations during a web transaction creates a focus on speed and reliability, whereas optimizing for model building and iteration creates more of a focus on flexibility. At Airbnb, engineering and data teams have worked closely together to develop a framework that accommodates both goals: a fast, robust scoring framework with an agile model-building pipeline.\n\nIn keeping with our service-oriented architecture, we built a separate fraud prediction service to handle deriving all the features for a particular model. When a critical event occurs in our system, e.g., a reservation is created, we query the fraud prediction service for this event. This service can then calculate all the features for the \u201creservation creation\u201d model, and send these features to our Openscoring service, which is described in more detail below. The Openscoring service returns a score and a decision based on a threshold we\u2019ve set, and the fraud prediction service can then use this information to take action (i.e., put the reservation on hold).\n\nThe fraud prediction service has to be fast, to ensure that we are taking action on suspicious events in near realtime. Like many of our backend services for which performance is critical, it is built in java, and we parallelize the database queries necessary for feature generation. However, we also want the freedom to occasionally do some heavy computation in deriving features, so we run it asynchronously so that we are never blocking for reservations, etc. This asynchronous model works for many situations where a few seconds of delay in fraud detection has no negative effect. It\u2019s worth noting, however, that there are cases where you may want to react in realtime to block transactions, in which case a synchronous query and precomputed features may be necessary. This service is built in a very modular way, and exposes an internal restful API, making adding new events and models easy.\n\nOpenscoring is a Java service that provides a JSON REST interface to the Java Predictive Model Markup Language (PMML) evaluator JPMML. Both JPMML and Openscoring are open source projects released under the Apache 2.0 license and authored by Villu Ruusmann (edit \u2014 the most recent version is licensed the under AGPL 3.0) . The JPMML backend of Openscoring consumes PMML, an xml markup language that encodes several common types of machine learning models, including tree models, logit models, SVMs and neural networks. We have streamlined Openscoring for a production environment by adding several features, including kafka logging and statsd monitoring. Andy Kramolisch has modified Openscoring to permit using several models simultaneously.\n\nAs described below, there are several considerations that we weighed carefully before moving forward with Openscoring:\n\nAfter considering all of these factors, we decided that Openscoring best satisfied our two-pronged goal of having a fast and robust, yet flexible machine learning framework.\n\nA schematic of our model-building pipeline using PMML is illustrated above. The first step involves deriving features from the data stored on the site. Since the combination of features that gives the optimal signal is constantly changing, we store the features in a json format, which allows us to generalize the process of loading and transforming features, based on their names and types. We then transform the raw features through bucketing or binning values, and replacing missing values with reasonable estimates to improve signal. We also remove features that are shown to be statistically unimportant from our dataset. While we omit most of the details regarding how we perform these transformations for brevity here, it is important to recognize that these steps take a significant amount of time and care. We then use our transformed features to train and cross-validate the model using our favorite PMML-compatible machine learning library, and upload the PMML model to Openscoring. The final model is tested and then used for decision-making if it becomes the best performer.\n\nThe model-training step can be performed in any language with a library that outputs PMML. One commonly used and well-supported library is the R PMML package. As illustrated below, generating a PMML with R requires very little code.\n\nThis R script has the advantage of simplicity, and a script similar to this is a great way to start building PMMLs and to get a first model into production. In the long run, however, a setup like this has some disadvantages. First, our script requires that we perform feature transformation as a pre-processing step, and therefore we have add these transformation instructions to the PMML by editing it afterwards. The R PMML package supports many PMML transformations and data manipulations, but it is far from universal. We deploy the model as a separate step \u2014 post model-training \u2014 and so we have to manually test it for validity, which can be a time-consuming process. Yet another disadvantage of R is that the implementation of the PMML exporter is somewhat slow for a random forest model with many features and many trees. However, we\u2019ve found that simply re-writing the export function in C++ decreases run time by a factor of 10,000, from a few days to a few seconds. We can get around the drawbacks of R while maintaining its advantages by building a pipeline based on Python and scikit-learn. Scikit-learn is a Python package that supports many standard machine learning models, and includes helpful utilities for validating models and performing feature transformations. We find that Python is a more natural language than R for ad-hoc data manipulation and feature extraction. We automate the process of feature extraction based on a set of rules encoded in the names and types of variables in the features json; thus, new features can be incorporated into the model pipeline with no changes to the existing code. Deployment and testing can also be performed automatically in Python by using its standard network libraries to interface with Openscoring. Standard model performance tests (precision recall, ROC curves, etc.) are carried out using sklearn\u2019s built-in capabilities. Sklearn does not support PMML export out of the box, so have written an in-house exporter for particular sklearn classifiers. When the PMML file is uploaded to Openscoring, it is automatically tested for correspondence with the scikit-learn model it represents. Because feature-transformation, model building, model validation, deployment and testing are all carried out in a single script, a data scientist or engineer is able to quickly iterate on a model based on new features or more recent data, and then rapidly deploy the new model into production.\n\nAlthough this blog post has focused mostly on our architecture and model building pipeline, the truth is that much of our time has been spent elsewhere. Our process was very successful for some models, but for others we encountered poor precision-recall. Initially we considered whether we were experiencing a bias or a variance problem, and tried using more data and more features. However, after finding no improvement, we started digging deeper into the data, and found that the problem was that our ground truth was not accurate.\n\nConsider chargebacks as an example. A chargeback can be \u201cNot As Described (NAD)\u201d or \u201cFraud\u201d (this is a simplification), and grouping both types of chargebacks together for a single model would be a bad idea because legitimate users can file NAD chargebacks. This is an easy problem to resolve, and not one we actually had (agents categorize chargebacks as part of our workflow); however, there are other types of attacks where distinguishing legitimate activity from illegitimate is more subtle, and necessitated the creation of new data stores and logging pipelines.\n\nMost people who\u2019ve worked in machine learning will find this obvious, but it\u2019s worth re-stressing:\n\nTowards this end, sometimes you don\u2019t know what data you\u2019re going to need until you\u2019ve seen a new attack, especially if you haven\u2019t worked in the risk space before, or have worked in the risk space but only in a different sector. So the best advice we can offer in this case is to log everything. Throw it all in HDFS, whether you need it now or not. In the future, you can always use this data to backfill new data stores if you find it useful. This can be invaluable in responding to a new attack vector.\n\nAlthough our current ML pipeline uses scikit-learn and Openscoring, our system is constantly evolving. Our current setup is a function of the stage of the company and the amount of resources, both in terms of personnel and data, that are currently available. Smaller companies may only have a few ML models in production and a small number of analysts, and can take time to manually curate data and train the model in many non-standardized steps. Larger companies might have many, many models and require a high degree of automation, and get a sizable boost from online training. A unique challenge of working at a hyper-growth company is that landscape fundamentally changes year-over-year, and pipelines need to adjust to account for this.\n\nAs our data and logging pipelines improve, investing in improved learning algorithms will become more worthwhile, and we will likely shift to testing new algorithms, incorporating online learning, and expanding on our model building framework to support larger data sets. Additionally, some of the most important opportunities to improve our models are based on insights into our unique data, feature selection, and other aspects our risk systems that we are not able to share publicly. We would like to acknowledge the other engineers and analysts who have contributed to these critical aspects of this project. We work in a dynamic, highly-collaborative environment, and this project is an example of how engineers and data scientists at Airbnb work together to arrive at a solution that meets a diverse set of needs. If you\u2019re interested in learning more, contact us about our data science and engineering teams!"
    },
    {
        "url": "https://medium.com/airbnb-engineering/engineering-culture-at-airbnb-345797c17cbe",
        "title": "Engineering Culture at Airbnb \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "If you had visited Airbnb\u2019s office yesterday you probably would have noticed something: clapping. I\u2019m not sure why, but sometimes a team will applaud a small victory, then more people will start clapping, then suddenly the entire product and engineering area is a din of applause and cheers. Most people don\u2019t know why they\u2019re clapping, they just want to show support and have fun.\n\nMaybe that\u2019s what good culture is about. Defaulting to an attitude of support and celebrating others\u2019 successes. Every company has some kind of culture. Some maintain it with meticulous attention, others just let it happen and hope for the best. Either way one fact remains: good culture creates an environment where people can do their best work, bad culture is soul-destroying.\n\nI\u2019ve been at Airbnb for a little over a year now. Previously I\u2019ve been an engineer and manager at many companies including Facebook and Yahoo. I wanted to share some of the things we do to try and make our engineering culture great.\n\nAt the core our philosophy is this: engineers own their own impact. Each engineer is individually responsible for creating as much value for our users and for the company as possible.\n\nWe hire primarily for problem-solving. When you have a team of strong problem-solvers, the most efficient way to move the company forward is to leave decision-making up to individual engineers. Our culture, tools, and processes all revolve around giving individual contributors accurate and timely information that they can use to make great decisions. This helps us iterate, experiment, and learn faster.\n\nMaking this environment possible requires a few things. Engineers are involved in goal-setting, planning and brainstorming for all projects, and they have the freedom to select which projects they work on. They also have the flexibility to balance long and short term work, creating business impact while managing technical debt. Does this mean engineers just do whatever they want? No. They work to define and prioritize impactful work with the rest of their team including product managers, designers, data scientists and others.\n\nJust as importantly, engineers have transparent access to information. We default to information sharing. The more information engineers have, the more autonomously they can work. Everything is shared unless there\u2019s an explicit reason not to (which is rare). That includes access to the analytics data warehouse, weekly project updates, CEO staff meeting notes, and a lot more.\n\nThis environment can be scary, especially for new engineers. No one is going to tell you exactly how to have impact. That\u2019s why one of our values is that helping others takes priority. In our team, no one is ever too busy to help. In particular, our new grad hires are paired with a team that can help them find leveraged problems. Whether it\u2019s a technical question or a strategic one, engineers always prioritize helping each other first.\n\nBeing able to decide what\u2019s impactful is possible with a clear company strategy to guide the decision-making process. That\u2019s why we\u2019ve designed our strategy for simplicity and quantifiability. It\u2019s simple enough to fit on a single page and every employee at Airbnb knows how their function relates to the big picture. Knowing what your team\u2019s goal is helps you decide how to use your time, which minimizes time-wasting debates about the existential stuff. And because each of our major goals has a numeric target, we can measure the effectiveness of various projects, learning quickly from our successes and failures.\n\nOur team structure also maps to our company strategy: we work in tight working groups of generally 10 people or less with efficient lines of communication. Teams are primarily comprised of engineers, product managers, designers, and data scientists, and some teams partner with other departments within the company. There is strong collaboration between functions. Payments includes people from finance, Internal Tools includes people from customer experience. It\u2019s common for engineers and designers pair up and figure out how to make something work in realtime. The best ideas come from close collaboration.\n\nThis year, we have ten teams focused on product development and four teams focused on technical infrastructure. Each team is concerned with a specific aspect of Airbnb as a business, and defines its own subgoals and projects on a quarterly basis, using the overall company strategy as a compass.\n\nAlthough each team owns non-overlapping pieces of the business, collaborating across teams is common and encouraged. For instance, we have discrete Host and Guest teams, since we tend to think of hosts and guests as separate user demographics, each with their own set of needs. But since the interactions between hosts and guests are what make Airbnb special, these teams contribute to their counterparts\u2019 roadmaps, share goals, and partner up on projects, while retaining enough separation to build specific expertise about their constituents\u2019 use cases and needs. Fostering collaboration across teams helps us cover gaps.\n\nIt\u2019s common for engineers to switch teams or contribute to areas beyond the scope of their immediate team. For example, it\u2019s routine for a product-focused team to contribute to improving our infrastructure in the workflow of their projects. Engineers have freedom to change teams when the work in another group more closely aligns with their interests and ability to drive impact. In fact, it is encouraged. Managers can facilitate this process, but it\u2019s up to the individual to find the team where he or she can have the greatest impact and initiate a move.\n\nThe development process at Airbnb is flexible by design. We don\u2019t want to build in different directions, but we also don\u2019t want to be so standardized that we miss out on better tools and methodologies when they emerge. We believe in shaping good judgment in individuals instead of imposing rules across the team.\n\nWhen our process changes it happens organically from within the team. Code reviews are an old but a good example of this. We had the mechanisms to do pull requests for years but we never mandated their use, and historically many engineers didn\u2019t adopt them as part of their workflow. In fact, in the early days it was common practice to merge your own changes directly to master and deploy the site. This is kind of like juggling chainsaws blindfolded \u2014 looks cool when you pull it off, but eventually you\u2019re going to lose a finger.\n\nAt some point a few motivated engineers started highlighting great code reviews at our weekly engineering all-hands meetings. They\u2019d highlight some of the most helpful or thoughtful code reviews they had seen over the week. Soon more engineers started adopting pull requests and a tipping point was reached where it became strange if you didn\u2019t ask for code review. Now it is just how we do development.\n\nAt the same time, this cultural shift was mirrored by advances in our tooling. A small team of engineers took it upon themselves to build out our continuous integration infrastructure, enabling the engineering team to run the entire test suite in minutes anytime they checked in a branch. Lowering the barriers to good behavior with tooling catalyzed the team\u2019s cultural change.\n\nThis is one example, but there are countless others including how we adopted our project management tools and bug tracker. When we discover a better way of doing things we facilitate awareness of the idea then let it stand on its own merit until it catches on (or doesn\u2019t). This way teams have a lot of flexibility with how they accomplish their work and we create opportunity for new good ideas to emerge.\n\nAny engineer can contribute to any part of the codebase. All repositories are open to all engineers. This is possible because of our culture of automated testing, our code reviews, and our ability to detect anomalies in production through detailed monitoring. The standard etiquette here is borrowed from the open source world: someone from the team that maintains the codebase you\u2019re touching should review your changes before you merge.\n\nThis model makes it easier for engineers to unblock themselves. Instead of getting onto another team\u2019s priority list and waiting for them to have time to get it done, you just do it yourself and ask them to review it. That code review happens quickly because, again, helping others takes priority.\n\nOnce code is merged engineers deploy their own changes. In a given day, we\u2019ll deploy the site 10 times or more. Our build-and-test process takes under 10 minutes to run and we can complete a full production deploy in about 8 minutes. Because it\u2019s so fast, we ask engineers to deploy their changes as soon as they\u2019re merged. Smaller change sets to production mean less chance for conflict and easier debugging when something goes wrong. It\u2019s common etiquette to be present in our engineering chatroom as you deploy your changes. Our bot announces when the deploy starts and completes and the engineer announces they have verified their changes in production. During this time the engineer is also responsible for watching the metrics to make sure nothing bad happens.\n\nOf course, bad things do happen sometimes. In these cases we may rollback the site, or fix and roll forward. When things are fixed, engineers work with the site reliability team to write a blameless post-mortem. We keep all post-mortems in an incident reporter tool that we developed internally. Post-mortems heavily inform proactive work we do to make infrastructure more reliable.\n\nAnother one of our beliefs is that engineers can progress just as far as individual contributors as they can as managers. There are two tracks by which engineers can progress in their careers: management and individual contribution. The pay scales are parallel, so there\u2019s no compensation advantage for getting into engineering management at Airbnb.\n\nIn fact, becoming a manager isn\u2019t about getting promoted; it\u2019s about changing the focus of your work. Managers are facilitators. They exist to get obstacles out of engineers\u2019 way. That can be career obstacles, prioritization, or technical help; pretty much anything. Their primary responsibility is to support the people around them.\n\nWe also value technical strength in our managers. Each manager is involved in dozens of technical decisions a week. Without a strong technical background, their influence in that process can lead to poor results. For this reason, all managers start as individual contributors. They can transition into management when they\u2019re familiar with the code and development practices and, more importantly, when it feels like a natural move. We don\u2019t airdrop managers.\n\nAn individual contributor\u2019s primary responsibility is technical execution that drives impact to the business. They are responsible for finding and doing high impact work. In that process another value is to leave it better than you found it. Every project should improve our technical foundation. That responsibility falls to individual contributors and this means that engineers are driving technical decisions and holding each other to high standards of technical work. It also means that engineers negotiate feature trade-offs and deadlines to make sure enough time is given to do quality engineering.\n\nAnother way that we help engineers progress is by helping them build their individual profiles outside the company. We do this through blog posts on our nerds blog and through open source. We believe that anything that isn\u2019t core to our unique business is fair game to be pushed to open source. We always want to be contributing useful technology back to the community. We encourage it as a way to help increase awareness around the engineering work we\u2019re doing and to showcase some of the best work by our engineers.\n\nRight now, we are still establishing the foundation and practices that will carry us forward over the next several years. Things that seem like trivial decisions today will be amplified 10x down the road when we\u2019re a much bigger team. That\u2019s a lot of pressure, but it\u2019s also fun to see experiments that work out and become part of the culture, or have something fail and get discarded right before your eyes.\n\nNot fucking up the culture is paramount. When you\u2019re growing quickly, it\u2019s important to keep the environment creative and fun. Our engineering team meets every Friday for an hour of technical presentations, animated GIFs, applause, appreciation and cheers. We do multi-day hackathons twice a year that are each worthy of their own posts. I meet with small groups of engineers every week just to ask questions and listen to ideas on how we can improve. We have a nerd cave where engineers can hang out and listen to records while they work. We could probably do an entire post on how we stay connected and have fun as a team but I\u2019ll save that for another day.\n\nWhat makes Airbnb special is that our culture connects engineers to the company mission and to each other more strongly than anyplace else I\u2019ve seen. Engineers own their impact here, prioritize helping others, default to sharing information, and continually leave the code better than they found it. Our culture empowers engineers to do their best work, and helps them get excited to come to work every day."
    },
    {
        "url": "https://medium.com/airbnb-engineering/experiment-reporting-framework-f3faca569e0c",
        "title": "Experiment Reporting Framework \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "At Airbnb we are always trying to learn more about our users and improve their experience on the site. Much of that learning and improvement comes through the deployment of controlled experiments. If you haven\u2019t already read our other post about experimentation I highly recommend you do it, but I will summarize the two main points: (1) running controlled experiments is the best way to learn about your users, and (2) there are a lot of pitfalls when running experiments. To that end, we built a tool to make running experiments easier by hiding all the pitfalls and automating the analytical heavy lifting.\n\nWhen designing this tool, making experiments simple to run was the primary focus. We also had some specific design goals that came out of what we\u2019ve learned from running and analyzing experiments with our previous tool.\n\nFor the rest of this post, let\u2019s consider a sample experiment we might want to run and how we\u2019d get there \u2014 from setting it up, to making the code changes, to seeing the results.\n\nHere is our current search results page, on the left we have a map of the results and on the right, images of the listings. By default, we show 18 results per page, but we wanted to understand how showing 12 or 24 results would affect user behaviour. Do users prefer getting more information at once? Or is it confusing to show too much? Let\u2019s walk through the process of running that experiment.\n\nFor declaring experiments we settled on yaml since it provides a nice balance between human and machine readability. To define an experiment, you need two key things \u2014 the subject and the treatments. The subject is who you want to run this experiment against. In this case, we choose visitor since not all users who search are logged in. If we were running an experiment on the booking flow (where users have to log in first) we could run the experiment against users. For a more in-depth look at the issues we\u2019ve seen with visitor versus user experiments, check out our other post. Second, we have to define the treatments; in this case we have the control (of 18 results per page) and our two experimental groups, 12 and 24 results per page. The human_readable fields are what will be used in the UI.\n\nThe next step is to implement this experiment in code. In the examples below, we\u2019ll be looking at Ruby code but we have a very similar function in Javascript that we can use for running experiments on cached pages.\n\nThe first argument is just the name of the experiment (from above). Then we\u2019ve got an argument for each treatment above as well as a lambda function. The deliver_experiment function does three main things, (1) assign a user to a group (based on the specified subject), (2) log that the user was put into the treatment group, and (3) execute the provided lambda for the treatment group. You\u2019ll also notice one more argument, :unknown. This is there in the case we run into some unexpected failure. We want to make sure, even in the case that something goes horribly wrong, we still provide the user with a good experience. This group allows us to handle those cases by rendering that view to the user and logging that the unknown treatment was given (and, of course, also logging the error as needed).\n\nThis design may seem a little unorthodox, but there is a method behind the madness. To understand why we chose lambdas instead of something simpler like if statements, let\u2019s look at a few examples of doing it differently. Imagine, instead, we had a function that would return the treatment for a given user. We could then deploy an experiment like this:\n\nThis would work perfectly, and we could log which treatment a user was put into in the get_treatment function. What if, however, someone is looking at site performance later on and realizes that serving 24 results per page is causing the load times to skyrocket in China? They don\u2019t know about the experiment you\u2019re trying to run, but want to improve the user experience for Chinese users, so they come to the code and make the following change:\n\nNow, what\u2019s happening? Well, we\u2019re still going to log that Chinese users are put into the 24 results per page group (since that happens on line 1) but, in fact, they will not be seeing 24 results per page because of the change. We\u2019ve biased our experiment. While you could do that with the lambda too, we\u2019ve found by making it very explicit that this code path is related to an experiment, people are more aware that they shouldn\u2019t be putting switching logic in there.\n\nLet\u2019s look at another example, what about the following two statements?"
    },
    {
        "url": "https://medium.com/airbnb-engineering/experiment-reporting-framework-4e3fcd29e6c0",
        "title": "Experiment Reporting Framework \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "At Airbnb we are always trying to learn more about our users and improve their experience on the site. Much of that learning and improvement comes through the deployment of controlled experiments. If you haven\u2019t already read our other post about experimentation I highly recommend you do it, but I will summarize the two main points: (1) running controlled experiments is the best way to learn about your users, and (2) there are a lot of pitfalls when running experiments. To that end, we built a tool to make running experiments easier by hiding all the pitfalls and automating the analytical heavy lifting.\n\nWhen designing this tool, making experiments simple to run was the primary focus. We also had some specific design goals that came out of what we\u2019ve learned from running and analyzing experiments with our previous tool.\n\nFor the rest of this post, let\u2019s consider a sample experiment we might want to run and how we\u2019d get there\u2013from setting it up, to making the code changes, to seeing the results.\n\nHere is our current search results page, on the left we have a map of the results and on the right, images of the listings. By default, we show 18 results per page, but we wanted to understand how showing 12 or 24 results would affect user behaviour. Do users prefer getting more information at once? Or is it confusing to show too much? Let\u2019s walk through the process of running that experiment.\n\nFor declaring experiments we settled on yaml since it provides a nice balance between human and machine readability. To define an experiment, you need two key things\u2013the subject and the treatments. The subject is who you want to run this experiment against. In this case, we choose visitor since not all users who search are logged in. If we were running an experiment on the booking flow (where users have to log in first) we could run the experiment against users. For a more in-depth look at the issues we\u2019ve seen with visitor versus user experiments, check out our other post. Second, we have to define the treatments; in this case we have the control (of 18 results per page) and our two experimental groups, 12 and 24 results per page. The human_readable fields are what will be used in the UI.\n\nThe next step is to implement this experiment in code. In the examples below, we\u2019ll be looking at Ruby code but we have a very similar function in Javascript that we can use for running experiments on cached pages.\n\nThe first argument is just the name of the experiment (from above). Then we\u2019ve got an argument for each treatment above as well as a lambda function. The deliver_experiment function does three main things, (1) assign a user to a group (based on the specified subject), (2) log that the user was put into the treatment group, and (3) execute the provided lambda for the treatment group. You\u2019ll also notice one more argument, :unknown. This is there in the case we run into some unexpected failure. We want to make sure, even in the case that something goes horribly wrong, we still provide the user with a good experience. This group allows us to handle those cases by rendering that view to the user and logging that the unknown treatment was given (and, of course, also logging the error as needed).\n\nThis design may seem a little unorthodox, but there is a method behind the madness. To understand why we chose lambdas instead of something simpler like if statements, let\u2019s look at a few examples of doing it differently. Imagine, instead, we had a function that would return the treatment for a given user. We could then deploy an experiment like this:\n\nThis would work perfectly, and we could log which treatment a user was put into in the get_treatment function. What if, however, someone is looking at site performance later on and realizes that serving 24 results per page is causing the load times to skyrocket in China? They don\u2019t know about the experiment you\u2019re trying to run, but want to improve the user experience for Chinese users, so they come to the code and make the following change:\n\nNow, what\u2019s happening? Well, we\u2019re still going to log that Chinese users are put into the 24 results per page group (since that happens on line 1) but, in fact, they will not be seeing 24 results per page because of the change. We\u2019ve biased our experiment. While you could do that with the lambda too, we\u2019ve found by making it very explicit that this code path is related to an experiment, people are more aware that they shouldn\u2019t be putting switching logic in there.\n\nLet\u2019s look at another example, what about the following two statements?\n\nIn this case we have identical logic and the same users will see the treatment. The problem is that because the tests are short-circuited in the if statement, in the first case we correctly log only when a user actually sees the treatment. In the second case we have the same problem as above, where we log that Chinese users are seeing the 24 results per page treatment even though they are not.\n\nFinally, once that\u2019s all done and deployed into the wild, we wait for the results to roll in. Currently we process the experiment results nightly, although it could easily be run more frequently. You can see a screenshot of the UI for the search results per page experiment in the image at the beginning of the post. At first glance, you\u2019ll see red and green cells. These cells signify metrics that we think are statistically significant based the methods presented in our previous post (red for bad and green for good). The uncolored cells with grey text represent metrics for which we are not yet sufficiently confident in the results. We also plot a spark line of the p-value and delta over time, which allows a user to look for convergence of these values.\n\nAs you can also see from the UI, we provide two other mechanisms for looking at the data, but I won\u2019t go into too much detail on those here. These allow for filtering the results, for example by new or returning users. We also support pivoting the results, so that a user could see how a specific metric performed on new vs. returning users.\n\nOnce we have significant results for the metrics we were interested in for an experiment, we can make a determination about the success (or failure) of that experiment and deploy a specific treatment in the code. To avoid building up confusing code paths, we try to tear down all completed experiments. Experiments can then be marked as retired, which will stop running the analysis, but retain the data so it can be still referred to in the future.\n\nWe plan to eventually open source much of this work. In the meantime, we hope this post gives you a taste of some of the decisions we made when designing this tool and why we made them. If you\u2019re trying to build something similar (or already have) we\u2019d love to hear from you."
    },
    {
        "url": "https://medium.com/airbnb-engineering/experiments-at-airbnb-e2db3abf39e7",
        "title": "Experiments at Airbnb \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Airbnb is an online two-sided marketplace that matches people who rent out their homes (\u2018hosts\u2019) with people who are looking for a place to stay (\u2018guests\u2019). We use controlled experiments to learn and make decisions at every step of product development, from design to algorithms. They are equally important in shaping the user experience.\n\nWhile the basic principles behind controlled experiments are relatively straightforward, using experiments in a complex online ecosystem like Airbnb during fast-paced product development can lead to a number of common pitfalls. Some, like stopping an experiment too soon, are relevant to most experiments. Others, like the issue of introducing bias on a marketplace level, start becoming relevant for a more specialized application like Airbnb. We hope that by sharing the pitfalls we\u2019ve experienced and learned to avoid, we can help you to design and conduct better, more reliable experiments for your own application.\n\nExperiments provide a clean and simple way to make causal inference. It\u2019s often surprisingly hard to tell the impact of something you do by simply doing it and seeing what happens, as illustrated in Figure 1.\n\nThe outside world often has a much larger effect on metrics than product changes do. Users can behave very differently depending on the day of week, the time of year, the weather (especially in the case of a travel company like Airbnb), or whether they learned about the website through an online ad or found the site organically. Controlled experiments isolate the impact of the product change while controlling for the aforementioned external factors. In Figure 2, you can see an example of a new feature that we tested and rejected this way. We thought of a new way to select what prices you want to see on the search page, but users ended up engaging less with it than the old filter, so we did not launch it.\n\nWhen you test a single change like this, the methodology is often called A/B testing or split testing. This post will not go into the basics of how to run a basic A/B test. There are a number of companies that provide out of the box solutions to run basic A/B tests and a couple of bigger tech companies have open sourced their internal systems for others to use. See Cloudera\u2019s Gertrude, Etsy\u2019s Feature, and Facebook\u2019s PlanOut, for example.\n\nAt Airbnb we have built our own A/B testing framework to run experiments which you will be able to read more about in our upcoming blog post on the details of its implementation. There are a couple of features of our business that make experimentation more involved than a regular change of a button color, and that\u2019s why we decided to create our own testing framework.\n\nFirst, users can browse when not logged in or signed up, making it more difficult to tie a user to actions. People often switch devices (between web and mobile) in the midst of booking. Also given that bookings can take a few days to confirm, we need to wait for those results. Finally, successful bookings are often dependent on available inventory and responsiveness of hosts \u2014 factors out of our control.\n\nOur booking flow is also complex. First, a visitor has to make a search. The next step is for a searcher to actually contact a host about a listing. Then, the host has to accept an inquiry and then the guest has to actually book the place.. In addition we have multiple flows that can lead to a booking \u2014 a guest can instantly book some listings without a contact, and can also make a booking request that goes straight to booking. This four step flow is visualized in Figure 3. We look at the process of going through these four stages, but the overall conversion rate between searching and booking is our main metric.\n\nA very common source of confusion in online controlled experiments is how much time you need to make a conclusion about the results of an experiment. The problem with the naive method of using of the p-value as a stopping criterion is that the statistical test that gives you a p-value assumes that you designed the experiment with a sample and effect size in mind. If you continuously monitor the development of a test and the resulting p-value, you are very likely to see an effect, even if there is none. Another common error is to stop an experiment too early, before an effect becomes visible.\n\nHere is an example of an actual experiment we ran. We tested changing the maximum value of the price filter on the search page from $300 to $1000 as displayed below.\n\nIn Figure 5 we show the development of the experiment over time. The top graph shows the treatment effect (Treatment / Control \u2014 1) and the bottom graph shows the p-value over time. As you can see, the p-value curve hits the commonly used significant value of 0.05 after 7 days, at which point the effect size is 4%. If we had stopped there, we would have concluded that the treatment had a strong and significant effect on the likelihood of booking. But we kept the experiment running and we found that actually, the experiment ended up neutral. The final effect size was practically null, with the p-value indicating that whatever the remaining effect size was, it should be regarded as noise.\n\nWhy did we know to not stop when the p-value hit 0.05? It turns out that this pattern of hitting \u201csignificance\u201d early and then converging back to a neutral result is actually quite common in our system. There are various reasons for this. Users often take a long time to book, so the early converters have a disproportionately large influence in the beginning of the experiment. Also, even small sample sizes in online experiments are massive in the scale of classical statistics in which these methods were developed. Since the statistical test is a function of the sample- and effect sizes, if an early effect size is large through natural variation it is likely for the p-value to be below 0.05 early. But the most important reason is that you are performing a statistical test every time you compute a p-value and the more you do it, the more likely you are to find an effect.\n\nAs a side note, people familiar with our website might notice that, at time of writing, we did in fact launch the increased max price filter, even though the result was neutral. We found that certain users like the ability to search for high-end places and decided to accommodate them, given there was no dip in the metrics.\n\nHow long should experiments run for then? To prevent a false negative (a Type II error), the best practice is to determine the minimum effect size that you care about and compute, based on the sample size (the amount of new samples that come every day) and the certainty you want, how long to run the experiment for, before you start the experiment. Here is a resource that helps with that computation. Setting the time in advance also minimizes the likelihood of finding a result where there is none.\n\nOne problem, though, is that we often don\u2019t have a good idea of the size, or even the direction, of the treatment effect. It could be that a change is actually hugely successful and major profits are being lost by not launching the successful variant sooner. Or, on the other side, sometimes an experiment introduces a bug, which makes it much better to stop the experiment early before more users are alienated.\n\nThe moment when an experiment dabbles in the otherwise \u201csignificant\u201d region could be an interesting one, even when the pre-allotted time has not passed yet. In the case of the price filter experiment example, you can see that when \u201csignificance\u201d was first reached, the graph clearly did not look like it had converged yet. We have found this heuristic to be very helpful in judging whether or not a result looks stable. It is important to inspect the development of the relevant metrics over time, rather than to consider the single result of an effect with a p-value.\n\nWe can use this insight to be a bit more formal about when to stop an experiment, if it\u2019s before the allotted time. This can be useful if you do want to make an automated judgment call on whether or not the change that you\u2019re testing is performing particularly well or not, which is helpful when you\u2019re running many experiments at the same time and cannot manually inspect them all systematically. The intuition behind it is that you should be more skeptical of early results. Therefore the threshold under which to call a result is very low at the beginning. As more data comes in, you can increase the threshold as the likelihood of finding a false positive is much lower later in the game.\n\nWe solved the problem of how to figure out the p-value threshold at which to stop an experiment by running simulations and deriving a curve that gives us a dynamic (in time) p-value threshold to determine whether or not an early result is worth investigating. We wrote code to simulate our ecosystem with various parameters and used this to run many simulations with varying values for parameters like the real effect size, variance and different levels of certainty. This gives us an indication of how likely it is to see false positives or false negatives, and also how far off the estimated effect size is in case of a true positive. In Figure 6 we show an example decision boundary.\n\nIt should be noted that this curve is very particular to our system and the parameters that we used for this experiment. We share the graph as an example for you to use for your own analysis.\n\nA second pitfall is failing to understand results in their full context. In general, it is good practice to evaluate the success of an experiment based on a single metric of interest. This is to prevent cherry-picking of \u2018significant\u2019 results in the midst of a sea of neutral ones. However, by just looking at a single metric you lose a lot of context that could inform your understanding of the effects of an experiment.\n\nLet\u2019s go through an example. Last year we embarked on a journey to redesign our search page. Search is a fundamental component of the Airbnb ecosystem. It is the main interface to our inventory and the most common way for users to engage with our website. So, it was important for us to get it right. In Figure 7 you can see the before and after stages of the project. The new design puts more emphasis on pictures of the listings (one of our assets since we offer professional photography to our hosts) and the map that displays where listings are located. You can read about the design and implementation process in another blog post here.\n\nA lot of work went into the project, and we all thought it was clearly better; our users agreed in qualitative user studies. Despite this, we wanted to evaluate the new design quantitatively with an experiment. This can be hard to argue for, especially when testing a big new product like this. It can feel like a missed marketing opportunity if we don\u2019t launch to everyone at the same time. However, to keep in the spirit of our testing culture, we did test the new design \u2014 to measure the actual impact and, more importantly, gather knowledge about which aspects did and didn\u2019t work.\n\nAfter waiting for enough time to pass, as calculated with the methodology described in the previous section, we ended up with a neutral result. The change in the global metric was tiny and the p-value indicated that it was basically a null effect. However, we decided to look into the context and to break down the result to try to see if we could figure out why this was the case. Because we did this, we found that the new design was actually performing fine in most cases, except for Internet Explorer. We then realized that the new design broke an important click-through action for certain older versions of IE, which obviously had a big negative impact on the overall results. When we fixed this, IE displayed similar results to the other browsers, a boost of more than 2%.\n\nApart from teaching us to pay more attention to QA for IE, this was a good example of what lessons you can learn about the impact of your change in different contexts. You can break results down by many factors like browser, country and user type. It should be noted that doing this in the classic A/B testing framework requires some care. If you test breakdowns individually as if they were independent, you run a big risk of finding effects where there aren\u2019t, just like in the example of continuously monitoring the effect of the previous section. It\u2019s very common to be looking at a neutral experiment, break it down many ways and to find a single \u2018significant\u2019 effect. Declaring victory for that particular group is likely to be incorrect. The reason for this is that you are performing multiple tests with the assumption that they are all independent, which they are not. One way of dealing with this problem is to decrease the p-value by which you decide the effect is real. Read more about this approach here. Another way is to model the effects on all breakdowns directly with a more advanced method like logistic regression.\n\nThe third and final pitfall is assuming that the system works the way you think or hope it does. This should be a concern if you build your own system to evaluate experiments as well as if you use a third party tool. In either case, it\u2019s possible that what the system tells you does not reflect reality. This can happen either because it\u2019s faulty or because you\u2019re not using it correctly. One way to evaluate the system and your interpretation of it is by formulating hypotheses and then verifying them.\n\nAnother way of looking at this is the observation that results too good to be true have a higher likelihood of being false. When you encounter results like this, it is good practice to be skeptical of them and scrutinize them in whatever way you can think of, before you consider them to be accurate.\n\nA simple example of this process is to run an experiment where the treatment is equal to the control. These are called A/A or dummy experiments. In a perfect world the system would return a neutral result (most of the time). What does your system return? We ran many \u2018experiments\u2019 like this (see an example run in Figure 9) and identified a number of issues within our own system as a result. In one case, we ran a number of dummy experiments with varying sizes of control and treatment groups. A number of them were evenly split, for example with a 50% control and a 50% treatment group (where everybody saw exactly the same website). We also added cases like a 75% control and a 25% treatment group. The results that we saw for these dummy experiments are displayed in Figure 10.\n\nYou can see that in the experiments where the control and treatment groups are the same size, the results look neutral as expected (it\u2019s a dummy experiment so the treatment is actually the same as the control). But, for the case where the group sizes are different, there is a massive bias against the treatment group.\n\nWe investigated why this was the case, and uncovered a serious issue with the way we assigned visitors that are not logged into treatment groups. The issue is particular to our system, but the general point is that verifying that the system works the way you think it does is worthwhile and will probably lead to useful insights.\n\nOne thing to keep in mind when you run dummy experiments is that you should expect some results to come out as non-neutral. This is because of the way the p-value works. For example, if you run a dummy experiment and look at its performance broken down by 100 different countries, you should expect, on average, 5 of them to give you a non-neutral result. Keep this in mind when you\u2019re scrutinizing a 3rd party tool!\n\nControlled experiments are a great way to inform decisions around product development. Hopefully, the lessons in this post will help prevent some common A/B testing errors.\n\nFirst, the best way to determine how long you should run an experiment is to compute the sample size you need to make an inference in advance. If the system gives you an early result, you can try to make a heuristic judgment on whether or not the trends have converged. It\u2019s generally good to be conservative in this scenario. Finally, if you do need to make procedural launch and stopping decisions, it\u2019s good to be extra careful by employing a dynamic p-value threshold to determine how certain you can be about a result. The system we use at Airbnb to evaluate experiments employs all three ideas to help us with our decision-making around product changes.\n\nIt is important to consider results in context. Break them down into meaningful cohorts and try to deeply understand the impact of the change you made. In general, experiments should be run to make good decisions about how to improve the product, rather than to aggressively optimize for a metric. Optimizing is not impossible, but often leads to opportunistic decisions for short-term gains. By focusing on learning about the product you set yourself up for better future decisions and more effective tests.\n\nFinally, it is good to be scientific about your relationship with the reporting system. If something doesn\u2019t seem right or if it seems too good to be true, investigate it. A simple way of doing this is to run dummy experiments, but any knowledge about how the system behaves is useful for interpreting results. At Airbnb we have found a number of bugs and counter-intuitive behaviors in our system by doing this.\n\nTogether with Will Moss, I gave a public talk on this topic in April 2014. You can watch a video recording of it here. Will published another blog post on the infrastructure side of things, read it here. We hope this post was insightful for those who want to improve their own experimentation."
    },
    {
        "url": "https://medium.com/airbnb-engineering/hacking-word-of-mouth-making-referrals-work-for-airbnb-46468e7790a6",
        "title": "Hacking Word-of-Mouth: Making Referrals Work for Airbnb",
        "text": "Referrals are a frequently used growth tool, but at Airbnb our antiquated referrals system was underutilized and underperforming. It was barely findable on the website and didn\u2019t exist on our mobile apps! I\u2019d like to share with you some tips we used to increase the program\u2019s user signups and bookings by over 300% per day.\n\nWord of mouth is a huge growth source for Airbnb, in part because Airbnb experiences are so personal. People use Airbnb to unlock incredible experiences \u2014 anything from weekend getaways with friends, cultural exchanges, and once in a lifetime events like honeymoons. The referrals program encourages inviting friends by giving both the sender and recipient $25 of travel credit at Airbnb when the invited user completes their first trip. The idea is to leverage our community\u2019s inherent tendency to tell their friends about Airbnb and amplify the effect.\n\nWe decided to relaunch Referrals on all three of our platforms: our website, our Android app, and our iOS app. We also decided to support accepting and sending referral on all three platforms; very few apps accept referrals on mobile despite the majority of emails read on mobile.\n\nReferrals was an exciting project to undertake because it embodies growth: it\u2019s highly measurable, scalable, and is all about identifying a growth pattern that\u2019s already happening but amplifying it at a key moment.\n\nBefore we coded anything, we defined what success would look like. We chose a few metrics to keep score. Adam Nash had spoken at the company at an Airbnb fireside chat recently and had inspired us to \u201ckeep score and win games.\u201d We knew that in this game data would be critical, so we started with our success metrics.\n\nFor referrals we defined success according to these metrics which form a funnel:\n\nFor each metric, we built three different forecasts:\n\nWe based our forecasts on how we think our old, neglected referrals program could perform given a rebirth. We also benchmarked ourselves against other successful referral programs such as Dropbox and Voxer.\n\nBefore we coded any product, we built mechanisms to track and report our progress on our success metrics.\n\nAt Airbnb we have an in-house event logging platform called air_events. With air_events we can call the same method from any of our platforms and log an event to a centralized Hive store. We built client libraries for air_events across our entire stack: Ruby (back-end web), Javascript (front-end web), Objective C (iOS), and Java (Android).\n\nFor referrals we defined a rich logging taxonomy of over 20 user events that happen during the referral invitation and sign up journey. With this tracking in place we\u2019d be able to follow an invitation from invite page impressions to referred users\u2019 making bookings or becoming hosts. We had iOS, Android, and the website all log according to this taxonomy. That way, we could easily graph and compare performance for a single metric across platforms.\n\nSomething that really helped us with this \u201cKeep Score\u201d phase is Airbnb\u2019s engineering team structure. Every team has a data analyst embedded with the team. Our embedded analyst extraordinaire Vaughn Quoss designed the taxonomy and approach. Vaughn also built dashboards and graphs so that we could monitor performance from day one.\n\nAt Airbnb we have a fun practice of renting out Airbnb listings near the office and working from them for a few days. To build referrals, we had two off-sites, both in the Lower Haight neighborhood of San Francisco.\n\nWe built support for web, Android, and iOS simultaneously. Two of the engineers on the team wanted to learn iOS and Android, so they did for this project. The core mobile team also helped by sending team members to pair program with us at the off-sites. At Airbnb we train engineers in-house on new technologies they\u2019re interested in, and give them the opportunity to build with it while working with mentors. It was awesome to ramp up on new skills building something.\n\nAlong the way we thought of a few product features.\n\nWe thought that with personalized referral codes and URLs, users would feel more ownership of their invitation landing pages and emails.\n\nOpening the app after installing through an invitation link takes you to a special referrals landing page.\n\nOne of our engineers Jimmy Tang was a cofounder of Yoz.io, where he built analytics tools for mobile. He thought of a unique way to leverage Yozio\u2019s product to make our referral experience better for mobile. Yozio built a tool to track which link a user clicked before they downloaded the app with reasonable accuracy. From this we can determine if the download came from a referral link that was shared to them and give them a different first time app open experience. When the user clicks the link, Yozio does its first fingerprint of the user. Then when the user downloads the app and opens, it will fingerprint the user again and match the user to the link click. It will then pass this information back to the app and then you can present a different view.\n\nThe result is visible today. If you go to the app store today and download the app, you see this:\n\nHowever, if you click on an invitation link and then download the app, when you open the app for the first time, you\u2019ll see this:\n\nHere\u2019s how we did: referrals increased bookings by over 25% in some markets.\n\nBecause we had built dashboards, we could see on launch day one how users were interacting with the referrals product. It was gratifying to measure our impact from Day 1.\n\nWe can build product levers to try to boost each of our core metrics defined in Step 1. For example:\n\nIt\u2019s been a few months since we first launched. This time we didn\u2019t want to launch and forget our referrals program. Instead we doubled down coming up with new ways to optimize the program. For example, we\u2019ve started promoting referrals at strategic moments when we think the user is most likely to refer, for example just after booking a reservation, or after leaving a positive review. We segment performance data by entry point so we know how these are all doing.\n\nWe\u2019re also now in the process of A/B testing promotional emails. One experiment I was particularly excited about was communicating very different value propositions for the same product. In one email, we emphasized that you can earn $25 for inviting a friend (self-interested). In the other email, we emphasized that you are sharing $25 with your friend (altruistic).\n\nWe\u2019ve also seen that reception to the program depends on culture. For example, the referrals program is astoundingly popular in South Korea. We\u2019re also segmenting our A/B test results by culture to see what messaging performs better where.\n\nIt was fun to go through a classic Growth project end-to-end. The process serves as a template for all our Growth projects at Airbnb: set measurable goals, plan metrics and logging, build the product with instrumentation, measure impact, iterate."
    },
    {
        "url": "https://medium.com/airbnb-engineering/testing-at-airbnb-199f68a0a40d",
        "title": "Testing at Airbnb \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "When I was considering an offer from Airbnb early last year, one of the things I made sure to ask about was testing. At my last startup I\u2019d worked hard to cultivate a culture of testing; my team took pains to make sure that everything we shipped had a full test suite, and we often built new features using test driven development. I wanted to make sure that I wasn\u2019t going to be joining a team that didn\u2019t care about testing, or worse, actively opposed it.\n\nWhen I posed the question to a few Airbnb engineers, the consensus seemed to be, \u201cwell, we all really believe in testing, but we\u2019ve got a long way to go\u201d. I got the impression of a team that was slowly trying to bring order to a large, monolithic application, and saw testing as an essential part of that transformation.\n\nNearly a year later, I\u2019m amazed by the progress we\u2019ve made. We went from a handful of brittle tests to a large, resilient suite. We\u2019ve gutted our continuous integration stack and reduced our build times from 1 hour+ to about 6 minutes, while simultaneously handling many times the number of builds. Most importantly, we\u2019ve gone from an engineering culture in which most new code shipped without a single test, to one in which a proposed change without tests will quickly be called out and corrected. In short, we\u2019ve not only become a team that believes in tests; we\u2019ve become one that actually writes them.\n\nFor the payments team, where I\u2019ve spent most of my time, this transformation has been especially significant. Airbnb processes huge volumes of transactions in dozens of different currencies across a wide array of payments processors every day, and a small undetected bug could have a huge impact on our guests and hosts. Deploys used to be harrowing for us; verifying that a change wasn\u2019t introducing regressions was often a matter of manual testing and stats watching. Now, our test suite is the largest in the codebase, and we can generally trust that a green build truly means we haven\u2019t broken anything.\n\nI\u2019d like to share with you how we managed this evolution. First, I\u2019ll talk about how we changed our culture to make testing a first class citizen. Next, I\u2019ll discuss some of the tooling we\u2019ve put in place to make it easier for our team to write and run tests. Finally, I\u2019ll talk about some of the testing challenges we\u2019ve faced while working with specialized code and an increasingly service oriented architecture.\n\nAt a tiny startup, changing a team\u2019s behavior is relatively easy. You and your team sit down together, discuss the new behavior you want to adopt, and then adopt it. When you have dozens of engineers spread across different teams, however, changing how people work requires a bit more strategy. One approach is to rule by edict, but given our culture of engineer autonomy this would have been very poorly received. The other approach is to lead by example and build a movement. For us, this movement began with pull requests.\n\nIn the early days of Airbnb, as at many startups, most commits were pushed directly to master; if you wanted somebody to review your changes, you\u2019d call them over or IM them a link to your (usually already merged) SHA. With deploys going out many times a day, this resulted in a lot of questionable code hitting our production servers, and our uptime suffered for it. As we grew, this process became increasingly problematic.\n\nEventually, a few people decided to do something about it and started submitting pull requests for their changes. This was never introduced as a mandatory policy; we never disabled pushing to master or shamed people for doing so. But as those few, then a team, then several teams started doing this, two things happened. First, it became clear that this process of peer review lead to less bad code hitting production, and therefore fewer outages. Second, it began to seem horribly old-fashioned to push directly to master. This was accelerated by team growth: since each new engineering hire was briefed on the importance of PRs, the percentage of the team using them kept increasing regardless of whether existing engineers made the switch or not. Eventually, even the cowboys of the old guard started to feel sheepish when they snuck changes into master.\n\nAdoption of pull requests held a number of advantages for our team. It improved our stylistic consistency, gave us a forum to discuss code structure and architectural decisions, and increased the likelihood that typos and logical errors would be caught before they reached our users. By acting as a channel through which all new code must pass, it also gave individuals on the team much greater visibility into what was shipping. This increased visibility, in turn, enabled us to begin a cultural transformation around testing.\n\nAirbnb has had a spec/ folder in place on our primary Rails app for quite a while, but the state of affairs a year ago was pretty grim. The suite was slow and error-prone, our CI server was barely limping along, and most people had no idea how to run tests locally. New hires were told that testing was important, but when they saw that nobody was paying any attention to tests they quickly forgot about them. When those of us most committed to testing discussed this last spring, it was clear that a multi-pronged strategy was necessary to start turning things around.\n\nFirst, we needed to take a lesson from the PR rollout and lead by example. This meant including tests with any PRs we submitted, whether it was new feature development, bug fixes, or refactors. The increased visibility provided by pull requests meant that many people would see this happening.\n\nSecond, we needed to start educating the team: we spoke in engineering meetings, showed teammates how to write tests, held office hours, and shared links and recommended reading. I began publishing a weekly newsletter of testing news and PR highlights, showing off well-written specs and giving people props for including tests with their changes.\n\nThird, we needed to exploit our growth and make new hires into champions of testing. I revamped our testing bootcamp and emphasized the importance of testing in fighting cruft and keeping us from being trapped under the weight of a monolithic codebase that we couldn\u2019t safely refactor. I explained how much progress we\u2019d already made, and encouraged our new engineers to bear the torch. As adoption of testing increased, the sense of inertia lifted and new hires began to treat test writing as a matter of course. As with PRs, the old guard eventually started to be won over.\n\nObviously, all of this cultural shift wasn\u2019t worth much if it was too painful to write and run tests. In the next section I\u2019ll look at the changes we made to our tooling during this period to make testing as painless as possible.\n\nOne of my friends at Airbnb likes to say that the bar to writing tests should be, \u201cso low you can trip over it.\u201d I tend to agree. As I mentioned above, this certainly wasn\u2019t the case a year ago; at that point, the bar was well overhead \u2014 possibly hidden in a tree or a passing cloud. To start lowering it, we needed a few critical components to be in place: a way to run individual tests quickly and reliably on a dev machine, a way to run the full suite quickly and painlessly in the cloud, and a way to make sure that every PR had a passing build before it was merged.\n\nThe best way to ensure that local testing was possible was to normalize people\u2019s dev environments. For this we chose Vagrant. This, combined with Chef, allows us to do our local dev in sandboxed Linux instances running locally via VirtualBox in a configuration as similar to production as possible. In addition to making dev environment setup much easier than it used to be, this ensures that each engineer has a consistent environment that is ready to run tests out of the box. The user SSHs into the local linux server and runs spec commands like they would on their host OS, and generally everything Just Works. Most people on our team combine this with Zeus, which allows the Rails environment to be preloaded for lightning fast (relatively speaking) test runs. Both Vagrant and Zeus have their share of issues, but in practice we\u2019ve found them to be a huge time saver.\n\nDue to the way our primary Rails application evolved, we\u2019ve got a lot of tests that have large dependency graphs, interact extensively with the DB, instantiate tons of objects, etc. In a single process, running the entire suite takes several hours \u2014 far longer than anybody should have to wait to find out if their changes are causing regressions. There are a ton of excellent strategies for speeding up Rails test suites \u2014 aggressive use of stubbing/test doubles, decoupling logic from models, avoiding loading Rails entirely \u2014 but given the size of our codebase and the velocity with which we\u2019re moving, most of these weren\u2019t immediately feasible. We needed a build system that would allow us to parallelize our test suite so that the real time taken to run the suite was manageable.\n\nOur SRE team went through several different continuous integration solutions in the last year before settling on Solano. Each of the previous systems had some issue: instability, memory consumption, poor DB management, poor parallelization, painful web UI, you name it. What Solano gives us is an on-premise solution with excellent native support for fanning out tests to multiple threads, running them in parallel, and then assembling the results. It has a great web UI, CLI support, and impressive performance. Since we started using it, our deploy workflow has grown noticeably faster, and the number of wails and anguished GIFs from frustrated engineers is at an all-time low.\n\nHaving a CI server building all commits across all branches was a huge first step, but to make this useful we needed to surface the outcome of these builds. This is where GitHub\u2019s commit status API comes in. Every time our CI server begins a build, it pings GitHub\u2019s commit status endpoint, and every time it completes a build it hits the endpoint again with the outcome. Now every open PR includes a yellow/red/green indicator for the branch in question, with a direct link to the build status page on our CI server. In practice this means more transparency, faster feedback cycles, and a guarantee that every branch merged into master has a passing test suite. This integration has been a huge help in keeping our master branch green, and has thus greatly reduced our deploy times (since engineers aren\u2019t waiting on build failures to be resolved in master).\n\nThe combination of a standardized local test environment with a fast, reliable CI server and GitHub integration has helped us move that testing bar a lot closer to the ground. Where I guess it will eventually, ideally, be tripping everybody all the time or something.\n\nBefore I joined Airbnb, I was becoming something of a testing purist. I wanted my unit tests to run in total isolation, my test setup to be as minimal as possible, and my whole suite to run as close to sub-second as possible. I watched Corey Haines\u2019 talk at GoGaRuCo and my eyes twinkled with wonder.\n\nThese days, I\u2019ve become a bit more of a pragmatist. The reality of automated testing with a large legacy codebase is that you\u2019ll need to make some compromises on testing purity. That\u2019s fine. The important thing is to test all the f***ing time. Even when it\u2019s hard. Especially when it\u2019s hard. (Just make sure you have a fast, parallelizing build server).\n\nHere are some examples of hard things we\u2019ve had to test, and how we approached them.\n\nI recently worked on a reconciliation project for our finance team. The goal was to pull in reports from all our payments partners, normalize them, and match them to records in our system. It was a Ruby project, but for performance reasons all the heavy lifting was done with raw SQL transactions.\n\nAt first I was worried about the difficulty of testing this, and even briefly considered punting on it. Thankfully, I reconsidered; the project would have been a nightmare without the sprawling test suite I ended up building. I approached each stage of the ETL (extract, transform, load \u2014 a common design pattern in data warehousing) as a discrete unit of work, and tested each one by asserting on the initial state and final state of the database tables it was modifying. The tests were pure acceptance; they had no idea how the SQL was accomplishing the task. By treating the system\u2019s implementation as a black box, my test suite allowed the SQL to be arbitrarily refactored (for performance or clarity) with a high degree of confidence, and also served as documentation for the behavior of a very complex system.\n\nAs Airbnb has grown, we\u2019ve started moving a lot more critical functionality out into services. In our development environments, we\u2019re operating either full or stripped down versions of most of the services that our core Rails app relies on, but we still want to be able to run our tests in isolation. I\u2019ve yet to see a perfect solution to this problem, but one approach that\u2019s worked well for us is to include stub functionality in the client libraries themselves (which are generally provided by the service). It\u2019s not a perfect solution \u2014 I don\u2019t love the idea of test-specific behavior being introduced to the client libs, for one \u2014 but it has a couple significant benefits. First, it eliminates the need for our test suite to know very much about the services the app consumes, and second, relatedly, it ensures that potentially breaking interface changes to the service will be surfaced by test failures (since an update to the service will be accompanied by an update to the client, and therefore to its stubbed implementation). Drop me a note in the comments if you hate this idea or have a better solution.\n\nWhile there\u2019s still plenty of room for improvement, automated testing at Airbnb has been greatly improved in the last year. We would never have been able to achieve this without a dedicated grassroots effort, and this effort would have failed if it hadn\u2019t been combined with huge improvements in our testing tooling. These initiatives, combined with education and a healthy dose of pragmatism, have gotten us to a point where testing is ubiquitous and relatively painless, and where green builds actually mean something.\n\nBuilding good habits around testing is hard, especially at an established company. One of the biggest lessons I\u2019ve learned this year is that as long as you have a team that\u2019s open to the idea, it\u2019s always possible to start testing (even if you\u2019ve got a six year old monolithic Rails app to contend with). Once you have a decent test suite in place, you can actually start refactoring your legacy code, and from there anything is possible. The trick is just to get started."
    },
    {
        "url": "https://medium.com/airbnb-engineering/hammerspace-persistent-concurrent-off-heap-storage-3db39bb04472",
        "title": "Hammerspace: Persistent, Concurrent, Off-heap Storage",
        "text": "According to Wikipedia, \u201chammerspace is a fan-envisioned extradimensional, instantly accessible storage area in fiction, which is used to explain how animated, comic, and game characters can produce objects out of thin air.\u201d\n\nWe recently built a library that stores strings off the ruby heap, but still allows fast access of those strings from within ruby. Applications can use it to produce strings out of thin air, so we named the gem hammerspace. Hammerspace provides persistent, concurrently-accessible off-heap storage of strings with a familiar hash-like interface. It is optimized for bulk writes and random reads.\n\nWe are pleased to announce that hammerspace is now an open source project \u2014 the code is available on GitHub.\n\nThe weekly performance report arrived in my inbox, but I already knew what it would say. For the seventh week in a row, our overall application response time was up. And I had no idea why.\n\nWe had looked at all the obvious things \u2014 traffic was not up significantly, and there were no big jumps in response time after a code change was deployed. None of our external service dependencies had regressed significantly. In fact, a steady increase in garbage collection time seemed to be the biggest contributor to the regression. But that just added to the mystery.\n\nWe theorized that a slow regression over many weeks must be caused by data growth, and that led us to translations.\n\nAirbnb\u2019s web site and mobile apps have over 80,000 translatable strings. Using our translation tools, our community of translators localizes Airbnb for over 30 locales. The number of translated strings grows every day as we introduce new strings, translate more strings, and localize Airbnb for more locales. And the growth looked more or less like our response time.\n\nEach locale\u2019s translated strings are stored in a database. This allows our translators to update translations and see the results immediately without having to do a code deploy. When a rails process renders a page in a given locale for the first time, the locale\u2019s translations are loaded from the database into a ruby hash.\n\nTranslations are bulk loaded into memory because accessing translations is very sensitive to latency. Rendering a page may require accessing hundreds of translations. Incurring a 2ms delay to access each one from an external cache would be prohibitively slow. Fetching all translations for a page at once is not straightforward, since the strings required to render a page are not known ahead of time.\n\nTranslations are updated relatively infrequently. If a translator updates a string, the subsequent request must reload the updated locale\u2019s translations to provide the translator with instant feedback, but otherwise translations can be stale by tens of minutes or more. Thus, it is efficient to incur the cost of loading all translations for a locale once if the translations can be accessed quickly thereafter.\n\nOver its lifetime, a process accumulates many translations. This poses a number of problems. First, the translated strings are stored on the heap, where the garbage collector must scan over the objects on every run (at least in the case of Ruby MRI). For a mature process with all the translations loaded, this is almost 1 million objects! Second, each process has its own copy of the strings. These strings total over 80mb when all the translations are loaded. Since each process has its own copy, this 80mb is multiplied by each process on the machine. Third, each machine only has a finite amount of memory. As each process on the machine reaches maturity, memory pressure builds and a process must be sacrificed to keep the machine from running out of memory. (We have a watchdog process that kills ruby processes that are using too much memory \u2014 this keeps the kernel\u2019s out-of-memory killer at bay.)\n\nOf course, we can prevent processes from being killed by adding more memory to each machine or by limiting the number of processes that can run on each machine \u2014 in fact, we employed both of these measures as stopgap solutions. However, these solutions are not scalable because the number of active translations is constantly growing.\n\nOver the summer, as the number of translated strings grew, we saw processes being killed more and more frequently. As process lifetimes became shorter, bulk loading translations became less efficient because the up-front cost of loading the translations was no longer being offset by a long period of fast accesses.\n\nWe needed a solution that would store translations off the heap, allow sharing between processes, and persist translations across process restarts \u2014 while still providing fast access.\n\nWe considered storing translations in a local memcache instance. This would get them off the heap, allow sharing between processes, and persist them across process restarts. However, accessing memcache over a local socket was still several orders of magnitude slower than a ruby hash access, so we ruled out memcache.\n\nNext, we benchmarked cdb and sparkey. These libraries essentially provide on-disk hash tables. They are optimized for bulk writes and random reads. The numbers were encouraging \u2014 writing a locale\u2019s translations was almost as fast as a ruby hash, and reads were only about twice as slow. The filesystem cache helps a lot here \u2014 moving the files to an in-memory filesystem made little difference. Unfortunately, neither library supports concurrent writers, so files cannot be shared between processes because multiple processes might try to load the same locale at the same time. Another drawback is that these libraries require special usage patterns, so we would have to tightly couple our translation tools with these libraries.\n\nWe needed a layer on top of these lower-level libraries that would support concurrent writers. Since we had to build something anyway, we made the interface as similar to ruby\u2019s hash as possible. This solved the problem of tight coupling with the application code \u2014 we could just substitute a ruby hash with a disk-based object that acted like a hash, and the application code would function just the same.\n\nSo we built hammerspace. Hammerspace adds concurrency control to allow multiple processes to update and read from a single shared copy of the data safely. Its interface is designed to mimic Ruby\u2019s Hash to make integrating with existing applications simple and straightforward. We chose to build hammerspace on top of sparkey, but support for other low-level libraries can be added by implementing a new backend class.\n\nWe are using hammerspace to store translations, but it should be generally applicable to any data that is bulk-loaded from some authoritative external source and benefits from low-latency reads. If your application downloads a bunch of data or loads a bunch of data into memory on startup, hammerspace may be worth a look. Hammerspace is an open source project \u2014 the code is available on GitHub.\n\nFor the most part, hammerspace acts like a Ruby hash. But since it\u2019s a hash that persists on disk, you have to tell it where to store the files. Hammerspace objects are backed by files on disk, so even a new object may already have data in it.\n\nYou should call close on the hammerspace object when you're done with it. This flushes any pending writes to disk and closes any open file handles.\n\nMultiple concurrent readers are supported. Readers are isolated from writers, i.e., reads are consistent to the time that the reader was opened.\n\nMultiple concurrent writers are also supported. When a writer flushes its changes it will overwrite any previous versions of the hammerspace.\n\nIn practice, this fits our use case because hammerspace is designed to hold data that is bulk-loaded from some authoritative external source. Rather than block writers to enforce consistency, it is simpler to allow writers to concurrently attempt to load the data. The last writer to finish loading the data and flush its writes will have its data persisted.\n\nFlushing a write incurs some overhead to build the on-disk hash structures that allows fast lookup later. To avoid the overhead of rebuilding the hash after every write, most write operations do not implicitly flush. Writes can be flushed explicitly by calling close.\n\nBehind the scenes, hammerspace maintains several versions of the sparkey files in separate directories. The current version of the files is pointed to by a current symlink. Each writer begins by copying the current version of the files into its own private directory. Writes are then made to the private files. When the writes are flushed, the current symlink is atomically updated to point to the directory with the updated files, and the previous target of the current symlink is unlinked. Updating the symlink atomically ensures that the hammerspace will never be left in a corrupt state, even if a writer should crash while performing the update. Also, unlinking a file does not actually remove the contents of the file until all open file handles are closed, so if there are readers that still have the old files open they can continue to use them.\n\nIntegrating hammerspace with our translation tools was mostly straightforward, although there were a few tricky bits.\n\nThe code was already structured to do a bulk write from the database to a hash, but we needed to ensure that clearing the existing translations, writing the new translations, and setting the \"last updated\" timestamp are all flushed in a single write. Otherwise, multiple writers might overlap and we would end up with inconsistent data.\n\nWe also needed to ensure that strings are encoded as UTF-8 going into hammerspace so that we can force the encoding to be UTF-8 when pulling the strings back out. Sparkey just stores a sequence of bytes. Every hammerspace access results in a new string object created from the bytes stored in sparkey, so we cannot rely on the original string's encoding to persist.\n\nWhen we were using ruby hashes, translations were updated when a translator made a request, or when the process was killed and a new process was spawned. New processes were spawned fairly regularly, but a code deploy served as an upper bound -- all processes would be restarted and we were guaranteed to reload the latest translations from the database. With hammerspace, the loaded translations persist across process restarts, so the only thing that causes translations to be reloaded is a request made by a translator. It is unlikely that our translators can hit every machine in every locale in a timely manner, so we needed a new trigger for reloading translations, or at least checking to see if updated translations were available.\n\nWe considered checking for updated translations on every request, but that seemed overkill. Ideally, we could have a single process check for updated translations every few minutes. If there are updated translations available, the process would load them into the shared hammerspace files on the machine for all other processes to use. A rake task scheduled by a cron job fit the bill perfectly. And as a bonus, a rake task takes translation loading out of the request cycle entirely (except for translators).\n\nThe final piece of the puzzle was a before filter that runs on every request. The before filter simply closes any open hammerspace files so they are reopened on first access. Hammerspace reads are consistent to the time the reader is opened, so without the before filter, processes would continue to read from old hammerspace files indefinitely.\n\nOver the four hours between 17:15 and 21:15, we launched hammerspace to 20%, 50%, then 100% of our application servers. The graphs below show a few key metrics during the rollout.\n\nThe first graph shows the number of locales loaded and where they are loaded from. There are two trends in this graph. First, the number of locales loaded from the database drops by 86.5%, while the number of locales loaded from hammerspace increases. Second, the total number of locale loads from both sources drops by 72.6% as processes lifetimes increase. The second graph, processes killed, confirms this. Less processes are killed, meaning processes are staying alive longer.\n\nThe third and fourth graphs show runs of the scheduled rake task and the number of processes using the hash store versus the hammerspace store. These are straightforward and trend as expected.\n\nThe fifth graph shows a decrease in total in-request garbage collection time. By looking at some other metrics, we were able to determine that the time of each garbage collection run remained roughly the same, so this decrease in total time is due to fewer garbage collection runs. In-request garbage collection runs decreased by 67.3%; total time decreased by 66.3%.\n\nThe final graph shows the number of locales available in each process. When locales were loaded into memory, processes were never able to get all locales loaded before they were killed. With hammerspace, it is possible for all locales to be loaded.\n\nThanks to the improvements in garbage collection time and less time spent loading locales and doing other process startup tasks, overall application response time dropped by 17.3%!\n\nWe had originally set out to curb a garbage collection regression. Along the way, we also fixed some inefficiencies in the way we load translations. But none of us expected such a dramatic improvement. In the future, we will definitely pay more attention to lower-level issues such as memory usage, garbage collection time, and application startup time."
    },
    {
        "url": "https://medium.com/airbnb-engineering/guest-experience-on-ios7-396ac6857156",
        "title": "Guest Experience on iOS7 \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Airbnb has always been a company focused on design and pixel-perfection. The iOS7 redesign represented an opportunity to break the mobile application down and build it back up again. We developed an internal suite of tools to allow our design team to select from a predefined set of swatches (fonts, colors, paddings, etc.), which leads to increased cohesion by decreasing the number of \u201cone-off\u201d design elements. In addition, the code is cleaner than ever; designers and programmers alike can know at-a-glance that the spec and the implementation are at parity.\n\nFlat design has become mainstream with the introduction of iOS7, and in our redesign of the Airbnb application we certainly wanted to adjust to this new paradigm. That said, flat design does not mean that apps need to completely avoid any sense of depth. The Z-axis can be used sparingly to increase its effectiveness, while avoiding cluttering screens with drop-shadows. Used properly, depth can create a sense of boundless space, which ultimately leads to more exploration by the user. The new Airbnb navigation system, dubbed AirNav, flies open to reveal the two halves of the Airbnb experience (Hosting and Traveling):\n\nThe search results, discover section and wishlists have all been redesigned to take maximum advantage of the beautiful photography of many Airbnb listings. Our color picking algorithm can select relevant hues to serve as backgrounds to title bars; it runs asynchronously to quickly select bright colors from the image while ignoring the white/black extremes (check out the GitHub link for more details). Such techniques can be challenging to calibrate, but the effect is to create an experience which is simultaneously polychromatic and cohesive:\n\nWhen a user opens up a listing, the header image is blurred and placed behind a semi-transparent white background. This creates more procedurally generated variety: each listing has a unique look and feel, yet the effect is subtle enough to not be jarring. Proprietary blurring algorithms (since Apple has not release theirs) cannot be executed real-time (eg, even on high end devices it takes more than 1/30th of a second to blur an image). In addition, blurring adds a second step to downloading an image from our content servers (it would be visually jarring to show a sharp image, and then replace it with a blurred version). However, these steps are hidden through use of fade animations.\n\nNot only is visual impact maximized through large images, but the lack of dividing lines and other constraining UI elements leads to a boundless feel. As the user scrolls down, many views use a parallax effect and rule of thirds to hide header images and make even more room for content.\n\nAs the guest proceeds through the booking process, the app has one last surprise. Modal windows (such as the share, book, and contact views) take advantage of animations to smoothly morph one screen into the next. UIDynamics anchors the experience to the real world, through the detection of gravity and acceleration, in a subtle but compelling way.\n\nIncreasingly, good app design has become about showing only the right things at the right time (especially important on small screens). This tactic is only effective, however, if users are encouraged through enjoyable interactions to explore the application: otherwise the true power will never be discovered. The iOS7 redesign of the Airbnb application accomplishes this by staying out of the user\u2019s way as much as possible, while calling attention to the most compelling aspects of travel."
    },
    {
        "url": "https://medium.com/airbnb-engineering/isomorphic-javascript-the-future-of-web-apps-10882b7a2ebc",
        "title": "Isomorphic JavaScript: The Future of Web Apps \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "At Airbnb, we\u2019ve learned a lot over the past few years while building rich web experiences. We dove into the single-page app world in 2011 with our mobile web site, and have since launched Wish Lists and our newly-redesigned search page, among others. Each of these is a large JavaScript app, meaning that the bulk of the code runs in the browser in order to support a more modern, interactive experience.\n\nThis approach is commonplace today, and libraries like Backbone.js, Ember.js, and Angular.js have made it easier for developers to build these rich JavaScript apps. We have found, however, that these types of apps have some critical limitations. To explain why, let\u2019s first take a quick detour through the history of web apps.\n\nSince the dawn of the Web, the browsing experience has worked like this: a web browser would request a particular page (say, \u201chttp://www.geocities.com/\"), causing a server somewhere on the Internet to generate an HTML page and send it back over the wire. This has worked well because browsers weren\u2019t very powerful and HTML pages represented documents that were mostly static and self-contained. JavaScript, created to allow web pages to be more dynamic, didn\u2019t enable much more than image slideshows and date picker widgets.\n\nAfter years of advances in personal computing, creative technologists have pushed the web to its limits, and web browsers have evolved to keep up. Now, the Web has matured into a fully-featured application platform, and fast JavaScript runtimes and HTML5 standards have enabled developers to create the rich apps that before were only possible on native platforms.\n\nIt wasn\u2019t long before developers started to build out entire applications in the browser using JavaScript, taking advantage of these new capabilities. Apps like Gmail, the classic example of the single-page app, could respond immediately to user interactions, no longer needing to make a round-trip to the server just to render a new page.\n\nLibraries like Backbone.js, Ember.js, and Angular.js are often referred to as client-side MVC (Model-View-Controller) or MVVM (Model-View-ViewModel) libraries. The typical client-side MVC architecture looks something like this:\n\nThe bulk of the application logic (views, templates, controllers, models, internationalization, etc.) lives in the client, and it talks to an API for data. The server could be written in any language, such as Ruby, Python, or Java, and it mostly handles serving up an initial barebones page of HTML. Once the JavaScript files are downloaded by the browser, they are evaluated and the client-side app is initialized, fetching data from the API and rendering the rest of the HTML page.\n\nThis is great for the user because once the app is initially loaded, it can support quick navigation between pages without refreshing the page, and if done right, can even work offline.\n\nThis is great for the developer because the idealized single-page app has a clear separation of concerns between the client and the server, promoting a nice development workflow and preventing the need to share too much logic between the two, which are often written in different languages.\n\nIn practice, however, there are a few fatal flaws with this approach that prevent it from being right for many use cases.\n\nAn application that can only run in the client-side cannot serve HTML to crawlers, so it will have poor SEO by default. Web crawlers function by making a request to a web server and interpreting the result; but if the server returns a blank page, it\u2019s not of much value. There are workarounds, but not without jumping through some hoops.\n\nBy the same token, if the server doesn\u2019t render a full page of HTML but instead waits for client-side JavaScript to do so, users will experience a few critical seconds of blank page or loading spinner before seeing the content on the page. There are plenty of studies showing the drastic effect a slow site has on users, and thus revenue. Amazon claims that each 100ms reduction in page load time raises revenue by 1%. Twitter spent a year and 40 engineers rebuilding their site to render on the server instead of the client, claiming a 5x improvement in perceived loading time.\n\nWhile the ideal case can lead to a nice, clean separation of concerns, inevitably some bits of application logic or view logic end up duplicated between client and server, often in different languages. Common examples are date and currency formatting, form validations, and routing logic. This makes maintenance a nightmare, especially for more complex apps.\n\nSome developers, myself included, feel bitten by this approach \u2014 it\u2019s often only after having invested the time and effort to build a single-page app that it becomes clear what the drawbacks are.\n\nAt the end of the day, we really want a hybrid of the new and old approaches: we want to serve fully-formed HTML from the server for performance and SEO, but we want the speed and flexibility of client-side application logic.\n\nTo this end, we\u2019ve been experimenting at Airbnb with \u201cIsomorphic JavaScript\u201d apps, which are JavaScript applications that can run both on the client-side and the server-side.\n\nAn isomorphic app might look like this, dubbed here \u201cClient-server MVC\u201d:\n\nIn this world, some of your application and view logic can be executed on both the server and the client. This opens up all sorts of doors \u2014 performance optimizations, better maintainability, SEO-by-default, and more stateful web apps.\n\nWith Node.js, a fast, stable server-side JavaScript runtime, we can now make this dream a reality. By creating the appropriate abstractions, we can write our application logic such that it runs on both the server and the client \u2014 the definition of isomorphic JavaScript.\n\nThis idea isn\u2019t new \u2014 Nodejitsu wrote a great description of isomorphic JavaScript architecture in 2011 \u2014 but it\u2019s been slow to adopt. There have been a few isomorphic frameworks to spring up already.\n\nMojito was the first open-source isomorphic framework to get any press. It\u2019s an advanced, full-stack Node.js-based framework, but its dependence on YUI and Yahoo!-specific quirks haven\u2019t led to much popularity in the JavaScript community since they open sourced it in April 2012.\n\nMeteor is probably the most well-known isomorphic project today. Meteor is built from the ground up to support real-time apps, and the team is building an entire ecosystem around its package manager and deployment tools. Like Mojito, it is a large, opinionated Node.js framework, however it\u2019s done a much better job engaging the JavaScript community, and its much-anticipated 1.0 release is just around the corner. Meteor is a project to keep tabs on \u2014 it\u2019s got an all-star team, and it\u2019s raised $11.2 M from Andreessen Horowitz \u2014 unheard of for a company wholly focused on releasing an open-source product.\n\nAsana, the task management app founded by Facebook cofounder Dustin Moskovitz, has an interesting isomorphic story. Not hurting for funding, considering Moskovitz\u2019 status as youngest billionaire in the world, Asana spent years in R&D developing their closed-source Luna framework, one of the most advanced examples of isomorphic JavaScript around. Luna, originally built on v8cgi in the days before Node.js existed, allows a complete copy of the app to run on the server for every single user session. It runs a separate server process for each user, executing the same JavaScript application code on the server that is running in the client, enabling a whole class of advanced optimizations, such as robust offline support and snappy real-time updates.\n\nWe launched an isomorphic library of our own earlier this year. Called Rendr, it allows you to build a Backbone.js + Handlebars.js single-page app that can also be fully rendered on the server-side. Rendr is a product of our experience rebuilding the Airbnb mobile web app to drastically improve pageload times, which is especially important for users on high-latency mobile connections. Rendr strives to be a library rather than a framework, so it solves fewer of the problems for you compared to Mojito or Meteor, but it is easy to modify and extend.\n\nThat these projects tend to be large, full-stack web frameworks speaks to the difficulty of the problem. The client and server are very dissimilar environments, and so we must create a set of abstractions that decouple our application logic from the underlying implementations, so we can expose a single API to the application developer.\n\nWe want a single set of routes that map URI patterns to route handlers. Our route handlers need to be able to access HTTP headers, cookies, and URI information, and specify redirects without directly accessing window.location (browser) or req and res (Node.js).\n\nWe want to describe the resources needed to render a particular page or component independently from the fetching mechanism. The resource descriptor could be a simple URI pointing to a JSON endpoint, or for larger applications, it may be useful to encapsulate resources in models and collections and specify a model class and primary key, which at some point would get translated to a URI.\n\nWhether we choose to directly manipulate the DOM, stick with string-based HTML templating, or opt for a UI component library with a DOM abstraction, we need to be able to generate markup isomorphically. We should be able to render any view on either the server or the client, dependent on the needs of our application.\n\nIt turns out writing isomorphic application code is only half the battle. Tools like Grunt and Browserify are essential parts of the workflow to actually get the app up and running. There can be a number of build steps: compiling templates, including client-side dependencies, applying transforms, minification, etc. The simple case is to combine all application code, views and templates into a single bundle, but for larger apps, this can result in hundreds of kilobytes to download. A more advanced approach is to create dynamic bundles and introduce asset lazy-loading, however this quickly gets complicated. Static-analysis tools like Esprima can allow ambitious developers to attempt advanced optimization and metaprogramming to reduce boilerplate code.\n\nBeing first to market with an isomorphic framework means you have to solve all these problems at once. But this leads to large, unwieldy frameworks that are hard to adopt and integrate into an already-existing app. As more developers tackle this problem, we\u2019ll see an explosion of small, reusable modules that can be integrated together to build isomorphic apps.\n\nIt turns out that most JavaScript modules can already be used isomorphically with little to no modification. For example, popular libraries like Underscore, Backbone.js, Handlebars.js, Moment, and even jQuery can be used on the server.\n\nTo demonstrate this point, I\u2019ve created a sample app called isomorphic-tutorial that you can check out on GitHub. By combining together a few modules, each that can be used isomorphically, it\u2019s easy to create a simple isomorphic app in just a few hundred lines of code. It uses Director for server- and browser-based routing, Superagent for HTTP requests, and Handlebars.js for templating, all built on top of a basic Express.js app. Of course, as an app grows in complexity, one has to introduce more layers of abstraction, but my hope is that as more developers experiment with this, there will be new libraries and standards to emerge.\n\nAs more organizations get comfortable running Node.js in production, it\u2019s inevitable that more and more web apps will begin to share code between their client and server code. It\u2019s important to remember that isomorphic JavaScript is a spectrum \u2014 it can start with just sharing templates, progress to be an entire application\u2019s view layer, all the way to the majority of the app\u2019s business logic. Exactly what and how JavaScript code is shared between environments depends entirely on the application being built and its unique set of constraints.\n\nNicholas C. Zakas has a nice description of how he envisions apps will begin to pull their UI layer down to the server from the client, enabling performance and maintainability optimizations. An app doesn\u2019t have to rip out its backend and replace it with Node.js to use isomorphic JavaScript, essentially throwing out the baby with the bathwater. Instead, by creating sensible APIs and RESTful resources, the traditional backend can live alongside the Node.js layer.\n\nAt Airbnb, we\u2019ve already begun to retool our client-side build process to use Node.js-based tools like Grunt and Browserify. Our main Rails app may never be entirely supplanted by a Node.js app, but by embracing these tools it gets ever easier to share certain bits of JavaScript and templates between environments.\n\nYou heard it here first \u2014 within a few years, it will be rare to see an advanced web app that isn\u2019t running some JavaScript on the server.\n\nIf this idea excites you, come check out the Isomorphic JavaScript workshop I\u2019ll be teaching at DevBeat on Tuesday, November 12 in San Francisco, or at General Assembly on Thursday, November 21. We\u2019ll hack together on the sample Node.js isomorphic-tutorial app I\u2019ve created to demonstrate how easy it really is to get started writing isomorphic apps.\n\nAlso keep tabs on the evolution of the Airbnb web apps by following me at @spikebrehm and the Airbnb Engineering team at @AirbnbEng."
    },
    {
        "url": "https://medium.com/airbnb-engineering/host-experience-on-android-a6f81f64af73",
        "title": "Host Experience on Android \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "One of our goals is to push the envelope of Android design. We strive to craft and build an innovative, elegant, and distinctly Android experience. The new Host Home on Android is an excellent example of this. With Host Home we want to provide our Airbnb hosts with a one-stop shop in the mobile app that has all the tools they need. We enable them to easily manage their listings, organize their calendar, receive important inquiry and reservation alerts, and communicate with their guests efficiently.\n\nThe redesigned inquiry and reservation UI organizes information into sections across three swipeable tabs: Itinerary, Profile and Messages. Each tab serves one primary function:\n\nThese three components together form the core of an Airbnb reservation for our guests and hosts as they manage their travels and listings.\n\nThe three tab design presents us with an interesting technical challenge: we have one single parallax, collapsible top image with a sticky toolbar that must be preserved as the top section across the three ViewPager tabs. We also want to maintain the the ability to horizontally swipe primary content on the screen.\n\nWe encapsulated the implementation of the three ViewPager tabs inside individual Fragments. We used different UI building blocks for each tab. The Itinerary and Profile tabs are ScrollViews, so each tab can flexibly display multiple content sections of variable length. The Messages tab is a ListView, so we can take full advantage of view recycling that comes free in a standard ListView. Each individual fragment-based tab still needs to scroll with the top image and toolbar together, which poses a few challenges.\n\nOur initial thought was to put the ViewPager, with its three scrollable view components, inside an outer ScrollView, and let the system take care of scroll interaction and user touch events. Unfortunately, the system cannot determine which of the two ScrollViews should handle a touch event and sometimes fails to deliver it, resulting in nondeterministic and undesirable scroll behavior.\n\nOur Messages fragment is based on a ListView. Placing a ListView inside a ScrollView negates all the benefits of using a ListView, as you lose view recycling and the important optimizations in ListView for dealing with large lists. This forces the ListView to display all of its items and to fill up the entire ScrollView container.\n\nThe solution we came up with is to use a RelativeLayout as our outer container, which hosts the ViewPager and consists of three scrollable fragments. This outer RelativeLayout also hosts the top image and toolbar.\n\nLet\u2019s dig into how this was built.\n\nEach of the ViewPager\u2019s fragments implements a callback that is responsible for keeping track of, and updating its scroll position and the top offset within the parent RelativeLayout. As the user scrolls up and down, or side-swipes to a different tab, the callback in the individual ViewPager fragment intelligently adjusts the contained ScrollView\u2019s or ListView\u2019s internal top offset and scroll position to reveal the correct amount of parallaxed image underneath, achieving the visual effect desired.\n\nBelow are some simplified code samples that illustrate our solution approach.\n\n2. The ScrollView implementation of the callback \u2014 used in Itinerary and Profile Fragment\n\n3. The ListView implementation of the callback \u2014 used in Messages Fragment\n\n4. Set up ViewPager.OnPageChangeListener to invoke the callback. This gets triggered when the users navigate to a different tab either by side-swipe or tapping on the top tab bar buttons.\n\n5. Set up ViewPager\u2019s parent view\u2019s ViewTreeObserver.OnGlobalLayoutListener to invoke the callback. This gets triggered when the user scrolls vertically.\n\nLeveraged the Android RenderScript framework to dynamically blur the top listing image. Photos with blurred effect in a few key views of the app enhance our narrative.\n\nImplemented our own Shader-based custom ImageView to download user profile pictures and round them with an optional, adjustable border.\n\nWe believe this is but one of the examples of a culmination of great design and dedicated engineering to produce an experience that is both unique to Airbnb, yet feels right at home on the Android platform."
    },
    {
        "url": "https://medium.com/airbnb-engineering/smartstack-service-discovery-in-the-cloud-4b8a080de619",
        "title": "SmartStack: Service Discovery in the Cloud \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "SmartStack is an automated service discovery and registration framework. It makes the lives of engineers easier by transparently handling creation, deletion, failure, and maintenance work of the machines running code within your organization. We believe that our approach to this problem is among the best possible: simpler conceptually, easier to operate, more configurable, and providing more introspection than any of its kind. The SmartStack way has been battle-tested at Airbnb over the past year, and has broad applicability in many organizations, large and small.\n\nSmartStack\u2019s components \u2014 Nerve and Synapse \u2014 are available on GitHub! Read on to learn more about the magic under the hood.\n\nCompanies like Airbnb often start out as monolithic applications \u2014 a kind of swiss army knife which performs all of the functions of the organization. As traffic (and the number of engineers working on the product) grows, this approach doesn\u2019t scale. The code base becomes too complicated, concerns are not cleanly separated, changes from many engineers touching many different parts of the codebase go out together, and performance is determined by the worst-performing sections in the application.\n\nThe solution to this problem is services: individual, smaller code bases, running on separate machines with separate deployment cycles, that more cleanly address more targeted problem domains. This is called a services-oriented architecture: SOA.\n\nAs you build out services in your architecture, you will notice that instead of maintaining a single pool of general-purpose application servers, you are now maintaining many smaller pools. This leads to a number of problems. How do you direct traffic to each machine in the pool? How do you add new machines and remove broken or retired ones? What is the impact of a single broken machine on the rest of the application?\n\nDealing with these questions, across a collection of several services, can quickly grow to be a full-time job for several engineers, engaging in an arduous, manual, error-prone process fraught with peril and the potential for downtime.\n\nThe answer to the problem of services is automation \u2014 let computers do the dirty work of maintaining your backend pools. But there are many ways to implement such automation. Let\u2019s first outline the properties of an ideal implementation:\n\nWe can use these criteria to evaluate potential solutions. For instance, manually changing a list of backends which has been hard-coded in a client, and then deploying the client, immediately fails many of these criteria by wide margins. How do some other approaches stack up?\n\nMany commonly-used approaches to service discovery don\u2019t actually work very well in practice. To understand why SmartStack works so well, it can be helpful to first understand why other solutions do not.\n\nThe simplest solution to registration and discovery is to just put all of your backends behind a single DNS name. To address a service, you contact it by DNS name and the request should get to a random backend.\n\nThe registration component of this is fairly well-understood. On your own infrastructure, you can use dynamic DNS backends like BIND-DLZ for registration. In the cloud, with hosted DNS like Route53, simple API calls suffice. In AWS, if you use round-robin CNAME records you would even get split horizon for free, so the same records would work from both inside and outside AWS.\n\nHowever, using DNS for service discovery is fraught with peril. First, consumers have to poll for all changes \u2014 there\u2019s no way to push state. Also, DNS suffers from propagation delays; even after your monitoring detects a failure and issues a de-registration command to DNS, there will be at least a few seconds before this information gets to the consumers. Worse, because of the various layers of caching in the DNS infrastructure, the exact propagation delay is often non-deterministic.\n\nWhen using the naive approach, in which you just address your service by name, there\u2019s no way to determine which boxes get traffic. You get the equivalent of random routing, with loads chaotically piling up behind some backends while others are left idle.\n\nWorst of all, many applications cache DNS resolution once, at startup. For instance, Nginx will cache the results of the initial name resolution unless you use a configuration file hack. The same is true of HAProxy. Figuring out that your application is vulnerable can be costly, and fixing the problem harder still.\n\nNote that we are here referring to native use of DNS by client libraries. There are ways to use the DNS system more intelligently to do service discovery \u2014 where we use Zookeeper in SmartStack, we could use DNS too without losing too much functionality. But simply making an HTTP request to myservice.mydomain.com via an HTTP library does not work well.\n\nIf you\u2019re convinced that DNS is the wrong approach for service discovery, you might decide to take the route of centralizing your service routing. In this approach, if service a wants to talk to service b, it should talk to a load balancer, which will properly route the request. All services are configured with the method of finding the load balancer, and the load balancer is the only thing that needs to know about all of the backends.\n\nThis approach sounds promising, but in reality it doesn\u2019t buy you much. First, how do your services discover the load balancer? Often, the answer is DNS, but now you\u2019ve introduced more problems over the DNS approach than you\u2019ve solved \u2014 if your load balancer goes down, you are still faced with all of the problems of service discovery over DNS. Also, a centralized routing layer is a big fat point of failure. If that layer goes down, everything else goes with it. Reconfiguring the load balancer with new backends \u2014 a routine operation \u2014 becomes fraught with peril.\n\nNext, what load balancer do you choose? In a traditional data center, you might be tempted to reach for a couple of hardware devices like an F5. But in the cloud this is not an option. On AWS, you might be tempted to use ELB, but ELBs are terrible at internal load balancing because they only have public IPs. Traffic from one of your servers to another would have to leave your private network and re-enter it again. Besides introducing latency, it wreaks havoc on security groups. If you decide to run your own load balancing layer on EC2 instances, you will end up just pushing the problem of service discovery one layer up. Your load balancer is now no longer a special, hardened device; it\u2019s just another instance, just as prone to failure but just especially critical to your operations.\n\nGiven the problems of DNS and central load balancing, you might decide to just solve the problem with code. Instead of using DNS inside your app to discover a dependency, why not use some different, more specialized mechanism?\n\nThis is a popular approach, which is found in many software stacks. For instance, Airbnb initially used this model with our Twitter commons services. Our java services running on Twitter commons automatically registered themselves with zookeeper, a central point of configuration information which replaces DNS. Apps that wanted to talk to those services would ask Zookeeper for a list of available backends, with periodic refresh or a subscription via zookeeper watches to learn about changes in the list.\n\nHowever, even this approach suffers from a number of limitations. First, it works best if you are running on a unified software stack. At Twitter, where most services run on the JVM, this is easy. However, at Airbnb, we had to implement our own zookeeper client pool in ruby in order to communicate with ZK-registered services. The final implementation was not well-hardened, and resulted in service outages whenever Zookeeper was down.\n\nWorse, as we began to develop more services for stacks like Node.js, we foresaw ourselves being forced to implement the same registration and discovery logic in language after language. Sometimes, the lack or immaturity of relevant libraries would further hamper the effort. Finally, sometimes you would like to run apps which you did not write yourself but still have them consume your infrastructure: Nginx, a CI server, rsyslog or other systems software. In these cases, in-app discovery is completely impossible.\n\nFinally, this approach is very difficult operationally. Debugging an app that registers itself is impossible without stopping the app \u2014 we\u2019ve had to resort to using iptables to block the Zookeeper port on machines we were investigating. Special provisions have to be made to examine the list of backends that a particular app instance is currently communicating with. And again, intelligent load balancing is complicated to implement.\n\nSmartStack is our solution to the problem of SOA. SmartStack works by taking the whole problem out-of-band from your application, in effect abstracting it away. We do this with two separate applications that run on the same machine as your app: Nerve, for service registration, and Synapse, for service discovery.\n\nNerve does nothing more complicated than what Twitter commons already did. It creates ephemeral nodes in Zookeeper which contain information about the address/port combos for a backend available to serve requests for a particular service.\n\nIn order to know whether a particular backend can be registered, Nerve performs health checks. Every service that you want to register has a list of health checks, and if any of them fail the backend is de-registered.\n\nAlthough a health check can be as simple as \u201cis this app reachable over TCP or HTTP\u201d, properly integrating with Nerve means implementing and exposing a health check via your app. For instance, at Airbnb, every app that speaks HTTP exposes a /health endpoint, which returns a 200 OK if the app is healthy and a different status code otherwise. This is true across our infrastructure; for instance, try visiting https://www.airbnb.com/health in your browser!\n\nFor non-HTTP apps, we\u2019ve written checks that speak the particular protocol in question. A redis check, for instance, might try to write and then read a simple key. A rabbitmq check might try to publish and then consume a message.\n\nSynapse is the magic behind service discovery at Airbnb. It runs beside your service, and handles making your service dependencies available to use, transparently to your app. Synapse reads the information in Zookeeper for available backends, and then uses that information to configure a local HAProxy process. When a client wants to talk to a service, it just talks to the local HAProxy, which takes care of properly routing the request.\n\nThis is, in essence, a decentralized approach to the centralized load load balancing solution discussed above. There\u2019s no bootstrapping problem because there\u2019s no need to discover the load balancing layer \u2014 it\u2019s always present on localhost.\n\nThe real work is performed by HAProxy, which is extremely stable and battle-tested. The Synapse process itself is just an intermediary \u2014 if it goes down, you will not get notification about changes but everything else will still function. Using HAProxy gives us a whole host of advantages. We get all of the powerful logging and introspection. We get access to very advanced load-balancing algorithms, queueing controls, retries, and timeouts. There is built-in health checking which we can use to guard against network partitions (where we learn about an available backend from Nerve, but can\u2019t actually talk to it because of the network). In fact, we could probably fill an entire additional blog post just raving about HAProxy and how we configure it.\n\nAlthough Synapse writes out the entire HAProxy config file every time it gets notified about changes in the list of backends, we try to use HAProxy\u2019s stats socket to make changes when we can. Backends that already exist in HAProxy are put into maintenance mode via the stats socket when they go down, and then recovered when they return. We only have to restart HAProxy to make it re-read its config file when we have to add an entirely new backend.\n\nSmartStack has worked amazingly well here at Airbnb. To return to our previous checklist, here is how we stack up:\n\nAfter using SmartStack in production at Airbnb for the past year, we remain totally in love with it. It\u2019s simple to use, frees us from having to think about many problems, and easy to debug when something goes wrong.\n\nBesides the basic functionality of SmartStack \u2014 allowing services to talk to one another \u2014 we\u2019ve built some additional tooling directly on top of it.\n\nCharon is Airbnb\u2019s front-facing load balancer. Previous to Charon, we used Amazon\u2019s ELB. However, ELB did not provide us with good introspection into our production traffic. Also, the process of adding and removing instances to the ELB was clunky \u2014 we had a powerful service discovery framework, and then we had a second one just for front-facing traffic. Finally, because ELB is based on EBS, we were worried about the impact of another EBS outage.\n\nWe were already putting Nginx as the first thing behind the ELB. This is because our web traffic is split between several user-facing apps \u2014 https://www.airbnb.com is not served by the same servers as https://www.airbnb.com/help, for example. With Charon, traffic from Akamai hits one of a few Nginx servers directly. Each of these servers is running Synapse, with the user-facing services enabled. As we match the URI against locations, traffic is sent, via the correct HAProxy for that service, to the optimal backend to handle that request.\n\nIn effect, Charon makes routing from users to our user-facing services just like routing between backend services. We get all of the same benefits \u2014 health checks, the instant addition and removal of instances, and correct queueing behavior via HAProxy.\n\nWhat we do for user-facing services, we also do for internal services like dashboards or monitors. Wild card DNS sends all *.dyno requests to a few dyno boxes. Nginx extracts the service name from the Host header and then directs you to that service via Synapse and HAProxy.\n\nThe achilles heel of SmartStack is Zookeeper. Currently, the failure of our Zookeeper cluster will take out our entire infrastructure. Also, because of edge cases in the zk library we use, we may not even be handling the failure of a single ZK node properly at the moment.\n\nMost recent work has been around building better testing infrastructure around SmartStack. We can now spin up an integration test for the entire platform easily with our Chef cookbook and Vagrant. We hope that with better testing, we can make headway on making SmartStack even more resilient.\n\nWe are also considering adding dynamic service registration for Nerve. Currently, the discovery that Nerve performs is hardcoded into Nerve via its config file. In practice, because we manage our infrastructure with configuration management (Chef) this is not a huge problem for us. If we deploy a new service on a machine, the Chef code that deploys the service also updates the Nerve config.\n\nHowever, in environments like Mesos, where a service might be dynamically scheduled to run on a machine at any time, it would be awesome to have the service easily register itself with Nerve for health checking. This introduces tighter coupling between the services and SmartStack than we currently have, so we are thinking carefully through this approach to make sure we build the correct API.\n\nWe would love for SmartStack to prove it\u2019s value in other organizations. Give it a shot, and tell us how it works for you!\n\nProbably the easiest way to get started is by using the Chef cookbook. The code there can create a comprehensive test environment for SmartStack and run it through automated integration tests. It is also the code we use to run SmartStack in production, and provides what we think is a solid approach to managing SmartStack configuration. There is also excellent documentation on running and debugging SmartStack included with the cookbook.\n\nIf you would like to configure Nerve and Synapse yourself, check out documentation in their respective repositories. We\u2019ve included example config files to get you started with each.\n\nScaling a web infrastructure requires services, and building a service-oriented infrastructure is hard. Make it EASY, with SmartStack\u2019s automated, transparent service discovery and registration: cruise control for your distributed infrastructure."
    },
    {
        "url": "https://medium.com/airbnb-engineering/making-breakfast-chef-at-airbnb-8e74efff4707",
        "title": "Making Breakfast: Chef at Airbnb \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Airbnb is a dynamic code environment. As we build an SOA to cope with our rapid growth, we have dozens of people creating, launching, and wiring together the services which create the user experience you see on our site.\n\nAll of these services have to run on some machines \u2014 in our case, virtual machines in Amazon EC2. We\u2019ve gone through several approaches to launching and configuring these machines. Last year, we open-sourced a tool we were using at the time, called CloudMaker. At the time, this tool was a major improvement over our previous manual approach. However, it didn\u2019t scale for us. There wasn\u2019t enough power in the YAML configs to specify all of the configuration we wanted to do, and so we grew complicated trees of versioned shell scripts stored in S3 that would do the bulk of the work and were completely unmaintainable.\n\nIn December of 2012, we began a migration to Chef. We started out using Opscode\u2019s excellent hosted chef, which enabled us to hit the ground running. Within a few weeks of getting started, we had all our common configuration ported over and were running Chef-enabled services in production. As we added more Chef users inside the company, however, we realized that our chef server workflow was not scaling. As users worked on different parts of the system, they would push different cookbooks, roles, and data bags to the server, sometimes clobbering the code other users had pushed previously. We needed a way to enable all of our users to work in parallel without affecting the work of other users, or the deployed code in production.\n\nSo, we ended up moving over to Chef Solo, and thus arrived where we are today. We have a set of tools and workflow which make machine administration a breeze. We would like to share our approach with the community, in the hopes that our lessons might come in handy in other organizations\n\nWe use a single git repo for all of our configuration management. All of our own cookbooks as well as any cookbooks we require as dependencies are imported into this repo.\n\nA single branch, called production, contains the authoritative configuration code which configures production instances. The SRE team reviews all merge requests into the production branch, acting as gatekeepers of the infrastructure.\n\nPeople who work on features for their services or on changes to the common configuration work in branches. For testing, our engineers spin up Vagrant boxes or EC2 instances using the chef code in their branch, and they submit their pull requests once they\u2019ve already tested their changes.\n\nAlmost all of our engineers now have some exposure to Chef. Many have gone through internal training sessions. We also have very extensive internal documentation, which covers everything from explaining the basics of chef to the nitty-gritty of cookbook testing at Airbnb.\n\nAny team which maintains a service running on it\u2019s own machines also maintains it\u2019s own cookbook for that service. Our Site Reliability (SRE) team is responsible for the common cookbooks that set up the base system as well as some shared cookbooks, such as the ones that install java or configure nginx. We\u2019ve also created internal abstraction definitions, such as java_service and rails_service. Using these, it is often possible to set up a service using a recipe containing just that one single resource.\n\nWe consider three primary attributes which define how a machine is going to be configured:\n\nOur environments are fairly minimal, and contain overarching configuration information, such as the addresses of the zookeeper servers in that environment. Cookbooks also usually use the environment to determine which credentials files to load during a run; machines in the development environment never get production credentials.\n\nWe always assign a single role to a box. We occasionally have SPOFs \u2014 roles which are only used by a single machine \u2014 but usually a role will be shared by a group of machines. A role is the main identifier for machines. Some examples: mobile-web-worker, sphinx, pricing.\n\nFinally, the branch attribute of a machine determines which chef repo branch will be used to configure the machine during a chef-solo run. Most of our boxes are on the production branch, but people will run boxes on branches when they\u2019re developing their cookbooks.\n\nThe ability to run boxes on git branches is an amazingly powerful thing. Testing chef code means pushing to a git branch \u2014 no knife commands! Engineers working on cookbooks can easily bring up machines running their new code, without any fear that this code will affect production machines in any way. While running on a branch, they can be sure that nothing else is changing in their machine configs except for the changes they themselves are making. We can do all of this even while avoiding strict and painful versioning of cookbooks and specialized tools like knife-spork \u2014 all you need to know is git.\n\nBecause all of our service owners want to stay current with the changes that SRE is making in the common cookbooks, they have an incentive to get their code merged into production. But they don\u2019t have to do it until they\u2019re ready; running on a branch is just as good most of the time, and if there are some critical updates they need they can always rebase or cherry-pick.\n\nWe store the chef attributes in three files \u2014 /etc/chef/environment, /etc/chef/role and /etc/chef/branch. Grabbing a machine from production to test a new configuration is as simple as sshing in and editing /etc/chef/branch.\n\nEvery one of our production instances have a converge command, which is a simple Bash wrapper around running chef-solo. The converge command first pulls down the latest copy of our chef repo, and checks out the correct branch \u2014 the one listed in /etc/chef/branch. Then, using the environment and role specified in /etc/chef/role and /etc/chef/environment, it generates a JSON hash and a runlist and passes those to chef-solo.\n\nWe loved being able to easily bring up instances using knife ec2. When we moved over to Chef solo, we implemented our own version of this, which we call stemcell.\n\nStemcell calls the AWS API to create the correct kind of instance, sets the proper tags, and then bootstraps the box to run Chef. Initially, we passed all information about launching instances to stemcell by hand on the command line \u2014 or created little bash wrapper scripts to properly invoke the command. Now, we keep role metadata in the role files themselves, and allow stemcell to parse the role information to determine what to do. Most of our roles have an attribute hash that looks like this:\n\nStemcell works with ubuntu\u2019s cloud-init for the initial configuration of instances. The script we place in the AWS machine\u2019s user data goes through the following steps:\n\nChef-server provides you with the ability to list all of your nodes, via the web UI or via a knife command. We wanted to implement similar functionality, so we came up with a simple service we call optica.\n\nAt the end of every run of chef, our custom optica handler pings to the service over HTTP and reports on changed resources, the success/failure of the run, and any custom parameters we would like to report. Optica also supports a convenient query API. For instance, to get all of our web worker nodes on the igor-test branch, we could do curl -sf http://optica/?role=web-worker&branch=igor-test.\n\nOptica expects and returns JSON. For routine operations, our engineers use optica with JQ on the command line to learn about our infrastructure.\n\nWe also enjoyed using knife-ssh to get information or to force a converge on a particular role. To replicate this functionality, we chose fabric. Our fabfile automatically populates the role list by quering optica, so we can do something like:\n\nThe fabfile takes care of the rest! The optica repo has more information, along with all of the necessary scripts and tools to get started.\n\nCurrently, our chef repo has over 50 individual contributors \u2014 the bulk of our engineering team. Most of the contributors work on cookbooks responsible for deploying their own services. For instance, engineers from our operations team work on the lantern cookbook, which runs the internal tool for our customer service team.\n\nEngineers are free to develop and test their own code in branches. However, it goes through a rigorous peer review with a member of the SRE team before it can be merged into the production branch. This means both that the code quality across all of our cookbooks is consistently high, and that the knowledge of the configuration is distributed through the team.\n\nAlthrough our workflow right now is solid and works very well for us, we recognize that there is a lot of room for improvement. We\u2019re working on a number of changes to this at the moment.\n\nStemcell is currently being internally deprecated in favor of a service which handles the launching of instances. This is because we would like to keep instance launching more consistent and reliable, and also to minimize the number of people in our team with credentials to launch instances.\n\nMost of our testing is manual at the moment. We provide basic Vagrant configs that enable sanity-testing of cookbooks locally, and it\u2019s easy to test in ec2 using branches. However, we are developing a comprehensive testing tool built on top of docker, which allows quickly testing changes without having to run through the entire runlist. We\u2019ll say more about this tool (internally called garcon at the moment) in a future blog post.\n\nFinally, we are always interested in hearing from the you about how we could improve. One of the reasons we chose Chef is because of its consistently excellent, creative, dedicated user community, which has made Chef the amazing tool it is today. We\u2019re paying attention on the Chef mailing list and on our repos in GitHub \u2014 drop us a line!"
    },
    {
        "url": "https://medium.com/airbnb-engineering/postmortems-at-airbnb-dde936fd7877",
        "title": "Postmortems at Airbnb \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "At Airbnb, postmortems are an important part of our culture. We have a custom tool for tracking incidents along with what went wrong and what went right. This gives us a huge base of knowledge to draw upon whenever we observe issues. It\u2019s also a great way to keep the team on the same page about the sort of problems that we are currently facing.\n\nWriting tons of postmortems could quickly become a dismal affair, especially for those of us on the reliability teams. We practice blameless postmortems where the primary goal is learning. We also have a lot of fun with our postmortems. This makes them more enjoyable to read and less onerous to write. We\u2019ve had everything from Sam Spade detective fiction to Karl Marx as themes for write ups. It\u2019s not uncommon to hear people unrelated to an incident asking each other whether they\u2019ve read the latest postmortem.\n\nRecently, we had a mysterious bug in our integration tests that prevented us from getting more than a few green builds per day. I spent about three days trying to narrow down the bug and figure out the fix. It was a pretty intense debugging session, and I ended up writing a postmortem themed after one of my favorite movies. We\u2019d like to share that report with you today.\n\nThis was our CI situation:\n\nWe weren\u2019t able to get tests to pass, which was annoying and slowed down our deploys.\n\nThis postmortem is a long one, but the short of it is that our version of db-charmer does not get along with threads.\n\nThis week our integration tests were frequently red, causing extreme delays in our deploys. Morale around here was getting low, and all of us were becoming increasingly frantic.\n\nI was on R&R off on some humid beach miles from civilization when I started to hear murmurings about the issue. The reports from the front were frenzied, inconsistent, and most of all unexplained. For reasons unknown Col. Jenkins was sending back ramblings of the forms:\n\nEveryone has a breaking point. You and I have one. Col. Jenkins has reached his. And very obviously, he has gone insane.\n\nThe second I reported in for duty at HQ, General Davide pulled me aside and gave me my mission \u2014 to terminate these errors with extreme prejudice.\n\nMy descent into madness can be retraced via the branch: break-jenkins-to-remake-jenkins [Ed. Note: This branch contains a set of commits with increasingly nonsensical commit messages that were preserved to show my debugging technique and level of confusion].\n\nStarting out, there was little to go on other than that it was thread related and only appeared in integration tests. It was assumed that all three errors were somehow related. Integration tests run threads for a number of purposes, the two main ones are an RSpec thread that executes the specs and a Webrick thread that exposes a webkit server. The Webrick thread also spins up a new thread for each request. Other possible threads would be the work queues for Airbrake/Squash (which may or may not exist in integration tests), sharded connection threads, or various utility threads for communication with WebKit. We were fighting a war and we didn\u2019t even know who the beligerents were.\n\nProgress up-river was further complicated by the inconsistency of reproduction of the behavior. About 2/3rds of the tests I queued up would fail, but only about 1/3rd of the failures were the Thread error that I figured would lead to the culprits.\n\nLogging identified that the two competing threads were a Webrick request thread, and the RSpec thread. Interestingly, the specific clash usually happened during the setup phase of a test run, indicating that requests triggered from the previous test were still in flight while the test suite had moved on. From here, additional tests confirmed that the violation was related to a thread querying on a connection was not registered to it in the connection pool. This was complicated during observation because it was possible for the connection pool state to have changed while we were trying to read out the values, though in the end this lead proved to be correct.\n\nOne tricky aspect to this was that the site of the error was far removed from the place where the conditions could be set up for the error to occur.\n\nInvestigation proceeded into whether there was some unthreadsafe corner case in ActiveRecord::ConnectionPool. This seemed initially likely as several more methods were synchronized in rails 3.2 than were synchronized in rails 3.0 and 3.1. Monkey patches were deployed and then the tests continued to fail.\n\nDbCharmer was an early contender, as it does mess with connections, but the connection messing happens like so via alias_method_chain:\n\nis uninteresting (though it does play into one other issue)\n\nis a wrapper around a map from class_name to a DbCharmer::ConnectionProxy (or nil if it is just the default ActiveRecord::Base connection, in our case airmaster)\n\nis just the regular (thread safe) ActiveRecord::Base#connection, this is called if db_charmer is nil\n\nThe ConnectionProxy should be thread safe in the specific case of two threads sharing the same connection as the implementation proxies to the underlying AR::Base#retrieve_connection\n\nOne interesting thing is the use of BasicObject, if one were to call on the ConnectionProxy, the return would be the class of the object being proxied to, in this case Mysql2Adapter. This shouldn\u2019t be an issue as the only thing identifying as a Mysql2Adapter stored in db_charmer_connection_proxy should actually be a ConnectionProxy.\n\nRunning out of time and succumbing to the fever of the jungle of the code, I started poking at this assumption. The first step was monkeypatching in a method called on both Object and DbCharmer::ConnectionProxy that would tell the actual class. I then hacked in logging to determine whether there was any place that was setting the db_charmer_connection_proxy to a fully realized connection object (which would be owned by a thread).\n\nThis hack showed me that there was a place in DbCharmer that would set db_charmer_connection_proxy to a connection instead of a connection proxy. The circumstances for this occur through this call stack.\n\nThe first step is an alias method chain of the method in ActiveRecord::Base that constructs an ActiveRecord::Relation. This is the method used to create the relations with syntax like . Here db-charmer stores either a realized connection (not a connection proxy) or a connection proxy on the relation. A connection proxy is stored if the connection has already been switched out (for instance inside of a model\u2019s on_slave block or if the model is not on the default database). This happens because would return . A regular connection is stored if the model is on the default master database, because would return\n\nThis should be fine, as long as relations are not shared between threads.\n\nThis method (and a couple others) hint at the problem. This method is on the ActiveRecord::Relation.\n\nHere we call ActiveRecord::Base\u2019s method with db_charmer_connection, which is an actual thread specific connection. The on_db method does some book keeping (which causes one of the other issues) and then calls into where the thread violation occurs.\n\nis called with the relation\u2019s , which sets the (thread global) to the thread specific connection.\n\nis specific to the ActiveRecord model class (to allow UserFraudInfo to be mapped to different dbs than User, for instance) but it is thread global. During the time that another thread is using a relation of any model to query, any queries on that model will get the other thread\u2019s connection instead of their own, causing the clash.\n\nThis explains the \u201cthis connection is in use by: Thread\u201d error. The other two have similar underlying causes:\n\nThe undefined_method comes because db_charmer_remapped_connection (the first option inside ) tries to call that method on a connection (stored in ) instead of a connection proxy. The specific reason that this is triggered comes from the global nature of the counter, which allows threads to interfere with what constitutes a top level connection. This combined with the setting of to a connection instead of a proxy triggers this bug.\n\nThe third error occurs when you start a transaction on one connection and then that connection is switched out for another one (that knows nothing of the transaction) by another thread during the course of the transaction. This could happen because of the common cause, but could also occur if a different thread switched to make a connection on slave.\n\nArmed with this knowledge, I was able to quickly patch DbCharmer in test mode. The solution was to wrap the execution of and in a Monitor. This guarantees mutual exclusion and also allows these methods to be called recursively without deadlock. The patch is only applied in test mode, as it\u2019s kind of dirty and shouldn\u2019t be a problem in production.\n\nThe longer term fix is to make our version of DbCharmer threadsafe as well as further audit/test its behavior.\n\nWhile debugging this, I also discovered that the thread error occurs excruciatingly rarely in production. This is interesting in its own right as we should not be making db calls from multiple threads. This suggests that a relation is being realized from the squash or airbrake threads, which could be a clue for something that I have suspected for a long time, but have been unable to track down."
    },
    {
        "url": "https://medium.com/airbnb-engineering/launching-airbnb-jp-in-record-time-52f8b0af965d",
        "title": "Launching airbnb.jp in record time \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Airbnb has hosts in 192 countries, and two-thirds of Airbnb trips cross a country border. Being totally international is not optional for us! Many things contribute to Airbnb growing in a new part of the world: an enthusiastic community of hosts, a natively-translated website and mobile apps, locally relevant search ranking, local payment methods, multilingual customer support, and lots more.\n\nIn this post, I\u2019ll walk through the life of airbnb.jp, our Japan website.\n\nLate last year, actor/investor Ashton Kutcher and Airbnb CEO Brian Chesky made a last minute trip to meet the growing Airbnb community in Tokyo. Having lived in Tokyo for 3 years, I found this news quite exciting, but Airbnb didn\u2019t yet have a Japanese website! Brian and Ashton\u2019s visit was happening in one week \u2014 could we launch airbnb.jp in time?\n\nAirbnb\u2019s websites and mobile apps have 400,000 words of English content. We couldn\u2019t translate all of it to Japanese in just a few days. It was important to prioritize so that the most visible webpages, email templates, and core flows of the site were delightfully localized before launch.\n\nWe set up our translation infrastructure so everything goes through one t() method on our Rails server. For example:\n\nWe instrument t() to help us know how to prioritize phrases for translation:\n\nAltogether, this allows us to present Japanese translators with a view of all pages/emails on our site, and order pages by translation completeness in Japanese and visibility (the \u2018importance\u2019 column represents the # of page views in the last 4 days):\n\nTo get the most important parts of airbnb.jp translated, we just had to instruct translators to start at the top of this list of pages/emails and work their way down.\n\nMany features have evolved on Airbnb since its inception 5 years ago, so we show translators only phrases that have had t() called on them in the last 4 days. To make this possible, t() increments a count of how many times the phrase has been translated today. This ensures that translators are only translating text that users are actually going to see.\n\nIn addition to associating each phrase with the controller/action pair or email template, t() captures the HTML of the response and asynchronously sends it to a server running a headless browser to take a screenshot. The goal is to have a screenshot of each phrase on each page it appears on, so that the translator knows exactly the context the phrase is used in. Here, while translating the phrase \u201cnever,\u201d a translator can see that it is used to describe when the host last updated their availability calendar:\n\nWe also give translators a contextual translation tool. This lets them edit translations while using the site:\n\nThis tool works by making t() wrap its return value in <phrase id=\u2026>\u2026</phrase> and then instrumenting these tags with JavaScript. After airbnb.jp went live the night of Ashton and Brian\u2019s visit, our community translators were able to spiff it up in-place with this tool.\n\nContent rendered client-side is naturally much harder to screenshot or instrument on the server. (We call t() on the server and then render the translations in JavaScript with our own Polyglot.js library.) For content rendered client-side, and for our mobile apps, we don\u2019t have screenshots and can\u2019t edit translations in-line. So there is no substitute for manually poring over the apps to find translation problems.\n\nWe\u2019ve further instrumented t() to automatically queue phrases for translation. If the phrase key (emails.reference_request.title in the above example) does not exist in our phrases database, t() creates a new row in our phrases database. This ensures that new content on the site is instantly translatable.\n\nFurthermore, when a translator changes a translation, it updates a site-wide per-locale timestamp. Every time an Airbnb translator or employee makes a request, the Rails server checks if this timestamp is more recent than its in-memory copy of translations for the request\u2019s locale. If there is a new version of translations to download, it grabs them from the translations database before responding to the request. So if a translator makes a change to airbnb.jp, they will see their improvements live immediately. This lets us iterate on translations as fast as feedback comes in.\n\nIn addition to all these tools, we needed to find Japanese translators pronto! As always, relying on our awesome community of Airbnb hosts was our first choice. Initially a host in Tokyo and a friend of a friend in San Francisco were keen to help, and we\u2019ve expanded the community from there. It is wonderful for community members who have actually traveled or hosted on Airbnb to provide the voice that introduces Airbnb to Japan. Our community members know our service best and are passionate about making Airbnb accessible in their respective language \u2014 and just like professional translators, we pay them for their services by the word for their kind help. (We were excited to meet one of them at Airbnb headquarters recently :)\n\nAirbnb is off to a great start in Japan. Listings in Tokyo alone have grown over 180% from this time last year. There is of course still much to do help Airbnb grow in Japan. For example, the bounce rate of visitors to airbnb.jp is twice that of airbnb.fr. We can do a better job educating Japanese users how and why to use Airbnb, and improve the homepage so it\u2019s easier to browse popular destinations in Asia.\n\nIn April I attended our second community event in Tokyo, which maxed out its 500-person capacity. I stayed with absolutely delightful hosts and was blown away by our Japanese community\u2019s energy, passion, and diversity. They inspire me to continue making Airbnb truly international and radically local."
    },
    {
        "url": "https://medium.com/airbnb-engineering/list-your-space-and-manage-listing-on-mobile-62f4ca293a99",
        "title": "List Your Space and Manage Listing on Mobile \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "As part of our continuing focus on mobile, we recently shipped List Your Space and Manage Listing on mobile. A rapidly-growing team came together for a simultaneous release on Android and iPhone, allowing hosts to quickly and easily create and manage their listing whenever and wherever they want.\n\nTwo big challenges we wanted to tackle with List Your Space on mobile were verifying the location of your listing and easily capturing and uploading photos of your space. Both of these steps can be complicated through a web browser, but the mobile platforms lend themselves well to these tasks. However, this doesn\u2019t mean that you are limited to a single platform. We support managing your listing on all platforms at every stage during the process of creation, pre-list, and post-list!\n\nSpeaking of managing beautiful photos of your space, we also built really slick photo rearranging and captioning into our new mobile tool box. We feel this especially stands out on Android, where smoothly-animating, responsive photo rearrangement is simply not found. All of this amazing user experience is supported back to Android 2.2 (Froyo), just like everything else in our app. Let\u2019s dive into how we made the new PhotoRearranger View class on Android. To start, here are the main building blocks we used:\n\nThe PhotoRearranger is built entirely upon the concept of mapping photo indices to rectangular portions of the screen. These rectangles are calculated in onSizeChanged(), so they are cached and always reflect the current size and orientation of the screen. After the user long-presses on a photo, we offset the photo\u2019s location based on drag events (tracked by maintaining a touch pointer id) and scale it over time based on an interpolator to indicate selection. Each frame, we calculate the current index based on the photo\u2019s screen location. If this differs from the last frame\u2019s index, we adjust the location of all other photos in the collection over time, based on two more interpolators, one for horizontal translation and the other for vertical translation. When the user releases the photo by lifting his or her finger, we calculate the current offset from the target screen rectangle and interpolate smoothly until the scale and location of the photo match that target rectangle.\n\nEvery photo is responsible for reporting to the PhotoRearranger if it is currently animating. The View will then use this information to decide whether or not the next draw should be scheduled. This concept is similar to GLSurfaceView.RENDERMODE_WHEN_DIRTY and serves to save your device\u2019s battery life while admiring those wonderful photos! All photos are drawn to the View\u2019s Canvas with the method Canvas.drawBitmap(Bitmap, Rect, RectF, Paint) with cached rectangles for maximum efficiency. If you\u2019re coming from a graphics background, you can think of the Rect object as texture coordinates in the range of (0, 0) to (w, h) and the RectF object as screen coordinates. Fortunately, all of the images from our server are guaranteed to have the same aspect ratio, but the PhotoRearranger will maintain aspect ratio during scaling if it detects something is amiss. Of course, all of this manual drawing also means we have to handle drawing the multiselect CheckBoxes and updating their states based on TouchEvents, but this is trivial since we have screen locations for each photo precomputed.\n\nSince we needed to be able to scroll while dragging a photo, we also included a Scroller object that we could use to manually control scroll behavior while maintaining the standard Android flywheel scrolling behavior and feel. By defining regions at the top and bottom of the screen that will cause a scroll, the user is able to move a photo from the last index to the lead position (or even the trash can, which lives under the ActionBar overlay) easily and intuitively.\n\nStill with me? Great! The final piece to the PhotoRearranger logic is the fact that the lead image spans the entire View, while all other indices are confined to columns (more on this later). Because of this size difference, all photos maintain yet another interpolator responsible for updating scale over time. This is separate from the selection scale interpolator for simplicity and is updated whenever the photo\u2019s index is modified. With this final interpolator, photos not only move from index to index smoothly, but also scale smoothly when entering or leaving the lead position. Speaking of the photo columns, since this is a custom View, many custom attributes may be overridden in XML, such as photos per row, photo padding, haptic feedback duration, interpolated animation duration, selection scale factor, edge scrolling threshold and speed, progress spinner resources, checkbox state resources, and several more!\n\nBe sure to check out the new List Your Space and Manage Listing features on Android and iPhone, and stay tuned for more great mobile host tools coming your way."
    },
    {
        "url": "https://medium.com/airbnb-engineering/secret-ingredients-to-building-airbnbs-international-payments-platform-523d20b6a0cf",
        "title": "Secret Ingredients to Building Airbnb\u2019s International Payments Platform",
        "text": "Airbnb has become a trusted global marketplace for finding the most interesting places to stay in the world. Part of what has helped build trust in the community has been our safe and secure payments platform. Payments through Airbnb takes the awkwardness out of the guest and host interaction, while still adding security for guests, as hosts are not paid until 24 hours after guests arrive. Security, reliability, and convenience are expectations from our community across all payment and payout methods. Each month Airbnb collects payment in 32 currencies and remits payment in 65 currencies in 192 countries. Building and improving the global pipes that collect and distribute money are daily challenges for payments engineering at Airbnb.\n\nBefore joining Airbnb, I primarily worked in investment banking building trading algorithms and applications in an environment with over 3 trillion dollars in assets. Bugs or optimizations resulted in literally hundreds of millions of dollars lost or gained. I once wrote a fractional penny optimization algorithm akin to the one from Office Space. This is not the approach at Airbnb. Our mindset is how we can build payments into a platform that can really help our guests and hosts. We\u2019re not building payments for money managers, we\u2019re building a payments experience that is simple, easy to use, and creates trust for our community. Often times we hunker down and knock out bugs that we know are impacting real user experiences rather than focusing strictly on downstream, backend optimizations. We\u2019ve also been known to set Airbnb free by refactoring our payments platform in about 5 hours to help victims from Sandy.\n\nOur community-minded approach to engineering goes further than the product. Unlike most banks which use proprietary and expensive software, at Airbnb, we use (and contribute to) open-source technology. Although there are differences from a community-minded and technology perspective, there are many similarities from a design pattern and practices perspective. I believe the engineering secrets to success in payments are more about design patterns and practices, and less about specific technologies (although I will later mention an example of improved performance achieved through the use of a different technology).\n\nWhen asked about our payments platform, many people are surprised to see how fast we build and iterate with a small team. Airbnb\u2019s payments team is responsible for everything related to a dollar flowing through the site: reservation booking, collection & distribution of money, pricing, iOS & Android payment experiences, financial reporting, internal payment APIs, and more. Our growth and scale has been exciting and has created unexpected challenges. We\u2019ve learned a lot of practical engineering patterns and practices along the way which I would like to share. As with all things engineering related, we are learning and evolving each day, but we hope the tips shared here help you continue to build innovative systems.\n\nSo, what are these supposed secret ingredients you ask? Well good friend, you have a first-class seat into my delicious explosion of ideas organized into a few themes.\n\nAn ongoing goal (for all software systems) should be to reduce complexity. Complex systems lead to slower turnaround and a higher likelihood of bugs. Eliminating unused features and proactively refactoring code should be the norm, not the the exception.\n\nDe-duplication is easy to forget when creating utility and helper modules. In other words, you should constantly sweep the code base for duplicate logic and reduce that logic into a common place.\n\nDatabase schema denormalization is sometimes good to help avoid nested snowflake like structures, especially in an online booking system when compared to an offline accounting system (see the decoupling section below for more on booking vs accounting systems). For example, most use cases don\u2019t require a fully normalized currency relationship:\n\nLoose coupling and modularity are known design patterns in software engineering. One thing to remember is that the pattern should be applied to all levels or dimensions of a system: functions within a class or module, class or modules themselves, entire directory structures, interactions between services, boundaries across the stack, etc.\n\nTo be more specific, a payments platform should almost always separate the booking system from the accounting system. The pattern held true both at the investment banks that I previously worked at and at Airbnb. Online systems (where transactions are processed) often have very different data and application requirements from offline systems (where internal reports are generated).\n\nFrom a class or module perspective, encapsulating rounding, foreign exchange (FX) conversion, and more can help identify new opportunities. Since Airbnb is so international, we've abstracted our two-sided transactions which can have any combination of two currencies into a complete, directed graph with self loops (i.e. a Canadian guest transacting in CAD can travel to an Australian host transacting in AUD, or a guest can travel to a host in the same country, or each vice versa).\n\nWhen coding in the present, it is always a useful exercise to think about an engineer (not necessarily yourself) in the future. Making sure that code is easily comprehendible and extensible is a good proactive thing to keep in mind when structuring and designing code.\n\nWhen building a payments system, self-defense is something to keep at top of mind. Defensive programming is a great strategy to help maintain predictability. Here is a simple example:\n\nThis might come off as obvious but validate EVERYTHING. Enumerations, values, and relationships between values should be validated before the time of persistence, computation, and retrieval.\n\nIn many programming languages, there may be features that you should avoid using. Adhering to a style guide and minimizing unexpected magic helps prevent bugs. For example, default scopes in Ruby on Rails is a feature we don't like using with payment models so we instead define custom class methods.\n\nAt the database layer, it's great to have read replicas alongside a master, but in the payments world you need to watch out for replica lag since it is likely to cause data inconsistencies. Maintaining database consistency is crucial in avoiding duplicate processing of the same action (i.e. paying a host out more than once for the same transaction).\n\nBatch processing is at times preferable to stream processing especially when we talk about payments. PayPal provides a mass payments API which is great for sending money to multiple recipients at a better transaction fee rate. If you need to bulk insert records into a database with ActiveRecord, I highly recommend the activerecord-import gem.\n\nFollowing the secret ingredients listed in this post truly enables Airbnb's engineering team to move at the speed it does today. As alluded to above, when Hurricane Sandy struck, we partnered with the City of New York to quickly create a platform for New Yorkers to provide free housing to those who were in need. Check out the story on CNNMoney. Zero dollar transactions is not a common feature of most payment systems but I was able to implement the ability in about 5 hours without disrupting regular payment flows. Remembering to Protect Yourself at All Times, I implemented heavy validation throughout all relevant models to control places where zero should and should not be. We've since generalized this code so we can respond to a disaster anywhere at any time.\n\nOne trick to staying agile is to always release in phases by staging rollouts. At Airbnb, we deploy to production more than 10 times a day which means we don't have big release cycles. Automated tests are great to avoid drawn out manual QA cycles.\n\nWhen introducing new services or porting logic, parity testing is a powerful strategy to help compare values online and prove correctness. For example, when we ported high throughput pricing logic from Ruby to Java (explained more in the performance section below) we ran both services sequentially while in-flight comparing results:\n\nCan you spot a potential problem with the code example just above? A big bang has already been introduced since each pricing request incurs a call to both services. Instead, the new pricing request can be incrementally ramped up by percentage (checkout Airbnb\u2019s open source feature launching tool Trebuchet) or the individual service invocations can be made in parallel (although it's a bit tricky in a language like Ruby).\n\nIn the process of building an international payments platform we've learned to store data in special ways. Date times should be stored in UTC epoch. Amounts should be stored in cents (integer column types are nicer to deal with instead of floating point).\n\nIn addition to precision at the database layer, one must never forget at the application layer to convert to specific time zones and convert to specific numeric values (i.e. ceil, floor, round, to_i, to_f). Precision and granularity matter and they can bite you in subtle ways:\n\nAnother special case is race conditions between systems (internal, external, or a combination of both). For example, when our system asynchronously interacts with PayPal's system, we process data only after a certain time buffer has passed to make sure the database is in a consistent state. Even with synchronous interactions we highly suggest keeping track of activity before and after main points of contact (i.e. keeping track of the number of attempts).\n\nBackground processing (scheduled tasks) can also bite you in subtle ways. Remember to guarantee mutual exclusion in places where it is needed. For example, if you have a frequent payout task, then appropriate locks should be put in place in the event that the task runs over and a subsequent run tries to execute in parallel. In addition to mutual exclusion, background processing may also require a specific ordering. This can only be achieved by task chaining. Airbnb's open-source replacement for cron, called Chronos, supports arbitrarily long dependency chains.\n\nOftentimes payments logic requires many distinct cases when handling a dollar. For example, Airbnb provides a rebooking feature in the event that a host cancels on you, in addition to providing bonus credits at the time of rebooking. It turns out there are 6 unique cases which force our code implementation to get quite complex. The tip here is to over-invest in documentation in places where complexity is unavoidable.\n\nThe peril of premature optimization is not a secret. Part of keeping things simple is knowing when to save optimization for later. At Airbnb, we allow hosts to set pricing at a daily, weekly, and monthly level for specific dates and in general with proration or without. When guests view our site they can browse pricing for any future start date. Price computation was previously written in Ruby as part of our main stack but we soon noticed that performance was an issue when both stack trace and object allocation analysis identified specific bottlenecks. Tight loops were hard to get around in Ruby so we decided to port the logic to Java where we can invoke things in parallel. The end result yielded a 4.5X performance boost and the pricing service now returns results in a matter of milliseconds.\n\nIt's often better to lazily optimize so that you can focus on what matters today. For example, just recently we hit performance bottlenecks with a high volume email that is sent out to hosts when they get paid. Airbnb already had a high throughput email service which was built previously for unrelated use cases. We decided to port the email to the new service only after observing that it was an issue.\n\nPayment gateway interactions should not only be tested for correctness. Response failures should also be stubbed and mocked to make sure your code handles all scenarios. Sometimes making assumptions is a bad thing especially in a production environment where things can happen that otherwise will never in a non-production environment unless explicitly simulated.\n\nIn addition to writing automated tests, running tests in a production environment obviously requires an A/B testing strategy. One idea that's easy to forget is that kill switches (i.e. an on/off switch) should be used when introducing new functionality in the event that unexpected things happen.\n\nBookkeeping and row level tracking are characteristics of a solid payments platform. It's good to remember that various dates can be associated with a single object (i.e. created at, updated at, charged at, booked at, state changed at, etc). Also, immutability is great for data management but in the event that you don't have pure immutability tracking, storing all changes to a record definitely helps. Time series data is ideal for accounting systems (also known as data warehouses).\n\nFrom an application perspective, explicit use of whitelists and blacklists helps control data flows and helps future engineers easily extend code. It\u2019s good to remember to take the effort to define reusable constants instead of hardcoding in various places. For example:\n\nBeing truly international leads you to unexpected and challenging places when integrating with local payment providers. Creating abstractions and APIs definitely helps hide implementation details. But sometimes APIs simply aren't available, as highlighted by our integration with Western Union for host payouts to South America and other parts of the world. Airbnb loves to use Apple laptops, but there is exactly one Windows laptop that we own which was required by the Western Union integration where we process and generate fixed width format files. Ah, the wonders of byte order mark (BOM).\n\nDo you get excited about payments and have more ideas to improve the experience for our guests and hosts? Comment here or send me your thoughts ian (at) airbnb (dot) com."
    },
    {
        "url": "https://medium.com/airbnb-engineering/how-we-measured-americas-most-hospitable-cities-3b1fdc119be0",
        "title": "How We Measured America\u2019s Most Hospitable Cities \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Lately, we\u2019ve been thinking about how we can promote and share exceptional hosting practices. We know that some hosts on our site consistently receive exceptional reviews. What are the common characteristics of these hosts?\n\nAs a first step in our investigation, we created a \u201cHospitality Index\u201d that measures host quality across cities. Immediately, we saw stark regional trends in host quality and hospitality.\n\nTo build the index of America\u2019s most hospitable cities, we looked to reviews, our richest source of data about how a trip went. After each trip, we ask guests to rate a number of specific dimensions:\n\nThere\u2019s a long history of criticism surrounding 5-star review systems. For example, scores tend to be binary (5 or 1). But we can be confident that a 5-star score is a good experience, at minimum. So for the index we looked at the percentage of trips (not reviews, which would be biased by review rates) where guests give 5-star scores for all of the above criteria.\n\nWith over a million trips completed in the United States in the past couple years, we have a rich set of experiences, spread across hundreds of cities, to draw upon. And to ensure the signal is sufficiently strong, we filtered the list of cities to those that had hosted at least 500 trips in the last 2 years. Finally, we limited the trips to just those where the guest booked a private room in someone\u2019s home, as opposed to staying alone in the entire home.\n\nThe cities with the highest percent of trips resulting in 5-star hospitality experiences included the following:\n\nIt makes sense, doesn\u2019t it? Southern hospitality rings through loud and clear with cities like Tampa, Raleigh, Memphis, and Nashville topping the list. Western creative enclaves like Mendocino and Eugene, along with Sedona and Santa Fe (in the top 20) also perform well. The list can be interpreted many ways; another lens we could use is age and employment. These are generally locations with big retirement and/or college communities, two groups of people with limited income and more free time.\n\nConspicuously absent are big cities and the Northeast. This isn\u2019t to say that we don\u2019t have any exceptional hosts in these areas \u2014 we definitely know of many amazing hosts around the country \u2014 but in aggregate, they are outshined by our Southern and Western hosts.\n\nGoing a step further, we wanted to understand what determines high hospitality scores. We ran a linear probability model to identify specific factors that influenced review ratings. The tables below show what factors either help (positive number) or hurt (negative number) host ratings compared to the baseline category (0%).\n\nOlder hosts tend to be more hospitable (possibly because they have more free time and experience). On the other hand, younger guests tend to be more generous with their reviews. This could be because they are more used to leaving reviews on websites or because they have lower expectations for the stay.\n\nGender doesn\u2019t matter much on the guest side, but female hosts tend to get higher ratings than male hosts.\n\nAs far as trip attributes, large groups give much lower ratings than small groups (we wonder what the host thought of these large groups?). As far as the length of stay, very short or very long trips tend to review at lower rates.\n\nGuests who plan ahead end up happier, at least according to the above table. The longer in advance a guest books, the more highly they rate their host. This could be because they have a better selection to choose from, or because they\u2019ve more carefully thought through their trip and have a clear sense of what they need in a host.\n\nAnother factor we were concerned might skew results was guest origin. If guests from certain regions or countries tend to be more or less generous with reviews, could that be influencing results? We ran the regression controlling for guest origin and found it didn\u2019t change the results in any meaningful way.\n\nWe were pleased to see regional hospitality trends reflected in Airbnb data, and to also take a step towards understanding where our most hospitable hosts tend to live. Stay tuned for an international version of our Hospitality Index."
    },
    {
        "url": "https://medium.com/airbnb-engineering/working-with-boston-university-researchers-to-improve-our-mobile-site-ddbe7ae00c25",
        "title": "Working with Boston University Researchers to Improve Our Mobile Site",
        "text": "Recently, we were contacted by a team of researchers at Boston University about an issue with our mobile site. Some recent changes to our internal JSON API affected other applications that consumed it. As a result of these changes the source code on some pages on the mobile site included data fields that were not rendered to end users in their browsers on the mobile site\u2019s pages. The fields were limited to details about reservations associated with reviews, and addresses for listings which are traditionally geolocated and visible on our maps. The fields did not include any financial or personal information of guests. We corrected those changes to the JSON API in less than 24 hours and added additional automated tests to help ensure our JSON API is performing as expected going forward.\n\nWe thank the team of researchers at BU for pointing out this issue in a responsible manner:\n\nThey carefully reported and explained the issue to us which assisted in the diagnosis and remediation. They were great to work with while we ensured that we had corrected the changes.\n\nThe team also helped us identify the importance of an additional direct line for responsible disclosure of certain issues. If you are a researcher and find a similar issue on Airbnb, please see our Responsible Disclosure Policy."
    },
    {
        "url": "https://medium.com/airbnb-engineering/performance-tuning-e10ac94916df",
        "title": "Performance Tuning \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "This is one of the graphs that you prefer to watch go down and to the right: median site load time over the last three months.\n\nPage load time has been shown to correlate directly to user frustration; users are more likely to leave after the first visit and never come back as a site loads slower. With the benefit of great tooling and advice, we\u2019ve been able to cut our site load time by about 25% and create a clear vision of how to continue on the path to an ever-faster website.\n\nTwo types of tools are essential for our performance testing: the tools to track what makes page loads slow, and the tools to log historical page load data and correlate it against code changes.\n\nThe first place to start is with a network waterfall graph. Chances are, unless you have a particularly fast website already, optimizing the number, order, and size of your assets will net you the largest gain. In all likelihood, the 700ms it takes your browser to download a blocking JavaScript file will outweigh any server change you could make by an order of magnitude, and will be easier to fix. If your goal is to drive down page speed, this is a good place to start.\n\nYour favorite browser will have dev tools that show a list of network requests, known as a \u201cwaterfall\u201d. Chrome\u2019s looks something like this:\n\nThe waterfall will also show you information about asset size, load order, and what assets are blocking rendering and downloading. You can also use Chrome\u2019s dev tools to watch the headers of individual requests to check for large cookie sizes and gzip compression. A related resource for performance suggestions will come from installing and running PageSpeed Insights, which will point out which assets are too large, uncompressed, uncached, loaded improperly, or otherwise unoptimized.\n\nAnother fantastic tool for finding performance issues is the free WebPagetest. It allows you to test site performance from a variety of worldwide locations, and provides a ton of information in its output. For example, here\u2019s a recent test of the Airbnb homepage. We can select the median result, then look at what assets are loaded and when page rendering begins and ends. It has several comparison views, and will even export a video. The visual aid it provides may show that although time-to-pageload is low, visual completion might come significantly later.\n\nOn the server, we use New Relic, which will highlight server inefficiencies and allow you to track down things like cache misses. It also has some tremendously useful information about performance in general, such as which pages users spend the most volume of time loading, so you know where to invest your optimization efforts. You can also watch as deploys go out and compare the current average load time against prior time periods to look for both regressions and the results of your optimizations.\n\nLastly, we use a RUM tracker to watch performance trends in conjunction with historical WebPagetest data. That RUM data is compiled using episodes.js and sent to a Hive cluster, which we then query for data based on page, location, or composite medians and averages.\n\nFollowing the list of PageSpeed Insights suggestions will get you most of the way there. In our experience, most gains have been had from the simplest of rules: \u201creduce requests.\u201d Sprite images in stylesheets, combine and minify multiple JavaScript files, and embed small images into data URIs; watch the dev tool waterfalls and make sure that you don\u2019t have assets blocking the download of other assets; and finally, put JavaScript at the bottom of the page. An extension of that is to lazy-load content; for example, on our homepage, additional gallery images are loaded after page load. (Note that lazy-loading is done not with jQuery\u2019s $(function) handler, but rather with $(window).on(\u2018load\u2019, function); where possible. This allows the DOM to render and higher-priority scripts to execute while things like hidden gallery images are loaded.)\n\nBecause we use Rails, we get automatic JavaScript and CSS combining / minifying through Sprockets. Sprockets also provides data-uri helpers, so you can embed small images with ease. You can use tools like browserify and Grunt in Node, or equivalents in your environment of choice. Google has also released a PageSpeed Insights plugin for nginx and Apache that will apply some of these rules for you automatically.\n\nWe also cache non-personalized pages aggressively with Akamai. For example, a recent change to begin caching the homepage has dropped load time for users by several hundred milliseconds:\n\nWith this approach, we can reduce the time to load the page by as much as possible by allowing users to load directly from edge CDN servers close to them. If you\u2019re using Dyn DNS or Amazon\u2019s Route 53, you can experiment with sending users to different CDNs based on latency or geography.\n\nWe use domain sharding across three domains for static assets: images, CSS, JavaScript, and web fonts, from cookieless domains. This allows users to download multiple assets at once. Once we have SPDY enabled, however, we\u2019ll reduce the domains to a single asset domain in order to take advantage of SPDY\u2019s HTTP pipelining.\n\nWe have our work cut out for us. Next we\u2019ll be working on:\n\nSpecial thanks to Steve Souders for the fantastic tech talk he gave at our office in March, and for the advice on how to make our pages faster.\n\nBe sure to check out his tech talk here: Dive into Performance"
    },
    {
        "url": "https://medium.com/airbnb-engineering/a-git-riddle-for-the-patient-reader-56ecd5d4e336",
        "title": "A Git riddle, for the patient reader \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Airbnb has been using Git for a long time. With nearly 200 central repositories, our engineers have seen and learnt a lot. Today, we\u2019d like to share some of the advanced features of Git references, and explain some of Git\u2019s idiosyncrasies along the way.\n\nWe\u2019ll deconstruct the following command, step by step:\n\nIf you can already decode it, congratulations, please feel free to jump to our open positions.\n\nNote that we take a few shortcuts and adopt some simplifications. We ask for your understanding and refer you to the Git manual pages for formal specifications.\n\nWe are using the following variant from git-diff(1):\n\nWhat about \u2014 W w? These are the paths we\u2019re interested in. We could achieve the same result by taking the output of git diff \u2026w1@{1w}\u00b2~3, then filtering only changes to entries in the W or w directories. However, git can do that much more efficiently by skipping the rest of the tree.\n\nWe\u2019ll assume w1 is a branch, making this the reference refs/heads/w1. Note that it could have been a tag, for example, which would have made this the reference refs/tags/w1. It could have been both, in which case we could have used those canonical forms for disambiguation.\n\nReferences are stored as plain files in the git repository; so you can inspect it with:\n\nWe are looking at the history of the branch itself. References have a history.\n\n\n\nLet us insist that this is not the history behind the reference, but of the reference itself.\n\nIf references were implemented with a C pointer, void * w, their history would get a new entry when performing:\n\nBut not when performing:\n\nYou can consult the history of a reference with git reflog. It is stored in .git/logs, and is purely local. Whilst many Git operations, like fetch, pull, push, etc. will perform some form of synchronization between local and remote references, their history isn\u2019t ever shared, merely affected.\n\nSo w1@{1w} designates the commit that the w1 branch pointed to on my machine, a week ago.\n\nThis designates the second parent of w1@{1w}.\n\nWhen designating a commit\u2019s parent in git, you implicitly refer to its first one. But as you probably already know, a commit can have multiple parents. Think of merge commits.\n\nWe take the great grandparent of w1@{1w}\u00b2.\n\nThat would be the first parent of the first parent of the first parent of the second parent of w1@{1w}.\n\nYou might be familiar with the former, but not the latter. Instead of looking at the difference between a and b, it will look for the closest common ancestor between a and b, and show how b differs from it.\n\nLet\u2019s assume HEAD is not an ancestor of w1@{1w}\u00b2~3, but that their history diverged 3 commits before HEAD and 2 commits before w1@{1w}\u00b2~3.\n\nThen git would show you the changes between their first common ancestor, HEAD~3 = w1@{1w}\u00b2~5, and w1@{1w}\u00b2~3.\n\nYou could see the 2 commits involved with git log \u2026w1@{1w}\u00b2~3."
    },
    {
        "url": "https://medium.com/airbnb-engineering/verified-id-c1323c4c307d",
        "title": "Verified ID \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "I\u2019ve been working on Verified ID since we first conceived of the idea almost a year ago. We launched this product last month, but I wanted to dig into more of the technical reasoning behind the decisions we made. I read this blog post on hacker news last night, and I\u2019ve also seen other comments online, and I wanted to address some of them.\n\nFirst, the rest of the team and I want to apologize for any difficulties people have faced so far in using Verified ID. It\u2019s a work-in-progress that we\u2019re continuing to develop. We built Verified ID to try to create more trust in the Airbnb community, but not at the expense of a good user experience. There have been some bugs, and mistakes we\u2019ve made, and we\u2019re working hard to fix them.\n\nWe\u2019re faced with some unique challenges at Airbnb. In addition to dealing with the usual risk problems like fraudsters and spammers, we want to help create a community where users feel confident sharing a living space with somebody they\u2019ve never met. It is a deeply personal experience where you are often sleeping in someone else\u2019s bed. Trust means more for Airbnb than the typical web company, and this is the main driver for Verified ID.\n\nBecause of its importance to us, we wanted to approach trust and verification in a new way. One reason we ask for both an offline and online verification is that it makes circumvention much more difficult. Offline verification is something that identifies you in the real world: a government issued ID, last four of your SSN/national ID number or some personal information akin to a credit check. Online verification is something that identifies you online, such as a Facebook or LinkedIn account. It is highly unlikely that someone will both hack a person\u2019s Facebook account and steal their ID, because they are two very different vectors of attack.\n\nWhen we started working on this product a year ago, we knew it would be a huge challenge. We\u2019ve come a long way, but we still have a long way to go. We believe that reducing anonymity will ultimately create better Airbnb experiences. But we also acknowledge that the product is not finished \u2014 it needs improvement. I\u2019m going to focus on online verification, since that\u2019s where most of the recent concern has been. There are several problems here that we would like to acknowledge:\n\nSo, to everyone affected by this, I\u2019d like to apologize for the bugs outlined above. Things could have been explained better, and we\u2019ll do a better job explaining why we ask for certain information going forward. There\u2019s nothing nefarious going on here, we\u2019re just a team of engineers trying to make the site better. These are the kinds of details that are hard to put in a public press release, but for those of you that do care about them, I hope this explanation helps."
    },
    {
        "url": "https://medium.com/airbnb-engineering/android-an-exercise-in-ship-it-ed73b6487013",
        "title": "Android: An Exercise in Ship It \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "The magic of Airbnb becomes real when you\u2019re actually climbing into your treehouse or docking at your own private island. You\u2019re not in front of a computer; you\u2019re participating in the locality in which you are immersed. In the course of this adventure, you might be contacting your host or guest, getting directions to the nearest sights, or giving tips on how to get to the coolest secret bistro nearby. And you\u2019re probably doing it on your mobile device.\n\nAirbnb exists in over 192 countries and 35,000 cities worldwide. As such, Airbnb builds products to meet the needs of our mobile and international audience. It\u2019s no secret that our iPhone app rocks, but what about Android? You may have noticed that the (previous) version of our Airbnb Android app hadn\u2019t received much love lately. Being that Android is actually the more internationally popular OS, we decided that this was unacceptable. We\u2019ve now made a commitment to bringing a refined Airbnb experience to Android.\n\nWe started building the in-house Android team just a couple months ago. In a few short weeks we overhauled the look and feel of app: in addition to a slew of new features, we gave it an interface and style that feels much more consistent with the strong design culture of Airbnb.\n\nWe had a hard deadline to launch the new Google Wallet integration features: May 15th at Google I/O convention. We decided we couldn\u2019t simply throw away all of the existing implementation, and concluded that the best course of action was to turn the old app into a library project which was then included in the new code. In addition we used Android\u2019s styles framework to tune the look and feel of the old content to our raised expectations.\n\nIn the process of re-architecting the codebase we found instances in which we needed to reference newly implemented classes from the existing code. This proved frustrating because we wanted to keep the division between projects as clean as possible. In order to launch a \u201cnew\u201d activity from the code in the included library project, we used reflection to let the app compile:\n\nNormally, in this situation, we would have a compiled reference to PropertyDetailsActivity.class, but since it only exists in the main project, we have to dynamically load the class at runtime.\n\nAnother issue that we encountered was the need to update the way we handle internationalization practices. In fact, we discovered possibly one of the more hilarious i18n one-liners we have ever seen:\n\nSpeaking of internationalization, we found it much more convenient to have strings be located in a single file, and have things such as plurals referenced there. So, for example, instead of the following:\n\nThis flattens the format of what actually needs to be translated to make internationalization much easier on internal tooling.\n\nWhat made this process unique was the aggressive timeline in which we needed to redesign and add to a full-featured app. However, we also wanted to have a clean break from the original contractor-implemented app.\n\nLooking to the future, we intend to solidify Airbnb\u2019s Android experience as a first-class citizen. Working closely with our incredible design team, engineering at Airbnb has the opportunity to solve formidable technical challenges to create one of the most elegant and compelling experiences in the world, for the whole world.\n\nGive the app a look here!"
    },
    {
        "url": "https://medium.com/airbnb-engineering/how-i-got-started-at-airbnb-79ab1e910894",
        "title": "How I Got Started at Airbnb \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Last fall Airbnb acquired Fondu, a company that Gauri Manglik and I founded. Fondu was helping people discover fresh new restaurants through a beautiful mobile experience. Rather than reading 300 reviews about a restaurant, we felt it would be more useful to follow your friends and respected foodies and see where they were eating.\n\nSince the acquisition, I\u2019ve gotten a lot of questions about the process and the transition. The biggest question I get whenever people hear about the acquisition is \u201cDo you miss working on a small team?\u201d My gut response to this isn\u2019t a yes or no, but actually just the realization that I still feel like I\u2019m on a small team. The product team I work on, Trust, is just as small as Fondu was, and operates with significant autonomy. We have our mission: to make Airbnb the most trusted community on the internet. The only real difference is that instead of using off-the-shelf everything, we\u2019re sitting next to other teams that are building the latest and greatest for us to use. Teams like Data Infrastructure, SRE, Search, Mobile, and many others. Instead of building everything from scratch, we have access to battle-tested functionality that lets us ship good stuff, even faster than the hacky way. Both ways allow quick iteration, but having supporting teams means that the solutions tend to scale better and be more reliable.\n\nSimilar to dating. After the initial introduction was made, we started talking in earnest, getting a feel for how we each worked, what our values were, and generally if there was chemistry. The more we talked, the more we realized how well we would fit into the Airbnb culture. There was a full battery of interviews, and a negotiation process that lasted a few rounds. We had a few deals on the table, but we went with Airbnb because of its amazing culture and compelling vision. We were also fortunate to have amazing investors who supported us through the entire process.\n\nHonestly, I get to focus on code and product a lot more. There are fewer distractions, and most of the details that suck time are eliminated. Some days I miss the uncertainty and the struggle, but that\u2019s been traded for some nice perks. I\u2019ve stopped eating at the same places every day and instead get to enjoy Chef Sam\u2019s diverse meals while chatting with some of the most people-oriented engineers I\u2019ve ever met. Having time to go to the gym daily has been transformational to my health as well. I\u2019m currently working on repairing 2 years worth of physical neglect due to life \u201cin the trenches.\u201d I\u2019ve even been able to start traveling again and actually wrote the first draft of this post from our offices in Delhi.\n\nThe scale of data at Airbnb is orders of magnitude greater than what we were dealing with at Fondu. We also deal with people\u2019s safety and real money, so our standards for protecting them are celestial. Having more engineering support means that the quality of the systems and infrastructure is also higher, as it needs to be. The tradeoff is that having a larger infrastructure means more moving parts need to be connected and maintained for every project. It\u2019s no longer possible to keep every part of the system fresh in your head because the product does so much.\n\nI\u2019ve been working on keeping our users safe and preventing fraud. My first major project was building a graph traversal system that continuously relates users sharing common attributes. We use this to identify duplicate accounts created by bad actors. The challenge was that it had to be really fast, as it is accessed on almost every authenticated request. As malicious users become more sophisticated, we\u2019re developing smarter techniques to stay ahead of them and neutralize attack vectors. Since then I\u2019ve been involved in multiple projects, such as Verified ID, related to improving our trust/anti-fraud infrastructure. Some components of those projects have been generalized and are getting internal adoption (such as Saddle).\n\nTo be honest, the past 7 months have been a real transition, but also a massive opportunity for personal growth. Airbnb\u2019s vision is inspiring, my colleagues challenge me daily, and the problems we\u2019re solving are legendary. If my life was a book, this is a chapter I would not dream of missing out on."
    },
    {
        "url": "https://medium.com/airbnb-engineering/location-relevance-at-airbnb-12c004247b07",
        "title": "Location Relevance at Airbnb \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Here at Airbnb, as you can probably imagine, we\u2019re big fans of travel. We love thinking about the diversity of experiences our host community offers, and we spend a fair amount of time trying to make sense of the tens of thousands of cities where people are booking trips every night. If Apple has the iPad and iPhone, we have New York and Paris. And Kavaj\u00eb, Au\u00dfervillgraten, and Bli Bli. The tricky thing is, most of us haven\u2019t been to Bli Bli. So we try to come up with creative ways to help people find the experience they\u2019re looking for in places we know very little about. The key to this is our search algorithm \u2014 a system that combines dozens of signals to surface the listings guests want. In the early days, our approach was pretty straightforward. Lacking data or personal experience to guide an estimate of what people would want, we returned what we considered to be the highest quality set of listings within a certain radius from the center of wherever someone searched (as determined by Google).\n\nThis was a decent first step, and our community worked with it resiliently. However, for a company based in San Francisco, we didn\u2019t have to look far to realize this wasn\u2019t perfect. A general search for our city would return great listings but they were scattered randomly around town, in a variety of neighborhoods, or even outside of town. This is a problem because the location of a listing is as significant to the experience of a trip as the quality of the listing itself. However, while the quality of a listing is fairly easy to measure, the relevance of the location is dependent upon the user\u2019s query. Searching for San Francisco doesn\u2019t mean you want to stay anywhere in San Francisco, let alone the Bay Area more broadly. Therefore, a great listing in Berkeley shouldn\u2019t come up as the first result for someone looking to stay in San Francisco. Conversely, if a user is specifically looking to stay in the East Bay, their search result page shouldn\u2019t be overwhelmed by San Francisco listings, even if they are some of the highest quality ones in the Bay Area.\n\nSo we set out to build a location relevance signal into our search model that would endeavor to return the best listings possible, confined to the location a searcher wants to stay. One heuristic that seems reasonable on the surface is that listings closer to the center of the search area are more relevant to the query. Given that intuition, we introduced an exponential demotion function based upon the distance between the center of the search and the listing location, which we applied on top of the listing\u2019s quality score.\n\nThis got us past the issue of random locations, but the signal overemphasized centrality, returning listings predominantly in the city center as opposed to other neighborhoods where people might prefer to stay.\n\nTo deal with this, we tried shifting from an exponential to a sigmoid demotion curve. This had the benefit of an inflection point, which we could use to tune the demotion function in a more flexible manner. In an A/B test, we found this to generate a positive lift, but it still wasn\u2019t ideal \u2014 every city required individual tweaking to accommodate its size and layout. And the city center still benefited from distance-demotion. There are, of course, simple solutions to a problem like this. For example, we could expand the radius for search results and diminish the algorithm\u2019s distance weight relative to weights for other factors. But most locations aren\u2019t symmetrical or axis-aligned, so by widening our radius a search for New York could \u2014 gasp \u2014 return listings in New Jersey. It quickly became clear that predetermining and hardcoding the perfect logic is too tricky when thinking about every city in the world all at once.\n\nSo we decided to let our community solve the problem for us. Using a rich dataset comprised of guest and host interactions, we built a model that estimated a conditional probability of booking in a location, given where the person searched. A search for San Francisco would thus skew towards neighborhoods where people who also search for San Francisco typically wind up booking, for example the Mission District or Lower Haight.\n\nThis solved the centrality problem and an A/B test again showed positive lift over the previous paradigm.\n\nHowever, it didn\u2019t take long to realize the biases we had introduced. We were pulling every search to where we had the most bookings, creating a gravitational force toward big cities. A search for a smaller location, such as the nearby surf town Pacifica, would return some listings in Pacifica and then many more in San Francisco. But the urban experience San Francisco offers doesn\u2019t match the surf trip most Pacifica searchers are planning. To fix this, we tried normalizing by the number of listings in the search area. In the case of Pacifica, we now returned other small beach towns over SF. Victory!\n\nAt this point we were close to solving the problem, but something still didn\u2019t feel right. In the earlier world of randomly-scattered listings, there were a number of serendipitous bookings. The mushroom dome, for example, is a beloved listing for our community, but few people find it by searching for Aptos, CA. Instead, the vast majority of mushroom dome guests would discover it while searching for Santa Cruz. However by tightening up our search results for Santa Cruz to be great listings in Santa Cruz, the mushroom dome vanished. Thus, we decided to layer in another conditional probability encoding the relationship between the city people booked in and the cities they searched to get there.\n\nThe relationship between the two conditional probabilities we used is displayed in the graph to the right. While all of the cities in the graph have a low booking likelihood relative to Santa Cruz itself, they are also mostly small markets and we can give them some credit for depending on Santa Cruz for searches for their bookings. At the same time places like San Jose and Monterey have no clear connection to Santa Cruz, so we can consider them as completely separate markets in search. It\u2019s important that improvements to the model do not lead to regressions in other parts of the world. In this case, little changed for our bigger markets like San Francisco. But this additional signal brings back the mushroom dome and other remote but iconic properties, facilitating the unique experiences our community is looking for. The location relevance model that we built during this effort relies completely on data from our users\u2019 behavior. We like this because it allows our community to dynamically inform future guests where they will have great experiences, and allows us to apply the model uniformly to all of the places around the world where our hosts are offering up places to stay. * * *Huge thanks to\n\nStamen Design and OpenStreetMap.org for sharing their map tiles and data, respectively."
    },
    {
        "url": "https://medium.com/airbnb-engineering/behind-the-scenes-airbnb-neighborhoods-cef63242eab7",
        "title": "Behind the Scenes: Airbnb Neighborhoods \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Andy: Airbnb Neighborhoods was birthed from the Snow White project and our research showed location is the number one criteria for Airbnb travelers when choosing a place to stay. Our goal with Neighborhoods is for every Airbnb traveler to figure out where in a city to stay and to help them feel more connected to the local culture. We like to call this local intuition the sixth sense of traveling. Neighborhoods is also the successor of our previous company, NabeWise, which was acquired by Airbnb in 2012. At NabeWise, we provided movers and travelers with a comprehensive and essential neighborhood guide that chronicled 25 US cities. By working with Airbnb\u2019s passionate and international community of hosts and guests, we are able to evolve our product and offer truly global, yet locally-nuanced, solutions for enhancing aspirational travel.\n\nAndy: Neighborhoods itself doesn\u2019t deal with insane amounts of data. Instead we were able to offload the hard work to external services. One such service, Glop (Genome Location Pipeline), regularly associates our data with the neighborhood in which it occurs. It goes something like this:\n\nSan Marco. Glop looks something like this:\n\nIn order to capture neighborhood boundaries we also built a custom browser-based system for creating the neighborhood geometry. Zack Walker, our cartographer, works with this system to map out each city in very fine grained detail. We\u2019re then able to really play with the geometry and pass it through various filters before importing it into the front end facing app. By the time the front end gets the underlying data, it is relatively small and manageable.\n\nAndy: There are actually quite a few components, among which are:\n\nAndy: My biggest challenge was engineering the best way to give the front-facing Rails app (henceforth neighborhoods-core) access to all the data produced by the pipeline. Neighborhoods-core reads data produced by the pipeline to personalize pages and produce the community visualization. What we needed was a solution that could lookup resources by city or neighborhood. We also wanted our solution to be fast. Very fast. The \u201cresources\u201d we needed to fetch are de-normalized tuples representing a variety of types of data. A single resource tuple could represent a reservation, a listing or even a user. At first, it seemed we wanted a SQL database, as our data had relations. However, this was ruled out based on the need for mass updates. Next, we looked at an in-house NoSQL solution that we call Dyson. Dyson seemed to give us the flexibility we needed with writes and updates, so we tried it. For reference, Dyson is backed by Amazon\u2019s DynamoDB, a reliable, but limited, managed, NoSQL solution. In essence, if we put the data right into DynamoDB, then Dyson can serve it. This led to the creation of a DynamoDB cascading tap. Countless timeouts, headaches and late nights later, we had a working solution. However, there was a problem, namely DynamoDB\u2019s 65KB storage limit. When you\u2019re storing uncompressed JSON, that\u2019s a pretty easy target to reach. As a band-aid, we engineered a solution involving pages of tuples. To say this solution was sub-optimal is putting it mildly, and the performance was even worse. With launch quickly approaching, brilliant words saved the day: \u201cYou don\u2019t need a database, you need a [expletive deleted] cache\u201d 1. So that\u2019s what we did, we traded our database for a cache. Specifically, we switched from Dyson to Memcached. How does this story end? 35ms response times.\n\nBen: My biggest challenge was setting up the neighborhood page layout tools. We needed tools in place to allow our content editors, translators, and photographers to begin work before we were even close to final designs. We also realized pretty early on that we needed to allow considerable flexibility in how pages would be laid out. Additionally, this tool had to be easy to use so that it wouldn\u2019t waste our content editors\u2019 time, as that was the limiting factor in whether we would be able to ship. I ended up creating a drag-and-drop page creation system that could import images from Finder, iPhoto, or any other photo viewing software our people were using. Once imported, images could be edited, reflowed, and captioned on the page. We also ran into a ton of issues because all of our photos are very high resolution and took a while to process. To speed things up, I wrote a high performance image processing server in Clojure that essentially injected itself as a proxy in front of the Rails image upload endpoint. Unfortunately, we ran up against some fairly bad image quality issues for certain images that didn\u2019t occur when processing using imagemagick, so we were never able to fully roll it out.\n\nBen: It\u2019s important to consider performance from day one. On this project, we kept New Relic Development Mode open in a separate tab at basically all points during development. This allowed us to constantly monitor what our app was actually doing, rather than hoping we had written fast code and then trying to bolt speed on at the last second. We also made our app akamai friendly from the start, so static page caching was just a matter of setting the right headers. You can check out Neighborhoods here: airbnb.com/neighborhoods"
    },
    {
        "url": "https://medium.com/airbnb-engineering/rendr-run-your-backbone-apps-in-the-browser-and-node-a3481af49312",
        "title": "Rendr: Run your Backbone apps in the browser and Node",
        "text": "You may remember our blog post back in January that first introduced Rendr, our library for running Backbone.js apps seamlessly on both the client and the server. We originally built Rendr to power our mobile web app, and in the post we explained our approach and showed some sample code.\n\nWe\u2019ve been blown away by the response from the community. With 80,000 hits to the original blog post and a constant stream of questions and comments on Twitter, it quickly became clear that we\u2019d stumbled upon a Zeitgeist. Many developers shared the same pain points with the traditional client-side MVC approach: poor pageload performance, lack of SEO, duplication of application logic, and context switching between languages.\n\nThe number one question we received went something like this: \u201cWhen will u release Rendr???\u201d\n\nWell, we\u2019ve up and done it \u2014 Rendr is now an open source project, and you can check out the code over at github.com/rendrjs/rendr. A good place to start is the sample app: github.com/rendrjs/rendr-app-template. It provides some minimal boilerplate for a Rendr app, fashioned as \u201cGitHub Browser\u201d, a little app that consumes the public GitHub API. Rendr is meant to work with any RESTful API, so please give it a try with your API of choice. If you come up with something you\u2019d like to show off, send it to us at @rendrjs and we\u2019ll host a little gallery of Rendr apps.\n\nLast week at HTML5DevConf I got a chance to speak on Rendr. [Check out the slides]() for some more context on Rendr. The talk was also recorded, but the video isn\u2019t ready quite yet. If you want to see it sooner rather than later, let the conference organizers know by asking nicely to @html5devconf. I\u2019ll update this post when the video\u2019s live.\n\nThe conference was great; there were phenomenal speakers, and there was a lot of useful tips on how to build modern, maintainable JavaScript apps.\n\nIndeed, there were no less than two other talks on server-side Backbone. Tim Branyen showed previewcode and Lauri Svan showed backbone-serverside, both prototypes that demonstrate sharing Backbone code between the client and the server. We got to meet up over lunch and compare approaches, and it was really interesting to see how similar our code was, with a few important differences.\n\nFor example, both Tim and Lauri chose AMD via RequireJS for modules, while I chose CommonJS via Stitch. Also, we all used Express as a web server, but they patched Backbone.Router or Backbone.History to route and handle server-side requests, whereas I created separate ClientRouter and ServerRouter classes that delegate to Express or Backbone.Router, and a separate routes file. Tim and Lauri chose to stub out jQuery on the server using Cheerio; I opted to avoid DOM abstractions on the server in favor of strict string-based templating.\n\nWhat\u2019s important, though, is that we all arrived at this approach after struggling with the drawbacks of a traditional client-side Backbone app. And I\u2019m sure we\u2019re not the only ones: there must be a number of developers who\u2019ve hacked on similar approaches, just like Bones. There\u2019s an opportunity here to combine our efforts and work towards a library that we can all use and extend to fit our needs.\n\nHere is the simple set of design goals which are guiding Rendr\u2019s development.\n\nIndicating what data to fetch, which template to render, which route to match, how to transform a model\u2019s data \u2014 this logic can and should be abstracted from specific implementation details as much as possible.\n\nIn true Backbone style, Rendr strives to be a library as opposed to a framework. A small collection of base classes which can be built upon is easier to adopt and maintain than a batteries-included web framework. Solve the problem at hand without imposing unneeded structure on the application.\n\nIf your application has a bunch of conditions that look like this, then it means you\u2019re doing something wrong. It\u2019s a sign of a leaky abstraction. Of course, sometimes it\u2019s necessary to know which environment you\u2019re in, but that logic should be consolidated and abstracted away as much as possible. Which leads us to\u2026\n\nThere are a few really tricky problems that need to be tackled to achieve these other goals. The complexity of the solutions should be hidden in the library, keeping the application code clean, but remain accessible when it\u2019s time to override core behaviors.\n\nBackbone is great at integrating with a RESTful API. Let\u2019s follow that convention, keeping the data source separate from Rendr itself. It should be possible to write adapters for different data sources, such as Mongo, CouchDB, Postgres, or Riak, but the library shouldn\u2019t impose structure on your model layer.\n\nWe prefer string-based templating over using a DOM on the server because DOM implementations are slow, and, well, because it feels hacky. You shouldn\u2019t need a DOM to render HTML. However, I\u2019m curious how this will change though once WebComponents become commonplace.\n\nExpress is the de facto Node.js web server. Rendr should fit with Express convention, exposing a few simple middleware. A nice effect of this is that you can tack on Rendr routes onto any existing Express app, or have Rendr and non-Rendr content served from the same codebase as necessary.\n\nI\u2019d love to see Rendr evolve to become even more modular, pulling out components like the routing, templating, and view classes into separate modules, leaving the minimal set of glue required to sanely build a Backbone app that runs on the client and server. With a bit of work, it may even be possible to decouple Rendr from Backbone itself, allowing it to work with other client-side MVC libraries.\n\nWhile you can easily use the code we\u2019ve released to build a production-quality app (if you don\u2019t mind getting your hands a bit dirty), our intention for Rendr isn\u2019t for it to be the next [Rails, Meteor, Ember, Backbone]. Instead, we want to explore the problem of isomorphic JavaScript applications and stimulate discussion in the community.\n\nTo that end, we\u2019ve been talking with @mde, who maintains the Geddy project, the first and most complete web framework for Node.js. Geddy is more of a Rails-style server-side MVC framework, but Matt shares our vision for JavaScript apps that can run on both sides. We\u2019re looking at what it would take to allow Geddy to take advantage of Rendr\u2019s approach. What primitives and abstractions do we need to build applications that can seamlessly render views, route requests, and fetch data in both environments?\n\nComing from Rails, we Airbnb engineers were initially smitten with CoffeeScript. We\u2019ve found it increases our productivity and lets us avoid some of the more verbose parts of JavaScript. Thus, Rendr has been written mostly in CoffeeScript. But heeding the advice of the broader Node.js community, we\u2019re in the process of converting it over to plain-jane JavaScript. This will make it easier for more people to adopt and contribute, avoiding fragmentation. Interesting side note: when we show off Rendr to people from the Node.js community, they choke on the CoffeeScript without fail, but the Backbone community and front-end developers in general seem to prefer CoffeeScript.\n\nRendr comes hot on the heels of the announcement of Chronos, the open source replacement for cron built by our data-infrastructure team.\n\nWe\u2019ve also released a few JavaScript projects that are borne out of our efforts building web apps here at Airbnb.\n\nInfinity.js is a small library for managing infinite scroll. It\u2019s like UITableView for the web.\n\nPolyglot.js is a browser- and Node.js-compatible library for handling internationalization. The secret-sauce of Polyglot.js is its handling of pluralization rules for non-English languages. For example, Chinese and Korean have just a single form, whereas Russian, Czech, and Polish have three forms.\n\nWe\u2019ve got a few more exciting open source projects coming through the pipeline, so keep an eye out! Follow @AirbnbEng and @rendrjs for updates.\n\nAnd, as always, we\u2019re hiring talented engineers to work all over our stack: frontend, backend, search, trust & safety, machine learning, data-infrastructure, analytics \u2014 you name it."
    },
    {
        "url": "https://medium.com/airbnb-engineering/scaling-with-cacheobservers-5a87dac185e4",
        "title": "Scaling with CacheObservers \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Airbnb\u2019s traffic grows approximately 5x year over year. We\u2019re running in the AWS cloud, using most of their services: hundreds of EC2 servers, tens of RDS instances, and nearly a dozen Elasticache nodes. At around this time last year, we hit a wall. Traffic was growing faster than we could scale our databases, and we were running the largest RDS instances Amazon sold. There was no more vertical headroom. Unless our tiny engineering team made massive changes, our site would buckle in under a month. We of course made large refactors to our schemas, in many cases switched datastores for frequently-used columns (especially those being used for key-value data), and vertically partitioned tables into separate databases where possible. But these were large changes and were performed over the course of months, not weeks. One of the ways we bought time for these efforts was by rebuilding our caching infrastructure from the ground up.\n\nThe most frequently accessed pages on our site are our listing pages. They\u2019re also largely static, so we\u2019d been caching the rendered output in Memcached and using ActiveRecord callbacks to expire the pages after related models changed. Unfortunately, the pages displayed information from many data sources: user information (prominently, the host\u2019s\u2026 But also information for each review!), the listing itself, data derived from pricing and currency tables, and more. Often Models displayed on the listing pages would change, but would only change private data like home address, or would change public data like their personal description that isn\u2019t shown in a review. Since we were expiring our page caches every time that happened, the pages were being re-rendered far more frequently than necessary \u2014 resulting in extra load on the databases. We looked into using Sweepers, the Rails-3-omakase way of expiring caches. But Sweepers suffered the same problems as callbacks: any Model attribute change expired the page, even if it wasn\u2019t used during rendering. Since there didn\u2019t seem to be a solution that let us specify which attributes were \u201csafe\u201d to change, we built our own on top of ActiveRecord::Observers. ActiveRecord::Observers, for those not familiar with the deeper workings of Rails, are an implementation of the Gang of Four Observer pattern: they watch things \u2014 in this case, ActiveRecord Models \u2014 for changes, and when changes occur, take specific actions. Since ActiveRecord Models store a list of all \u201cdirty\u201d (changed) attributes, we were able to build an extension that watches Models for attributes deemed unsafe to change and then expires caches when those attributes do change.\n\nTraditional Observers are one-to-one with Models, not Views. But our CacheObservers needed to have knowledge of the View structure, which in our case often rendered multiple Models; we couldn\u2019t have them be one-to-one with Models. Sweepers are slightly different: they observe both Models and specific Controller actions. But we wanted something that knew just about specific views, not about what actions could impact it: every time we added a new edit action, we didn\u2019t want to also have to remember to call the related CacheObserver\u2019s expiration method. Instead of following the Rails Observer and Sweeper patterns, we made CacheObservers one-to-one with views and one-to-many with Models. CacheObservers have no knowledge of Controllers; they only know what data a view needs to render. There\u2019s a simple DSL for declaring data dependencies:\n\nInside each CacheObserver definition, there can be multiple caches_on blocks. Each caches_on block represents a Model class; the safe_fields are the fields that are safe to change without needing to expire the page cache. Of course, this isn\u2019t quite enough to implement a smart cache-busting Observer. After all: how can you expire a Listing cache given only a model? To auto-expire caches, CacheObservers need to control the cache keys as well, and have a little bit of extra information to figure out how to get from a User model to the Listing\u2019s cache key. First, to define the cache key generation:\n\nAll we\u2019ve added here is the surrounding class definition, along with one extra line:\n\nThere are many ways to configure key generation (in fact, at the base level it simply accepts a block that returns a string); however, in this case we\u2019ve chosen to use the generate_from_url built-in strategy. We\u2019ve passed it pieces of the hash needed by Rails\u2019 url_for method, but missing a crucial component: the listing id. Next, we tell the CacheObserver how to generate the key from the Listing model:\n\nThe cache.url_options block takes an instance of a model that\u2019s had unsafe attributes changed and returns any pieces of the hash left out of the generate_from_url hash. The CacheObserver then merges the hashes together and generates the key from the result. In this case, we simply return the listing.id as the :id parameter; we could\u2019ve also left out the :action parameter in the generate_from_url method and returned it here, but there\u2019s no reason to and it\u2019s easier to keep it in the class method instead of inside each caches_on block. You might wonder: if at the base key configuration level the configuration simply accepts a block that returns a string, how could there be a built-in url_options method? Is there tight coupling between the generate_from_url method and the CacheObservers? But there isn\u2019t: url_options is just an alias for key_options, which takes a block and passes its output as the input for the key configuration block. In this case, the URL strategy decorates the cache object to have the alias for clarity, and expects the given block to return a hash.\n\nExpiring Multiple Caches Associating Listing models with a Listing cache is easy \u2014 they\u2019re clearly one-to-one. If a listing changes, the cached page for it should be expired. But what about user models? Users could have multiple listings, so clearly returning a single hash won\u2019t work. Luckily, the generate_from_url strategy handles this case:\n\nThe url_options block can return an array of hashes, and the CacheObserver will expire all of the associated caches. Of course, in production you probably wouldn\u2019t write the code as above: you\u2019ll potentially generate a bunch of inefficient queries for listings, when all you need are the ids. For simplicity, we\u2019ve presented it this way; writing the more efficient query isn\u2019t difficult but is left as an exercise for the reader.\n\nThe above worked well for expiring full-page caches only when absolutely necessary. But often pages have portions that are highly dynamic, but other pieces that are generally static; for that, we cache pages in fragments so that even when the page expires we don\u2019t do extra queries for the static pieces. The trick with fragment caches is that if they expire, they should expire the full-page cache as well: but they shouldn\u2019t expire their sibling caches. What\u2019s more, nested fragments should expire their parents, who should in turn expire their own parents. This makes sure that your pages are always up-to-date, but maximally cached. This isn\u2019t the first time this concept has been blogged about; at around the same time as we were building our system, 37Signals began blogging about their \u201cRussian Doll\u201d caching architecture, which has now become the base for Rails 4 caching. CacheObservers work similarly with respect to fragment caches: caches have children (and grandchildren, and great-grandchildren\u2026), and they do what you expect and expire only what\u2019s necessary. The DSL is once again simple:\n\nEach cache is named, and can be accessed by name in the view. The models and safe attributes are separate from their siblings and parents, and in addition to being able to expire on different models, children can expire on different attributes on the same models (or, more unlikely, the same attributes on the same models) as their siblings or parents.\n\nAirbnb grows seasonally: in the spring and summer months we accelerate dramatically, then level off mid-autumn before beginning the relentless upward drive once again following the New Year. We built CacheObservers in pieces last spring: at first, just the full-page caches, then child fragment caches, and eventually even Etag-based extensions to short-circuit the rendering process. By midsummer our full-page cache hit rates had more than doubled, and our database efforts bore fruit. We\u2019d kept the site online, and we slowly transitioned our efforts away from disaster mitigation and towards overall site performance and building the foundation for the next year\u2019s marathon. Thanks to those efforts, we\u2019re doing better this year. We\u2019re well into the growth season, faster than we\u2019ve been in years, and we\u2019ve been keeping our site at three 9s thanks to the tireless efforts of the SRE and Performance teams, with the Core Services team building out future architecture for next year\u2019s spike in parallel. I\u2019m crossing my fingers for summer, but it\u2019s mostly out of habit. We learned our lesson last year. If you want to be a part of a fast-moving team tackling giant scaling problems, we\u2019re hiring."
    },
    {
        "url": "https://medium.com/airbnb-engineering/chronos-a-replacement-for-cron-f05d7d986a9d",
        "title": "Chronos: A Replacement for Cron \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Chronos is our replacement for cron. It is a distributed and fault-tolerant scheduler which runs on top of Mesos. It\u2019s a framework and supports custom mesos executors as well as the default command executor. Thus by default, Chronos executes SH (on most systems BASH) scripts. Chronos can be used to interact with systems such as Hadoop (incl. EMR), even if the mesos slaves on which execution happens do not have Hadoop installed. Included wrapper scripts allow transfering files and executing them on a remote machine in the background and using asynchroneous callbacks to notify Chronos of job completion or failures.\n\nChronos has a number of advantages over regular cron. It allows you to schedule your jobs using ISO8601 repeating interval notation, which enables more flexibility in job scheduling. Chronos also supports the definition of jobs triggered by the completion of other jobs, and it also supports arbitrarily long dependency chains.\n\nChronos is available on Github\n\nAt Airbnb, we heavily rely on data analysis to build great products. Our data-pipeline consists of many technologies such as Hadoop, MySQL, Amazon Redshift and S3. Our software engineers and analysts use a mix of Cascading, Cascalog, Hive and Pig for interfacing with Hadoop. We have scripts that export tables from a vast number of databases into S3 and we use various ETL (extract transform and load) processes to turn blobs of bytes into meaningful information. Many of these transformations consist of multiple steps and some tables are composed of a myriad of data-sources and joins.\n\nWe\u2019re not in a private datacenter, and we aren\u2019t running our own Hadoop cluster \u2014 we use a managed Hadoop product from Amazon, called Elastic Map/Reduce. High variance in network latency, virtualization and not having predictable I/O performance is an ongoing challenge in a cloud environment. There are many sources for errors. For example calls to web services are subject to timeouts.\n\nIn a complex processing pipeline every step increases the chance of failure. Until December last year, we were relying on a single instance with cron to kick off our hourly, daily and weekly ETL jobs. Cron is a really great tool but we wanted a system that allowed retries, was lightweight and provided an easy-to-use interface giving analysts quick insights into which jobs failed and which ones succeeded.\n\nWe also wanted a system that was highly available and could manage any workload, not just Hadoop jobs. Other requirements were that the system still could run BASH scripts and fan out the workload to many systems (as we are exporting many tables we didn\u2019t want to just execute on one box albeit we wanted to have central management). At the same time we began looking at Mesos for data-infrastructure. Thus we made the decision to build a new lightweight, fault-tolerant scheduling tool which we named Chronos that would run on top of Mesos, using Mesos\u2019 primitives for storing state and distributing work. Mesos also allowed us to dynamically add new workers to our pool without having to change the configuration of the existing cluster.\n\nChronos comes with a UI which can be used to add, delete, list, modify and run jobs. It can also show a graph of job dependencies. These screenshots should give you a good idea of what Chronos can do.\n\nOver the past weeks, we have open-sourced Chronos, you can check it out on our github page: https://mesos.github.io/chronos/\n\nHere\u2019s the video from our Tech Talk on Chronos:"
    },
    {
        "url": "https://medium.com/airbnb-engineering/redshift-performance-cost-b4aa8dc25d75",
        "title": "Redshift Performance & Cost \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "At Airbnb, we look into all possible ways to improve our product and user experience. Often times this involves lots of analytics behind the scene. Our data pipeline thus far has consisted of Hadoop, MySQL, R and Stata. We\u2019ve used a wide variety of libraries for interfacing with our Hadoop cluster such as Hive, Pig, Cascading and Cascalog. However, we found that analysts aren\u2019t as productive as they can be by using Hadoop, and standalone MySQL was no longer an option given the size of our dataset. We experimented with frameworks such as\n\nSpark but found them to be too immature for our use-case. So we turned our eye to Amazon Redshift earlier this year, and the results have been promising. We saw a 5x performance improvement over Hive.\n\nRedshift is Amazon\u2019s SQL based enterprise data warehouse solution for large scale and complex analytics. Under the hood, it is a distributed managed ParAccel cluster. It achieves its high performance through extreme parallelism, columnar data storage, and smart data compression. The setup process is very easy and fast (it took just a few minutes to provision a 16-node cluster) , and you can connect to the system via any Postgres compliant client.\n\nSchema Migration The first advice we can offer is to follow what the manual says closely when migrating your tables and queries. We started out by migrating a few large datasets generated by our existing Hadoop based ETL over to Redshift. The first challenge we had was schema migration. Even though Redshift is based on Postgres 8.0, the \u201csubtle\u201d differences are big enough, forcing you into the Redshift way of doing things. We tried to automate the schema migration, but the problem was bigger than we originally expected and we decided it was beyond the scope of our experiment. Indexes, timestamp type, and arrays are not supported in Redshift, thus you need to either get rid of them in your schema or find a workaround (only 11 primitive data types are supported at this point in time). This was the most lengthy and tedious part of the migration to Redshift, but it serves as a very good training and evaluation process (whether redshift is the right solution for you). When defining your schema, but careful with the distribution key, which determines how your data is distributed across the cluster. Check all the queries you run against the table and choose the column that gets joined most frequently to get the best performance. Unfortunately, you can only specify one distribution key and if you are joining against multiple columns on a large scale, you might notice a performance degradation. Also, specify the columns your range queries use the most as sort key, as it will help with the performance.\n\nData Loading The next step is loading our data into the system. Which probably sounds easy, but there are few gotchas. First, in order to load your data into Redshift, it has to be in either S3 or Dynamo DB already. The default data loading is single threaded and could take a long time to load all your data. We found breaking data into slices and loading them in parallel helps a lot. Second, not all the utf-8 control characters are supported. Some of our data originally came from other SQL databases and unfortunately it has all sorts of utf-8 characters. Redshift only supports control characters up to 3 characters long. Remember, Redshift is intended for analytics and I doubt those characters are of any use for the purpose, so clean them up before loading. If you don\u2019t have many of those cases the other options is to use MAXERROR option to skip them. Another issue we had was NULL values, since Redshift only supports one null value when loading data. If you have multiple \u2018NULL\u2019 values in your data what you need to do is to load them as a string into a temp table and cast them back to NULL. Last, we had some data in json format, and we had to convert those into flat files, since it is not supported in Redshift. After schema migration and data loading we are finally ready to play around with Redshift to see its power in action.\n\nRedshift \u2014 16 node cluster ($13.60 per hour \u2014 $0.85 per hour per node) Node Type: dw.hs1.xlarge CPU: 4.4 EC2 Compute Units (2 virtual cores) per node Memory: 15 GiB per node I/O Performance: Moderate Platform: 64-bit Storage: 3 HDD with 2 TB of storage per node\n\nSimple range query against a giant table with 3 billion rows, we saw 5x performance improvement over Hive!\n\nRedshift is 20x faster than the Hive version of the query!\n\nResults As shown above the performance gain is pretty significant, and the cost saving is even more impressive: $13.60/hour versus $57/hour. This is hard to compare due to the different pricing models, but check out pricing\n\nhere for more info. In fact, our analysts like Redshift so much that they don\u2019t want to go back to Hive and other tools even though a few key features are lacking in Redshift. Also, we have noticed that big joins of billions of rows tend to run for a very long time, so for that we\u2019d go back to hadoop for help.\n\nFrom our preliminary experiment with Redshift, although lacking a few features we would like it to have, it is very responsive and can handle range and aggregation against a fairly large dataset very well. Anyone with a little SQL background can get start to use it immediately, and the cost of the system is very reasonable. We don\u2019t think Redshift is a replacement of the hadoop family due to its limitations, but rather it is a very good complement to hadoop for interactive analytics. Check it out on Amazon and we hope you will enjoy the ride as well!"
    },
    {
        "url": "https://medium.com/airbnb-engineering/our-first-node-js-app-backbone-on-the-client-and-server-c659abb0e2b4",
        "title": "Our First Node.js App: Backbone on the Client and Server",
        "text": "Here at Airbnb, we\u2019ve been looking curiously at Node.js for a long time now. We\u2019ve used it for odds and ends, such as the build process for some of our libraries, but we hadn\u2019t built anything production-scale. Until now.\n\nThere\u2019s a disconnect in the way we build rich web apps these days. In order to provide a snappy, fluid UI, more and more of the application logic is moving to the client. In some cases, this promotes a nice, clean separation of concerns, with the server merely being a set of JSON endpoints and a place to host static files. This is the approach we\u2019ve taken recently, using Backbone.js + Rails.\n\nBut all too often, it\u2019s not so clean; application logic is somewhat arbitrarily split between client and server, or in some cases needs to be duplicated on both sides. Think date and currency formatting. You tend to have a Ruby (Python, Java, PHP) library that you\u2019ve been using for awhile, but all of a sudden you have to replicate this logic in a JavaScript library. The same is true for the view layer; some of your markup lives in Mustache or Handlebars templates for the client, while other needs to live in ERB or Haml so they can be rendered on the server for SEO or other reasons.\n\nIf you\u2019ve seen my tech talk or last blog post, then all this should sound familiar. Our thesis, four months ago when I gave this talk, was that, in theory, if we have a JavaScript runtime on the server, we should be able to pull most of this application logic back down to the server in a way that can be shared with the client. Then, as a developer you just focus on writing application code. Your great new product can run on both sides of the wire, serving up real HTML on first pageload, but then kicking off a client-side JavaScript app. In other words, the Holy Grail.\n\nThis Holy Grail approach is something we had dreamt about for a long time, but not having any experience with Node.js, we didn\u2019t quite know where to start. Luckily, we hired our first engineer who has experience running Node.js at large scale in production; meet Barbara Raitz, who comes to us from LinkedIn, where she built out the API that powers their mobile apps using Node.js. Now that we had some in-house Node expertise, Barbara and I set out to create our solution for the Holy Grail.\n\nI\u2019m proud to announce that we\u2019ve launched our first Holy Grail app into production! We\u2019ve successfully pulled Backbone onto the server using Node.js, tied together with a library we\u2019re calling Rendr. We haven\u2019t open-sourced Rendr quite yet, but you can expect it in the coming months, once we\u2019ve had a chance to build a few more apps with it and decouple it from our use case a bit.\n\nSo, the app: we re-launched our Mobile Web site using this new stack, replacing the Backbone.js + Rails stack that it used previously. You may have been using it for over a month without even knowing. It looks exactly the same as the app it replaced, however initial pageload feels drastically quicker because we serve up real HTML instead of waiting for the client to download JavaScript before rendering. Plus, it is fully crawlable by search engines.\n\nThe performance gains are an awesome side effect of this design. In testing, we\u2019re using a metric we call \u201ctime to content\u201d, inspired by Twitter\u2019s time to first tweet. It measures the time it takes for the user to see real content on the screen. Let\u2019s take our search results page, for example. Under the old design, before any search results could be rendered in the client, first all of the external JavaScript files had to download, evaluate, and execute. Then, the Backbone router would look at the URL to determine which page to render, and thus which data to fetch. Then, our app would make a request to the API for search results. Finally, once the API request returned with our data, we could render the page of search results. Keep in mind all of this has most likely happened over a mobile connection, which tends to have very high latency. All of these steps add up to a \u201ctime to content\u201d that can be more 10 seconds in extreme cases.\n\nCompare this with serving the full search results HTML from the server. Over a mobile connection, it may take 2 seconds to download the initial page of HTML, but it can be immediately rendered. Even as the rest of the JavaScript application is being downloaded, the user can interact with the page. It feels 5x faster.\n\nWe built upon tools we already know and love: Backbone.js and Handlebars.js. We ended up with a hybrid of Rails, Backbone, and Node conventions. For example, the app\u2019s directory structure will look familiar to Rails users (minus collections and models):\n\nIn a typical Backbone app, \u201ccontroller\u201d logic \u2014 fetching the appropriate data and instantiating view(s) for a particular page \u2014 lives in methods on your instance of Backbone.Router. As your app grows, however, the router quickly becomes bloated with all of these route handlers. This is why we\u2019ve created real controller objects, which group related actions into more manageable chunks. This abstraction also allows us to more easily add before and after filters if we want to. Here\u2019s what a users controller might look like:\n\nFirst off, you'll notice we're using CoffeeScript. It's a bit controversial, I know, but we're fans. Secondly, notice the module.exports = at the top. That's a tell-tale sign of a CommonJS module. CommonJS is what Node.js uses to require modules, and we're able to reuse the same syntax in the client using Stitch, which was written by the Sam Stephenson, the same fellow who also wrote Sprockets for Rails.\n\nNow, keep in mind that this controller code gets executed on both the client and the server. For example, if a user lands on \"/users/1234\", and a route exists that routes that to users#show (more on that below), then the show action will be invoked.\n\nThe @app.fetch you see is our way of encapsulating resource fetching. In this case, a User model with id 1234 will be fetched from the API, instantiated, and passed to the view. Why not just instantiate the User model yourself in the controller? You could do that, but the fetcher provides a layer of indirection which allows us to do a bunch of fancy caching in both the client and the server-side. I'll leave out the details for now, but that could be the subject of a future blog post!\n\nWe need to be able to match a certain URL to a controller/action pair both on the client-side and the server-side. Inspired by Rails, we have a routes file that specifies these routes and any additional route parameters.\n\nNotice the optional parameter maxAge on the listings#search route; this is used to set caching headers. You can add any arbitrary parameters here and access them in the router. We also plan to make this more advanced as needed, such as adding parameter requirements.\n\nOur routes file format is heavily inspired by a really interesting project called ChaplinJS. Chaplin is an application framework built on top of Backbone. Chaplin is under active development and is quite well maintained; definitely worth a look if you want some more structure in your Backbone app.\n\nThese routes are parsed by a router, which delegates incoming requests to particular controllers. We have a ClientRouter, which delegates to Backbone.History and translates pushState events to controller actions, and a ServerRouter, which does the same for Express requests; both extend BaseRouter to share common logic.\n\nWe serve the app using Express, the de-facto web server for Node.js.\n\nIn Rendr, your views extend our BaseView, which in turn extends Backbone.View, adding a number of methods that allow it to easily render on both the server and the client. Here's an example ListingsIndexView for you:\n\nFirst you'll notice that we require BaseView with require('rendr/base/view'). This CommonJS module path is standard in Node.js for requiring files within NPM packages, and using a trick in our Stitch packaging, we can use the same path in the client.\n\nIn addition to the typical methods and properties from Backbone.View, we've added some custom ones to hook into the view's lifecycle. Notice the #postRender() method; this gets called only in the client-side, right after rendering. This is where you would put any code that needs to touch the DOM, such as initializing jQuery plugins like slideshows or sliders.\n\nEach view has a Handlebars template associated with it, and a View#getHtml() method. On the server, we simply call #getHtml() and return the resulting string. On the client, View#render() calls #getHtml() and updates the innerHTML of its element. We also have a #getTemplateData() method, which by default returns @model.toJSON(). You can override it to act as a view-model, adding any view-specific properties to pass to the template for rendering.\n\nWe decided on an entirely string-based templating approach, preferring not to depend upon a DOM on the server. One result of this is that it becomes necessary to push all HTML manipulation either into the Handlebars template or into a custom #getHtml() method. In other words, you cannot rely on any DOM manipulation code to construct your markup; if you have a habit of appending child views, loading spinners, or any other markup in #render(), get used to pushing that down to the template. Now, here's the tricky part: when we generate a page of HTML from a hierarchy of views on the server, we also have to ensure that once it reaches the client, all of the views' event handlers are properly bound to the DOM, and we have living, breathing view instances that can respond to user interactions. Our approach was to decorate the generated HTML with data-attributes, which specify which view class it represents. Here's an example from one of our listing pages:\n\nOn pageload, we find all DOM elements with the data-view=\"some_view_class\" attribute, instantiate view objects for each, and reconstruct the view hierarchy. That way, events are properly bound, and we preserve any parent-child relationships between our view objects so they can listen to and emit events on each other.\n\nSounds easy enough, right? Well, it ended up being more difficult to solve than we expected. One of the problems we ran into is that all data needed to instantiate a view needs to be extractable from the DOM. You'll notice in the above code snippet that we also have the data-attributes data-model_name and data-model_id. This allows us to pass in the correct model or collection into any view, fetching it from the client-side model cache (remember @app.fetch from our controller? it handles this for us). This is the pattern we came up with in order to ship; a better approach would be to assign a unique id to each view instance on the server, and have some sort of mapping from view id to view data which we could read from in the client, rather than looking up from the DOM.\n\nThere's another way to ensure views are properly bound to the DOM, which we decided against. It involves discarding the server-generated HTML upon pageload, regenerating the view hierarchy in the client, and swapping the view DOM tree into the page. The downside is the potential for weird UX interactions if, for example, a user starts to interact with elements on the page that then get destroyed and replaced.\n\nWe really like enforcing encapsulation within our views. We also wanted an easy way to declaratively nest subviews within any view. We came up with a nifty way of achieving both using Handlebars helpers. Check out this slightly contrived example.\n\nSo, you can arbitrarily nest views using a simple Handlebars helper, and simply call #getHtml() on the top-most view to get the entire hierarchy's HTML. Nifty, eh?\n\nWe found this useful enough as a standalone library that we've pulled it out into NestedView. Check it out on Github.\n\nNow onto models and collections.\n\nRendr has a BaseModel and BaseCollection, which extend Backbone.Model and Backbone.Collection. Here's the most basic User model:\n\nPretty thin, right? Of course, you can add whatever custom methods you want to your models.\n\nThe important part is the url property. Rendr expects to get all of its data from a JSON API over HTTP. This could be on the same server, but in our case, it's a preexisting API that powers a number of other apps. The url specified above points to the path on this API server.\n\nCalling Backbone's CRUD methods on the model or collection (#save(), #fetch(), #destroy(), and now #update() in the latest Backbone) will dispatch a request to this API. We override Backbone.sync() such that when these methods are called from the server-side, Rendr sends an HTTP request directly to the API server. When called from the client, Rendr will prepend /api to the model/collection URL, proxying the request to the API through the Rendr server. This allows us to do any additional formatting of the request or response, and to centralize the API request and response handling logic, at the expense of some network time.\n\nInternationalization (I18n) is incredibly important to us here at Airbnb. We support 30+ languages, and have localized web sites all around the world. We've been performing I18n in JavaScript for some time, but the need arose to make a more-robust library that can run in CommonJS environments as well. That's led to Polyglot.js, our open source JavaScript I18n library. The extra special sauce is our pluralization logic. Here's an excerpt from the docs:\n\nIn English (and German, Spanish, Italian, and a few others) there are only two plural forms: singular and not-singular.\n\nHowever, some languages get a bit more complicated. In Czech, there are three separate forms: 1, 2 through 4, and 5 and up. Russian is even crazier.\n\nPolyglot.js abstracts all that away from you, and just requires you to provide translations for whichever locales you're interested in.\n\nThere's still a lot of work to be done. Before we release Rendr, we need to build a few more applications with it and modularize the code a bit more. Luckily we have a growing need for rich JavaScript apps that are fast and SEO-friendly here at Airbnb. Keep an eye out here and follow @AirbnbNerds and @spikebrehm for more updates.\n\nI'll be speaking more about Rendr and Airbnb's experiences building rich JavaScript apps at HTML5DevConf on April 1-2 in San Francisco. It's shaping up to be a great conference. Sign up if you haven't already!"
    },
    {
        "url": "https://medium.com/airbnb-engineering/upgrading-from-ree-1-8-7-to-ruby-1-9-3-c304e2493b34",
        "title": "Upgrading From REE 1.8.7 to Ruby 1.9.3 \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "In a previous blog post, we discussed our path to upgrading to Rails 3.0 from Rails 2.3. At the time, a number of comments asked about our upgrade path from 1.8.7 to 1.9.3. We waited until the Rails 3.0 upgrade was complete and in production before beginning the Ruby upgrade. It is probably a good thing, since upgrading our Ruby version required significantly more work than we had anticipated.\n\nWe were really excited about the potential performance improvements that a number of other companies have reported after upgrading to Ruby 1.9.3. Harvest, ZenDesk, UserVoice, NewRelic, and Ngin all have released great blog posts reporting pretty significant performance gains after making the upgrade.\n\nThe first major milestone was getting our Rails app to start locally in Ruby 1.9.3. We had to upgrade a number of our gems (e.g. Zookeeper,libxml-ruby, hpricot) so that they would work in Ruby 1.9.3. For some gems, we only needed them in one environment. Gemfiles have a useful feature where you can specify the platform that you want a particular gem installed, like so:\n\nCurrently, Airbnb utilizes Ruby on Rails\u2019 Cookie Based Session Store. By default, the cookie based session store serializes data from the session using Ruby\u2019s Marshal. While this provides you with the ability to store complex objects in the session, it limits the portability of that data. For example, a Date object serialized by Ruby 1.8.7\u2019s Marshal will throw an exception if you try to deserialize it using Ruby 1.9.3.\n\nTo make the session cookie portable between Ruby versions, we monkey patched the code that serializes the session to use JSON instead. Interestingly, the MessageVerifier class in Ruby on Rails 3.2.3 provides support for specifying the serializer; however, ActionDispatch:: Cookies:: SignedCookieJar does not. So we pulled in the MessageVerifier from Rails 3.2.3 into our Rails 3.0 app, and monkey patched ActionDispatch:: Cookies:: SignedCookieJar to use JSON as the serializer. To minimize session resets during the transition period while we rolled this out, before loading the session from the cookie, we try to infer whether it was serialized with Marshal or JSON by reading the first couple characters of it. The code is included in this Gist for what we call our \u201cRuby on Rails JSON Cookie Session Store.\u201d\n\nWe had to clean up our codebase to make sure that we were only storing objects that could be serialized using JSON. Switching Date objects to be ISO strings was rather trivial. Somewhat surprisingly, the FlashHash, the object that Ruby uses when you call something like `flash[:notice] = \u201cSuccess!\u201d`, isn\u2019t portable between Ruby versions. To get around this, we use a custom middleware to move the flash messages to a separate cookie, where it is serialized using JSON. This bypasses ActionDispatch::Flash, which looks in the session for a FlashHash. At Airbnb, many of our pages are cached, so we actually use JavaScript to read this separate cookie with the flash and add it to the DOM in the client.\n\nIt\u2019s worth noting that we initially wrote this code so that we could share the session between services. Like many Ruby on Rails apps that reach some amount of scale, we\u2019re moving towards a Service Oriented Architecture. When we launched Airbnb\u2019s \u201cCommunities\u201d feature, which was built as its own service on Rails 3.2.3 and Ruby 1.9.3, it shared the session with Airbnb\u2019s main monolithic Rails application (a.k.a. monorail) which was running Rails 3.0 and Ruby 1.8.7 at the time. Using JSON to serialize the session will allow us to share the session with services written in other frameworks and languages altogether, like Node.js.\n\nRather than dealing with sharing data between Ruby versions in memcached, we setup a completely separate memcached cluster for the Ruby 1.9.3 servers. In general, this worked out pretty well for us.\n\nOne rather obscure issue that created some major headaches for us involved the fact that data serialized using Ruby Marshal apparently takes up more space in Ruby 1.9.3 than in Ruby 1.8.7. The default maximum object size in memcached is 1MB, and some data that we were serializing in memcached no longer fit when we switched to Ruby 1.9.3. Code that once cached values suddenly failed silently when we switched to Ruby 1.9.3.\n\nWe gradually updated our codebase so that it would work in both Ruby 1.8.7 and Ruby 1.9.3. The following is a guide on how to write code that works in both environments:\n\nAs many people have pointed out, encodings will be the biggest pain point when upgrading to Ruby 1.9.3 from 1.8.7. You\u2019ll have to add the \u201cmagic encoding comment\u201d on the top of every file that uses UTF-8 encoded characters.\n\nRuby 1.8 supports the American style date format, MM/DD/YYYY, so calling Date.parse on the string \u201c10/11/2012\u201d will return a Date object representing October 11th, 2012. But in Ruby 1.9.3, American style dates are no longer supported, and Ruby 1.9 appears to parse them in the European format of DD/MM/YYYY:\n\nWe use Jeremy Evans\u2019 American Date gem to keep this functionality consistent between Ruby 1.8.7 and Ruby 1.9.3.\n\nCalling .methods on a object in Ruby 1.8.7 returns an array of strings, while in Ruby 1.9.3, an array of symbols is returned. Instead of doing something like this:\n\nThere is a very subtle difference in how Ruby 1.8.7 and Ruby 1.9.3 handle regular expressions with UTF-8 encoded strings. In the example below, we attempt to write a regular expression that can isolate the name part from the greeting of a message \u201cHello Chlo\u00eb,\u201d:\n\nAs you can see, the third approach is the only version that works consistently between Ruby 1.8.7 and Ruby 1.9.3.\n\nThe meaning of the POSIX character class [:punct:] is subtly different between Ruby 1.8.7 and Ruby 1.9.3. In the following example, we attempt to replace all of the punctuation characters with a Unicode snowman:\n\nThe String class no longer supports the #each method. In Ruby 1.8.7, this method would allow you to iterate on each line of a string. This (odd) functionality was dropped in Ruby 1.9.3.\n\nHash#select returns an array of arrays in Ruby 1.8.7, but a proper Hash in Ruby 1.9.3. You can write code that is compatible with both 1.8.7 and 1.9.3 by wrapping the call in Hash[] like so:\n\nColons are no longer valid after \u201cwhen\u201d in a case statement. We prefer to use \u201cthen\u201d or a newline instead.\n\nIn Ruby 1.9.3, LOAD_PATH no longer includes because it was deemed a security risk. You can explicitly add it when requiring files, use absolute paths, or use require_relative.\n\nIn Ruby 1.9.3, Range#member? and Range#include? behave differently for ranges that are defined by begin and end strings. In Ruby 1.9.3, those methods only return true if an exact match is in the range, not just a prefix of the string.\n\nIn order to have consistent behavior between Ruby 1.8.7 and Ruby 1.9.3, we created a class called CSVBridge, and use that instead of CSV or FasterCSV:\n\nWe created a nothington class called RubyBridge to encapsulate a bunch of helper methods that we found ourselves using repeatedly to make our code compatible:\n\nThe following is a method that we added to application_controller as a before_filter for all actions to ensure that params were encoded with UTF-8:\n\nSome additional monkey patches related to handling data serialization in ActiveRecord and Thrift are included in this gist.\n\nIn line with experience of others, the bulk of the problems that we encountered with upgrading to Ruby 1.9.3 involved encodings. Once we got all of our specs passing, we needed to test the app with production traffic to uncover the more insidious encoding problems. We configured our build server so that we could maintain builds for both Ruby 1.8.7 and Ruby 1.9.3 at the same time. Rather than making the switch all at once, we deployed the Ruby 1.9.3 build to a handful of instances in our cluster so that they could get production traffic, added them into the load balance and then watched for exceptions. We\u2019d take the instances out of the load balancer, fix the errors and repeat.\n\nWith over 100,000 lines of code in our main Rails app and support for 21 different end-user languages, upgrading Airbnb to Ruby 1.9.3 was a significant undertaking.\n\nWas it worth it? We were hoping to see the type of performance gains that Zendesk and Harvest reported after they upgraded. While Zendesk reported 2\u20133x improvement in response time, we saw only a 20% improvement.\n\nHowever, in the past couple months, we have been able to tune our application in numerous ways (which we hope to document in a future blog post). As a result, our performance has improved by a margin more in line with what we had hoped for:"
    },
    {
        "url": "https://medium.com/airbnb-engineering/immediately-invoked-function-expressions-and-parentheses-eeea53b39e0b",
        "title": "Immediately-Invoked Function Expressions and Parentheses",
        "text": "A little while ago, Jonathan Cutrell opened an interesting issue about immediate-invoked function expressions (IIFEs) on our JavaScript style guide:\n\nAnd of course the short answer is yes, it could be either. Crockford makes a good case for the parens on the inside:\n\nIf you don\u2019t care about the return value of the IIFE, you can also write them like this:\n\nLet\u2019s explore the two IIFE methods using parentheses a bit more.\n\nBoth work the same. It starts to get interesting when one of the modules is missing a trailing semicolon:\n\nWith a missing semicolon, each set of parens is trying to immediately-invoke the preceding expression. That would be the return value of the preceding IIFE.\n\nSo the difference is when the TypeError happens. Let\u2019s check out what the arguments are up to. Note that console.log() returns undefined:\n\nNow let\u2019s do that same example with the Crockford way:\n\nThere\u2019s no TypeError because of the returned function. The returned function that logs the arguments is then getting invoked with the return value of module2, which is undefined.\n\nWith that understanding, let\u2019s go back to the original example, where there was a TypeError:\n\nThe (function{})(); and (function(){}()); IIFEs can act differently in the missing semicolon situation.\n\nUse a tool to make sure modules aren\u2019t missing trailing semicolons when working on modules.\n\nTo be extra safe add a leading semicolon to the IIFE:"
    },
    {
        "url": "https://medium.com/airbnb-engineering/our-javascript-style-guide-43b026f5b463",
        "title": "Our JavaScript Style Guide \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Check it out on Github: https://github.com/airbnb/javascript\n\nEveryone writes JavaScript a little differently. We finally decided it was time that we got together and agree on how we write JavaScript.\n\nWe describe it as a mostly reasonable approach to JavaScript.\n\nWe\u2019re releasing it under the MIT license, so please feel free to fork and use at your will. We don\u2019t expect everyone to agree with the way we do things, but we do hope this can help kick start your own style guide as a template or map of some sort."
    },
    {
        "url": "https://medium.com/airbnb-engineering/building-single-page-apps-ef865c4e74a4",
        "title": "Building Single-Page Apps \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Two weeks ago I had the opportunity to give a presentation on Single-Page Apps as part of our biweekly Tech Talk series at Airbnb HQ in San Francisco. Here\u2019s links to the slides and a video of the full presentation and Q&A session:\n\nThanks to all who attended! We had a packed house of about 150 engineers who got warmed up on fancy tacos and beer, courtesy of Airbnb\u2019s Chef Sam.\n\nThe talk included a number of specific tips and tricks on Backbone.js, Handlebars, and Rails that the team employed to build Wish Lists, which we launched in June. I mention how we bootstrap the app from Rails to Backbone, our data lazy-loading strategy, how we keep our Backbone views modular and DRY, how we handle CDN and asset fingerprinting when generating HTML client-side, and our approach to I18n in the client, among other things.\n\nOne of these examples that got the biggest reaction was was Airbnb\u2019s homegrown JavaScript I18n library, which we just open sourced under the name Polyglot.js. Polyglot is a tiny utility that makes it easy to use translated phrases in both the browser and in Node. Check it out and let us know what you think!\n\nIn the talk I also introduced what it means to build a single-page app and outlined two different approaches: the Easy Way and the Hard Way.\n\nThe Easy Way is my name for single-page apps that run almost entirely in the browser. Routing, view rendering, model layer, persistence, I18n, currency formatting, etc. all run in the client. The app is loosely coupled to the server, which mostly serves static files, and to some RESTful API, which may or may not be running on the same server. This approach is gaining popularity with the help of libraries and frameworks like Backbone.js and Ember.js. You can find tons of resources and tutorials online about how to architect these client-side apps.\n\nThere are two main drawbacks to the Easy Way: performance and SEO. The performance hit comes from the fact that the browser needs to download and evaluate all external and in-page scripts before the app can render any meaningful HTML. Twitter wrote a great blog post about this a few months ago. They replaced client-side rendering with server-side rendering and saw the \u201ctime to first tweet\u201d drop by a factor of five. That\u2019s huge.\n\nSEO is a huge consideration for a content-heavy app like Airbnb. We want to build a rich, interactive searching and browsing experience but we absolutely need to allow search engines to crawl us. It\u2019s hard to do both; we\u2019ve tried a few approaches for sharing view rendering between JavaScript in the browser and Rails on the server and the results have been awkward at best.\n\nIn contrast, the Hard Way is what I call a webapp that can run seamlessly both on the cilent and the server. Application logic and view rendering can run on the server to serve up HTML for performance and SEO, but can also run in the client to provide a rich, single-page experience.\n\nWe started our dive into Node.js with a survey of the open source application frameworks available now. The first we found were Geddy and Tower. These are heavily inspired by Rails, and so have a fairly traditional MVC architecture. A URL maps to a controller and an action, and the server returns a page of HTML for each request. These don\u2019t solve for the Hard Way, and in my opinion don\u2019t take advantage of Node\u2019s strengths.\n\nNext was SocketStream. SocketStream is neat; it\u2019s realtime at its core, quite modular, and it provides some neat adapters for using WebSockets to persist Backbone or Ember models to the server. However, it\u2019s made to support fully client-side apps: aka, the Easy Way.\n\nNext up was Meteor. Meteor is the most-hyped Node application framework of the bunch, and it does seem like a revolutionary way to write apps. Like SocketStream, it\u2019s realtime, but it provides much more structure to your application, and you have to buy into the Meteor Way. It even provides its own plugin system, eschewing NPM in favor of a Meteor-specific approach. That scares me off a bit; why reinvent the wheel? Finally, it doesn\u2019t solve for the Hard Way. This was a mistake in my talk \u2014 I said that rendering on the server and client was a core principle of Meteor, but it turns out it just has a hacky plugin that uses PhantomJS to boot up a headless WebKit instance on the server, and is labled as \u201cDon\u2019t Use\u201d in the documentation.\n\nFinally, Derby. Derby is the only Node project I\u2019ve seen that actually solves for the Hard Way as a design goal. It has a novel approach to sharing templates and application logic between client and server, relying heavily on its realtime, MongoDB-based model-synchronization engine Racer. However, like Meteor, it\u2019s not very modular, and it\u2019s very alpha. I was very stoked to give Derby a spin, as I mentioned in the talk, and afterwards I sat down with Nate Smith, core Derby contributor to see how we might be able to leverage it. As as I was afraid, it\u2019s not quite ready for prime time, especially if like us, your data comes from a set of RESTful APIs that are decoupled from the Node app.\n\nSo, we\u2019ve begun to prototype our own Node application framework. It\u2019s too early to give away any details, but we\u2019re starting by porting over Backbone-style Models, Collections, and Views to Node, and seeing what issues arise. We\u2019re taking some inspiration from Henrik Joretag\u2019s Capsule, and Mikito Takada\u2019s view.json. More exciting news on that to come! Let me know in the comments and on Twitter at @spikebrehm if this is an approach that interests you.\n\nCheck out our Tech Talk page to see videos of our past talks and schedules of upcoming talks. If you\u2019re a San Francisco nerd, come check \u2019em out!"
    },
    {
        "url": "https://medium.com/airbnb-engineering/integration-testing-with-selenium-and-capybara-3cfbf3d1b5b1",
        "title": "Integration Testing with Selenium and Capybara \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Life before frontend integration testing meant clicking links and buttons and testing everything by hand before launching any feature change. But as every programmer knows, insanity is doing the same thing over and over again, and not writing code to do it for you.\n\nThough it might seem daunting to set up a frontend integration testing environment, automating these tests is worth the effort.\n\nFor our frontend testing needs, we use a combination of Capybara and Selenium \u2014 Selenium because it\u2019s a mature solution for automating browser interactions; Capybara because of its rspec-like syntax for specifying browser interactions.\n\nUsing Selenium requires installation of the selenium-webdriver gem, and Capybara requires installation of the capybara gem and the launchy gem for screengrabs.\n\nThe auto-generated test will live in spec/requests look something like\n\nWithin the test, you can use Capybara to describe any user interactions on a page.\n\nThe selectors can be id, label, or text \u2014 they\u2019re lenient.\n\nit \u201clets the user login via modal and email\u201d, :js => true do\n\nAfterwards, you can run the tests with rake spec:requests. They also run as a part of bundle exec rake spec\n\nStay tuned for a followup post on how we integrated Capybara tests with Jenkins, our continuous integration server!"
    },
    {
        "url": "https://medium.com/airbnb-engineering/upgrading-airbnb-from-rails-2-3-to-rails-3-0-269b5b0f2ad0",
        "title": "Upgrading Airbnb from Rails 2.3 to Rails 3.0 \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "One of the major nerd goals for Airbnb in 2011 was upgrading to Rails 3. Our production instances made the final switch in the week leading up to Thanksgiving, but it didn\u2019t happen all at once.\n\nWe added the required pieces throughout the past year, and, looking back, breaking the upgrade into three major steps was easier to manage than trying to cram it into a single deploy. The first two steps are independent from actually upgrading to the Rails 3 gem, and any Rails 2.3 app out there should do them even if there\u2019s no intention of upgrading to 3.0.\n\nThis was our first step into Rails 3 land, and we did it at the end of 2010. Our app was running Rails 2.3.8 at the time, and the production instances were a mess. Each instance had its own set of gems and its own versions of those gems. Naturally, none of the instances matched.\n\nOne of our engineers picked the latest version of each gem initialized by config.gem in production to create our first Gemfile. He installed Bundler using their Rails 2.3 guide and added any missing gems when trying to start the app. He used the recommended ~> notation for gem versions at first, but that upgraded our Paperclip gem to 2.3.3. That version had performance problems in our environment, so the engineer specified exact versions for our gems instead, the versions we had fully tested.\n\nHe then ran each of our Rake tasks and added any gems those tasks needed to run. That was the last step before our Gemfile was ready to deploy in production.\n\nAdding Bundler at the end of 2010 was a one-engineer task that took about a week working on-and-off. If Bundler had been added the same time as the rest of the Rails 3 upgrade, almost a full year later, it would have been a much bigger ordeal.\n\nAfter installing Bundler, Airbnb ran without any thoughts of Rails 3 for a while. Our production environment was far cleaner, each production instance was running the same set of gems, and our deploy process was simpler.\n\nTemplates in Rails 3 auto-escape all strings, which is a major change from 2.3 and breaks a lot of assumptions. Auto-escaping HTML is a good thing, but many of our templates assumed Ruby strings would be interpreted as HTML.\n\nSkip ahead from our Bundler install about 6 months to Spring 2011. To bring the Rails 3 behavior into our app, Tobi Knaup, one of our engineers, chose to install the rails_xss plugin. The plugin escapes strings by default like Rails 3. He knew this was likely to break a ton, if not all, of our templates, so he setup a Rails XSS Hacksession to get every engineer\u2019s help in the installation.\n\nTobi listed every controller in our app at the time and divided them among the engineers. The engineers went through the templates used by each controller and made sure string output in templates was marked as html_safe or rendered with raw where appropriate.\n\nMost of the XSS work was done in a single day with all of the engineers\u2019 help, but there was cleanup on pages that weren\u2019t used very often that continued for several weeks after installing the plugin.\n\nWith rails_xss installed, Airbnb was more secure and closer to Rails 3.\n\nThis is the well-known upgrade step that includes updating the \u2018rails\u2019 gem to 3.x. Thankfully our Gemfile was already mature, and our templates expected most strings to be HTML-escaped when we got to this step around Thanksgiving 2011.\n\nWe knew the real upgrade required major code changes, so four engineers started the upgrade on a Saturday afternoon when there were minimal changes being committed. We asked other engineers to hold off on changes over the weekend to prevent any merge nightmares.\n\nWe kept the Rails upgrade branch separate with its own Gemset in case we needed a local working version of master before the upgrade was done. The most basic version of our workflow looked like this:\n\nThings that caused headaches during the upgrade:\n\nThe app was running on Rails 3 by the end of the weekend, and specs were passing, but there were plenty of pages that hadn\u2019t been tested.\n\nMonday morning all of the engineers took ownership of features they had recently worked on and thoroughly tested them similar to how we approached the Rails XSS install. By the middle of the week we merged the rails3 branch into master and deployed it to our production instances.\n\nIf everything goes smoothly, the Rails 3 upgrade should be transparent to users. That can make the upgrade a hard sell to anyone outside engineering considering the effort that goes into it. Installing Bundler was a small project done by a single engineer, but installing the XSS plugin and doing the actual Rails 3 upgrade took considerable hours that had to be planned.\n\nWe coordinated with our product team for each of the big steps. They were planned between major product updates to make sure the codebase wasn\u2019t changing while we were upgrading."
    },
    {
        "url": "https://medium.com/airbnb-engineering/css-box-shadow-can-slow-down-scrolling-d8ea47ec6867",
        "title": "CSS box-shadow Can Slow Down Scrolling \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Working on one of the Chromebooks Google lets you borrow on Virgin America flights, I noticed scrolling down the page on my airbnb.com dashboard was much slower than on my normal laptop. I chalked it up to weak Chromebook hardware, but other sites were scrolling just fine. box-shadow had caused slow scrolling on our search results page before, so I did some investigation.\n\nI used Chrome\u2019s Timeline tab to see the duration of paint events on the page. Before each test I forced a garbage collection and scrolled to the same window position using window.scroll(0, 140). Then I clicked the down arrow in the scroll bar twice, a 40px-scroll per click, and recorded the paint times.\n\nTo see if box-shadow was slowing down scrolling, I cut the blur-radius in half. The scrolling was far smoother, and the numbers showed why: paint events were taking half as long, which meant more paint events per time period.\n\nSince box-shadow was the obvious offender, I tried taking it out entirely.\n\nAnd then I set it to something huge. The Chromebook did not like painting a 300px blur-radius. It took 2 full seconds of paint time per scroll arrow click!\n\nWe dropped the blur-radius for the boxes on airbnb.com/dashboard to 3px and added a 3px offset to get a cleaner look that didn\u2019t tear up performance for devices with less processing power.\n\nYour Chromebook audience is probably pretty small, but Chrome is built on WebKit just like the iOS and Android browsers. If CSS is hurting performance on a Chromebook it\u2019s likely hurting it for mobile WebKit users visiting your full site too.\n\nIn case you try this at home, this is the Chromebook I was using: Google Chrome 14.0.835.204, Platform 811.154"
    },
    {
        "url": "https://medium.com/airbnb-engineering/how-we-improved-search-performance-by-2x-19489abc7213",
        "title": "How we improved search performance by 2x \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "If you haven\u2019t used our moving map search recently you should check it out now, because we made it more than twice as fast! Actually, every search is faster now but it\u2019s most noticeable in map mode. So how did we do that? Let\u2019s start with some background on our setup. We\u2019re a Rails shop, and we\u2019re using Sphinx as our search engine. The two are connected through ThinkingSphinx, an excellent Ruby gem that provides an easy to use query interface, and a DSL for defining indexes. The queries we run are a little bit different from what an average website does, because every single one filters results with spatial constraints (latitude/longitude). We also make heavy use of facets for the various filter options such as room type, neighborhood, or amenities.\n\nSphinx works great for most common use cases, but it\u2019s not optimized for spatial queries. While it gives you some basic functions to query and rank by distance, it doesn\u2019t perform any spatial indexing. The latitude and longitude fields are just floats, and spatial queries have to scan the whole index, which is of course not very performant or scalable. Also, it turns out that the configuration generated by ThinkingSphinx doesn\u2019t allow Sphinx to make use of multiple processor cores. Now while it sounds like this setup doesn\u2019t fit our requirements at all in terms of performance, Sphinx is very fast in general. Rewriting or switching to a different engine wasn\u2019t an option for us at the time so we wanted to make surgical changes to get the maximum out of it. We got help from Vlad and Rich at sphinxsearch.com, who are experts in tuning Sphinx.\n\nThe first objective was to allow Sphinx to use all available processor cores. To achieve this, we split the search index into multiple parts and configured Sphinx to use them as a distributed index. Sphinx then uses one thread to search each partial index, and merges the results afterwards. Here is an example configuration snippet that makes use of two cores:\n\nWhat\u2019s important here is to set dist_threads to the number of processor cores, and to configure one partial index per core. It\u2019s easy to split your data into multiple indexes if you have an id column with auto_increment. Simply use the mod operator in the source config blocks. Another big performance boost came from upgrading Sphinx from 0.9.9 to 2.0. It\u2019s currently in \u201cstable beta\u201d, which basically means that core features are production quality, whereas some newly added features might be less tested. The Sphinxsearch guys recommended it, and since we weren\u2019t using any of the cutting-edge features we felt confident to use it in production. The only downside to those changes is that we had to say goodbye to the ThinkingSphinx index configuration DSL. It doesn\u2019t support these advanced settings.\n\nThere are a few ways to get even more performance out of Sphinx. It has its own query language \u2014 SphinxQL, which allows you to bundle queries and execute them together. This is really helpful for combining multiple facet queries. It would require major changes in our app and getting rid of ThinkingSphinx though, so we\u2019ll save that for a later date. Another way to get more parallelism and scalability is to split the index across multiple machines. This works similar to same-machine distributed indexes and is easy to set up. Although Sphinx has been great for us so far, the lack of spatial indexing will become a problem at some point. We\u2019re currently exploring other architectures that provide this feature. Stay tuned."
    },
    {
        "url": "https://medium.com/airbnb-engineering/switching-to-a-html5-photo-manager-bigger-better-faster-stronger-96a6113b02ee",
        "title": "Switching to a HTML5 Photo Manager: Bigger, Better, Faster, Stronger",
        "text": "Photos are an incredibly important component of listings on Airbnb, and have a demonstrable impact on the effectiveness of a listing; so much so, that we completely foot the cost for professional photographers to take photographs of hosts\u2019 listings. It benefits both us and our users to make it easy as possible to upload and manage photographs of listings.\n\nOur old photo manager was becoming a little long in the tooth. To upload a file, users would have to click on a standard HTML \u201cfile\u201d input field, choose a single file in the Finder (or Explorer, on Windows) window, and then wait for the file to finish uploading and processing before choosing another file to upload. There was no indication of progress during this process; users would be stuck staring at a spinner until the response from the server was received. For large files, this spinner would stay up for quite some time, resulting in a frustrating experience.\n\nWhen the time came to do a once-over of our host tools, the photo manager received the bulk of our attention. We started by researching our users, and discovered that the majority of you \u2014 almost 70% \u2014 were using HTML5-capable web browsers built on the Gecko or WebKit rendering engine. As such, we decided to use a number of HTML5 technologies in implementing the photo manager, which allowed for such interactions as multiple-file drag-and-drop directly from your desktop, and asynchronous file uploads with progress state. We also relied on CSS3\u2019s animation capabilities to smooth out some of the transitions (such as the filling of the progress bar), and to provide some fun feedback about the state of the photo manager (such as emulating the iOS \u201cwiggle\u201d when reordering photos).\n\nAll of these changes in the UI/UX necessitated changes on the server side as well. Previously, all of the photo processing was done synchronously with the request, which increased the load on our nginx web frontends and resulted in elevated response times. We built a new photo processing backend using the delayed_job gem that relies on asynchronous job workers feeding off a queue of photo processing jobs. Our new backend allows us to scale the photo processing completely independently of our web frontends.\n\nTo further lighten the load on the web frontends, we decided to have the photo manager upload files directly to our Amazon S3 bucket. Performing cross-domain file uploads via AJAX requires implementation of the CORS specification in both the browser and the server. To provide some background, the CORS specification precedes any POST request to a server with an OPTIONS request, expecting the server to respond appropriately if CORS is allowed. If the expected response is not received, the browser blocks the followup POST request.\n\nTargeting the photo manager towards HTML5-enabled browsers gives us CORS support in the browsers by default. Unfortunately, S3 does not respond appropriately. Inspired by a blog post on the subject, we worked around this problem by configuring our nginx frontends to respond directly to the browser\u2019s initial OPTIONS request, and then proxy the followup POST request to S3. Here\u2019s the configuration that we used:\n\nUnlike an unproxied (direct) upload to S3, a proxied upload does consume some I/O bandwidth on our EC2 instances, but because we use nginx\u2019s built-in mechanisms to proxy, it does save our Passenger/Rails web application layer from having to process the upload.\n\nThe success of the new photo manager for listings meant that there was a big push to have it rolled out to other areas of the site, such as our user profiles and our new Stories feature, which we\u2019ve been able to do quite easily. The new photo processing backend has been performing admirably, even with the increased load of multiple features utilizing it simultaneously.\n\nOur focus on HTML5 browsers does beg the question: What do we do to support users on non-HTML5 browsers? In cases where a non-HTML5 browser is detected, we fall back to the old synchronous upload endpoint. Since we use CSS3 mainly to prettify transitions, the degraded state will also respond appropriately to user interactions.\n\nWe hope you enjoy our new HTML5 photo manager. Let us know what you think!"
    },
    {
        "url": "https://medium.com/airbnb-engineering/do-it-yourself-ach-direct-deposit-5c9359aad950",
        "title": "Do-it-yourself ACH direct deposit \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "One of the most frequent implementation questions that I am asked by entrepreneurs is how does Airbnb make ACH direct deposits into customers\u2019 bank accounts. There seems to be a wave of transactional marketplaces being built and everyone wants this capability. Surprisingly, there were few, if any, plug-and-play providers of this service when I implemented our solution for Airbnb, though I do know multiple companies who are building products around this now.\n\nThe do-it-yourself solution is quite straightforward though. Most commercial banks have an interface that can be activated by request that allows you to make ACH deposits. Like many startups in the Bay Area, we are a customer of Silicon Valley Bank, and, as a result, our original implementation used the SVBeConnect ACH Service. Each day our system would output a CSV file that contained batched deposit instructions (basically amount and beneficiary account details). Each weekday someone in our office would upload the file to the bank, though this step can be automated using secure FTP.\n\nValidation can be accomplished by issuing a \u201cpre note\u201d (a special type of transaction in which no money is transferred) or by making a micro deposit. If the account details are wrong, a notification is typically generated within 1\u20133 business days. Another manual step is collecting these error notifications each day and notifying the customer. Although it would be nice if an API or XML report existed for this, a manual, but streamlined workflow is sufficient for our needs.\n\nAirbnb migrated off the Silicon Valley Bank ACH platform as our needs grew beyond just ACH, though our current solution is still similar to the above. It is worth mentioning that at scale the cost of ACH is very cheap. We currently pay about 10 cents per transaction.\n\nIf you are going to collect and store bank information, make sure to educate yourself about SSL and how to do encryption properly. The official resource for ACH related information is www.nacha.org."
    },
    {
        "url": "https://medium.com/airbnb-engineering/when-the-cloud-gets-dark-how-amazons-outage-affected-airbnb-66eaf8c0f162",
        "title": "When The Cloud Gets Dark \u2014 How Amazon\u2019s Outage Affected Airbnb",
        "text": "We\u2019re big fans of AWS at Airbnb. All of our infrastructure runs on it, so just like many other websites, we were also affected by this week\u2019s AWS outage. However we survived it with only 30 minutes total downtime, and search being unavailable for three hours. We didn\u2019t lose any data. Compared to what happened to other sites, that\u2019s not too bad. I want to share what exactly happened, and some of the things we had in place that saved us from a more widespread outage.\n\nAmazon didn\u2019t provide too much detail but it seems that the initial problem was that a significant part of the network in two us-east availability zones became unreachable Thursday morning at about 1am. This triggered a number of automated actions. Like many AWS services, Elastic Block Store (EBS) is mirrored across multiple nodes to provide redundancy. If some nodes go down, the service automatically re-mirrors to new instances, to make up for the lost redundancy. This is expected to happen in a distributed system and AWS is built to handle it. Obviously you need to have some spare capacity to enable this. Apparrently the problem was so widespread that there wasn\u2019t enough extra capacity to handle all the re-mirroring. This lead to huge IO latency or completely unresponsive EBS volumes. One of the zones recovered after a couple of hours, but the other zone was still having issues on Friday.\n\nWe use RDS for all our databases and it runs on top of EC2 and EBS. Our databases became unresponsive when the event first happened, which took Airbnb down for 18 minutes starting at 12:50am Thursday morning. After that, the master db recovered and our website was back up because our app servers only use the master. Our slave db was hit harder and it remained unresponsive until Friday. We use it for analytics and background tasks. Our background tasks were failing until the morning hours which delayed outbound emails, reminders, etc. for up to 8 hours.\n\nThe next partial outage happened Friday morning at around 6am when our search daemon crashed. Search on Airbnb was unavailable until about 9am when we brought it back up. The rest of the website was working fine during this time.\n\nThe third outage happened Friday at 6pm. This time, the site was down for 11 minutes. Again, the master db became unresponsive due to EBS problems, and every query eventually timed out. In a twist of fate we deployed code during the outage, which stops app server processes running with old code, and starts fresh ones with new code. Because the old processes were stuck waiting for their queries to go through, they didn\u2019t terminate and were still running when the new processes came under full load. We ended up with double the number of processes, which exhausted the memory on the app servers. Some of the new processes died because they were out of memory, which made things worse.\n\nThe web frontend recovered by itself after the first downtime. To fix the background tasks, we pointed them to the master db, which was working fine. The Friday morning search downtime had the same underlying problem, so pointing the indexer to the master db until the slave was back fixed the problem. Friday afternoon we had to go in and manually kill the dead processes, then restart the site.\n\nThe main reason why things stayed pretty much under control is that we spent some extra money on redundancy of our data store. We blogged back in November about how we moved our database to RDS with a setup that provides redundancy and failover across multiple availability zones. We are very happy today that we did this migration. Problems would have been a lot worse with the old setup. While AWS services are extremely reliable, availability zone failures sometimes happen. This is nothing specific to Amazon or other cloud services. They run on servers and networks that are also found in regular datacenters, and these things can fail. It\u2019s the responsibility of the engineers who build on these services to install proper procedures to handle failure.\n\nWhile things went ok this time, we also identified some things that we could have done better. We\u2019ll follow up with another post about these changes."
    },
    {
        "url": "https://medium.com/airbnb-engineering/the-iphone-app-backstory-e6741bd0ae64",
        "title": "The iPhone App Backstory \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "We recently released our iPhone app. We are happy to report that not only has the feedback been very positive, but the app has been featured by Apple and was one of the top 5 apps within the travel category of the App Store.\n\nOne of the principles that I believe helped us gain such exposure was the idea that we weren\u2019t going to ship until we could truly impress people. It is one thing to impress your existing (passionate) users, but it is an even higher bar to convince new visitors to join based only on the experience that is offered within the app. There\u2019s a lot of noise in the App Store, so we had to be sure that we stood out. We showed the app to a lot of people and iterated many times before we were convinced that we had achieved this.\n\nSome people have asked what went into building the app. It actually started out as my summer internship project, an idea that was born from the fact that I had been building apps since the SDK was launched. By the end of the summer, the team was extremely excited by the amount that I had accomplished, and I decided that I should postpone returning to school to see the app through to completion. It took an additional two months of serious effort and help from visual designer Steph Tekano and Joe Gebbia, who helped work through the user interaction, to make the app into what you see today.\n\nWe are excited to see how people use the app to book and manage their travels."
    },
    {
        "url": "https://medium.com/airbnb-engineering/mysql-in-the-cloud-at-airbnb-336e5666bc94",
        "title": "MySQL in the cloud at Airbnb \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Airbnb is a rapidly growing marketplace and our infrastructure needs are evolving. Like many other websites, we were using MySQL as our main datastore, but we recently migrated it to RDS. RDS (Relational Database Service) is a web service by Amazon that lets you run a scalable MySQL setup in the cloud while hiding the administrative overhead that usually comes with it. This blog post is about the challenges we faced with MySQL, how RDS solves them, and how we migrated to RDS from our old setup.\n\nBefore we switched to RDS, Airbnb was running on a single MySQL instance on top of Amazon EC2. We have dozens of background tasks that handle things like payments and analytics. Many of the queries that are run by these tasks are expensive SELECTs, and we were starting to notice an impact on frontend performance. MySQL has an easy solution for read-intensive applications (as most websites are): just add a read-only slave server that uses asynchronous replication and divide read queries between the servers\u2026 Except setting up replication takes some care and on-going attention. Our engineering team is already maxed out, so adding that much complexity didn\u2019t seem like a good idea.\n\nWhile EC2 is generally very reliable, there is always a chance of an instance failure. The recovery process we had in place was completely manual. We would have had to launch a new instance and restart the site after we changed the host name. Our data backup solution for the old setup was to take a snapshot of the underlying EBS volume every couple of hours. In other words, the site would have been offline for a couple hours and there was a possibility of data loss.\n\nAs a website grows, the database eventually becomes a bottleneck. Scaling horizontally (sharding) is always a delicate task, and scaling vertically with our old setup would have involved a significant amount of planning and manual work. In order to have consistent data, we would have had to take the site offline, take a snapshot, then launch a bigger instance with that snapshot.\n\nAmazon RDS supports asynchronous master-slave replication, and it can be launched with the click of a button or an API call. The time-intensive administration tasks are all handled by RDS, and our engineers can spend more time developing features.\n\nRDS supports Multi-AZ deployments (multiple availability zones), where it transparently keeps a hot-standby master in a different AZ that is synchronously updated. Unlike asynchronous replication, there is no replication lag, meaning that the hot standby always has the same data as the live master. In case of an instance failure, network outage, or even unavailability of the whole AZ of the master, the hot standby is automatically promoted to be the new master. The fail-over process is completely automated: AWS changes the CNAME record of the master under the hood.\n\nA Multi-AZ setup gives you a good amount of redundancy, but there is always a worst-case scenario. What happens if the hot standby fails too? For those rare cases (or when you don\u2019t want to spend the extra money for Multi-AZ), RDS provides point-in-time recovery, which lets you boot a new database instance using a consistent snapshot of your data at any time within your data retention period. Yes, any second you want, and up to about five minutes before a failure! The data retention period is one day by default but can be extended by up to 8 days. In addition to that, RDS allows you to pull a consistent snapshot of your data at any time, much like an EBS snapshot. This is great for archiving.\n\nRDS provides a number of different instance types, similar to EC2. You can start with a small, 1.7GB 1-core server, and scale up to a massive quadruple extra large 68GB 8-core. This range should cover our needs for quite some time. This operation inevitably causes some downtime, but since the whole process is automated, it is kept to a minimum. And again, there is an API for that.\n\nEverything is straightforward if you are setting up a site for the first time, but what if you have an existing database with gigabytes of data that you want to migrate to RDS? This is where the fun begins. At the time we did the migration, we already had gigabytes of data and some tables with millions of records. RDS instances only expose MySQL and don\u2019t give you direct access to the machine via SSH or other tools, so unfortunately there is no way to do this on the file system level. The only practical way to move your data over is to do a mysqldump on the source machine, followed by a mysqlimport in RDS. This can take a significant amount of time if you have a lot of data, and unless you want to spend days or weeks figuring out a complex migration strategy, the only option is to take down the site during the process. We wanted to keep the downtime as low as possible, so we looked for simple ways to transfer a significant amount of our data while the site was still running. We came up with the idea to copy the data in tables that never receive UPDATEs or DELETEs, so we would only have to copy the diff later on. Once those tables were carefully selected, the procedure was pretty straightforward. The actual script that we used to do our migration is available at https://gist.github.com/671874\n\nThis approach allowed us to perform the whole migration with only 15 minutes of downtime. The new setup eliminates the effects of long-running queries on our frontend performance, and generally seems to perform significantly better than off-the-shelf MySQL on EC2. Even more important than that, we are now well prepared for our future growth."
    },
    {
        "url": "https://medium.com/airbnb-engineering/airbnb-engineering-blog-first-post-4d338cd82b84",
        "title": "Airbnb Engineering Blog \u2014 First Post \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "Welcome to the Airbnb engineering blog! Here we will be documenting some of the challenges that we tackle that may be useful and interesting to the broader community. These posts will come directly from our team members, and thus I hope to both give you direct access to them and give exposure to their individual contributions. In the process of doing so I hope you will get a better sense of who our team is, what engineering is like at Airbnb, and why we are all so excited about what we are working on."
    },
    {
        "url": "https://medium.com/airbnb-engineering/hard-problems-big-opportunity-4e1fac7fe75e",
        "title": "Hard Problems, Big Opportunity \u2013 Airbnb Engineering & Data Science \u2013",
        "text": "This post is being re-posted from our general audience blog.\n\nAirbnb is creating a new marketplace for space, and is facing many of the challenges that eBay faced back in 1998 when they created a new marketplace for goods. Though Airbnb doesn\u2019t move petabytes of data or process thousands of messages per second (yet), we facilitate a massive amount of commerce. This post is for the software engineers in the audience who are curious about the types of challenges that we face.\n\nHere are some illustrations of how we are innovating:\n\nPayments. Each month Airbnb collects and pays millions of dollars in over 100 different countries. To facilitate this exchange we integrate with dozens of local payment providers and maintain bank accounts in several currencies. What are the local services we can leverage? What are the benefits/limitations of those services?\n\nSearch. In New York City alone Airbnb has 3,600 diverse properties \u2014 \u2014 more choices than any other accommodation site. How do we compute relevance when the factors are at a minimum a combination of availability, location, price, reputation, type of space, etc? How do we sort by price when it is a function of dates and a multitude of business rules? How do you make the comparison process manageable to the user?\n\nStandardized Processes. Airbnb has over 40,000 available properties. The property owners ultimately dictate the quality of the Airbnb experience. Some of the owners are first-time renters; others have rented for years and have their own modus operandi. How do we get them to adopt a standard set of operating practices relating to cancelations, cleaning fees, communication, payment, scheduling, security deposits, etc? How do we train them to cooperate through a balance among education, incentives, and penalties?\n\nFraud. With commerce comes fraud. Deviants from around the world are trying to \u201cbeat\u201d our system everyday. We have to proactively identify suspicious behavior and quarantine users until additional verification steps can be taken. What are the patterns that we can identify? How do we avoid false positives? How do we allow users to prove their innocence? How do we share our intelligence with the greater community?\n\nUser Acquisition. Airbnb makes money when owners make money. Airbnb has a unique product that converts visitors to travelers very well. How do we use the targeting capabilities of ad networks combined with our backend tracking capabilities to maximize ROI? What we can learn from click tracking? How can we monetize our existing user base through follow up marketing? What network effects exist?\n\nWe are seeking talented engineers to solve problems like these. Please check out our job openings."
    }
]