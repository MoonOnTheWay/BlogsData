[
    {
        "url": "https://medium.com/@nxp251/language-modeling-using-lstm-dbc3190c417b?source=user_profile---------1----------------",
        "title": "Language Modeling Using LSTM \u2013 Nachiketh Prabhakar \u2013",
        "text": "In this blog post I will be talking about Long Short-Term Memory Neural Networks, their application in language modeling and my experiments with Children\u2019s Book Test (CBT) dataset published by Facebook.\n\nA language model is a statistical model that assigns a probability to a sequence of words by generating a probability distribution. You can use language models in different systems like for speech recognition, machine translation, information retrieval, word sense disambiguation etc. This is a challenging task as the amount of possible word sequences are high i.e it is a high dimensionality problem. The goal of a language model is to ensure that the output of these systems not only convey meaningful words and the correct lexical choices but also that these words are in the right order.\n\nRecurrent Neural Networks (RNNs) : They are a class of Artificial Neural Networks which have loops in them so that the information can persist. They are used to model sequential data. Below is an image of RNN, on the left side is the loop version and on the right side is the unrolled version.\n\nLong Short-Term Memory Neural Networks (LSTMs) : LSTMs were introduced by Hochreiter and Suchmidhuber in 1997 and were refined and popularized by many people. LSTMs were explicitly designed to handle long-term dependencies problem. The long term dependencies problem arrises due to vanishing or exploding gradients as the gradient flows back in time. RNNs have this problem and therefore cannot learn from the previous information. LSTMs were designed to remember information for a long time. Below is an image of a LSTM Network.\n\nIn the above diagram, the top horizontal line is the cell state. Information can be added to the cell state which runs through the entire chain. The information is added /removed using gates. The gates are composed of a sigmoid neural network and a point wise multiplication operation. The different gates are:\n\nForget Gate \u2014 This sigmoid gate decides what information is to be kept.\n\nInput Gate \u2014 This sigmoid gate decides which values are to be updated.\n\nG gate \u2014 This is the tanh layer gate that creates a vector of new candidate values, that could be added to the cell state.\n\nOutput Gate \u2014 This sigmoid layer decides what parts of the cell states are going to be outputted.\n\nLSTMs take vector from below in depth and before in time concatenate them and multiply with weight matrices. LSTMs can be summarized by the equations below:\n\nI have used TensorFlow for this task. TensorFlow is an open source software library for machine intelligenceigence. The code for this task can be found in my github account here.\n\nThe CBT dataset is 25.7 MB, contains 5.2 million words out of which 68031 are distinct. Training such a huge file on a machine without GPUs takes a long time, therefore I experimented by creating smaller datasets.\n\nDataset \u2014 I reduced the dataset to contain only 10 stories, this reduced the size from 25MB to 4.4MB. Converted all the words to lower case and removed words with hyphens in them.\n\nModel Configuration \u2014 These are the various parameters that can be tuned in order to make the performance of the system better. I experimented with different configurations. The configuration chosen affects the training time and the quality of results.\n\nMetric \u2014 Language Models are evaluated based on average perplexity across all words in a text. The less probable words get higher perplexity values. When you compare perplexity values the ones with lower perplexity are better modeling the given language. Other than this, the quality of language model can be assessed directly in the application it is being used, in this case generating children stories.\n\nI trained 2 different models of configurations small and medium. For the small configuration, the perplexity values on the training set was 74.998, on the validation set it was 168.931, on the test set it was 168.436 and took 1.61 hrs to train. A sample of the text generated is shown below.\n\nFor the medium configuration, the perplexity values on the training set was 54.982, on the validation set it was 117.173, on the test set it was 120.760 and took 9.2 hrs to train. Below is the sample of the text generated.\n\nEven though the results don\u2019t seem exceptional they give an insight into the capabilities of LSTMs and RNNs in real world applications. Training the model longer and with other configurations would yield better results. I hope this post gave you an insight into Language Modeling, RNN and LSTMs."
    },
    {
        "url": "https://medium.com/@nxp251/introduction-to-chatbots-b8af8259490f?source=user_profile---------2----------------",
        "title": "Introduction To ChatBots \u2013 Nachiketh Prabhakar \u2013",
        "text": "In this post I will introduce you to chatbots and show you an example of a simple ChatBot. Hope this post will show you how easy it is to build your own bot and encourage you to go on and develop your own chatbot.\n\nWhat is a chatbot? A brief history.\n\nA chatbot is a computer program that can interact with a human through a chat interface and is designed to stimulate a human. The very first chatbot dates back to the year 1966 developed by Joseph Weizenbaum and was know as ELIZA. The name comes from a character in Bernard Shaw\u2019s Pygmalion. If you have read the book, I believe you would get the reference. If you haven\u2019t that\u2019s alright, it is not a prerequisite for this post. Eliza was regarded as one of the first programs that was capable of passing the Turing test. Inspired from ELIZA came ALICE in 1995, from then on there have been numerous chatbots and nowadays chatbot frameworks and platforms have come to life.\n\nHow does a chatbot work? Anatomy of a chatbot.\n\nYou as an user of a chatbot can ask a question via one of the messaging platform like Facebook messanger or WeChat or Skype or even by just sending a SMS. If you want to know the weather then you ask - What is the weather today in Cleveland?\n\nYour question is then parsed by the parser and is converted into structured data. In this example, from your question it can be inferred that the intent in the message is weather, location is Cleveland and date is today. The parser can match the pattern using Regex. But, here is where AI can do a way better job. To be more specific Natural Language Processing and Machine Learning. You can think of this as a supervised learning problem.\n\nThe responder which has the output sentence structure, receives the above structured data, along with the data from Action which provides the weather API data. The responder returns the answer to your question to the bot and you can see this answer in your messaging platform.\n\nAn example of a ChatBot built on ChatFuel platform \u2014\n\nChatFuel is one of the many platforms that allows you to create an AI chatbot quickly on Facebook platform. On one side you have the human who asks questions like what kinda music do you like to listen to? This question goes to NachoBot and from there it goes to Chatfuel and response such as \u201cI like to listen to classical rock\u201d is sent back. Chatfuel is the brain that does all the work. It understands what the human has said and responds back with the answer. Chatfuel is not inherently smart and you have to train it to understand questions and answers.\n\nFirst step is to create response cards on ChatFuel. For example when you say Hi! You need a response such as Hi to you too! or How are you today? The next step is to map all similar words or sentences to a response card. This is done using Natural Language Processing. For example \u2014 Hi!, Hola! Hello, Hi there will all map to the same response card. Chatfuel allows you to connect to facebook messenger and test out the bot you have built. I believe you know enough to go and develop a simple chatbot on your own.\n\nAs you can see above I am chatting with a Bot. Voila! Its so simple right. Go on and try to develop your own chatbot."
    },
    {
        "url": "https://medium.com/@nxp251/developing-a-self-driving-car-ai-using-pytorch-and-kivy-53466383ae8f?source=user_profile---------3----------------",
        "title": "Developing A Self Driving Car AI Using PyTorch and Kivy",
        "text": "In this post I will be talking about how to develop a simple self driving car AI using PyTorch and Kivy.\n\nMy goal is to develop an Artificial agent that makes roundtrips from Airport(Top left corner of the screen) to Downtown(Bottom right of the screen). The main concepts you need to know are Reinforcement Learning, Markov Decision Process and Q-Learning.\n\nReinforcement Learning \u2014 Learning from the feedback given by the environment.\n\nQ-Learning \u2014 For a couple of actions \u2018a\u2019 and states \u2018s\u2019 (a,s) we will associate a numeric value Q(a,s). We say the Q(a,s) is \u201cthe Q-value of the action a played in the state s\u201d. The Q value at any given time \u2018t\u2019 is obtained by updating the Q value at time t-1 with the Temoporal difference.\n\nNow that you have a basic idea of the concepts required to develop the self driving car, I\u2019ll dive into the implementation of it. The basic componenets required are the car, map, sensors to locate what is straight, left and right of the car and some sand so the we can give the car some obstacles to avoid. Below is an image of the AI car on the map along with the 3 sensors.\n\nIn this post I\u2019ll walk you through the main classes required to implement the AI and the map. For the complete code you can find it in my github profile.\n\nLet me first talk about the AI which is implemented in ai.py. This is the file where the car is trained to self drive. Here, I start with the class Network which inherits all the modules from the torch.nn. I provide the network with 5 inputs which are 3 signal values from the 3 sensors and positive and negative orientation. The orientation is the angle of the car with respect to the goal. For this task I have chosen a neural network with a single hidden layer with 30 neurons (feel free to experiment with this) and an output layer with 3 neurons.\n\nNext I implement the class ReplayMemory which is conceptually the experience replay. Instead of considering just the previous transaction we consider the previous 100 transactions, as one time stamp is not enough to understand the long term correlations.\n\nNow comes the main part of the program. The DQN class that is the Deep Q Network class which basically trains our car to drive. It has 3 primary methods one is the select_action method which takes in the state of the car as input and output the right action. I use a softmax function, so as to get the best action to play at each time, but at the same time the car will explore different actions. I then implement the learn function which will train the deep neural network. I am using Adam Optimizer along with smooth_l1_loss loss function (feel free to experiment with this).\n\nI then implement the score function which computes and returns the mean of all rewards in the reward window. Finally, we\u2019ll need the save and load method which are used to save the model and load it at a later state respectively.\n\nNow that the AI is completed, let me briefly walk you through the map.py file which you need to run to see the AI in action. The two important classes in this file are the car and the game classes. In the car class I initialize the car\u2019s angle, rotation, velocities, sensors and signals. Then I update them when the car makes a movement in the move function. The game class contains the important update function which updates everything at each time t when reaching a new state. The UI was developed using Kivy. You can learn more about kivy here.\n\nFor the complete code check out my github profile. The code is heavily commented with the concepts, to give you a clear understaning of what each line does. Feel free to play with it. I learnt how to develop this simple app from a tutorial on Udemy."
    },
    {
        "url": "https://medium.com/@nxp251/image-recognition-with-keras-2-0-ae897b228f8c?source=user_profile---------4----------------",
        "title": "Image Recognition With Keras 2.0 \u2013 Nachiketh Prabhakar \u2013",
        "text": "In my last post I spoke about how to build a neural network using TensorFlow. In this post I\u2019ll tell you about how I built a simple image recognition model using Keras with TensorFlow. Before that let me introduce you to Keras.\n\nKeras is used to build state of the art machine learning models with just a few lines of code. Keras is the front end layer written in python that runs on popular deep learning toolkits like TensorFlow. When you use Keras with TensorFlow, it builds a TensorFlow model and runs the training process. Keras comes with several pre-trainined deep learning models for image recognition. You can adapt these models to create a custom image recognition system with your own data.\n\nWhen is using TensorFlow a better choice?\n\nWhen is using Keras a better choice?\n\nSo I would say, unless you have highly specialized needs or you are building a large system for millions of users, it is better to use Keras.\n\nKeras provides you with several built-in image recognition modules. They are all trained to recognize images from the imagenet data set. The imagenet data set is a collection of millions of pictures of objects that have been labelled, so that you can use them to train your system.\n\nIn this post, I will show you how I used the ResNet50 model to build my image recognition system. ResNet50 model was developed by Microsoft and is one of the best models available to recognize images.\n\nFirst let me import all the required modules from Keras.\n\nNext I will load the Keras ResNet50 model which is pretrained against the Imagenet database.\n\nNow lets load an image which we need to recognize, ResNet50 requires us to provide the input as a 224 X 224 pixels image. After this I convert the image into a numpy array. Keras expects a list of images, and therefore I\u2019ll add another dimension to the input. I\u2019ll scale the input to the same range used in the trained network.\n\nNow, I input the image array through the deep neural network to make a prediction. Finally, I\u2019ll get the value of the predicted classes and output them.\n\nAmazing isn\u2019t it! With just a few lines of code we have managed to build an image recognition system.\n\nHope you have got an understanding of what you can do with Keras and when you should use it. Go ahead and try Keras out today! Leave your comments and suggestions below!"
    },
    {
        "url": "https://medium.com/@nxp251/building-a-neural-network-in-tensorflow-d87d74b08420?source=user_profile---------5----------------",
        "title": "Building A Neural Network In TensorFlow \u2013 Nachiketh Prabhakar \u2013",
        "text": "Hey! This is my first post on medium. I will be talking about how I built a ReLU (Rectified Linear Unit) Neural Network, to predict the sales of a video game given the statistics of similar video games.\n\nTensorFlow is a software framework for building and deploying machine learning models. It is typically used to build deep neural networks. Python is the best supported and easiest language to use. I have used Python3 along with scikit-learn and pandas.\n\nThe training data set has 1000 samples and the test data set has 100 samples of 8 features and the target value of total_earnings. We calculate the total_earnings using the features shown in the picture below:\n\nFirst I import all the libraries required\n\nThen I read the training and testing data, followed by scaling it to a small range between 0 to 1 for it to work well. Make sure both the training and test sets are scaled to the same scaler.\n\nThen I initialize the hyper-parameters \u2014 learning_rate and training_epochs (number of iterations), followed by the number_of_inputs, number_of_outputs, layer_1_nodes, layer_2_nodes, layer_3_nodes.\n\nNow that I have everything initialized, I will add the layers of the neural network. This is a 4 layer network with 3 hidden layers and 1 output layer.\n\nAfter this I compute the cost \u2014 this is the mean of the squared differece between the predictions and the provided target values.\n\nOur goal is to minimize the cost function. To do so, I will use the AdamOptimizer for gradient descent.\n\nNow I initialize the session. A session is a class for running tensorflow operations. Inside session, I first initialize all the variables and layers of the neural network. I create 2 files training_writer and testing_writer to store the logs of training and testing. Now I run the optimizer and calculate the training and testing cost over the scaled training and test set.\n\nNow that the model is trained, I use it to predict the sale price on the test set and finally print out the predicted value and the real value. Finally I save the model as exported_model to use it in other data samples.\n\nAs you can see the prediction of our model is pretty close to the real value!\n\nThis is the first of a series of posts from my side. Hope this was informative and gave a good understanding on how to build Neural Network in TensorFlow! Please provide your reviews and comments below!"
    }
]