[
    {
        "url": "https://medium.com/@rgotesman1/learning-machine-learning-part-6-bias-variance-and-error-metrics-51321efc71e8?source=user_profile---------1----------------",
        "title": "Learning Machine Learning \u2014 Part 6: Bias, Variance and Error Metrics",
        "text": "Week 6 of the course was much lighter on math and focused on pragmatic decisions you will have to make when designing ML programs. I found this week very interesting and suspect it conveyed some of the most useful information of the course.\n\nThe first topic that we covered was the idea of machine learning diagnostics which are various ways of assessing the characteristics of your ML model. To calculate these characteristics we need to split our data into 3 sets: a training, validation and test set. Normally we allocate the majority of the data, say 60%, to the training set and split the remainder between the validation and test sets.\n\nSay we were interested in developing linear and quadratic functions to model our data. First we find the optimal parameters of these 2 candidate models based off the training set. We then test these models on the validation set and pick the one with the highest accuracy. Finally, we run this chosen model on our test set to see how generalizable it is to other data. You might ask why we split the data in this way and the reason is to ensure we preserve some data we have never seen before that can be used to test the accuracy of our model. If you test the accuracy of your model on any data that was used to train it, the accuracy will likely be exaggerated and we want to have an unbiased measure of the model\u2019s performance before testing it in the field. Splitting the data allows us to do that.\n\nWe can now introduce some important terms: bias and variance. Both are bad for different reasons. We say a model has high bias when it is too simple to capture the complexity of the data. This would be like using a linear model for data that has a quadratic form. In short, high bias means our model underfits the data. In contrast, high variance is when our model is too complex like using a 100 degree polynomial for that quadratic data. In this situation we overfit the data. This can be visually understood by considering the graphs below. Bias and variance are a problem because a model that has too much of either fails to generalize. It is common to refer to the bias-variance tradeoff because when we have more of one we have less of the other.\n\nAfter learning about these concepts I realized bias and variance had been with us since the start of the course. If you consider the cost function for regularized linear regression:\n\nyou\u2019ll see that it can be rewritten as:\n\nwhere, in a way, \u03bb dictates how we go about balancing the bias-variance tradeoff. Consider if \u03bb is very large. Then to minimize J we know we must make B small, but this means making most of our parameters small in which case we are dealing with a model that is more prone to having high bias rather than high variance. In contrast, if \u03bb is small, B is not much of a concern and instead we want to minimize A. The better fit our model is to the training data the smaller A will be and we see that in this case our model is more likely to have high variance than high bias. Finding the optimal value of \u03bb to balance the bias-variance tradeoff is key to coming up with a good model and can be achieved by testing various values of \u03bb in the validation set.\n\nWe can test if our models have high bias or variance through the use of learning curves. To create a learning curve we take m random examples from our training set, train our candidate model and calculate the error of said model on the m training examples and the entire validation set. We keep doing this for larger and larger values of m and plot the training and validation errors as a function of m. Different graphs will emerge depending on whether your model suffers from high bias or high variance.\n\nFor instance, with high bias, regardless of how much training data you have the model will be a poor fit. You can have all the data in the world but if you\u2019re only interested in horizontal lines you can\u2019t model much. So as m increases the training error will get large. The model will be just as bad in the validation set so we expect the validation error to be similarly large. If we consider what happens when our model has high variance we\u2019ll note that the training error will be very low, since we\u2019re overfitting, and as m increases the error will slowly creep upward. Turning to the validation error, since our model is overfitted it poorly generalizes so the validation error will be large. As m gets larger the model will learn from more representative data and the validation error will start to move down. These 2 scenarios are captured in the graphs below."
    },
    {
        "url": "https://medium.com/@rgotesman1/learning-machine-learning-part-5-backpropagation-5d07501b579a?source=user_profile---------2----------------",
        "title": "Learning Machine Learning \u2014 Part 5: Backpropagation",
        "text": "For week 5 of the course we continued to cover Neural Networks and focused on the backpropagation algorithm. I found backpropagation the most interesting, but also most challenging, algorithm learned thus far. I really struggled to understand how it worked and Andrew seemed to be pulling equations out of thin air.\n\nEventually I was lucky enough to stumble on the 1986 Nature paper where Geoffrey Hinton (the father of modern machine learning) first proposed backpropagation. I\u2019d strongly encourage everyone to read it. After doing so several times I walked away with a much greater understanding of backpropagation and was even able to write and publish an article about it on the Medium publication Towards Data Science!\n\nI won\u2019t go over it again here but essentially, by applying some properties of the chain rule (from calculus), we can use the error between what our neural network outputs and the true output to calculate the gradient of the cost function. I found it really cool how we can use the transpose of the weight matrices used in forward propagation to propagate the errors backwards. Once we have calculated the gradient of the cost function we can apply it to gradient descent as usual and optimize our weights.\n\nWe also went over the idea of symmetry breaking when it comes to initializing the weights of the network. Unlike linear or logistic regression, where we can simply set our initial weight vector to 0, in neural networks we need to be careful that our weights are not symmetrical. If you work through the math you\u2019ll see this is because it leads to nodes representing the same features, making our network highly redundant and incapable of learning a variety of features. We can get around this by randomly initializing our weight vector to some small values.\n\nOne last idea covered this week was gradient checking. Implementing backpropagation is complicated and you want a way to make sure that the gradient your algorithm is outputting is indeed correct. We can achieve this by using the limit definition of a derivative:\n\nEssentially what we do is pick a really small value, perhaps 0.0001, and use that as h in the equation above where J is the cost function. This will give us an estimate of the gradient and if our backpropagation algorithm returns the same value we know we have implemented backpropagation properly. It\u2019s important to note that we only use gradient checking to make sure backpropagation is working correctly for one or two cases in our training set and that we deactivate it before fully training our neural network. That is because gradient checking is computationally costly and we don\u2019t want to slow backpropagation down. Once we\u2019ve verified backpropagation is working, we have no more need for gradient checking.\n\nThat is all for now. Onward to week 6!"
    },
    {
        "url": "https://towardsdatascience.com/learning-backpropagation-from-geoffrey-hinton-619027613f0?source=user_profile---------3----------------",
        "title": "Learning Backpropagation from Geoffrey Hinton \u2013",
        "text": "All paths to Machine Learning mastery pass through back propagation.\n\nI recently found myself stumped for the first time since beginning my journey in Machine Learning. I have been steadily making my way through Andrew Ng\u2019s popular ML course.\n\nThen we got to neural networks and the algorithm used to train them: backpropagation. Despite Andrew\u2019s best efforts, I just couldn\u2019t grasp how the technique worked. Though Andrew assures us this is fine, that you can use neural networks without this deeper understanding and that he himself did so for a number of years I was determined to gain a better grasp of this concept.\n\nTo do so I turned to the master Geoffrey Hinton and the 1986 Nature paper he co-authored where backpropagation was first laid out (almost 15000 citations!). I\u2019d encourage everyone to read the paper. It\u2019s pretty short, only 4 pages, and after studying it in detail I came away with a much better understanding of backpropagation that I shall now try to impart.\n\nThe chain rule is a fundamental property of derivatives taught in any undergraduate curriculum. It states that if you have 3 functions f, g and h with f being a function of g and g being a function of h then the derivative of f with respect to h is equal to the product of the derivative of f with respect to g and the derivative of g with respect to h. This can be neatly written as:\n\nNow let\u2019s use this to figure out how backpropagation works.\n\nLet\u2019s imagine we are working with the simple neural network below. This network has 3 layers: an input layer in blue, a hidden layer in green and an output layer in red. Each unit in the previous layer is connected to each unit in the following layer. Every connection has a weight and whenever a unit sends information forward to another unit, that info is multiplied by the weight of that connection. A unit takes the sum of all the inputs connecting to it from the previous layer, performs a logistic function on this sum and passes the value forward.\n\nLet\u2019s imagine we are given m training examples:\n\nwhere x and y are 3 dimensional vectors and i refers to the i-th training example. For input x, we shall call the prediction/output of our neural network g and it is also a 3 dimensional vector with each vector element corresponding to an output unit. So for every training example i we have:\n\nWe want to find the values of the weights that lead to g being equal (or at least very similar) to y, given input x.\n\nTo do this we shall use an error function that we define as:\n\nTo calculate the total error we take every case i in our training set, and for each unit in the red output layer, compute the squared error between that unit\u2019s prediction and the true output. If we do this for every case we will get the total error.\n\nSince g, the value predicted by the network, is dependent on the weights of the network, we can see that the error will vary with the weights, and we want to find the weights that minimize E.\n\nWe can do this using gradient descent. But gradient descent requires us to find the derivative of E with respect to each weight. And that is what backpropagation allows us to achieve.\n\nWe shall now consider a single case and rather than 3 output units, assume we have an arbitrary number of output units n. For that single case our error now becomes:\n\nwhere we have removed the index superscript for convenience, since it is constant.\n\nWe might now ask ourselves, how does this error vary as the prediction of one of the output units vary. This is simply the derivative:\n\nInterestingly, it seems that as the predicted value of an output unit changes, the error will change at a rate equal to the \u201cerror\u201d or difference between the predicted value and true value. Cool!\n\nNow we might ask ourselves how does the error vary as the total input to one of these output units vary. Again, this is simply a derivative. Let\u2019s use z to refer to the total input to an output unit. Then we are interested in finding:\n\nbut since g is a function of z by applying chain rule we can rewrite this as:\n\nRemember how we said each unit applies the logistic function to its input before passing it forward. That means that g is a logistic function and z is its input so we can write:\n\nSo now we can say:\n\nand we have figured out how the error changes as the total input to an output unit changes. Amazing!\n\nWe are now in a position to find the derivative of the error with respect to some weights. Which is what we need for gradient descent.\n\nLet us call the prediction/output of a green unit g\u2019 and we\u2019ll refer to the weight of the connection between unit k in the green layer and unit j in the red/output layer as:\n\nConsider the total input z to the output unit in yellow below. We can calculate the input by taking the output of each green unit, multiplying it by the weight of the red arrow connecting the green unit to the yellow unit and adding them all up.\n\nRather than 4, if we have an arbitrary number of green units n (this n is different than the one we defined before) we can then say that:\n\nso it seems we can write z as both a function of the weights connecting to it and as a function of the output of the units connecting to it.\n\nWe ask ourselves how does the error vary as a weight connecting to an output unit varies. This can be written as:\n\nWe have just figured out the derivative of the error with respect to the weights connecting to the output layer. This is just what we needed to make gradient descent work.\n\nBut we\u2019re not done yet. We still need to figure out the derivative of the error with respect to the weights connecting the 1st and 2nd layers.\n\nThankfully, chain rule isn\u2019t done yet either.\n\nWe calculate how the error varies as the output of the k-th green unit varies as:\n\nand since there are j weights coming out of unit k we take this into account as:\n\nIf you think about it we are right back where we started. We have the derivative of the error with respect to the output of some unit. We can now completely ignore the red output layer, treat the green layer as the last layer of the network and repeat all our above steps to compute the derivative of E with respect to the incoming weights."
    },
    {
        "url": "https://medium.com/@rgotesman1/learning-machine-learning-part-4-neural-network-theory-f8ce728d7059?source=user_profile---------4----------------",
        "title": "Learning Machine Learning \u2014 Part 4: Neural Network Theory",
        "text": "This is a continuation of my Learning Machine Learning series. You can find Part 3 here.\n\nWeek 4 of the course covered the theory behind neural networks, a powerful technique for finding a non-linear hypothesis for your data.\n\nThe trouble with non-linear hypotheses is they explode your feature space. For instance, if you want to include quadratic terms and your data set has n features, there will \u2248 n\u00b2 terms. If n is 1000, suddenly we have a million terms in our model. Which is a lot.\n\nNeural networks offer an alternative way of finding non-linear hypotheses without having to include all million terms in your model. As well, rather than requiring the data scientist to decide a priori they want to include second or third order terms, the neural network will include ordered terms that best fit the data. It is more flexible in this way.\n\nNeural networks are all the rage these days and what I find most amazing is they can be applied to all sorts of datasets whether they be visual, audio or text. One example of where neural networks can be used is hand written digit recognition. Though I\u2019ve trained a network to do this on the MNIST dataset using Keras, I would like to implement one from scratch to get a deeper feel of how the network learns its features.\n\nThe motivation behind neural networks is the greatest learning machine humanity has discovered: our own brains. The neurons in a neural network are modelled after a human neuron with its dendrites for receiving inputs, cell body in which the the signals are summed and axon to carry the signal forward. Similarly, in a neural network each neuron receives inputs, processes them using some function (for this week it was a logistic function) and passes the output forward.\n\nThe way that the network is structured is referred to as the network\u2019s architecture. We break the network up into different layers. The first layer is called the input layer. The last is dubbed the output layer. Every layer in between is called a hidden layer and there can be as many hidden layers as you want. In the diagram below we have a neural network with an input layer, 1 hidden layer and an output layer. Note that layers only pass values forward and never back.\n\nIn this neural network each node connects to every node in the next layer. Though not shown, each path from one node to the next carries a weight. You can imagine that for the signal to move from one node to the next it must multiply its value at the starting node by the weight of the path and this product is what reaches the end node. Each node receives several of these products from the nodes in the previous layer that connect to it. It sums them all up, applies the logistic function and passes the output forward to the next layer where the process repeats. Neural networks learn by tuning the values of the weights for each path so that the output layer produces a hypothesis that can model your data. In this way, complex non-linear hypotheses can be formed and the more hidden layers you have the potentially more complex models can be generated.\n\nWhat I found most interesting in this week\u2019s content was how neural networks can be used to approximate binary logical operations such as AND or OR. Focusing on the case of AND, we know that x1 AND x2 will equal 1 only when x1 = x2 = 1 and otherwise will give 0. Looking at the neural network and truth table below, we see that this simple neuron can effectively approximate the logical AND operation. I\u2019m amazed by the versatility of neural networks, the sheer number of things they can model and am curious if there are any situations where such a neuron would be more useful than the conventional logic gates we find in circuits.\n\nOverall this week of the course was enjoyable and I\u2019m glad I got a bit of a better understanding of how neural networks work underneath the hood. The next post will go over the algorithms that allow neural networks to tune their weights and derive the optimal hypothesis."
    },
    {
        "url": "https://medium.com/@rgotesman1/learning-machine-learning-part-3-logistic-regression-94db47a94ea3?source=user_profile---------5----------------",
        "title": "Learning Machine Learning \u2014 Part 3: Logistic Regression",
        "text": "This is a continuation of my Learning Machine Learning series. You can find Part 2 here.\n\nWeek 3 of Andrew Ng\u2019s ML course covered the Logistic regression classification method. Logistic regression is useful for when you have 2 or more distinct groups or classes and you want to determine which group/class your data falls into.\n\nAn initial question might be why we can\u2019t use the linear regression technique we learned in the previous week to achieve this. And there are at least 2 good reasons.\n\nLet\u2019s imagine we have the situation on the left below with 2 classes, 0 and 1 and a set of training examples that fall into each category. We could fit our line (black) to the data and say something like, if for any new input our model predicts a value greater than 0.5, the input belongs to class 1, if it predicts less than 0.5 the input belongs to class 0. Fair enough. However if we add just one more training example as we do on the right below, we see our model changes drastically. Even though, to our eyes, its clear the extra blue star shouldn\u2019t change the model, it does. This sensitivity to outliers is one reason linear regression is a poor choice for classification problems.\n\nAnother reason is that linear regression models output values that are continuous and can be far greater than 1 and far less than 0. Since our classes are discrete, only consisting of 0 and 1, it does not seem appropriate to use linear regression to solve this problem. Logistic regression to the rescue.\n\nThe true utility of Logistic regression stems from curve we use to model the data which in its simplest form is given by:\n\nIf we plot this function we\u2019ll find that as z approaches infinity, the logistic function approaches 1 and as z approaches negative infinity the function approaches 0. We can view the output of this function as the probability that the input belongs to class 1. If the probability exceeds 0.5 then we say it belongs to class 1. If it is below 0.5 we say the input belongs to class 0.\n\nBut when will the logistic function be greater than 0.5. From the graph its immediately obvious this occurs when the input is greater than 0. So if we consider a parameterized version of logistic regression, where our input vector x has n features and we have a corresponding parameter \u03b8j for each feature xj we can rewrite the logistic function as:\n\nIf we consider a simple case where the input vector x has 2 features, x1 and x2, we can see that theta transpose x becomes:\n\nWhen this expression exceeds 0, h(x) >0.5 so we say x belongs to class 1 and when its less than 0, h(x)<0.5 so we say x belongs to class 0. In this particular case, the expression defines a line, with each side of the line belonging to class 1 or 0. We call this line the decision boundary though it need not be a line. It can be a curve or in higher dimensions a hyperplane. The point is it demarcates the regions belonging to class 1 and 0.\n\nNow that we have the general form of our hypothesis we need to figure out a way to optimize the parameters based on our training data. Once again, we shall achieve this using the idea of a cost function. The cost function for logistic regression is more complicated than for linear regression so we\u2019ll work up to it in two parts.\n\nWe\u2019ll begin by defining some initial cost as:\n\nWe can see that the cost varies based on the class the training input belongs to. If it belongs to class 1, the cost takes the form of -log(h(x)) (blue curve below). We can see why this makes sense since if our hypothesis outputs 1 when y=1, this is correct and we want the cost to be 0. As well, if the hypothesis outputs 0 when y=1, this is incorrect and we want the cost to be very high. If the input belongs to class 0 the cost takes the form of -log(1-h(x)) (red curve) and we can see why this is appropriate by similar logic.\n\nWe can take this cost and use it to write out the cost function J in a single line which becomes:\n\nThis is really the same thing since if y=0 the first term of the sum disappears and if y=1 the second term of the sum vanishes.\n\nWe want to find the parameters that minimize the value of the cost function and once again we can find these parameters through gradient descent. When we take the partial derivative of the cost function and plug it into our gradient descent formula, once again we get the familiar algorithm:\n\nYou\u2019ll note the summation term is the exact same form as the one you get when deriving gradient descent for linear regression. It is different however because in this case the hypothesis is a logistic function, not a linear one.\n\nThe tools we have developed for solving binary classification problems can also be applied to problems where we have 3 or more groups. We separately consider each group in the training set, give it a \u201cvalue\u201d of 1, give all the other classes a \u201cvalue\u201d of 0 and then run our regular logistic regression to get a decision boundary. If we have n different classes, this method will yield n different logistic regression models. When we plug in a new input x, each model will output the probability that x belongs to that class and by taking the maximum value we achieve classification.\n\nThe course then discusses the problem of overfitting, which is when our hypothesis tries so hard to model every detail of the training set that it generalizes poorly to other data. One method to reduce overfitting is called regularization which involves adding an additional weight to some parameters so that they are prevented from contributing to our model. This stops the model from becoming too \u201ccomplex\u201d and hopefully avoids overfitting.\n\nI have seen regularization pop up several times before in ML discussions and didn\u2019t find this week\u2019s regularization lectures that enlightening. Hopefully the topic will be covered in greater detail later on.\n\nOverall I learned a great deal this week and am looking forward to week 4 on Neural Network representation."
    },
    {
        "url": "https://medium.com/@rgotesman1/learning-machine-learning-part-2-multiple-linear-regression-41d227c28dac?source=user_profile---------6----------------",
        "title": "Learning Machine Learning \u2014 Part 2: Multiple Linear Regression",
        "text": "This is a continuation of my Learning Machine Learning series. You can find Part 1 here.\n\nWeek 2 of Coursera\u2019s Machine Learning course covers multiple linear regression. This is similar to univariate linear regression except rather than just a single independent variable, our model now includes two or more. Each of these variables comes with its own parameter so we can write our new hypothesis as:\n\nThough this formula may seem complex, understanding it is quite simple. Theta zero represents the value of the hypothesis when all independent variables have a value of zero. Every other theta, theta_i, can be thought of as how much h(x) increases when x_i increases by 1 unit. That is because, if we hold every x constant, except x_i which we increase by 1, then we can see that h(x_1,x_2\u2026(x_i)+1,x_(i+1)\u2026x_n)-h(x_1,x_2,\u2026x_i,x_(i+1)\u2026x_n) = theta_i*(x_i+1)-theta_i*(x_i)=theta_i.\n\nTo make the notation cleaner we can group all the thetas into a column vector theta, all the variables into another column vector x, and by defining x_0=1 we can write h(x) succinctly as:\n\nAs before, we want to find the parameters theta that minimize our cost function J\n\nand we achieve this through the gradient descent algorithm\n\nNote that the summation term is achieved by taking the derivative of J with respect to theta_i.\n\nUsing gradient descent we can start with initially random values of theta and step by step converge to parameters that optimize the fit of our hypothesis.\n\nThe course then moved on to consider more practical matters when running the gradient descent algorithm. The first was the importance of feature scaling. Having data variables with wildly different scales can increase the time it takes for gradient descent to converge.\n\nImagine you\u2019re trying to model data with 2 variables, one being household income, the second the number of children in the home. It\u2019s immediately clear that these 2 variables exist on different scales. The income variable can range from 0 to the millions or even billions. The second variable probably won\u2019t go over 10. In cases like these its important to scale the data which can involve dividing each variable by its value range or some measure of dispersion like standard deviation. Doing so will cause the variables to have the same scale and speed up the rate at which gradient descent converges.\n\nAnother pragmatic factor to consider is how to set the learning rate alpha. If alpha is too small gradient descent will take a while to converge. If it is too large, gradient descent may never converge. The best way to figure out the optimal alpha is through trial and error. Start with an alpha of 0.001 and increment upward to 1. Assess how the cost function converges which each alpha, does it decrease quickly, slowly or oscillate? Picking the optimal alpha will improve gradient descent\u2019s ability to converge.\n\nThe course then touched upon the normal equation which is a mathematical formula for figuring out the optimal values of theta.\n\nUnfortunately the course doesn\u2019t go over how it\u2019s derived but I found a nice post that goes over it. The cool thing about the normal equation is you don\u2019t need to rely on the iterative process of gradient descent to find your optimal values of theta. For datasets with a small number of features this is very convenient and efficient. However once you start to get 10000 or more feature, multiplying those enormous matrices together becomes incredibly inefficient and gradient descent is the way to go. In addition, the normal equation, while applicable to linear regression, can\u2019t be used for other ML methods like logistic regression while gradient descent can.\n\nThe week ended with an overview of key commands in the Octave programming language. Personally, I would have preferred the course to work with Python since this is what I will use for most of my ML projects but the knowledge is pretty transferable. One cool thing I figured out how to do is vectorize the gradient descent update algorithm so that the update for all values of theta is achieved in one line."
    },
    {
        "url": "https://medium.com/@rgotesman1/learning-machine-learning-part-1-linear-regression-cost-functions-and-gradient-descent-7b67d6543aff?source=user_profile---------7----------------",
        "title": "Learning Machine Learning \u2014 Part 1: Linear Regression, Cost Functions and Gradient Descent",
        "text": "I just finished going through week 1 of Andrew Ng\u2019s machine learning course on Coursera and there were a lot of interesting ideas. I thought I would summarize and discuss the more important ones.\n\nMachine learning is the science of teaching a computer how to solve a problem without explicitly programming the solution. Rather you allow the computer to come to its own empirical solution from looking at the data.\n\nThis is a very powerful method because it can generalize across many fields and be used everywhere from diagnosing breast cancer to suggesting a Spotify song.\n\nMachine learning is generally broken down into 2 main categories: Supervised and unsupervised learning.\n\nSupervised learning is used when you have a dataset with input x AND output y. It is labeled. You know that a relationship exists that somehow links x to y and you go about discovering it through machine learning.\n\nFor instance, if you wanted to predict a person\u2019s weight from their height, and you had a dataset consisting of pairs of height and weight, this would be a supervised learning problem. Both the independent and dependent variables are know. Specifically it is a regression, supervised learning problem since you are trying to predict a continuous variable.\n\nIf instead the dataset told you each person\u2019s weight and whether they were, quote unquote, healthy or unhealthy, and given somebodies weight you wanted to make this prediction, that would be a classification, supervised learning problem since you are trying to predict discrete classes.\n\nThe second main kind of machine learning is unsupervised learning which is when you have a dataset with no labels and you want to see if it can be divided into certain subgroups that give it some underlying order. A recent example of this was in the medical journal Lancet where researchers, using cluster analysis, were able to identify 5 subgroups of diabetes, compared to the usual 2.\n\nThe course then moves to the first learning model: linear regression with 1 independent variable or univariate linear regression.\n\nLinear regression has the mathematical form h of Fig 1 where parameters theta zero and theta one define the y-intercept and the slope respectively. For any dataset you want a learning algorithm that can determine which values of theta lead to h best modeling the data.\n\nThe obvious question is how do we measure how well h fits the data? And that is where the cost function J comes in. J can come in many forms but in this case we define the cost function as below. Note that it is a function of theta whereas h is a function of x.\n\nThe way this cost function works is we look at what our candidate linear regression model predicts the dependent variable should be based on an input x as well as the true value of the dependent variable associated with x, take the difference and square it. We do this for each element of the dataset before adding them all up and dividing by m, the total number of data elements, to obtain a value for the average square distance between what h predicts the dependent variable should be and what the dependent variable truly is. We divide this value by 2 just because it makes the number easier to deal with.\n\nClearly, if the linear regression hypothesis was perfect it would always predict the correct value of the dependent variable and lead to J=0. That is the goal. Depending on the values of theta that we give our model, J will acquire some value and the shape of J for univariate linear regression is given below (Fig 3). Rather than plugging in thousands, or tens of thousands of combinations of theta and seeing which gives the smallest value of J, we would like an algorithm that can get us to the values of theta that minimize J, regardless of the starting values of theta.\n\nThe algorithm that does this is called gradient descent. Consider some arbitrary cost function J with the shape in Fig 4 and imagine we initialized our learning model with values of theta given by the X. Gradient descent will update the values of theta, choosing ones that reduce J with each iteration until a local minimum is reached. Depending on the initial values of theta, gradient descent may converge to different local minima but this is not a problem for univariate linear regression.\n\nThe rule defining the gradient descent descent algorithm is as follows:\n\nA key feature of the algorithm is that each theta is to be updated simultaneously so the partial derivative is calculated on J for both theta zero and theta one before the values of theta are changed. Another important feature is alpha, the learning rate, which determines how quickly gradient descent converges. It is equivalent to step size. Make it too small and the algorithm will take a long time to converge. Make it too large and the algorithm might overstep the local minimum or even fail to diverge. The partial derivative gives a sense of the direction at which the function J is changing and you take a step in that direction, updating theta as you go until a local minimum is reached.\n\nThe week ended with a review on basic linear algebra properties but I knew most of this from a university course so nothing too interesting was shared.\n\nOverall I really enjoyed the first week of the course and am looking forward to learning more. Comment below if you have any questions or if anything was unclear."
    },
    {
        "url": "https://medium.com/@rgotesman1/learning-machine-learning-part-0-the-setup-7ad1fc053d8f?source=user_profile---------8----------------",
        "title": "Learning Machine Learning \u2014 Part 0: The Setup \u2013 Ryan Gotesman \u2013",
        "text": "Though I have implemented my fair share of machine learning models, from neural networks to logistic regression, most of my programs are rather simplistic. They usually involve importing a library, invoking a method or two and then checking the output.\n\nI feel like I don\u2019t truly understand the subject.\n\nAnd I\u2019d like to.\n\nI plan to remedy this by going through several top-rated machine learning courses.\n\nOne that focuses on theory: Learning from Data by Dr. Yaser\n\nAnd one that focuses on application: Machine Learning by Andrew Ng\n\nI will post my notes on lectures and problem sets so you can follow along and learn.\n\nOnce I complete these to my satisfaction I\u2019ll try to move on to others so feel free to suggest any good courses you enjoyed."
    },
    {
        "url": "https://medium.com/@rgotesman1/what-hygenic-bees-can-teach-us-about-behaviour-d22578f165cd?source=user_profile---------9----------------",
        "title": "What Hygenic Bees Can Teach Us About Behaviour \u2013 Ryan Gotesman \u2013",
        "text": "What Hygenic Bees Can Teach Us About Behaviour\n\nThe Nature versus Nurture debate is as old as time.\n\nWhat determines how we behave? Our genes? Or our environment?\n\nCertainly both play a part, but I have always leaned to the Nurture side of the argument and consider our surroundings a far greater influencer of behaviour than some genes buried deep within the nucleus.\n\nRecently I read an intriguing article about honey bees that made me rethink that position.\n\nLike most organisms, honey-bees are worried about bacteria infecting and colonizing their homes. One particularly nasty bacteria is called Bacillus larvae and it infects the larvae of the bees before quickly spreading through the hive.\n\nInterestingly, certain kinds of bees, dubbed hygenic bees, consistently survive B. larvea exposure while others routinely perish.\n\nHygenic bees earn their name from how they deal with colony infection by B. larvea. In hives, the growing larvae are covered with a wax cap to keep them warm and defend from any unpleasant elements. When a larvae gets infected in a hygenic colony a bee will rip this layer of wax off, grab the infected larvae and throw him out the door. This OCD-like behaviour is absent in non-hygenic bees and scientists were curious if they could find a genetic mechanism behind this.\n\nThey began by mating a queen from a hygenic colony with a worker from a non-hygenic one. The children, with genes from both hygenic and non-hygenic parents, displayed no hygenic properties and allowed bacterial infected larvae to grow in their hive.\n\nThis told the researchers they were dealing with a recessive gene, a gene that will only influence behaviour when there are 2 copies of it and not in the presence of its dominant gene.\n\nThe researchers then did something interesting. They took this non-hygenic child, with its copy of a hygenic and non-hygenic gene, and mated it with a hygenic queen. As expected, the offspring that inherited the hygenic gene would remove the wax cap and expel any infected larvae while children inheriting the non-hygenic gene left the wax cap and larvae untouched.\n\nAmazingly, something unexpected happened. A subset of these children would rip the wax cap off but put no effort into expelling the infected larvae from the hive. After removing the wax the bees would allow the larvae to simply sit there.\n\nIt appears that there are 2 genes controlling hygenic behaviour. One gene compels a bee to remove a wax cap and another completely separate gene makes them discard the infected child. The researchers realized some bees may be born with the predisposition to remove infected larvae but can\u2019t because they lack the 2 copies of the recessive wax-removal gene.\n\nTo put this to the test the researchers went in and manually removed the wax caps of infected larvae. Remarkably, some of the bees that previously displayed no hygenic behaviour whatsoever proceeded to eliminate the infected larvae! Just as predicted!\n\nI found these findings amazing because they demonstrate just how great an influence genes can have on behaviour. Sure, we humans are no bees and our powerful brains give us great liberty over our DNA but I hope this story makes you pause and think.\n\nJust how much of who you are is based on your environment and how much is predetermined by the sequence of nucleotides free-floating in your cells?"
    },
    {
        "url": "https://medium.com/@rgotesman1/reviewing-udemys-most-popular-android-course-e683307866bc?source=user_profile---------10----------------",
        "title": "Reviewing Udemy\u2019s Most Popular Android Course \u2013 Ryan Gotesman \u2013",
        "text": "Today I downloaded Android Studio and played around with some layouts. From what I can tell some of the main types of layouts are RelativeLayout, LinearLayout, ConstraintLayout and GridLayout and they can be used to dictate where objects reside on the screen\n\nIt\u2019s pretty simple to create a user interface. There are a bunch of pre-programmed buttons and views that you can drag and drop. In a couple of seconds I was able to throw together an email and password screen.\n\nThe problem is that hitting submit doesn\u2019t do anything. There\u2019s no logic written into the button and that\u2019s what I need to learn how to do.\n\nLooking around AndroidStudio there are a number of folders and sub-folders in the project window that warrant exploration. I have no idea what most of them are for although it looks like the res>drawable folder is where I store images.\n\nSo far the course is off to a great start. I learned how to create various buttons from scratch using Java. It\u2019s a lot more complicated then simply dragging and dropping to create a UI. You need to create an object for the layout, an object for the button/widget/view you\u2019re interested in using and another object to define the button\u2019s location. I enjoyed the insight and flexibility this offered but for now I think I\u2019ll stick with dragging and dropping my buttons onto the screen.\n\nToday was all about event handling or learning how to recognize when the user is doing different things to the screen. I made a simple program where one line of text appears when a button is clicked quickly and another appears when the button is held for a while. This was achieved using the setOnClickListener and setOnLongClickListener methods, respectively. Interestingly, these 2 methods are probably what\u2019s behind your app turning on when you click it but asking if you want to delete it when you hold down on it. Neat insight!\n\nI also learned how to recognize gestures using the GestureDetector library. Through this class you can detect finger motions on the screen that correspond to single taps, double taps, scrolling and flinging (violently scrolling). Once I learn how to import complex objects into the app this method will be very useful for detecting common gesture interactions.\n\nI also realized that rather than create each button from scratch in Java it\u2019s much easier to drag it into the UI and then \u2018import\u2019 it in the Java code using the findViewByID method.\n\nToday I learned about fragments which are basically a way to get 2+ activities on the same screen. For instance, the line and button are 1 activity and the picture is a separate activity in the picture below.\n\nAt this point I\u2019m not exactly sure why one would want to use fragments\u2026The below layout could easily be created without them but hopefully all will soon become clear.\n\nI then spent an excessive amount of time trying to figure out how to get a spinner (a drop down menu that appears when you click the arrow) to appear. I ran the program dozens of time but the spinner never appeared on the screen. Eventually I realized the spinner had a minimum height attribute set to 0 so though it was there I could not see it. After setting the minHeight variable to about 20 pixels, the arrow became visible and the drop down menu worked as expected.\n\nI also had an idea for a first app . I enjoy jogging on the treadmill and periodically glance at the number of calories to burned to see if its alright to eat that extra Lindt after dinner. But is that number accurate? I\u2019d like to make an app where you enter some info about your jog and get the number of calories burned in return. A simple app and a simple attempt to validate the number on the treadmill.\n\nI made a rough sketch of the UI below. Hopefully it turns out a little bit more colorful.\n\nToday was a slow day. I learned how to get the value of the spinner class. That\u2019s pretty much it. Hopefully I can work on the app tomorrow.\n\nI managed to get my idea working today. It\u2019s pretty simple but I\u2019m proud of it because I definitely would not have been able to get it working a week ago. It uses 5 TextViews (1 is invisible), 4 EditText views and 1 button. When the user clicks the button the app calculates the number of calories burned using the formula found here (https://www.livestrong.com/article/34973-calculate-treadmill-calories/). The accuracy of the number isn\u2019t as important as going through the steps of creating the app. Also if the user enters something besides numbers they are reminded to only enter numbers which was achieved with a try-catch.\n\nTomorrow I\u2019ll go through more of the course.\n\nToday I learned how to use overflow menus. These menus are used to open new tabs on Chrome for Android so it was exciting to learn a technique that I make use of everyday.\n\nI also learned the basics of animation. Animation is generally achieved by telling an object to move from point A to B but creating a delay so the action is not instantaneous and can be discerned by the human eye. In Android this is achieved using the TransitionManager class. I only touched upon this class today and I\u2019m eager to look more into it.\n\nI haven\u2019t been able to allocate as much time as I\u2019d like to Android over the last two days so I\u2019ll definitely focus more on it tomorrow.\n\nToday I learned about Intents which are objects you can use to run 1 activity from another activity. When you click a button in an app and a fresh screen appears, that screen was called using an Intent. Through Intents you are able to move from the blue colored activity to the red one whenever the button is pressed and similarly from the red activity to the blue activity.\n\nI also learned how to implement Threads which are a way to get 2 or more processes running at once. For instance if you wanted to load a screen while continuing to allow the user to press a button, you would employ a Thread. They are very powerful and definitely an area I need to explore.\n\nToday I spent a good deal of time trying to make an app that could play a video. In addition, I finally learned how to capture what was happening on my phone screen so now I can show you apps I make in action.\n\nInitially I tried to play some videos using the VideoView widget and though it can play .mp4 files, since Youtube doesn\u2019t provide videos in the format, VideoView essentially cuts you off from the Internet\u2019s largest source of video content. As such I decided to use the Youtube Android Player API and after following this tutorial I managed to get a video from Youtube to play!\n\nI also learned how to use the phone\u2019s camera and store a captured picture. I find this the coolest thing I learned so far because now I can easily bring complex user generated data into my apps.\n\nI spent most of the day learning how to create app notifications. These are the messages that pop up on your phone whenever you get a new text or an update for a game. Essentially it\u2019s a way for an app to tell you something happened without you having to open it. Pretty important stuff. And pretty cool. See video below.\n\nToday I learned about shared preferences which are a way to save user data. Through shared preferences you can ensure data is retained when an app is closed and is still there when the app is opened once more\n\nToday I learned about ListViews which are, well\u2026 a way to display lists of things. Pretty simple stuff. See video below.\n\nToday I learned all about databases using SQLite which is a database management program. I have some experience using SQL on a Linux system and SQLite was similar. You type in the SQL command just like you would on the command line except you store the command as a String and then pass it into the execSQL method for execution. I found it a bit tricky, you need to get the spacing in the String just right or it won\u2019t be understood by the computer and this led me to have a lot of trouble initializing a TEXT column.\n\nEventually I got it though and as you can see in the program below, when you enter a text string and click add the string is included in the database of words, which are retained even when the program is closed and reopened. Words can also be easily deleted from the databases by entering them and clicking delete. Overall I think SQLite offers more flexibility than the Shared Preferences I learned 2 days ago.\n\nToday I practiced a number of things I\u2019ve learned in the past two weeks by making a simple BMI app. The user begins at a home screen and presses a button to get to the main screen. There the user is prompted to enter the weight, their height and hit a button to calculate their BMI before being told if they are overweight, underweight or healthy. Moving between screens was achieved using Intents and the user interface created with EditText, Button and TextView widgets.\n\nToday I learned a bit about using the GoogleMaps API. After getting permission to use the API from Google, running it is pretty simple. For some reason when the app starts the marker points to Sydney Australia. I made it point to the CNTower instead. You can then zoom in and out and navigate like you would on the actual GoogleMaps. I\u2019d be interested to learn how to get the phone\u2019s GPS coordinates so that when you open the app it tells you exactly where you are and I\u2019d also like to learn how you could prompt the user for an address, deduce its latitude and longitude and place a marker on the map.\n\nToday I made a simple stopwatch app. There is a counter at the top of the screen that increases by 1 every second after you click the start button, it can be stopped by clicking the stop button and it can also be reset back to 0.\n\nI confess the above app does not satisfy me. If you look closely you\u2019ll see that if you hit stop the app will still increase the time by 1 before pausing. It took me a while to figure out why this was occurring and I finally realized it was because the scale I was using to count, seconds, was greater than the speed with which I could press the button. I rewrote the above app so that it counted time in milliseconds, not seconds and the problem went away. I suspect this is why the clock app that comes with your phone counts time in milliseconds and not seconds.\n\nThe above fix is still not sufficient. If you look at the video you\u2019ll note that time passes slower on the stopwatch app than in reality. After thinking about it for a while I realized this is occurring because of the way I keep time. I have a counter variable that I increment say by 100 to account for the passage of 100 milliseconds. Then I actually delay the program by 100 milliseconds before incrementing the counter variable again. The problem is that the time between increments of the counter variable may exceed 100 ms. Updating the textView with the current time might take an additional 5\u201320 ms, and perhaps the delay is 101 ms instead of 100. After a couple loops this leads to the app falling behind an actual stopwatch. The workaround is to use the System.currentTimeMillis() method which is how the phone keeps track of the time that has elapsed. By making the current time equal to System.currentTimeMillis() \u2014 (Time When App Started) you\u2019ll always have the amount of time that elapsed irrespective of whether updating the textView took 10 or 500 ms.\n\nI also finally learned out how to keep all the variables from resetting whenever the orientation of the phone is changed using the onSaveInstanceState method.\n\nToday I learned about checkboxes which as the name suggests are boxes that can store checkmarks. Its a fast and easy way to store the user\u2019s preference on a number of decisions.\n\nOf course the problem with using checkboxes is there are times when you only want to pick 1 option or the other, for instance turning something on or off. You can\u2019t be on and off at the same time. To give users this kind of pick 1 and only 1 option we use radioButtons, with an example below.\n\nToday I made a simple \u201ccoffee shop app\u201d mostly using listviews. The idea is the user is presented with a number of choices, clicks one of them to go to another screen with choices, picks one of them and gets some info. It was a good review of how listViews work\n\nToday I learned about the NavigationDrawer which is a slideable menu that allows for easy navigation through your app. I made a simple pizza app where using the NavigationDrawer the user could order pizza, view menus, look at the day\u2019s offers and extras as well as contact and rate the app. I really like it. The only drawback I can think of is when you access the NavigationDrawer the menu obscures the app which might disrupt user immersion.\n\nToday was a slow day, I spent it learning how to use the ActionBar which is the toolbar at the top of the app that allows you to add icons. Each icon can be associated with a specific action that is invoked when clicked. In the simple app below, clicking an icon causes the user to switch activities. Also I found a great stash of free icons which will useful in the future here: https://material.io/icons/\n\nToday I learned a bit about cardViews which are a way of placing information into layouts with rounded corners. Nothing too interesting but you may prefer the way it makes the app look.\n\nI also learned about scrollViews which I find one of the most interesting and exciting topics I\u2019ve covered so far in the course. These are necessary whenever you want to present content that is larger than what the screen can hold. I made a column of 4 cardViews that stretched down the screen and with a scrollView the user could look through all of them. Pretty cool.\n\nFinally I also learned how to use the accelerometer that comes with the phone. As you can see in the video below the phone can keep track of your relative X, Y and Z positions in 3d space. In the first few seconds the phone is sitting still on my desk, in the last few I try to wave it around hysterically which causes some change in the variables. I can see how you could train a program to recognize certain combinations of changes in X, Y and Z to be associated with certain movements and therefore teach an app to recognize when you\u2019ve completed a step as part of a pedometer program. Very neat.\n\nI almost reached the end of the course and there is one set of modules left, building a real estate application. I like this app because it requires you to use almost everything covered in the course: TextViews, Buttons, Intents, Clicking Events, Navigation Tool Bars, Google Map APIs, ScrollViews and more. At the end you get a pretty decent looking app with a login screen, a menu with options to view a scrollable list of images, a google map with marked locations of fictitious stores, a list of testimonials and a contact page. A nice way to end the course.\n\nIf you got this far I wanted to say thanks for reading and I hope this review can give you some insight on whether the course is for you."
    },
    {
        "url": "https://medium.com/@rgotesman1/hamilton-waterfalls-ae15e196c320?source=user_profile---------11----------------",
        "title": "Hamilton Waterfalls \u2013 Ryan Gotesman \u2013",
        "text": "Recently, it dawned on me how little I had explored Hamilton and I became eager to visit as many landmarks and beautiful locales as I could.\n\nFor some time I have also wanted to take up photography as a hobby.\n\nWith my ever atrophying artistic vision, camera and thirst for adventure I\u2019ve set out to explore Hamilton. Hopefully, these series of articles will provide any readers with a few pretty pictures, an idea or two for things to do if ever bored in Hamilton and motivation for me to continue practicing photography.\n\nHamilton is known as the waterfall capital of the world and for my last excursion I was eager to visit a few. I stopped at Albion and Buttermilk and they did not disappoint.\n\nButtermilk and Albion falls are right next to each other. I somehow managed to get lost in the woods while searching for buttermilk, must be because I was always a tea person :p \u2026 Eventually I did find it. The falls are quite lovely albeit a bit petite. I\u2019m told they are much more impressive right after a rain storm if you can summon up the courage to venture out.\n\nAlbion falls is like a bigger version of buttermilk. I only took pictures of it from the side though a frontal shot may have been more impressive. What special about Albion is the series of steps in the face of the rock that cause the water to cascade. It makes for a very cool effect.\n\nThis trip was also an interesting place to practice photography as it was the first time I\u2019d taken pictures of moving objects. While I am happy with my shots, this photography book I\u2019m reading says when capturing moving objects you should strive to convey the direction of movement. I\u2019m not sure if I achieved this and in the coming days I\u2019ll reflect on how this could better be done.\n\nThe waterfall capital of the world did not disappoint and I would definitely recommend visiting these and more waterfalls in Hamilton.\n\nThis will likely be my last Exploring Hamilton entry, hope you were inspired to go on an adventure in this beautiful city. Thanks for being part of the journey!"
    },
    {
        "url": "https://medium.com/@rgotesman1/hmcs-haida-f41112a6ed51?source=user_profile---------12----------------",
        "title": "HMCS Haida \u2013 Ryan Gotesman \u2013",
        "text": "Recently, it dawned on me how little I had explored Hamilton and I became eager to visit as many landmarks and beautiful locales as I could.\n\nFor some time I have also wanted to take up photography as a hobby.\n\nWith my ever atrophying artistic vision, camera and thirst for adventure I\u2019ve set out to explore Hamilton. Hopefully, these series of articles will provide any readers with a few pretty pictures, an idea or two for things to do if ever bored in Hamilton and motivation for me to continue practicing photography.\n\nThe HMCS Haida is a destroyer class vessel that operated from 1943 to 1963 and participated in both World War II and the Korean War. It has the distinction of sinking more enemy vessels than any other ship in the history of the Canadian fleet and I was eager to see this titan of steel while I was still in Hamilton.\n\nThe ship is a National Historic Site of Canada and currently serves as a museum. Note that until June 30 it is closed Monday to Wednesday and I learned this the hard way, being greeted by a locked gate, so I had to satisfy myself by only looking at the ship\u2019s exterior.\n\nThe site is very easy to get to from McMaster. Moored at Pier 9 it\u2019s a 15 minute drive or about 30 minutes by bus. Interestingly, the boat was not always there. It was at the West end of Toronto\u2019s waterfront at Ontario Place until 2002 when it was purchased by Hamilton and towed here.\n\nPhotographically speaking, the HMCS was the largest object (377 feet long!) I\u2019d ever taken pictures of and it was challenging to try to fit the entire ship into the image. It was also interesting trying to balance capturing the full ship vs. focusing in on smaller components of the vessel , capturing fine details but missing out on the sheer size of the ship.\n\nOverall I found the visit pleasant and enjoyed seeing the ship. I would recommend going when the museum is open just to get the full experience.\n\nLooking forward to future adventures in Hamilton!"
    },
    {
        "url": "https://medium.com/@rgotesman1/mcmaster-museum-of-art-a195d5edcc6f?source=user_profile---------13----------------",
        "title": "McMaster Museum of Art \u2013 Ryan Gotesman \u2013",
        "text": "Recently, it dawned on me how little I had explored Hamilton and I became eager to visit as many landmarks and beautiful locales as I could.\n\nFor some time I have also wanted to take up photography as a hobby.\n\nWith my ever atrophying artistic vision, camera and thirst for adventure I\u2019ve set out to explore Hamilton. Hopefully, these series of articles will provide any readers with a few pretty pictures, an idea or two for things to do if ever bored in Hamilton and motivation for me to continue practicing photography.\n\nThe McMaster Museum of Art is a public, non-profit university affiliated gallery situated near the McMaster Student Center and easily accessible by any student. It is home to many beautiful pieces of art and after 3.5 years at Mac I was embarrassed to say I\u2019d never been so was eager to see what it had to offer.\n\nThe gallery is split into a bottom floor, which plays host to temporary exhibits that stop by as they tour the country, and a top floor which houses the gallery\u2019s permanent collection. Understandably, I was not allowed to take pictures from the touring exhibit so all of these pictures are from the top floor.\n\nInterestingly, when you take the flight of stairs to the top floor, you are greeted by an enormous chalice, a goblet, wreathed in spikes. It\u2019s so large you could sit in it. The first time I saw it I was quite taken aback and I doubt I shall ever happen upon a larger cup.\n\nThe museum boasts an impressive collection of ancient antiquities, over 200 to be precise, with pieces from the Bronze age over 5000 years ago to the Modern era. They have pottery, glass, carvings and jewelry from places like China, Japan and Europe. One of my fondest memories is visiting the museums of Greece and drinking in the rich display of pottery and sculptures from ancient Athens. Walking through the collection of antiquities at McMaster Museum of Art I felt transported back there. Interestingly, this component of the exhibition is dedicated to Mr. Togo Salmon who it turns out was one of the first appointments to McMaster\u2019s classics department in 1930 and taught at the school for 43 years. I\u2019ve had a number of classes at Togo Salmon Hall throughout my undergrad so nice to finally know whose hall I\u2019m sitting in.\n\nMy favourite piece in the exhibit is probably the 5 faces above. There is something strangely ominous and eerie about them that frightens you but also draws you in. It almost seems as if the artist tried to replicate Picasso\u2019s cubism, deforming the faces of the subjects, but chose to use a medium of stone rather than paint on a canvas.\n\nThe Museum also has a nice Asian section which is very different from the rest of the exhibit which seems to focus on art with a European style and origin. The canvas paper used for the paintings was quite unique, it reminded me of the papyrus used in ancient Egypt more than anything and there were also some really colorful objects that looked like a mix of sculpting and origami.\n\nThis trip was also an interesting place to practice photography as it was a new challenge for me to try to fit an entire piece into the view of my camera without capturing any other pieces or the frame of a painting for instance. Step too close and you miss some of the detail, step too far and you capture more than you want. It was cool trying to find the right balance.\n\nI also found it challenging taking frontal pictures of paintings. The glass covering the paintings always caught my reflection which ended up making it onto the image. This forced me to take pictures of paintings at an angle and I am curious if there is a better workaround.\n\nNote the following pictures just give a taste of the great work the McMaster Museum of Art has to offer and there are many more delights waiting to be seen.\n\nI would highly recommend it to anyone looking to spend a pleasant afternoon enjoying art."
    },
    {
        "url": "https://medium.com/@rgotesman1/a-big-house-even-bigger-boat-21579a188ee5?source=user_profile---------14----------------",
        "title": "Dundurn Castle \u2013 Ryan Gotesman \u2013",
        "text": "Recently, it dawned on me how little I had explored Hamilton and I became eager to visit as many landmarks and beautiful locales as I could.\n\nFor some time I have also wanted to take up photography as a hobby.\n\nWith my ever atrophying artistic vision, camera and thirst for adventure I\u2019ve set out to explore Hamilton. Hopefully, these series of articles will provide any readers with a few pretty pictures, an idea or two for things to do if ever bored in Hamilton and motivation for me to continue practicing photography.\n\nThe first locale I visited was Dundurn Castle an 18000 square foot, National Historic site, mansion built in 1835.\n\nI visited the spot over the weekend when the building was closed so wasn\u2019t able to tour the interior, though the forty room estate normally functions as a civic museum. Interestingly, many famous people have visited this place, including our first prime minister and King Edward VII.\n\nThe building has an enormous lawn both in the front as well as the back and I can easily see having a picnic there one afternoon, stretched out along the grass."
    }
]