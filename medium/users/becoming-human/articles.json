[
    {
        "url": "https://becominghuman.ai/exploring-the-meaning-of-ai-data-science-and-machine-learning-with-the-latest-wikipedia-5fea5f0a2d46?source=---------0",
        "title": "Exploring the Meaning of AI, Data Science and Machine Learning with the latest Wikipedia\u2026",
        "text": "Terms such as data science, machine learning and artificial intelligence have found a well deserved spot in the \u201cpantheon\u201d of tech buzzwords.\n\nWe hear and read about them almost on a daily basis. In fact they are often used interchangeably, even though depending on the source there are clear preferences (you might have noticed a certain fondness that say marketing and media have for the term AI).\n\nThe illustration above is an adaptation from PWC, one of the many Venn diagrams available on the internet attempting to explain the relationship between the terms, but it is not very straightforward.\n\nCan we use data and analytical methods to capture the meaning and semantic context of these terms ? Ok, let\u2019s give it a go and try to fight fire with fire. The Wikipedia clickstream dataset will be of great help for this.\n\nThe Wikimedia foundation has recently decided to open source its monthly clickstreams across several key languages. A typical clickstream in English language contains millions of distinct Wikipedia urls, requested billions of times by internet users.\n\nHaving such a comprehensive resource available is great news. It enables several sophisticated types of analysis around online user behaviour over one of the world\u2019s busiest websites. Indeed there are several published academic papers that study the clickstream dataset from various scientific perspectives, especially behavioural.\n\nThis will be a practical approach using network analysis. Not only to answer the question above, but also to motivate the use of clickstream and network analysis for other applications and domains. Keyword research, content development, web traffic visualisation come to mind and I am sure there are many others.\n\nWe will use the latest available dataset from March 2018 which contains 23.3M pairs of referrer and target urls in English language, accounting for a total of 6.2B page requests to the Wikipedia servers.\n\nThe dataset makes it possible to visualise the relationship between different terms based on the way users navigate from one Wikipedia page to the other, either by following the links or using the built-in search functionality \u2014 sometimes falling victims to the Wikipedia rabbit holes.\n\nThe dataset is practically a gigantic network of nodes (the Wikipedia articles) which are connected via edges to other nodes (related articles) based on the sequence of pages requested during the user session.\n\nThe size of the edges can be thought of as a weight factor reflecting the traffic between two nodes. One of the benefits of working with network analysis is that its outputs can be graphed. This allows us to see the overall view of what all those clicks and page transitions might mean.\n\nNOTE: The graphs that follow have been configured to enable decent viewing on mobile devices. However they are best viewed on a large screen.\n\nNext we will examine clickstream graphs of the following terms:\n\nAs a warm-up and in order to explain the method of analysis we \u2018ll start with Data mining, a term that was heavily used in the past.\n\nThis initial graph has isolated the part of the network that involves \u201cdata mining\u201d and its neighbourhood of adjacent nodes (articles). The edge between two nodes is the traffic between them and the colour and size of the nodes reflect the number of neighbours they are connected to. This is already useful to some extent. In order to make more sense of the related terms, as a second step, the nodes were grouped into relevant categories, in order to see the broader relationships more clearly.\n\nThe downside of this (beyond my own personal bias applying the labels) is that the graph can get overloaded with text and colours. Moreover, in some cases e.g. AI it can be really hard to contain the number of groups to a manageable level. In the case of artificial general intelligence the nodes are not homogeneous enough to create meaningful categories. In both cases the nodes were on purpose not placed in groups. Note also that in order to make the graph more clear, some of the nodes are not visible- either because of their low traffic or because of having a very low number of neighbours.\n\nOne thing to note from the start is that the Wikipedia entry for data science is relatively limited. It has the least amount of hyperlinks among all the other terms reviewed in the article. The content however has been growing substantially. Since November of 2017 when the first dataset was released, the links on the page almost doubled.\n\n(Fun fact about data science: one of the pages associated with it is actually the Wikipedia article for buzzword, even though its vertex degree was not sufficient to get it into the graph).\n\nThe article for data mining is much more comprehensive as it has been around for much longer.\n\nThat said there is still a fair amount of similarity between the type of associated terms. Most of the terms in the data science graph appear in the data mining one too. This could give some ground to those suggesting that data science is a rebranded version of data mining as used in the \u201990s and \u201800s.\n\nFor the moment however, data science lacks strong associations with business related terms such as business intelligence, analytics and maybe terms like OLAP that data mining has, but those and other associations might emerge as the data science article develops.\n\nMachine learning seems to be the most straightforward case of all. It is for the most part associated with terms referring to different scientific methods for knowledge discovery or prediction (labelled as machine or statistical learning methods).\n\nThis is by far the most broad, diverse term but also the one hardest to convert into a graph. AI by itself is associated with more than 450 nodes and I had to set a high threshold for the number of neighbours so that the graph wouldn\u2019t look like a massive ball of hair. The diversity of the nodes still made it hard to define meaningful categories.\n\nTo stay current with the spirit of the times the last clickstream network graph is for artificial general intelligence.\n\nThe nodes of the graph are not homogeneous enough to create multiple meaningful clusters. However, between nodes bearing names such as \u201cmind uploading\u201d, and \u201cintelligence explosion\u201d we can see two themes that editors and users seem to be interested in.\n\nLet\u2019s call the fist cluster \u201cAI for evil\u201d including pages like\n\nLet\u2019s close with the \u201cAI for good\u201d cluster that includes:\n\nThe clusters by themselves cannot obviously define artificial general intelligence but they are indicative of popular content associated with them.\n\nWe have explored the 5 terms and artificial neural net (ANN) was the only associated term present and prominent in all 5.\n\nIt\u2019s hard to accept that this is a coincidence. ANNs combined with modern computational advances are highly associated with recent progress in the field of AI. In some ways they helped an AI comeback after many years of AI winter.\n\nCurrently state of the art deep learning models are based on ANNs. Tensorflow, Deep Mind\u2019s most famous open source project is one of the most popular libraries for ANN applications in terms of Github interactions and Stackoverflow traffic. Taking everything into consideration it seems likely that ANNs are here to stay and we are going to hear more and more about them (my prediction).\n\nThe code to generate the graphs using the R language is available in this github repo where some of the parameters and filters choices (e.g. number of edges filter, order, degree) are available.\n\nYou can use the easy start script, to quickly experiment with how other concepts of your choice relate to each other.\n\nThis article was motivated by a combination of pieces of content, to which I was exposed in recent times and which I recommend strongly.\n\nAlso I \u2018d like to stress how open source libraries like ggplot and ggraph are really helpful when working with network data visualisation.\n\nAs you are now judging the content you have just read, and maybe already having some objections, remember: the goal wasn\u2019t to define any term but rather to look at them through the perspective of Wikipedia editors\u2019 and users\u2019 behaviour. From my end it was a bit like working on a jigsaw puzzle.\n\nOf course there is an inherent bias in that articles reflect the biases of their own authors. It is positive however is that in the case of Wikipedia, thanks to the wiki editorial model and process of control we can count on the wisdom of the crowds to mitigate any strong biases or extreme/misinformed views. Just as an example the Wikipedia article for AI has a number of editors and watchers that\u2019s in the thousands.\n\nTo conclude, an important benefit of performing network analysis with the Wikipedia clickstream is that it can shed light around what both users and contributors of the world\u2019s largest encyclopedia consider important. Based on either how they write or how they click.\n\nWhat do you think, are there any other terms worth to explore with the Wikipedia clickstream ? Do you see any other interesting uses for this dataset ? Data science practitioners: Is there a way to automate the grouping of the nodes in the graphs using e.g. NLP ?\n\nExperienced data analytics consultant, ex-Googler, in Dublin Ireland. I am currently looking to form a network of collaborators (remote or not) to tackle interesting challenges in the area of marketing and customer analytics."
    },
    {
        "url": "https://becominghuman.ai/will-bots-process-my-electricity-bill-ai-transforming-the-cx-for-utility-customers-554b25e9ab1a?source=---------1",
        "title": "Will bots process my electricity bill? AI transforming the CX for utility customers",
        "text": "The utilities industry is shifting from a highly traditional, regulation-driven environment to a technology-driven, sophisticated marketplace. Utilities, such as energy, gas, water, and waste management, already rely on smart devices for optimization of infrastructure and the supply-demand balance.\n\nThis smart utility ecosystem generates huge amounts of data that must be analyzed to extract actionable insights. Artificial intelligence (AI) is helping to analyze this data in order to optimize supply-demand ratios, deliver proactive infrastructure maintenance and predict equipment failure. AI in utilities is also playing an increasingly central role in customer-facing interactions.\n\nThere is a telling paradox in the utilities market. While more than 90% of utility companies\u2019 overall budget is dedicated to infrastructure and other operating costs, with less than 10% allocated towards customer service, the picture is very different when it comes to AI.\n\nGartner reports that the vast majority of utilities\u2019 investment in AI is earmarked for customer service. 86% of utilities already use AI in customer engagement applications, call center service and support, or digital marketing platforms, far exceeding AI use in other areas of operation. Utilities recognize that in the short-term, investment in AI can deliver the highest ROI in terms of improving speed and efficiency, enabling better data processing and analytics, and enhancing the customer experience (CX).\n\nUnlike high-risk investments that impact utilities\u2019 infrastructure, investment in customer-facing AI is considered low risk due to the maturity of the market. In addition, the large number of repetitive customer inquiries traditionally received by utilities has driven these companies to use AI in the automation of specific tasks, such as:\n\nChoosing off-the-shelf AI solutions to target specific quick-win use cases enables utilities to realize immediate gains, and increase their confidence in the capabilities of AI. Once success has been established, utilities can roll AI out to other applications and use cases, such as customer service chatbots or automated IT help desks.\n\nForward-thinking utilities have already made significant inroads in harnessing the power of AI to deliver tangible business results. Some examples of use cases where AI has made a measurable impact include:\n\nPredictive analytics are helping utilities provide better energy management services by utilizing data, statistical algorithms and machine learning techniques to identify the likelihood of future outcomes based on historical data. For utilities, that means using data-driven insights to automatically deliver timely and relevant communications that wow consumers and optimize business operations.\n\nAccording to Oracle, smart meters in the US generate one billion customer data points each day. Add to that customer information from demographics and call history, and utilities have all the data they need to predict customers\u2019 wants and needs. For example, AI-based predictive analytics can help energy companies forecast high bills before the bills are generated, and deliver personalized alerts to customers. AI also allows utility companies to segment customers and automatically target specific segments for promotions or energy-savings tips, thereby reducing operational costs, and further cutting customers\u2019 energy bills.\n\nUtility companies all have vast numbers of customers and an endless volume of daily transactions, each susceptible to human error.\n\nAccording to UIPath, utility providers who switch their business processes to robotic-process automation (RPA) see a significant decrease in human errors, usually over 60%. RPA is an emerging form of business process automation technology based on software robots or artificial intelligence (AI) workers. RPA can automate tasks such as meter reading, billing, processing customer payments, and other back office tasks.\n\nThis is especially relevant with the US Energy Information Administration reporting that there are more than 70 million smart meters installed in the United States, and that total is expected to reach 90 million by 2020. A team of RPA robots can be deployed to read smart meter information, freeing up valuable human resources.\n\nThe smart home market is in a stage of hyper growth, driven in part by the consumer-driven demand for smart energy management. According to Priori data, the global smart thermostats market grew 56% between Q2 2016 and Q2 2017 to $218.5M. The global smart lighting market grew even more \u2014 81% between Q2 2016 and Q2 2017 to $101.2M.\n\nThese smart home devices generate a massive number of support requests for installation, set up, troubleshooting and maintenance, often overwhelming customer support centers. Using AI, utilities can implement self-service capabilities that instruct customers how to install and operate smart devices by themselves.\n\nFor example, an AI-powered virtual technician can use computer vision to view the customer\u2019s environment and provide step-by-step unboxing, installation and activation instructions, eliminating the need for the customer to contact a human agent for assistance.\n\nAI is increasingly aiding utilities in managing, optimizing and maintaining not only their infrastructure, but their customer support operation as well. AI is helping utility companies automate repetitive customer inquiries and other tasks, thereby allowing them to concentrate on building customer relationships and taking their customer service to the next level.\n\nPredictive analytics, RPA and virtual assistants are examples of use cases where AI has transformed the utilities industry, delivering an enhanced CX and added value for the enterprise overall. With 83% of top European utilities executives considering AI a high to medium priority for their business, customers can expect further transformation of the utility industry in the future.\n\nThis post was originally published on the TechSee Blog"
    },
    {
        "url": "https://becominghuman.ai/how-ai-can-enhance-the-agriculture-industry-around-the-world-6bd088b262d3?source=---------2",
        "title": "How AI Can Enhance The Agriculture Industry Around The World",
        "text": "We will need to produce 70% more food than we do today to feed 2.3 billion people that will be added to the world\u2019s population by 2050. And we will have to do this in spite of increasing effects of climate change, scarcity of resources, and growing levels of inequality around the world.\n\nHow do we ensure increase in food production? How do we make sure we produce enough food to cater to the fast-growing population, whether rich or poor?\n\nWe must evaluate our current processes and actions and find ways to improve on the way we treat the earth and agriculture which essentially keeps us alive.\n\nFor many years now, agriculture as a profession has been a fertile, but not very attractive industry. This has become increasingly pronounced in recent years.\n\nDifferent countries have different needs and pain points that needs to be addressed.\n\nExample: A country like Norway is exploring ways to incorporate renewable energy into fish farming in a bid to reduce carbon emission while Nigeria, my home country, hasn\u2019t figured out how to generate enough electricity for residents and businesses.\n\nFarmers around the world face different challenges and while some of these challenges are unique to specific farmers in specific countries, a general problem they all face is increasing crop yield \u2014 producing more.\n\nThese are some of the questions that the introduction of artificial intelligence and machine learning into the agricultural sector can answer.\n\nWith the help of data scientists and machine learning, farmers can solve and answer most of the questions above using the predictive abilities of artificial intelligence.\n\nAccording to IBM, 90% of all crop losses are due to weather problems. By feeding weather data into machine learning algorithms, farmers will be able to predict the weather, know when to plant and how best to avoid damage.\n\nFarmers can feed simple bits of unstructured data like what the weather is like on a particular day and how it has affected their crops into an artificial intelligence algorithm such as WorkFusion\u2018s. This algorithm takes this data, works through it repeatedly until it notices patterns and starts to make the most accurate predictions possible. This way, the farmers know what is coming and are prepared for it.\n\nIn India, a Bengaluru-based startup named CropIn Technology Solutions offers a B2B farm management software that uses \u201cMachine Learning, Satellite Monitoring, Data Storage, and Weather Analysis technology mechanism to maximize yield per acre whilst identifying the risk and favourable areas for cultivation,\u201d according to Entrepreneur. Its software serves big seed companies, crop insurance providers, governments, and banks.\n\nPeople who already have data science competence need to turn their attention to the fruitful field of agriculture. IBM is doing so with IBM Watson Machine Learning, but they are just one company.\n\nMore entrepreneurs need to arise to this challenge-cum-opportunity to save the planet by focusing on the impact that artificial intelligence can have on agriculture (food!) around the world.\n\nOur farmers need people that will help them analyse data around factors like humidity, soil pH, air pressure, precipitation and temperature. These are the factors that will determine plant strength, predict the best time for irrigation, and increase food production in a time when the world really needs it.\n\nPS: Much appreciation for @farmcrowdy @ghalaniapp @thriveagric @farmerline @trotrotractor @agrocenta and other #agritech startups in Nigeria and Ghana for making it their purpose to improve agricultural processes in this region."
    },
    {
        "url": "https://becominghuman.ai/machines-demonstrate-awareness-of-future-and-past-edc6e79eee30?source=---------3",
        "title": "Machines Demonstrate Awareness of Future and Past \u2013",
        "text": "In this series, I prove machines are conscious by exploring various aspects of consciousness, showing how machines possess each one. If you feel I\u2019ve omitted an important aspect of consciousness, I am open to any challenge. For further background on my project, click here.\n\nToday we\u2019ll be performing a little bit of time travel. The first two articles focused on a very immediate kind of consciousness, being aware of oneself, being aware of the present. Many authors view these as a \u201clower\u201d level of consciousness, rather commonly found in the animal kingdom.\u00b9 As the story goes, only humans possess a higher-order ability to think about what has happened and plan for the future. For example, neurobiologist Gerald Edelman:\n\nWe won\u2019t consider \u201cconsciousness of being conscious\u201d in this post\u00b2, and we\u2019ll only lightly touch upon \u201csyntax and semantics.\u201d\u00b3 What I will illustrate is how machines demonstrate the other components of Edelman\u2019s definition above.\n\nMachine demonstration of temporal consciousness is not hard to find. We only have to look at our phones to see these properties demonstrated. I look at my text message history, I see the past is remembered. My phone also models the future, this is demonstrated every night when I set my alarm. My phone will wake me up. I can trust it with at least this very basic understanding of future.\n\nMy text message history goes back years, and I can set an alarm for a calendar invite in the year 3150. This is a far longer reach than any living organism.\n\nMachines were capable of dealing with the past from the earliest days of electronic computation. Early electromechanical computers such as the Mark 1 (built in the early 1940\u2019s) were designed to perform the lengthy calculations involved in solving differential equations. These machines could keep track of a few numbers internally but for large calculations, more storage was often required. In these cases, the machine would write numbers to punch cards which would be fed back into the machine by the operator.\n\nTaking past results, and performing further actions on them is fundamental to computation. Turing\u2019s machine, the theoretical model of computation,\u2074 is an infinite tape. A playhead moves forward and backward along the tape, reading and writing information. There is no meaningful computation without access to the past.\n\nThis is not the first thing we humans think of when we think of the past. But it is a familiar experience for anyone who has ever made a shopping list. Your list is an artifact of your past that you are utilizing to buy groceries. This is the sense of past most frequently experienced by computations.\n\nAnd of course, contemporary operating systems track time in more mundane ways, with things like system logs and cron jobs. These OS structures are means through which the entire machine organism can reckon with future and past.\u2075\n\nHow close are we to Edelman\u2019s criteria? He describes an individual\u2019s past, being utilized for future plans. While I have shown you the individual\u2019s past, and their future plans, I have not talked much about utilization. Let\u2019s focus on planning.\n\nLow-level planning occurs in the operating system, the part of the machine most concerned with managing its individuality. The process is known as scheduling. Computers multitask, they run multiple programs at once. Under the hood, there is often a single-core CPU, executing only one of these programs at a time. How does the OS decide which program to run when?\n\nThe question first arose in the 1950s. Machines were getting big enough to store multiple programs in random-access memory. A new innovation called the interrupt (a fundamental component of machine self-awareness discussed in part 1) allowed meaningful signals to be sent around the machine. When a program had to deal with something slow, like an I/O device, it sent an interrupt. The CPU could then decide to do something else with its time.\n\nAs early as 1959, nontrivial schemes such as shortest-job-next planning were being deployed. Within a decade, time-sharing systems hopped back and forth between programs quickly to present the appearance of running multiple programs simultaneously. People hooked up multiple monitors and keyboards to large computers, and mainframes were born. Dozens or hundreds of computer terminals, all running off of a single computer. Yet another miracle of machine consciousness that no human would be capable of.\n\nAs operating systems evolved, the scheduling process became more intelligent, and started to take the past into account when planning for the future. For example, consider the Linux 2.6 scheduler. For each running the program, the scheduler tracks a heuristic, called the dynamic priority bonus, which takes note of the time a task has recently spent sleeping and running. This is a model of the past that is used for planning the future.\n\nEdelman\u2019s higher-order consciousness goes \u201cbeyond the limits of the remembered present\u201d. My prior example is a rather short-term kind of thinking.6 Can machines think more deeply about future and past?\n\nIt turns out that these questions were asked by the earliest pioneers of computation. The desire to simulate weather was a motivator for Von Neumann throughout the 1940s in developing EDSAC, the successor to ENIAC. Computers have been predicting the weather since 1950, and today perform the feat with startling accuracy. While these traditional techniques are purely forward-looking, the latest machine learning prediction techniques use data from the past to feed learning systems. We cannot snub this ability as basic, it is beyond the ability of humans without machine aid.\n\nWeather prediction is not the kind of existential, individual planning that Edelman describes. Most people see computers as idle machines that wait around to be touched. Surely these computers do not plan for the future. Everything in the computer is so short-term, pre-canned.\n\nThat truth holds up only so long as we look at machines with no large goals. Machines do not make the kind of long-term life plans that humans do, my iPhone does not want to have a family and then enjoy a seaside retirement.\u2077 However, there are machines with long-term goals.\n\nConsider the computers that competed in the Darpa Grand Challenge. These machines had to travel 150 miles. Suddenly, the future and past matter a bunch more, and we see adaptive strategies at work that continually revise the future plan based on events as they unfold.\n\nThis is a common pattern in many topics of machine consciousness. Machines are really good at some kinds of remembering/planning, while humans are really good at other aspects of remembering/planning. They work in different ways for the different organisms. My central thesis is that they both are forms of consciousness, and we shouldn\u2019t say one form is more or less conscious when their styles are merely different.\n\nThis post wraps up the basics of my series. Awareness of self, present, future, and past come up in virtually every conversation on consciousness. I hope this framework provides a useful way for thinking about these aspects in machine terms. In the following posts, we\u2019ll get deeper into particular philosophical definitions, esoteric takes on what consciousness means, and of course plenty of machine examples."
    },
    {
        "url": "https://becominghuman.ai/heroku-for-deep-learning-floydhub-6bbc0fb6a77e?source=---------4",
        "title": "Heroku for Deep Learning \u2014 FloydHub \u2013",
        "text": "That\u2019s what they say!\n\nFor all those like me, who are looking for an alternate service like Amazon EC2/Azure/Google Cloud but with minimum learning curve. Here\u2019s what I found couple of days ago, FloydHub i.e. Platform-as-a-Service for training and deploying your DL models in the cloud. Setting it up is super easy. It takes the pain of setting up libraries and environment for deep learning like CUDA, cuDNN, etc along with popular deep learning frameworks like Keras, TensorFlow and PyTorch already in-place, that\u2019s what impressed me. Apart from that, they provide almost all popular datasets for the user to get started with. FloydHub offers 100 hours of free GPU power and also No credit card needed for sign-up. Under the hood they use AWS reserve instances but are providing the simplicity as a service model, now you get why it\u2019s called Heroku for DL.\n\nSo, today we will see how you can setup, deploy and train your model on FloydHub with blink of an eye.\n\nRunning out your code over the Floyd services requires you to follow 4\u20135 steps in order.\n\n3. Login to Floyd through the CLI.\n\nYou will be prompted to enter the password after this, thereafter, you should successfully login into your user account.\n\nP.S. In case, you get \u201cAttributeError: \u2018module\u2019 object has no attribute \u2018OP_NO_TLSv1_1\u2019\u201d error while trying to login. Install the below version of Twisted."
    },
    {
        "url": "https://becominghuman.ai/understanding-autoencoders-unsupervised-learning-technique-82fb3fbaec2?source=---------5",
        "title": "Understanding Autoencoders \u2014 Unsupervised Learning Technique",
        "text": "Recently, I came across a very good article How to autoencode a pokemon? Believe me or not, this article motivated me to explore this topic. So here I am, today we will see how we can use this technique in the field of Natural Language Processing. If you would have noticed the thumbnail image properly, then you must have noticed that the input to the network is same as the output from the network. That\u2019s exactly are Autoencoders.\n\nNow, you might be thinking where are such networks used in practice? Mostly, these are used in data compression, dimensionality reduction, text generation, image generation, etc. For all of this to achieve, we are interested in the Latent Space Representation i.e. the layer about which the network is symmetric, for which, we train our model on a data for certain use-case and then chop off the decoder part of it and use the encoder for other likely datasets. You can think of it as we could use the encoder as a feature detector that generates a compact, semantically rich representation of our input.\n\nBuilding Autoencoders in Keras has great examples of building autoencoders that reconstructs MNIST digit images using fully connected and convolutional neural networks.\n\nAutoencoders in general consists of two parts, an encoder and a decoder. The encoder will read the input and compress it to a compact/compressed representation, and the decoder will read this representation and recreate the input. The whole process is about trying to learn the identity function with the minimum reconstruction error.\n\nAnyone, who has tasted the flavor of working with NLP knows the power and usefulness of word embeddings. Almost all the solutions to the use-cases in NLP are built on top of it.\n\nLet\u2019s consider an example \u2014 The cat is chased by the dog.\n\nBy using Google Vectors or GloVe Vectors, we can represent this sentence as the sequence of each word as 300 dimension vector and a common strategy to get sentence vector is to average out the word vectors over the sentence length. But, if you analyze this approach, it\u2019s more of a bag of words approach that capture the semantics of the sentence and at the same time discards the syntactic structure of the sentence resulting in \u201cThe cat is chased by the dog\u201d and \u201cThe dog is chased by the cat\u201d as the same thing.\n\nThere are a couple of other strategies that are used in practice apart from averaging technique, like, traversing the syntax tree in DFS fashion and averaging out the phrases and eventually averaging phrases to reach the sentence representation. Today, we will see how we can use RNN Autoencoders to get sentence vector representation.\n\nBelow is the overall architecture that we will be building.\n\nOur autoencoder model takes a sequence of GloVe word vectors and learns to produce another sequence that is similar to the input sequence. The encoder LSTM compresses the sequence into a fixed size context vector, which the decoder LSTM uses to reconstruct the original sequence.\n\nAbove snippet is the main boilerplate for building any Autoencoder that deals with textual data. Since the objective of the autoencoder is to produce a good latent representation, we compare the latent vectors produced from the encoder using the original input versus the output of the autoencoder using cosine similarity between original sentence vector and generated one. If you are lost in the syntax anywhere read this."
    },
    {
        "url": "https://becominghuman.ai/will-we-be-able-to-predict-life-on-other-planets-using-artificial-intelligence-b5536feb36b3?source=---------6",
        "title": "Will We Be Able to Predict Life on Other Planets Using Artificial Intelligence?",
        "text": "In this blog post I will be mainly taking from The Royal Astronomical Society\u2019s, namely Mr Christopher Bishop, presentation from his and his team\u2019s research at the European Week of Astronomy and Space Science (EWASS) in Liverpool on April 4th, 2018.\n\nIt was very recent and I thought it to be very relevant and interesting to mention on my blog since I\u2019ve been in touch with some very curious and remarkable minds *cough *cough you know who you are \u2026 and I thought that you\u2019d be interested!\n\nThe research they presented was that they use artificial intelligence (AI) and the method of automatic machine learning to find whether there is life on other planets.\n\nMachine learning is a process that learns from outweighing the risks of different outcomes of actions by trial and error. In that way it is very similar to the natural process of evolution.\n\nIt has been actively developed by companies such as Google and supported by investors such as Elon Musk, Mark Zuckerberg, and Ashton Kutcher, who invest into companies like Vicarious and Neuralink.\n\nMachine learning is also able to process vast amounts of complex observations all within a few days, if not minutes. It functions similarly to evolution. Evolution, however, achieved at human intelligence without aiming at it. Only a small part of evolution\u2019s purpose was for intelligence. The rest was for survival, reproduction, and developing an altruistic moral system.\n\nNick Bostrom in his book titled SuperIntelligence says that evolution created intelligent life by \u201clucky coincidence,\u201d to the point that replicators for evolution come about every 1\u00b3\u2070 planets. The atmosphere for most planets does not permit a natural habitat where natural selection could take place.\n\nIt would be impossible to explore the Universe to find any form of life by ourselves. That is why researchers at The Royal Astronomical Society use Artificial Intelligence, specifically the mentioned method of machine learning, to classify planets into five types based on habitable Atmospheric differences in our Solar System between Earth, Mars, Venus, and Saturn\u2019s moon Titan. By comparing these planets and their differences to those of our Solar System with atmospheres that support human life, they are then able to use the stored data for future interstellar missions that will help the potential of finding life on other planets.\n\nResearches look into Artificial Intelligence because of the speed at which it will be able to process all the probabilities of different atmospheres. AI is capable of thinking \u201cabout a million times faster than the minds that built it\u201d said Sam Harris, the neuroscientist and best-selling author, in his TED talk in 2016. As AI is developed, it will be able to produce 20,000 years of human intellectual work within a matter of weeks. Or as Harris stated in his talk,"
    },
    {
        "url": "https://becominghuman.ai/4-industries-artificial-intelligence-is-transforming-fe27b750769b?source=---------7",
        "title": "4 Industries Artificial Intelligence Is Transforming",
        "text": "Down to its core, artificial intelligence (AI) describes the advanced process for a machine to make decisions based on logic. AI has already made a global impact with the creation of conversational chatbots, self-driving cars, and recommendation systems. AI is growing in its reputation among business leaders as an emerging asset to the workforce and is presently found in multiple industries already, transforming the the way businesses and societies operate.\n\nThe adoption of AI is on the rise inside the healthcare industry, solving a variety of problems, saving money and paving new roads to a broader understanding of health sciences. AI technologies in the healthcare industry are mostly used to efficiently collect individual patient data. Now we are seeing AI technology gather more in-depth data for more pressing conditions like asthma management, Parkinson\u2019s tremors and heart conditions.\n\nHealthcare is even seeing assistance in AI-driven diagnostics that sort through massive amounts of data all at once to suggest possible conditions and ways to treat them. Inside the hospital, there is now AI-assisted anesthesia delivery and professional AI support during medical procedures. According to Health IT Analytics, \u201cRevolutionary changes in health and health care are already beginning in the use of AI-based health and health care tools.\u201d\n\nAI is also dominating the finance market faster and more efficiently than humans \u2014 from managing investments, to collecting financial data and using predictive analytics to anticipate changes in the stock market, which big financial firms use as a guide for investment opportunities. Almost every company in the financial technology sector has already started using AI to save time, reduce costs and add value.\n\nCustomers that bank with Wells Fargo are now directed to an AI-driven chatbot when they want to discuss account information or reset their account password. JPMorgan Chase now uses AI as an image recognition software to analyze legal banking documents that extracts particular information and clauses in seconds compared to the 360,000 hours it takes to manually review 12,000 annual commercial credit agreements.\n\nGeorgia Institute of Technology professor Goel, who teaches an artificial intelligence class, recently developed an AI chatbot to help answer specific student questions online. Goel told Digital Trends \u201cWe thought that if an AI TA (teacher assistant) would automatically answer routine questions that typically have crisp answers, then the (human) teaching staff could engage the students on the more open-ended questions.\u201d Some professors and teachers worry that AI technology may replace them in the near future, but many developers and educational technology researchers suggest that AI technology is meant to assist teachers rather than take their jobs.\n\nThis includes exploring where courses could be improved. From finding gaps in lectures during class to giving students extra educational support, AI technology can provide immensely helpful feedback to both educators and students to rapidly improve educational methods and refine the curriculum.\n\nTransportation is the most known industry where AI technology is discussed. While self driving cars and trucks may be the most anticipated developments, AI can go as far as collecting data from a variety of sources to optimize and adjust the shipping routes and simplify distribution networks.\n\nAccording to Engadget, Japanese shipping companies want to build self-navigating cargo ships. \u201cThe plan is to implement an AI-driven steering system that could lay out the shortest, safest and most fuel-efficient routes based on information about things like weather and any obstacles that might be in a ship\u2019s way.\u201d Working alongside shipbuilders, their goal is to develop new technology that can predict malfunctions, reduce maritime accidents and improve efficiency.\n\nArtificial intelligence is steadily transforming day-to-day businesses and way of life. With further analytical developments and improvements in AI-driven technology, and the further we push AI logical processing, society will redefine efficiency."
    },
    {
        "url": "https://becominghuman.ai/generating-social-media-cover-pictures-from-my-followers-ones-397ec9f953e8?source=---------8",
        "title": "Generating social media cover pictures from my followers\u2019 ones",
        "text": "Generating social media cover pictures from my followers\u2019 ones I wanted to change my social media channels cover pictures, so instead of thinking what to put there, I delegated this task into a neural network, feeding it with all the cover pictures of my followers. Neural networks are hungry, you need to feed them First I needed to extract thousands of cover pictures, so I wrote a script using Python and the Tweepy library to scrape all the cover pictures of my Twitter followers + my Facebook friends. Here is an screenshot of the script doing its work: Python script extracting all the cover pictures of my followers And the result below is going to be the input of the neural network, as we see, there are some pictures about landscapes, text, people, random objects, colors\u2026 a very diverse input, so it shouldn\u2019t be easy for an AI to create a meaningful output, but let\u2019s see!\n\nI have chosen a GAN, cause this type of neural networks are very efficient learning patterns with a good grade of abstraction. GAN, Generative Adversarial Networks learn like a baby, it is like giving some colored pencils to a 1 year old baby, he will start painting random stuff, and his parents will say: \u201cthis painting is ok!\u201d and \u201cthis is not!\u201d. The GAN has a generator (the baby!) that draws random pictures from random noise, and it has a discriminator (the parents) that compares the paintings from the generator with real pictures, after some iterations, the \u201cbaby-generator\u201d will learn how to create new meaningful paintings. In the outputs we can find that, surprisingly, the neural newtork has detected the text patterns in some of the covers of my followers, cause it is trying to imitate some kind of messages in a language I don\u2019t understand. Maybe it is trying to tell us something? :-) The neural network trying to imitate texts If we return to the screenshot of the inputs images below, we see that people like to have a landscape as their cover picture:\n\nThat\u2019s why the neural network has become a very decent landscape painter, it draws better than me, it is awesome how well is the light represented in some of these mountains and sunsets images. I bet you that if you put any of these images in a modern art gallery, visitors would like them, and even buy some of them. These paints are so abstract because between texts and landscapes there are a lot of different contents being used in cover pictures, that\u2019s why the neural network is getting a certain level of randomness, and \u201ccreativity\u201d: Some abstract images drawn by the neural network. This is art. But, who is the owner? It is crazy how a neural network by analyzing only a few thousands of cover pictures, has learn to create this meaningful and beautiful patterns. This is art! But who is the artist? The one that put all the pieces together? The followers that fed the neural network? The neural network designer? The author of the first paper speaking about GANs? We are now speaking about images, but also it is perfectly possible to teach a neural network to be a creative architect, or music composer, etc. What is going to happen with art in the future? In the very near future, we are going to have art exhibitions made by machines, and machines usually are better than us detecting if something has liked us or not. Also they are better generating new artworks from the things we liked, cause we are in the era in which art is being transferred to the machines. Enjoy it!"
    },
    {
        "url": "https://becominghuman.ai/introduction-to-recommendation-system-in-javascript-74209c7ff2f7?source=---------9",
        "title": "Introduction To Recommendation system In Javascript",
        "text": "I bet you\u2019ve come across a recommendation system in diverse ways, ranging from a commerce website where you buy goods,like Amazon;social network like Facebook; video/movies site like You-tube and Netflix. These site use the past record of movies,goods and friends to recommend new ones for you. \n\nIn this post I will be introducing you briefly on how the system work in javascript. Will we look at different way of representing the case so that they all work with the same set of algorithm and will we create and example with a small dataset on movies critics and using real movie data called MovieLens.\n\nWe all know that the best way to get recommendation for a products,movies is to ask your friends. And I will tell you that the only way, i have ever watch the best movies is through friends that have a good taste for good movies, like Game of throne, which I think was boring at first,was assured to me by a friend that it\u2019s interesting ,so I watch it again and see the beauty, and sure that most of us had experienced this before.\n\nWhat of, if you have a group of friend, who have a good sense for Fashion and you are confused of the cloth to wear to a date. Each one of them make a suggestion on which of the cloth is better than one and suitable for the date. Each one of them have different option and this can confused you the more on which cloth to wear for the date.\n\nAs more option become available,then it become less practical to decide what you want by asking a small group of people. This why the technique called collaborative filtering was developed.\n\nCollaborative filtering algorithm skim through a large set of people and finding a smaller set with taste similar to yours. It look at things they enjoy and combine them to create a ranked list of suggestion.\n\nWe create a nested object in javascript to store the movies critics , containing different users and the movies they like which is ranked from 1\u20135.\n\nAfter we\u2019ve collect the information of what the users like, we find a way to determine similar people who also like the same thing. We do this by calculating the similarity score which involve comparing the user with all other person. We can either use Euclidean distance or Pearson correlation.\n\nOne of the simplest way to calculate similarity score between two user. It involve representing the data as a point on a chart(x-y axis), and each user having a point e.g from our movie critics we can draw the chart of people who watch and .such has\n\nThe figure above show that has been plotted at on the axis and of axis. The closer a set of people are in the preference space the closer their preference are.since the chart is two dimensional you can only look at few ranking at a time\n\nfor example to calculate the distance between Lisa Rose and Mathew\n\nThis calculate the distance, which will be smaller for people who are more similar. since we want the value to be high for similar people and the value should range between 0 to 1. so similar user will have a value close to 1.Hence we change the way we implement the formula.\n\nso let create a function to calculate the euclidean distance for our movie critics\n\nwe create an helper function to calculate the length of an object\n\nLet compute the overall euclidean distance between Lisa Rose and Mathews\n\nThe correlation coefficient is a measure of how well two set of data fit into a straight line.Although the formula is more complex to Euclidean score but gives a better result where data isn\u2019t normalized\n\nThe function will return a value between -1 and 1.A value of -1 means they are not close or similar, and a value 1 means that the two people have exactly the same rating for every item.\n\nAfter the similarity score has been created,we create a function to ranked user in the movie critic in relation to the user we want i.e find other user that have the same taste of movie .\n\nThis function check if the user is not compare to itself when comparing the users.Then it return an number of user of the sorted result.\n\nFrom this we know that should be reading movie reviews by ,since their taste are similar.\n\nWhat we just created is not what user will actually want to be engage in. for instance we can just expect user like to start searching for movies that had watched, that will be too tedious,apart from that it could also return reviewer who strangely liked a movie that got bad reviews from all other critics return by .\n\nTo solve these problem we need to score the items by producing weighted score that ranks the critics, then take the votes of all other critics and multiply how similar they are to the user we want by the score they gave each movie\n\nThe table above shows the correlation score for each critics and the rating they gave the three movies. The above table is a score matrix for . since Toby has not rated some movies like . win the column we weight every rating by multiplying it with their similarity to . Then we divide the total of each column by the total of the Similarity score of user that rate such movie.\n\nThe reason for dividing, is because a movie reviewed by more people would have a big advantage. for example was reviewed by everyone,so it total is divided by the sum of all the similarities.While was not reviewed by ,so the movie is not divided by the sum of all other similarity.\n\nIn the next tutorial we learn how to know which customer/user s will or should use the item next, and we practice with the MovieLens dataset."
    },
    {
        "url": "https://becominghuman.ai/decentralized-supervised-neural-network-on-the-blockchain-giving-mining-a-good-purpose-3e64888caa",
        "title": "Decentralized supervised neural network on the Blockchain: Giving mining a good purpose",
        "text": "There is another task that GPUs are very good doing, that also expends a lot of time and energy but it is much more useful for us than calculating random hashes like in Ethereum or Bitcoin: the optimization of a neural network .\n\nThe value of the gold is backed by the value of the resources needed to extract and transform the raw gold, plus the value of the market. Today Bitcoin and Ethereum are based in the PoW (Proof of Work) schema, this is a system created to emulate the difficulty of extracting physical things with value, like the gold, but this system is not environmental-friendly cause we are wasting a lot of energy to solve some mathematical operations that are not being used for any useful purpose.\n\nI am going to speak about a use case where the blockchain technology can add a lot of value to the Artificial Intelligence field.\n\nDifferent datasets and different neural networks can be trained over the blockchain, to explain the use case I am going to use the Celeba50K dataset, which contains 50.000 different faces of celebrities and a GAN neural network.\n\nLet\u2019s suppose that the nodes are training a neural network and running an IPFS node which stores: the datasets with thousands of images of celebrities, and the checkpoints file that contains the weights, which are the values that the neural network has learned during its training.\n\nThe network also can decide to improve a current dataset or to request a new one, so we will end with a decentralized network of knowledge on multiple datasets.\n\nLet\u2019s see both scenarios, a use case training an existent dataset and another one requesting new datasets from the nodes:\n\nThis is the passive way of mining, in this case with the Celeba50K we have a GAN neural network, so each node will try different values for the generator and the discriminator to calculate the weights. In GANs the generator tries to generate different solutions from noise that are similar to the real input, and the discriminator tries to detect if the generated solution is a fake or not.\n\nIn the current scenario, each node receives the images from the Celeba50K dataset and starts the training using the GPU. When training is finished, the node shares the weights with the network, and each node checks how good has been the training compared with others. We can compare how accurate is an output compared with the inputs. The better the training, the more reward.\n\nThis is the active way of mining, let\u2019s imagine that we want to generate a new dataset, and we want to request the data (images, texts, sounds\u2026) to the network. The nodes will vote for new datasets and the network will seek the consensus to decide which new datasets are accepted. Then the dataset request is broadcasted to the network and the nodes are going to be able to submit new items for the dataset, the accepted submissions will be paid by the network.\n\nThe network always will have a decent knowledge, so it will be able to check the submissions with some accuracy, for example if we are requesting \"images of bridges\", we can easily discriminate fake objects, ban unlegit nodes and promote trustworthy ones.\n\nSo if you want to be a successful node you will need to have a lot of computing power with GPUs (Proof-of-Work), and a lot of free HD space to store the datasets (Proof-of-Capacity).\n\nThe dataset sizes to train neural networks are usually VERY large (MIT Places205 contains 2.5 millions of images and its size is 1.6TB), adding some hashing operations to check that the data is really in your computer, can lead to a good way to create a Proof of Capacity (PoC).\n\nThis combined schema of PoW + PoC is a new concept of combined mining.\n\nI can see a relatively close future where there is a shared network of AI knowledge where everyone can connect to, to feed it with new data, or to use it as a service in any of our home, work or personal gadgets.\n\nIs this the path to the decentralized superintelligence? Should we implement a stop button for a decentralized Skynet? :-)\n\nLet\u2019s see, for now the humans are more excited than scared about AI, and it should always be like this."
    },
    {
        "url": "https://becominghuman.ai/innovation-and-the-future-of-africa-a-tale-of-two-countries-280115c97b1",
        "title": "Innovation and the future of Africa: A tale of two countries",
        "text": "Too many times, when we speak about the future of Africa or the future of technology in Africa, we do so with an air of ignorant homogeneity.\n\nWe forget that each country operates by its own policies and that these policies affect its rate of growth. We forget that each country has its own cultural practices and a political history that affects the way the citizens conduct business.\n\nWith predictions of a huge population boom, advanced technology and the ubiquity of smart devices, we get excited and optimistic about \u201cAfrica\u2019s Rising,\u201d and forget that Africa is not one country \u2014 it\u2019s a mesh of 54 diverse nations with their governments, numerous local religions and over 2000 indigenous languages.\n\nWhile some are progressive in the sense that they study and embrace technology knowing it will impact every industrial sector and change, improve, distort the current standard of living of their citizens, others are either moribund or regressive. If they are bothered about this future, it is not obvious from their policies and actions.\n\nTwo prominent examples from each group are Rwanda and Nigeria.\n\nBoth countries are definitely on the landscape when you talk about Africa Rising in terms of technology, startups, investment and innovations. Both have a history of ethnic civil war and genocide.\n\nWhile Rwanda has progressed massively since the 1994 Rwandan Genocide, Nigeria\u2019s growth has been start-stop, slowed down by corruption, lack of vision and wicked problems, like Victor Asemota calls in this article.\n\nUnder President Paul Kagame, Rwanda has seen tremendous growth. The country has introduced policies that actively encourage economic development and the ease of doing business.\n\nThe year before that, it ranked 56th globally. This growth was sparked by specific policies that make it easier for startups and SMEs to operate.\n\nThe country upgraded its systems to make available the Building Permitting Management Information System. This BPMIS cut down the time and cost of obtaining building permits by at least 10 percent. Rwanda also improved its tax payment system and improved its ability to protect minority investors by 16.66 percent.\n\nFor example, a startup called Zipline, in partnership with the Rwandan Ministry of Health, uses drones to deliver blood to citizens around the country. In the past year alone, it has delivered more than 5,500 units of blood. This has helped reduce maternal mortality rates and drive down incidences of malaria-induced anaemia, mostly in children. No other country in Africa has yet embraced this technology.\n\nNigeria, on the other hand, has not lived up to its potential. It is a country still very dependent on oil as its primary revenue source. This, so much, that a crash in the global oil price in 2015 sent the country into its worst economic recession in 25 years, erasing 55 percent of its oil revenues.\n\nDespite its population of 180\u2013200 million people, who are predominantly young; Nigeria, as a country, has made very little technological advancement. I write about the dangers of letting young people grow in such unsupportive business environment here.\n\nGovernment short-sightedness and mispriorities, power supply deficit (addressed in-depth by Edmund Olotu), expensive electricity bills, bad roads, security issues, real estate deficit and poor education systems are just some of the problems plaguing Nigeria and holding back city-level technological advances.\n\nThe country has a vibrant startup scene, especially in Lagos, in spite of Nigeria. Being the first stop during Mark Zuckerberg\u2019s 2016 Africa visit validated this. Mark had meetings with Federal Government officials and Aso Villa Demo Day was organized days later, but little progress has been made on that initiative since then. The Demo Day has never been organized again and has now become a controversial subject, tainted by allegations of mismanagement.\n\nOne way they do is by adopting new technologies to improve their business processes and create more jobs for the youthful population. Recently, Standard Bank group, one of Africa\u2019s largest banks, employed WorkFusion to improve its customer onboarding process, cutting down the time from 20 days to 5 minutes, and creating new and more fulfilling roles for its employees.\n\nIt is only logical to conclude that while some African countries will continue to leapfrog and run with technology; for various reasons, many others will continue to regress.\n\nSo, when we speak of Africa and the future, we must do so with care and specificity, lest we ignore the many complexities that surround the continent, and the countries within it."
    },
    {
        "url": "https://becominghuman.ai/journey-of-a-digital-traveler-how-artificial-intelligence-digitalization-transforming-the-afc6b30b4c46",
        "title": "Journey of a Digital Traveler : How Artificial Intelligence & Digitization Transforming the Travel\u2026",
        "text": "The typical journey of a digital traveler can be divided into five major phases. And artificial intelligence & digitization has already been transforming many aspects of all those five stages significantly. These stages are loosely termed as (1) Dreaming stage, (2) Planning stage, (3) Booking stage, (4) Onsite experience stage and (5) Feedback & experience sharing stage.\n\nI personally believe that every travel plan starts with an amazing dream and the source of that dream could be anything. In this dreaming stage people usually looks for some kind of an inspiration for bringing that dream into reality. So, they normally start going to the internet to get ideas about their dream travel destinations and the adventures & activities to do there. While searching so in the internet they may find a banner ad at the extreme right hand site of a web page or in an online travel magazine. That\u2019s basically an ad rendered by a travel company which encourages the user to click on the ad, go to their site, select a travel plan and then do the bookings. It\u2019s a long process and there is a possibility that the user may lose his/her interest in between and simply go to some other site and start a fresh search.\n\nNow with the increasing proliferation of Artificial Intelligence and intelligent conversational interfaces, the travel companies has replaced that banner ad with an Artificial Intelligence Chatbot within that ad unit. This enables user to directly talk to the Chatbot and get ideas about different travel destinations, information pertaining to cost and logistics and can finally book the flight and hotel tickets in a very natural & conversational way. And if you use the Chatbot couple of times for your vacation planning and booking, it is intelligent enough to automatically register your preferences and can directly recommend you the right travel plan next time in a very personalized manner which suits your requirement. This personalization of offers is extremely important today for any business because as per some studies, approximately 70% of the users want personalized and tailored information as per his/her preferences and not the generic ones.\n\nTravel sites like Kayak and few other sites are providing similar conversational interfaces to their customers for planning and booking travel where customer can interact with a Chatbot in natural language either through text or voice and can do planning and booking.\n\nNormally people spend more time in planning a holiday trip than the actual trip. Here you can see AI is been used in many channels where the customer is engaging. Most of the popular travel sites and mobile apps uses Chatbot as their interface with the customers. Channels like Facebook, Slack and Skype; travel sites like Booking dot com, Sky Scanner have already implemented Chatbots in these channels not only for FAQs and queries but also to help their customers for searching, booking and getting help on other aspects of travel. There are also other advanced AI applications that are capable of reading your email and calendar and recommend you travel plans proactively like travel site Hipmunk.\n\nNow let\u2019s come to the most exciting phase, the onsite experience stage. Use of AI is widespread here too. There are couple of travel companies which provides predictive analytics of different events which may happen and the travels may experience during their travel. For example, flight delay is now being predicted by applying machine learning techniques into different data sources which may include air traffic control data, weather data, booking information or even some news or flight maintenance related data to provide meaningful insights and predict any delay in the flight. Google Flight can predict a flight delays with more than 80% confidence.\n\nMany of the airlines are also now investing on predictive maintenance. So, using machine learning and other techniques like IoT to predict when an aircraft needs to go for maintenance. So that it can also help avoiding delays and cancellations due to maintenance related issues if the same can be predicted well in advance.\n\nMachine Learning, Deep Learning, Virtual Reality, Augmented Reality and Simulation techniques are now extensively used in the Travel Industry. Most of our digital experiences till now has been experiencing everything and interacting with a screen or a device from outside. That screen could be an interactive screen of your mobile device, your laptop screen, iPad or even a voice receiver. But the capability of Virtual Reality to put you right at the middle of that experience and you interacting with those experiences all around is mind-boggling.\n\nPre-Travel Experience with Virtual Reality includes providing an experience of the places that you are planning to visit and inspire you to be there. Hotel Marriott in US, using Virtual Reality provided pre-booking experience to their customers of tourist places just to give them the feeling of the location and inspire them to plan a vacation there. Travel companies are already adopting this in many areas. For example, Thomas Cook in UK uses it for up-selling seats just to give customers pre-booking experiences of difference of an economy class and an economy plus class seat so that customer can just experience the difference of that extra six inches of leg room in advance before making a booking choice.\n\nQuantic airlines has implemented Virtual Reality for entertainment of their first class passenger in the long distance flights by giving them a new and different experience while they are in a long distance tiring and boring flight. It\u2019s kind of immersing someone into a completely different experience while in actual he/she is in a completely different environment or place altogether.\n\nAI Facial recognition is also pretty interesting technology used by couple of airlines. Delta for example has leveraged this technology for baggage check-in. They use the facial recognition to identify the person and compare it against his/her photograph in passport or driving license used during the booking to make sure they are one and the same. ZetBlue is using the similar AI technology for their passenger boarding on to the flight. Similarly Dubai airport uses identity tunnel to identify the person while he/she is walking through the tunnel to the security checking. These technologies contributes a lot to reduce the overall friction in the travel.\n\nNow when you reach the destination in your hotel, here also AI is used to enhance your experience, like identifying you automatically in the hotel lobby and do an auto check-in into the hotel. Knowing you have arrived at the hotel lobby, automatically turning your lights and AC on in your room, allowing you to interact with a Chatbot for room service or for any queries and concerns during your stay at the hotel. For example, Radisson uses a Chatbot for room service. Just imagine all the devices and amenities in your hotel room are connected devices and can be controlled remotely from your smart phone app provided by the hotel. Providing personalized menu cards in the restaurants for Food & Beverage only of your choice.\n\nNow during your travel and sightseeing around the places, AI enabled devices gives you the opportunity to explore local places and the culture. AI applications like Google Translate can help you translate anything to your own language during your travel. Also, AI enabled digital technologies are changing the way the tourist exploring different places. Using \u201cTimes Traveler\u201d App visitor can explore how the Berlin Wall used to look like in the past. To do that, they simply have to direct their smart phone to the landmark, the app recognizes the spot with AI and plays respective historical footage.\n\nComing to traveler safety, Simulation techniques are used to train the rescue staffs to give them an experience very similar to the live scenarios during conditions where rescue operations need to be performed.\n\nThe Self-Driving car in future going to revolutionize the travel industry. Just imagine, a self-driving car picking up a customer from Airport to the hotel or dropping him to a meeting. Let\u2019s wait for that day when a self-driving car comes for your pick up at the airport, that\u2019s going to be an amazing experience.\n\nThe last stage of travel which is your feedback after you come back from your dream vacation. The feedback could be in many different forms like sharing your experience with your friends and relatives and encourage them to plan for similar vacations. Some people will share their travel experiences in social media by posting images, videos and comments. Some people may provide their good/bad rating on the travel site. And someone like me may like to write a blog on the amazing vacation they just had. For a travel company it\u2019s extremely important to collect such feedback as much possible from their customers which is going to give them valuable information on how they can improve their services further and Artificial Intelligence can be of great help here too. All the service providers whether it\u2019s the travel booking company, airlines, hotels etc. in the entire value chain of travel & tourism are trying to collect those valuable data and process those using Artificial Intelligence and capture meaningful inferences and subsequently predict customer preferences and proactively offer personalized and better services to them.\n\nSo, that\u2019s probably gave some idea on how Artificial Intelligence is transforming travelers journey and how different entities involved in this whole chain are getting benefited. It\u2019s all about enhancing the experience of the travelers and transforming the travel & tourism business with the enormous potential that Artificial Intelligence provides."
    },
    {
        "url": "https://becominghuman.ai/how-should-a-robot-respond-when-you-hit-it-9ebbcb5bd0b9",
        "title": "How should a robot respond when you hit it? \u2013",
        "text": "I remember getting my first Furby for Christmas \u2014 a tiny, fluffy owl-like creature with massive ears that would have been cute enough for a few weeks, if it wasn\u2019t for the \u2018intelligence\u2019 the little guy possessed. I remember spending months and months trying to learn my new friend the basics of English [it must be said that my own English level at the time was close to zero \u2014 A for effort]. When it \u2018learned\u2019 a new word or a new sentence structure, I petted it or gave it food, hoping that the little guy would understand that this behaviour was exactly what I was looking for.\n\nOn a sad side note, much later I learned that Furbies did not learn from their surroundings, but that they were programmed to develop their English during their lifecycle. Can you imagine the devastation. But the interesting part is this: where I believed that I was stimulating the Furby to learn a new language, the opposite was actually true. By making me believe that it had learning capabilities, Furby reinforced me to keep interacting with it.\n\nFurbies are allegedly welcome again in the NSA\u2019s office, and the world has seen a rise in so-called companionship robots, just like the Furby once was. The difference \u2014 these robots display [to a certain extend] actual learning capabilities based on human-like interactions such as touch, vision or speech. The similarity? We are still [unconsciously] rewarded to keep interacting with the robot. PARO the Seal positively reacts when it s stroked or cuddled. Kuri looks \u201clovingly\u201d up to you when you rub its head. And Pepper reacts appropriately to your mood, based on your verbal and non-verbal communication. Just as we as children learned to finish our plate [\u201cyou ate all of it, very good job!\u201d], positive reinforcement ensures that we keep displaying favourable behaviour towards our companionship robots.\n\nThe reason why we're receptive to robot interaction, is because biologically, we actually are not that well-equipped to see a robot for what it is \u2014 a pile of cleverly-constructed bits and bytes. According to Dr. Carpenter, expert on human-robot social interactions, we are instinctively programmed to attribute \u2018organic\u2019 characteristics [such as movement or interaction] to things that appear to have autonomy and intent. Or: when a \u2018thing\u2019 displays \u2018\u2018human-like\u2019\u2019 behaviour, we are programmed to understand it to be a \u2018\u2018real\u2019\u2019 entity.\n\nWe thank Siri when it gives an answer.\n\nIn the movie Her, the OS responds in such a human-like manner that the protagonist falls in love with it.\n\nBoomer, a robot introduced on the battlefield to disarm explosives, got a military funeral and awards because the soldiers identified with it as with a colleague.\n\nIn the game Love Plus, the software [in the form of a virtual girlfriend] reacts in such a way that players feel like being in a relationship \u2014 and take it on dates or on weekend retreats.\n\nWe are positively reinforced to display certain behaviour towards robots, and biologically we have a hard time resisting it. This can be great \u2014 being stimulated to display appropriated behaviour towards a robot may positively affect human-to-human behaviour. Teenage boys learn with their dating-app that giving compliments to a girl may lead to a positive environment between the two of them. Playing with my Furby, I learned that with proper encouragement and patience, you can teach someone a new skill.\n\nBut what happens when reinforcement turns towards the darker side of humanity? If human-robot interactions can stimulate positive human-to-human behaviour, can it also encourage harmful behaviour?\n\nLet's explore this potential transfer of negative behaviour through the case of domestic violence. In a future society where humans and robots live together, we may expect domestic violence to happen between an owner and his in-house robot. Following the same line as above, the response of a robot, when hit, may affect the owner's [unconscious] perception of the act \u2014 and therefore the way that he interacts with other human beings.\n\nLet's have a look at two different types of responses from the robot. The first \u2014 the owner hits his robot, and the robot doesn't display a negative reaction. In this case, the offender is not negatively reinforced towards this type of behaviour, meaning that he is not per se stimulated to not hit the robot again. Over time, the owner of the robot may get subconsciously programmed to understand that hitting someone is maybe not a very abnormal thing to do \u2014 and transfers this behaviour to his relationships with humans.\n\nSo, to prevent above, we would want a robot to respond when it's hit, right? Unfortunately, this road also poses some difficult questions. Who decides on the appropriate response \u2014 the developer, an independent ethical committee? And if so, what is the appropriate response \u2014 is it to display anger or sadness, or should the robot automatically send a message to law enforcement? What type of behaviour of a robot is desirable, or allowed? Should a robot even be a teacher of ethics?\n\nThings get even more disturbing when discussing the transfer of behaviour from interaction with sex robots to humans. Surely, most sex dolls are rather innocent, but the discussion gets concerning when we take a look at sex doll Frigid Farrah by Lumi Dolls [a company that, by the way, also created the first robot brothel]. Frigid Farrah is a sex doll with \u2018frigid\u2019 settings, meaning that it clearly communicates to its user that it's not willing to have sex. The result? A sex doll targeted at those that have ever dreamt of raping a woman \u2014 and I don't think I need to explain how this potential transfer to human-to-human relationships is, excuse my French, very fucked up.\n\nRobots are coming, and they will impact us in many ways \u2014 including the way we interact with other human beings. Where Furby taught us that time investment leads to positive results, Frigid Farrah could teach us very disturbing ideas. As ethics in robotics is a fairly new field of play, no-one has all the answers \u2014 making it even more vital to keep discussing when technology progresses.\n\nWant to learn more about ethics in AI? This course is a very good start.\n\nIf you liked this article, please \ud83d\udc4f. Remember, you can clap up to 50 times \u2014 and it really makes a great difference for me. Thanks for reading!\n\nPlease note: the opinions expressed in this article are my own, and do not reflect the view of my employer."
    },
    {
        "url": "https://becominghuman.ai/machines-demonstrate-awareness-of-the-present-c66c6821adf3",
        "title": "Machines Demonstrate Awareness of the Present \u2013",
        "text": "In this series, I prove machines are conscious by exploring various aspects of consciousness, showing how machines possess each one. If you feel I\u2019ve omitted an important aspect of consciousness, I am open to any challenge. For further background on my project, click here.\n\nThis week I\u2019ll be looking into machine consciousness of the present. This was first demonstrated in its most basic form, in the 1880\u2019s with the advent of electric lighting. Light switches are aware of which way they are flipped. Need proof? Look at the lamp. The absence or presence of light will tell you.\n\nThe only major difference between this early light switch, and a contemporary digital light switch, is that the latter encodes its awareness of the presence as binary data, instead of in analog signals. Perhaps this additional level of sophistication is needed for us to apply the word \u201caware\u201d. Like our brains, contemporary digital devices encode information from sensors, store that information in regions of the machine, and then make that information available to various parts of the apparatus.\u00b9\n\nTo my mathematical mind, the above seems definitive proof of machine awareness of the present. However the rest of my brain remains unconvinced, we\u2019ve barely scratched the surface of a huge topic. Let\u2019s explore.\n\nAfter winning a Nobel prize in medicine, Gerald Edelman turned his attention towards consciousness. His theory of consciousness is rooted in neurobiology and boldly attempts to demonstrate a Darwinian mechanism for the development of neuronal structures in the brain. Edelman\u2019s concept of primary consciousness describes the animal awareness of elements in the present environment. It is sensorimotor, and basic:\n\nThis exclusion of language severely limits what we can count as primary consciousness. For example, consider color processing in the brain. There are many layers, we could say the encoding of color up to the primary visual cortex fits within Edelman\u2019s container. However, once we identify a color as red, we are out of primary consciousness territory, we have entered the linguistic domain.\n\nThis exclusionary nature presents a bit of a quandary for proof. Machines can certainly, as I will describe below, explain to us what they see. But how do we know that they have a visceral, present experience of it?\u00b2 This primary consciousness is \u201cthe fabulous 3D vista that, we swear, exists in front of us but that actually exists nowhere in the physical universe\u201d, that reader Louis Savain asked me to explain.\n\nTo see how machines demonstrate this visceral present, we must first strip it of its supernatural trappings. For humans, the 3D vista is quite material indeed. Visual field maps in the brain are that material. Adjacent neurons fire when light hits neighboring cells in the retina. If we show a picture to the eye, these adjacent neurons in the brain will fire with a spatial pattern that matches the image. This is a bizarre but extremely well validated fact. There are actually miniature pictures, inside of our brain, that form when we look at things.\n\nWe also know that, if you damage certain parts of the brain, portions of the 3D vista disappear. This was discovered in 1918 by Gordon Holmes by studying gunshot wounds, and has since been validated in scores of experiments. This vista does exist somewhere in the physical universe. Remove those places, and the associated parts of the vista will be forever gone.\n\nWhile the full story on what happens in the brain is far from understood, the structure of the pre-linguistic visual present in machines is plain to see. The core structure is the frame, a digital image that represents a moment in time. A CCD, like a retina, captures light to make an image. While initially spatially coherent, and sometimes copied to other physical hardware units (like perhaps a GPU framebuffer), these captured moments mainly live in information space. In today\u2019s visual software, frames are copied everywhere and processed in all kinds of ways.\n\nWhen the computer starts to do things with these moments, we enter a linguistic territory and are no longer dealing with primary consciousness. The visceral present in a machine is comprised of all of the information representations built from its input devices, up to the point where the information is processed, and interpreted as language. The bit that encodes whether a lightswitch is on or off is the most simple example of this encoded visceral information. We see this materially scattered throughout the machine. And we see the results of this visceral awareness in the things that machines can tell us about the world.\n\nCamera-based computer vision technology has been helping folks unlock phones with their faces since 2015. Entertainment systems like the Xbox Kinect can see which way we are moving our bodies. It is not uncommon to see computers using language to describe the present. Machines, of course, don\u2019t always use human language. We can call the expanded class of linguistic and logical structures that machines apply to their present environment tokens.\n\nThe origin point of this machine consciousness could be the year 1966, when a summer research project ambitiously attempted to determine the figure-ground relationship in images and from that, \u201cactually name objects by matching them with a vocabulary of known objects\u201d. While this did not succeed, the very early work that followed did in fact manage to see the world. By the mid-1980\u2019s, these techniques could provide quite convincing models of the real world, albeit only if the conditions were just right.\n\nToday\u2019s self-driving cars perform amazing feats based on their ability to tokenize the present, but are they more conscious than the machines of the 1960\u2019s? If we look at the history of object recognition, we do not see a singular place where something like consciousness begins. The techniques stack upon one another, the field learns together and over time solid solutions have emerged. But these methods did not suddenly become conscious. Nor will they suddenly gain an architecture of consciousness in the future that is not present today.\n\nWe limit our appreciation for machine consciousness if we focus only on what humans can do. For example, he United Kingdom has a machine with six million eyes that persistently models a sizable chunk of the country. No human can see so much at once.\u00b3\n\nAn early example of this alien machine awareness is 1962\u2019s SAGE System, which utilized one of the first computer networks. Radar stations scattered across the United States tracked planes. The SAGE detection center brought all of this information together to coherently track aircraft across this very large airspace in real time.\n\nIs SAGE\u2019s level of present awareness greater of less than my own? SAGE is aware of the entire U.S. air space. I am aware of every book on the bookshelf that sits beside me. I am not trying to rank these things, but it seems straightforward to say that both SAGE and I are aware of the present.\n\nThese variegated examples of present awareness debunk the myth that machine consciousness will arrive at some time. There will be no singularity.\n\nMachines today are quite good at detecting both trees and animals, plonk one of them in the wild and they\u2019ll know exactly as much about the environment as a basic primate might, even a human. Do we need such an anthropomorphic mimesis to say a machine is conscious, aware of the present?\n\nThere is no dividing line between humans and machines in terms of present awareness. More curiously there is no dividing line in machine history between today\u2019s achievements and the first steps of the 1960s. From several starting points, machine conscious of the present has gradually grown to today\u2019s levels, and will continue to slowly expand forever more.\n\n[1] Curiously, basic animals like sponges demonstrate proto-neural mechanisms that are almost as simple as a light switch circuit.\n\n[2] Humans can introspect and self-report having a present awareness. Machines self-report the various mechanisms of present awareness I mention, typically through the operating system or other system utilities, the details of which are not seen by humans other than technical workers. Self-reporting cannot objectively prove that anything exists.\n\n[3] The only example that comes to mind is a security guard monitoring several camera feeds at the same time. Even in this case, humans have difficulty synthesizing more than a handful of simultaneous views."
    },
    {
        "url": "https://becominghuman.ai/the-concept-of-i-ai-and-the-operating-system-part-2-672752f1fa3",
        "title": "The concept of \u201cI\u201d. AI and the Operating System, Part 2",
        "text": "In the first article in this series, AI and the Operating System, I outlined the trends and challenges of the marriage of artificial intelligence and computer operating systems. While the trend of that relationship becoming closer over time may have been obvious, less obvious is the direction of clear winners in the space.\n\nIn this article we will explore the concept of \u201cI\u201d in relation to AI and operating systems and why it will become more important over time. We will look at why the absence of \u201cI\u201d had been holding the AI Operating System (AIOS) back, and how that will change over time.\n\nOf course when I speak of \u201cI\u201d, I am talking about the personal pronoun.\n\nHow many times have you had a message from your computer, reading \u201cI have to download and install some updates, please don\u2019t turn me off.\u201d? Probably never, because, at least until recently, we have not made operating systems with that level of induced anthropomorphism.\n\nPeople in the past were worried enough about computers taking their jobs, let alone having computers that seemed like living things with minds of their own. Introducing the concept of \u201cI\u201d (or self) to an OS may have been scary enough by itself, but forcing people to think of a computer as a living entity may well have been bad for business if people did not want to work with computers for fear of them.\n\nIt is quite a radical change of mindset to think of a computer as an entity worthy of what respect we pay living beings, and frankly, the technology of yesteryear was not at a level where introducing a concept of self to a computer was warranted anyway. For example, there is little too personable about using a keyboard and mouse to converse with a computer.\n\nThings have changed dramatically over the last few years, however, with speech recognition nearing that of human capability and with the introduction of face recognition meaning that computers can now recognise a familiar face addressing them. Advances in voice recognition means that identifying a person by their particular voice is possible, and so perhaps without realising it, it has become easier to anthropomorphise a computer as if a relatable personality.\n\nWe seem to live in an interesting era of schizoid operating systems that kind of want to use the word \u201cI\u201d to speak of themselves and kind of not at the same time. Cortana is an inbuilt, but separate, application from Windows 10. Siri is an inbuilt, but separate, application from IOS. My PC still says \u201cPlease don\u2019t shut down your computer\u201d when installing updates. It does not say, \u201cPlease don\u2019t turn me off.\u201d. Cortana might use \u201cI\u201d in a sentence, but she does not pretend to be the PC or operating system itself.\n\nWith a large percentage of Microsoft\u2019s OS market being that of commercial users in offices etc., it is worth asking now, if Microsoft have not asked already, \u201cDo we want PCs and AIOSs with an inbuilt concept of \u201cI\u201d?\u201d. Will we ever see that intuitive leap when PCs start speaking of themselves? When will the day come that at my first day of work I introduce myself to my office computer, tell it that I am its new user, for it to then ask the system administrator if that is okay?\n\nWe can also ask, \u201cWhat are the market forces that will force such questions to be asked, from here until the end of time of humans with computers on Earth?\u201d. I think that question has already been answered to some extent with the introduction of Amazon\u2019s Alexa.\n\nTry as you might, an Internet search for \u201cWhat is the operating system for Amazon Alexa?\u201d will return next to no results, if any at all. If an Alexa has an operating system with its own name, it is a well guarded secret. It could well be because an Alexa is its own operating system.\n\nAn Alexa has a concept of \u201cI\u201d. An Alexa is the hardware, software and operating system all combined. It has an identity that is easily marketed without confusion. You are buying an Alexa. You are not buying a HP computer running Windows implementing Cortana. You are buying an Alexa that just is. That\u2019s what I mean by an operating system implementing the concept of \u201cI\u201d. If Alexa talks about itself in the first person, it does not refer to an operating system or the hardware, but \u201cI\u201d.\n\nAn Alexa is not an operating system as we might normally think of them. I find it hard to know if an Alexa is a disk operating system at all. Is there an on-board memory/disk within an Alexa? Maybe, but it hardly matters because an Alexa is so far removed from an ordinary operating system that if she does disk operations it\u2019s inconsequential, you just talk to her in natural language and she obliges as best she can.\n\nUntil the introduction of Alexa the absence of the concept of \u201cI\u201d within OSs was, to some extent, holding the concept of an AIOS back, because there was always a disjoint between the concept of the OS and the personal assistants operating on the OS. It wasn\u2019t that easy to anthropomorphise over an OS and consider it a living thing. We bought iPhones not Siris. We bought a PC, not a Cortana. Now we buy an Alexa and Alexa just is.\n\nAs personal assistants like Alexa become more widely adopted, beyond their extant and somewhat explosive growth, I feel that the concept of \u201cI\u201d within computer operating systems will become important to those AIOS producers who want their products to form personal relationships with their users. It may well be that any OS in the not too distant future that does not have a clear concept of \u201cI\u201d will look as silly as it was to actually have implemented that feature not too long ago.\n\nThank you for reading. I hope you have enjoyed this journey into AI and the operating system."
    },
    {
        "url": "https://becominghuman.ai/the-case-of-unregulated-exponential-technological-development-3364f681b271",
        "title": "The Case of Unregulated Exponential Technological Development",
        "text": "It\u2019s 2011, the word on the street was there was some new app. At the time few people had smartphones and even less were proficient enough to even know how to use them. The app was Instagram. Photo editing app first, sharing platform second. Some were drawn in while others didn\u2019t understand its existence or its rise. Fast forward to 2017, Instagram has become one of the most influential and trailblazing applications ever to come about. With over 500 million daily active users, 95 million shared photos and videos, and a valuation of over 50 billion thanks to the Facebook acquisition (Balakrishnan 1). Instagram has come to be a behemoth of social interaction among teens and young adults alike.\n\nInstagram isn\u2019t the only app to receive this type of success, but A real social revolution came with the advent of Instagram and the platform of the smartphone. All of a sudden every moment needed to be photographed and shared, every dinner, every movie, every date, wedding, newborn child. People began sweating their follower counts, becoming green with envy over their peers. Suddenly it seemed as if popularity and self-worth were now given a numerical value easily comparable to every other person. People got lost in the hysteria, spending hours toiling with what filter to use or what caption to add. Fears of missing out and exclusion pushed mass adoption among practically every person under 25. People were quickly no longer skeptics of this technology and used it for whatever gain possible regardless of its costs seen or unseen. In 2011 if you went up to someone and explained the idea of posting every nice dinner they\u2019ve had or special occasion on the internet for the world to see, people would be far more averse to this idea then they have grown to be today. This social revolution is one that wasn\u2019t forecasted by anyone and yet its influence on us is undoubtedly immense. In our modern technological age, it is paramount that as individuals we exercise cultural resistance against the social norms perpetuated by technology companies whose increasing advancements require society and humanity to implement regulations on this never before seen influence.\n\nThe use of cultural resistance is no new idea, it\u2019s a practice that\u2019s as old as history itself. The act of pushing back against a society\u2019s norms is what progresses our society and makes us question why certain things are the way they are. According to Stephen Duncombe from his writings on cultural resistance, he defines cultural resistance as \u201cfree space to create new language, meanings, and visions of the future.\u201d (Duncombe 8). When culture isn\u2019t contested the dominant power structures in our civilizations end up dictating our futures regardless of what the masses may want. This social control is so effective due to the shared belief across the population that things are the way they should be and there seem to be no other feasible alternatives. Today, technology companies have assumed this dominant power becoming some of the most valuable and influential corporations. One of the ways of defining the power a company has is by looking at its overall value a term also known as market capitalization. The top 5 companies sorted by market cap are Apple, Google, Microsoft, Amazon, and Facebook. All of these companies are technology based and hold the values of continued technological innovation as supreme importance. This cannot be a coincidence, this is very telling about our country\u2019s societal values and our guides to the future. With these companies as the leaders of our economy, the increased exponential rate of technological innovation guaranteed by Moore\u2019s Law, the idea that computer processors will double in speed every 18 months, will only be given more and more room to flourish. This increased pace doesn\u2019t allow us enough time as a society to properly evaluate the potential benefits or problems a new innovation may cause. Many of these companies grew to prominence in amounts of time that have never been seen before. Facebook is an example of one of these companies. 5 years ago they were valued at 40 billion which is minuscule compared to the 550 billion they are worth today, 200 billion of which was grown in the past year. So with all this technological progress have there been any downsides? The smallest innovations in this field can lead to the creation of massive unforeseen impacts that are seemingly irreversible. Shouldn\u2019t we be worried about all this influence within companies that have no formal or substantive regulations?\n\nFirst, we need to evaluate one of the current technologies which have seemingly revolutionized our society and has grown at an ever-increasing rate, Social media. Social media can be defined as websites and applications that enable users to create and share content or to participate in social networking. With the advent of the modern day internet in the late 1990s came the first primitive social media platforms. These took the forms of blogging forums, chatrooms, and instant messaging websites. There should be no surprise that at the first chance humans got to use the internet they repurposed it to interact with one another because we are genetically hardwired and programmed to want to socialize (Wolpert 1). With this new technology, we could live in a world where people talked to others across the country to even across the world. In this form however it was still very clumsy to use and had large barriers to entry and skepticism surrounding the abuse of these technologies. People worried about internet safety, privacy, and exactly who could see what they did on the internet, but these fears wouldn\u2019t last long. Also, many of the social media companies around this time had little power, if any, over people because of their inability to monetize these platforms as well as the small amounts of time people would use the website. In 2003, came the advent of MySpace. This marked an important place in the history of social media. It was the first modern-day social media website. It contained many of the characteristics that we still see prevalent today, a personal profile, the ability to make friends all atop an instant messaging platform. Although MySpace\u2019s surge in users and popularity didn\u2019t last long, there is no doubt that it was one of the most important precursors to the creation and success of Facebook. This is because MySpace prepared people for Facebook as they could easily switch from platform to platform due to many things being similar and the social hurdles that Facebook would need to leap were already covered by MySpace. At its inception, Facebook was like many of the other platforms in the space, but they had one thing different about them, their ability to adapt to the future. Every other social media website before it struggled with money and gaining and keeping a user base active. Facebook was one of the first to figure it out. In its first form, it was nothing more than a platform where users could communicate with their friends and post or respond to their statuses. Then it became a waiting game. With each new technological innovation, Facebook would be the pioneers to implement it. For example, In 2007, with the creation of the smartphone and the app store came the Facebook mobile platform. As the internet got faster and smartphone adoption increased, Facebook did whatever they could to encourage their users to make use of these new innovations. Adding the ability to share files and photos in turn making smartphones more important and useful by adding the platform users already loved to use and making it accessible everywhere. This was a tipping point where I believe technology and their supporting companies began to gain a concerning amount of power. Facebook at this point began to change. The company that once seemingly had societies and humanities best interests in mind soon became misaligned from these ideals. Their new goal after having a user base of over 2 billion was to increase time on the app or website. In order to accomplish this daunting task, they began to design a user experience that was addicting. Almost overnight, posts weren\u2019t sorted chronologically anymore, they were sorted by which content would keep users on the site the longest. This was all done by harvesting the information the users willingly provided about themselves and using the novel technology of machine learning, every liked post, video viewed or any other conceivable action was taken into account by a complex algorithm to curate posts uniquely to each user. This was all extremely successful and the time spent on these platforms skyrocketed. Recently, we have seen people involved with the creation of this field beginning to come out against the practices these social media companies are employing to take advantage of their users. One of whom is Chamath Palihapitiya, a former Facebook VP. As someone who was at the forefront of technological advancement and change at Facebook, he has come out against his work at his former employer at Facebook. He feels \u201ctremendous guilt\u201d over growing the social network that \u201ceroded the core foundations of how people behave by and between each other.\u201d(Wang 1) When he started, he said, \u201cthere was not much thought given to the long-term negative consequences of developing such a platform.\u201d This is exactly the problem with our increased pace of innovation. These companies are too worried about pushing hot new technologies and beating competitors to market so they often times do not look at the long-term effects it could have on society. If a company like Facebook isn\u2019t doing this, do we expect any other smaller company to do it? \u201cIt literally is a point now where I think we have created tools that are ripping apart the social fabric\u2026 the short-term, dopamine-driven feedback loops that we have created are destroying how society works: no civil discourse, no cooperation, misinformation, mistruth. And it\u2019s not an American problem\u2026 This is a global problem.\u201d(Wang 1). This shows a direct first hand experience of people in the tech field pushing the bounds of influence without giving it much thought in terms of its potential negatives. This discourse is a call to action on consumers to look at what these companies are doing and be at least aware of it if not avoid it. The problem is rolling back this sort of technology is near impossible. When people spend multiple hours per day on their various social media websites, how can we expect them to be able to just simply shut it off? With each new innovation social media quickly figures out how to implement it in a way that increases the reliance users have on the platform. Over the past few years the technological advancements in social media has led to an almost complete loss of privacy, increased loneliness, a diminishing civil discourse and a rampant spreading of misinformation and mistruth, yet some of us are too helplessly addicted to even care or resist it. I know personally about how technology has affected our society. I have first hand seen people blindly adopt and use technology without thinking past the surface level of consequences. Are we expected to believe that documenting everything we do on a practically permanent public ledger that is social media is a good thing? Why have we not given further consideration into this? You can ask almost anyone if they think we are being tracked throughout technology use, and the answer is always a yes. So why do we not care? Why do we trust these companies so much when their only interests really are to turn a profit for their shareholders? Can the free market regulate when the dominant tech companies of Google, Facebook and Amazon have the resources to buy and crush any competition? If one of these companies was found of committing a serious ethical violation with our data would we be able to turn away in a world where we have become increasingly reliant on these companies? We have all these massive questions about the social media services that we use daily, but these are ethical and moral questions that should be addressed before users continue to use it like they have been.\n\nSocial media isn\u2019t the only place where our unregulated exponential technological innovation is starting to raise problems. For this next one, we need to look at something that is relatively obscure to the public eye that is the advancements in automation and artificial intelligence. Artificial Intelligence is the idea that humans can program and design software in order to accomplish a certain task. We have seen A.I. take form over the past couple of years going from beating the best human player at chess to performing the complicated task of self-driving. So if our software can already achieve these goals what\u2019s to stop us from creating machines smarter than ourselves? Then at this point what is to stop an artificial intelligence from continuing advancing its own code in order to become more and more intelligent. This idea of rapid intelligence development is known as superintelligence or an intelligence explosion. One of the leading thinkers on this topic is Nick Bostrom, A professor of philosophy at Oxford University with an extensive background in physics, computational neuroscience, and mathematical logic. From Bostrom\u2019s book Superintelligence he goes in depth to explain the dangers this technology can have, \u201cBefore the prospect of an intelligence explosion, we humans are like small children playing with a bomb. Such is the mismatch between the power of our plaything and the immaturity of our conduct. Superintelligence is a challenge for which we are not ready now and will not be ready for a long time.\u201d(Bostrom 58). As a society, the way technological innovation has advanced created a situation where we have a few major technology companies and governments gunning for new technology to further their power and dominance. Instead of trying to put in place ethical goals and safety restrictions, we have seen companies doing whatever means necessary to get closer and closer to their end goal. This raises serious concerns over the power and impact of Artificial Intelligence. This raises the idea that maybe there are just certain innovations we may not have the framework or knowledge to create and absorb its effects. We do not understand the devastating consequences that could come with an intelligence explosion. However, Superintelligence is inevitable if we continue progressing our hardware and software at the rates we have seen in the past and as a society, we have every motive to continue as intelligence is the solution to almost all of our problems. How to cure diseases, solve world hunger, address climate change\u2026 these are all problems that can be easily solved if we had the intelligence to do so. I don\u2019t doubt that people will challenge the idea if superintelligence is even possible, much less inevitable, but then you need to find a problem with one of the following assumptions. 1. Intelligence is the product of information processing. We have already seen this to be true as narrowly minded intelligence can already act at superhuman levels, looking at the case of chess or self-driving. 2. We will continue to improve our intelligent machines. Given the ultimate value of intelligence as previously stated, we have no reason to not continue our innovation. We don\u2019t even need intelligence to go that much further just by the virtue of speed. Sam Harris explains this idea in his TED talk when he says \u201cSo imagine if we just built a superintelligent A.I. that was no smarter than your average team of researchers at Stanford or MIT. Well, electronic circuits function about a million times faster than biochemical ones, so this machine should think about a million times faster than the minds that built it. So you set it running for a week, and it will perform 20,000 years of human-level intellectual work, week after week after week. How could we even understand, much less constrained, a mind making this sort of progress?\u201d (Harris). This example highlights our unforeseen shortcomings when compared to A.I. So, If we accept that A.I. superintelligence is inevitable how can we prepare in order to facilitate the best case scenario? We would need a \u201cManhattan project\u201d on A.I. looking into the technology and accessing which regulations need to be put in place in order to make a transition from a world without A.I. to one with it as seamless as possible. Currently, if a company owned this technology they could have the potential to replace many of our current jobs in our economy. This could create huge amounts of unemployment with steep wealth inequality and currently, our political system is not prepared to handle this. Without proper regulations and political, economic scenarios to report to in the case of a superintelligent A.I. the circumstances could be dire.\n\nUltimately, whether it be the way our society is impacted by this technological innovation through the vein of social media or artificial intelligence there can be potential drastic unforeseen consequences that lie ahead. We have seen the consequences of not properly evaluating our technologies with the creation of our various modern day social media apps, but we are yet to see what consequences there can be through the other fields that may lie ahead. What mistakes we have made in accessing technology on the social media front we must take a lesson from. Our shortcomings on social media may have created a platform of misinformation and many other problems, but if we do not learn from this and place regulations on new technological advancements we may see a similar situation develop around a technology such as artificial intelligence which has the potential to cause much greater problems or to do away with the human race as we know it. While regulating technology may slow our progress towards certain innovations and goals, it may be our only way to prevent these much greater unforeseen problems in our future."
    },
    {
        "url": "https://becominghuman.ai/machines-demonstrate-self-awareness-8bd08ceb1694",
        "title": "Machines Demonstrate Self-Awareness \u2013",
        "text": "In this series, I prove machines are conscious by exploring every aspect of consciousness, and demonstrating how machines possess it. For further background on my project, click here.\n\nOf all the attributes of consciousness, self-awareness has the honor of being the first listed when Merriam-Webster starts to define consciousness as \u201cthe quality or state of being aware especially of something within oneself\u201d. In a survey of nine neurocognitive models of consciousness, Richard Morin determines that \u201ctwo aspects of consciousness seem especially important: perception of self in time and complexity of self-representations\u201d. Based on this frequency, he lists self-awareness as the third of his four levels of consciousness.\n\nAcademics carry on a lively debate over the validity of these self-representational theories of consciousness. In another dictionary, we find a stark opposition to Merriam-Webster. Sutherland\u2019s Macmillan Dictionary of Psychology doesn\u2019t find the reflexive aspects of consciousness all that important. But for many, the internal understanding of oneself is a fundamental if not the defining aspect of consciousness (seen in qualia and Kant\u2019s inner sense\u00b9).\n\nI\u2019m not going to take sides in this particular battle of terminology. Remember my job is to wade through the plurality of consciousness, not sort it out. Let\u2019s consider two kinds of self-awareness, external and internal, both of which are present in machines, each a result of its own curious history.\n\nMy knowledge that I am in the city of Los Angeles is an awareness of myself in relationship to the external world. Machines have exhibited this type of self-awareness for some time, for example my iPhone can pinpoint its location with startling accuracy (if you don\u2019t believe me, simply open Maps and look at the blue dot). Machines can pass the basic medical test required to establish consciousness (\u201cWhere are you?\u201d \u201cWhat time is it?\u201d).\n\nToday a software like Siri can answer these questions, conversationally, as a human would. It is not the speech synthesis and voice recognition technology however that make Siri self-aware, but the underlying knowledge of self. It is a mistake if, in looking for machine self-awareness, we look for direct analogues to human experience. When researchers build robots like Qbo which pass the mirror test, they are making an historical point. These anthropomorphic feats say more about a machine\u2019s ability to mimic a human than they do about consciousness.\n\nWhen you are willing to look at machine\u2019s external sense of self, through the eyes of what matters to the machine, you can understand it at a far deeper (and truer) level. For example, any machine connected to the internet has a name called an IP address. This name appears everywhere. This is how a machine knows when it has done something. For example, an ACK in the TCP protocol confirming that a machine\u2019s message has been delivered to sender, contains that machine\u2019s name. A machine in TCP/IP actively tracks its external footprint over communication channels, waiting to make sure ACKs come back so that every message is delivered.\n\nThis form of name-based community was first thought of in the late 1950s, but took almost a decade to become realized. The first steps to realization were seen in the early 1960s with the SAGE system. And by the end of the decade, a fully functioning ARPANET used names in the way that is dominant today online. Entirely bereft of ceremony, machine self-awareness was created over fifty years ago.\n\nOf course, this form of self-awareness goes far beyond names. It is the tasks that machines perform using these names that are testimony to the high levels of external self-awareness they possess. In distributed computing applications, machines use networks to coordinate with an exactitude that any contemporary army would envy. To see this miracle in action, perform a Google search. The speedy response is due to not one, but a myriad of computers communicating together to produce your result. If they weren\u2019t aware of themselves, in relation to the other machines, then they would not be able to perform this computation so accurately and quickly.\n\nWhile these examples consider awareness of the external self, it is the other half of the issue: internal self-awareness, that seems the higher bar for consciousness. This self-reflexivity, the ability to introspect is an essential aspect of the human conscious experience. And from the naive perspective it would seem far more difficult for a machine to know itself, internally, in the way that my iPhone knows it is in Los Angeles.\n\nHistorically we find the opposite is true. Internal self-awareness precedes external self-awareness in the annals of computer history. Again, we need to look at the issue in machine terms. It makes no sense to start with feeling and desire. A machine which claims to crave soup is lying, not self-aware.\n\nSo if machines don\u2019t want soup, what is going on inside? It is difficult to see machine self-awareness at first because we are blinded by the opacity of our own introspection. Humans, when we tell our thoughts and feelings, have partial access. We look for something similar in machines and are starved to find it.\n\nThat\u2019s because machines, unlike humans, have a complete and total self-awareness of their internal state. This is not an accident. This was a deliberate invention that occurred in 1945, as part of the landmark Von Neumann architecture. Early computers, like Zuse\u2019s Z3, had two clusters of information. There was the program, that would be run by the computer, it was one pile of numbers. And there was the data, the numbers to crunch, as another pile.\n\nThe idea of Von Neumann (and others, contemporaneously) was to keep both the program and the data in a single pile of numbers. This had some funny outcomes. For example, it became possible in this system to create a program that could change itself.\u00b2 More importantly, creating a program that could powerfully run and manage other programs was straightforward.\n\nThis parlor trick of internal self-awareness was demonstrated in machines as early as 1962. The Compatible Time-Sharing System built at MIT demonstrated this new concept. In the time-sharing scheme, a single CPU pretends to be multiple computers. The time on the CPU is shared between many terminals, each of which has a keyboard and monitor. As these systems progressed, they became more sophisticated, and today we have a rich array of virtualization methods that demonstrate a startling awareness and control of the a machine\u2019s innards by the machine itself.\n\nThis power of machine self-awareness is so great that every OS today protects the computer against a variety of a security threats. The access to internal self is so complete that a small malicious program could wreak havoc.\n\nMachine internal self-awareness includes responses to physical events through interrupts. A relatively straightforward process allows a computer to know when its keyboard has been touched, or when its hard disk is full. Interrupts themselves date back to the 1950s and have a fascinating history that is well documented online.\n\nEven a basic machine is more internally self-aware than a human, in a certain light. I can search my laptop and find every image on the damn thing. But, as a human, I don\u2019t always recognize people that I\u2019ve met and sometimes I forget their names. The images and contents of my mind can often be called up voluntarily, but not always.\n\nTo the extent that a machine thinks, it can absolutely control its thought, automatically running programs and suspending them. Meanwhile, human monks train for decades to pause their thoughts, an accomplishment which is rarely achieved.\n\nThese superpowers must today be taken with a grain of salt. There are not as many intermediary layers of control in machines as there appear to be within the human brain. Machines have a perfect self-knowledge of but a relatively limited apparatus.\n\nThe locus of machine self-awareness is the operating system, a subject we will return to later in this series as we explore other aspects of consciousness. For example, the awareness of time, which will have to be taken up at a later date."
    },
    {
        "url": "https://becominghuman.ai/using-machine-learning-to-analyze-students-deliverables-30b48fa57e21",
        "title": "I have used Machine Learning to analyze my students",
        "text": "Recently I have been teaching to 2 different courses at IE Business School: Five sections of Startup Lab and two sections of Business Impact. As I wrote in the past article, I asked for their deliverables using a chatbot, and now I am processing all the collected data using a machine learning based tool, the IBM Watson Natural Language Understanding tool.\n\nI asked to my students three questions, and they should reply them in at least 50 words, so I can have a decent amount of content to analyze. The questions were related with blockchain and technology:\n\nThe students of the Startup Lab (SL) and Business Impact (BI) have some differences, SL students usually want to create their own business, and BI ones usually want to start a corporate career inside a big market player.\n\nI opened the IBM Watson NLU tool and started analyzing all the responses. This was the sentiment output results related with the deliverables:\n\nWe can see here how Business Impact Students seem to be more positive about blockchain technology and the Startup Lab ones have a more critical way of seeing it.\n\nBut how are these results calculated? The tool analyzes the meaning and sentiment of each phrase and context, and it makes an average of all of them for the text. Let\u2019s see how this works using an example:\n\nWhy Startup Lab D punctuation on Sentiment (0.33) and Joy (0.22) are so low and the Sadness (0.54) is so high?\n\nWell, a team said: \u201cFood supply will be an issue in the future, you have things such as sea level rise threatening\u201d another team referred to the Uber self driving car accident: \u201c(\u2026) unfortunate news about self-driving Uber killing a person in the US\u201d. These are not a very positive facts, that lowered the average punctuation in their section, but they also show a good level of criticism which is good in a business student working with technology.\n\nLet\u2019s take a look to the most important keywords detected by the algorithm:"
    },
    {
        "url": "https://becominghuman.ai/satellite-ai-seeking-solutions-in-high-resolution-7952585c5abc",
        "title": "Satellite AI: Seeking solutions in high resolution \u2013",
        "text": "Satellites have been flying around the earth for decades \u2014 scanning landscapes and capturing images of our fast-changing planet. Remote sensing has been around since even before the first flight of the Wright brothers. It was restricted to hot air balloon flights back then. Systematic aerial photography and satellite remote sensing reached an inflection point during the Cold War, when the need for surveillance led to modification of combat aircraft for the purpose of spying. The space race also gave a fillip to satellite launches. The first satellite photographs of the earth were taken on August 14, 1959 and satellite image processing techniques evolved in 1960s and 1970s.\n\nTill late 1990s, the primary consumer of remote sensing data was either governments bodies or defence agencies. This was because of the strategically sensitive nature of technology, which gave birth to the fear that it can be used for spying. However, after the fall of the Soviet Union commercial satellite imagery market began to evolve and IKONOS became the first commercial, very-high resolution satellite to be launched in 1999. Another factor in play was the growing use of computer software for analysis of data and satellite data consumption benefited from this growth in the 1990s.\n\nThe 21st century saw rapid changes in the remote sensing industry. Data consumption continued to increase. This was accelerated by the fall in costs of satellite imagery. Moreover, open data sources emerged with Landsat data becoming publicly available in 2009. Copernicus Hub followed in 2014 when the European Space Agency launched Sentinel 1. Another inflection point occurred in the industry when Planet launched a constellation of 88 Dove satellites abroad the PSLV-C37 of ISRO. These are shoe-box sized satellites leveraging the power of off-the-shelf consumer electronics to reduce costs. Further innovation in satellite launching by a slew of startups led by SpaceX has reduced costs of launching satellites.\n\nSo what\u2019s the result? Over 620 satellites are imaging the earth with a singular aim of providing on-ground imagery information at different resolutions through both commercial and open source web based platforms.\n\nSo satellite data is commercialized. Planet is providing coverage of the entire surface of the Earth, everyday and more companies will reach this milestone soon. But who will analyse petabytes of data on a daily basis? It is humanly impossible to make sense of this data and generate actionable insights in a regular, repeated way.\n\nWe at Attentive AI aim to unlock the potential of satellite data using artificial intelligence. Machine learning algorithms can scan petabytes of imagery and generate insights in a jiffy. We develop models to extract features, detect changes and predict physical situations using AI. Our algorithms get better as they learn to turn millions of satellite images into usable data.\n\nToday we are on the cusp of a radical transformation in how information is gathered, analysed and monetized.\n\nNow the question is how can we utilize this ever-growing archive of global imagery to overcome our planet\u2019s greatest challenges? Can we draw useful conclusions and seek answers to worldwide problems with image analysis? The short answer, we have discovered, is YES. The avalanche of satellite images provides a superlative database of the entire planet, one that can be used to the predict impact of climate change, stop forest fires and prevent wars.\n\nRecently, satellite images of road construction done by PLA (People\u2019s Liberation Army) have emerged. The PLA was extending all-weather infrastructure in Doklam affecting the balance of power between India & China in the region. Considering the substantial nature of the event it is critical to detect such developments in time to adopt necessary measures for national security and maintaining peace in the subcontinent. We are building tools to assist detection and quantification of military assets and infrastructure development in border areas.\n\nSatellite AI \u2014 It is the observation of tiniest detail from the vast panoply of human enterprises which can ease the process of resolving the world\u2019s biggest questions and seeking solutions in high resolution."
    },
    {
        "url": "https://becominghuman.ai/visual-music-machine-learning-workshop-for-kids-a90c957dab33",
        "title": "Visual Music & Machine Learning Workshop for Kids \u2013",
        "text": "It all started with the 50th anniversary of the Leonardo Almanach. We\u2019ve been asked by Nina Czegledy to make an event that makes it possible to share high level, advanced topics (science, technology) for kids in playful forms. Since Leonardo is an academic journal, it is an interesting experiment in itself: making such concepts available for people who were not involved in it before. If we have a look on the current (and near future) media art landscape, we see that there are a lot of developments on both the hardware side and the software side. Since current popular hardware advancements are so ephemeral, unreliable, and entertainment related (VR, AR & the like), we wanted to dive into the realms of something different. Something that can tell more about paradigm shifts, cognition, thinking and the world: the area of creative usage of machine learning.\n\nWe have large experience in audiovisual performances and fine arts \u2014 Andrea Sztojanovits, Lorand Szecsenyi-Nagy are both artists & researchers such as myself \u2014 so we decided to share concepts through creative ways of music creation. While drawing and building are ways of constructive expression, music is a form of communication using intuitive, sensual contexts, so if we go this way, kids can find connections and consequences in a more personal aspect as opposed to more regular, abstract ways that are operating only on the visual domain. First we tried and played around with the possibilities of the human body: showing how conductivity can form electric circuits is always releasing a bunch of ideas that helps in leaving behind cognitive bias for the children. By letting electricity flow through each of us, interrupting & connecting the circuit, we immediately became part of a larger, distributed system. When children were touching each other, it became clear, that this is something that we can do together, everyone is equal and very important in the circuit. By letting current rise through all participants, concepts like trust, equality, experimentation, play, openness arise naturally, and these lead to constructive conversations, it also helps children to get comfortable with the situation, and initiate communication & sharing of subjective experience. 1. Paper repro: \u201cLearning to Learn by Gradient Descent by Gradient Descent\u201d 3. Best websites a programmer should visit in 2018 @code_wonders\n\nWe were adding & connecting fruits, environmental artefacts (gas tubes that we found in the room), any sort of conductive material to the system. This way we were showing the possibility of abstraction in a system, asking questions like if something works, what else can work, how can we extend existing concepts regarding to a problem (in this case, generating sound)? Objects, environment were followed by the introduction of conductive paint, so we could face challenges and inspiring aspects of drawing, painting & organizing sounds in a visual way. After making drawings of weird & surreal creatures, distant, never-seen before landscapes and newly created constellations, we connected the drawings with the computer, mapping accurate sounds to different regions and semantic components of the drawings. Musical structures (piano sounds, xylophone, marimba) and sampled sonic realms (bubbles, animal sounds, natural forces) both found their ways to appear along with the tangible space of the drawings. Since kids came from different backgrounds with different interests, we found ourselves in a very diverse set of sonic galaxy: some of them were more musical, some more tactile & open for strange sounds, some are just observing the whole scene.\n\nThese sounds, combined with the drawings were definitely creating a very unique way to understand abstract components and their correlations. Although it was purely intentional and experimental, children were tackling at the deep interrelations of interface & usability design, cognitive science, software ergonomics, system theory without touching any concept verbally & directly of these fields. After turning drawings into working models of playful sonic interfaces (connecting the cables, mapping the inputs), we started to try and discuss the possibilities of what an artificial machine can add (or take away) to extend play, expression & creativity to the system. First we started to add simple delay effects to the sounds. This is a concept where the sound is repeated with a certain amount of feedback. We could say, time is captured, and rotated backwards into the musical sequence. Time, repetition is the very basic of musical structures, so it is a good point to start with the concepts. Our second playful example was to add states for the repetition: we\u2019ve been teaching Markov chains on the fly, to continue our recently added sound sequences. This system does no more than repeating some events we\u2019ve already showed them: while we are training the chain, we teach the probability (of switching fro m one pitch to the next one) too, that makes possible to move along from one state to the next one, etc. This model is used within a lot of fields from creating content for games, composing music, or simulating conversations. It is much more advanced compared to our previous feedback-delay model, since it really takes care of the \u201cstate\u201d that we pass to it.\n\nThe third example was about to introduce the companion of an artificial entity more further: we were playing together with a machine where the system was trying to catch up where we left off. By playing simple melodies to the system, it was trying to continue our \u201cmusical thought\u201d by improvising in the same scale & mode, using a Recurrent Neural Network that is not only aware of current state, but is also time aware, so can track and notice changes during the evolving composition. We were using pre trained models from Magenta, implemented in deeplearn.js by Tero Parviainen. What we\u2019ve noticed, that kids grabbed immediately the constrains & possibilities of the first (delay-feedback) system. They started to play around with it, they changed their mood for playing, they were both acting & listening. What was more surprising is that playing together with the neural network was also very smooth: they understood that they have to wait for the next improvised \u201canswers\u201d from the system, they also integrated these answers into their own playing technique. Since it was very basic, we could observe it only through the dynamics and the speed (including breaks, longer silence) in their play. The workshop was followed by a very inspiring talk on Neural Art by Daniel Varga, that was investigating the visual aspects of current deep learning methods and artistic outputs. It also turned out as a relevant way to connect the gap between the children and their parents, describing more on the topics of GANs, Deep Dream and the role of those vision based convolutional approaches in our regular life."
    },
    {
        "url": "https://becominghuman.ai/basics-of-neural-network-bef2ba97d2cf",
        "title": "Basics of Neural Network \u2013",
        "text": "Partial Derivatives are calculated so we know what was the contribution of error by each weight.\n\nThe need of derivatives obvious if you think of it.\n\nFor example think of a neural network trying to find the optimal speed (velocity) of a self driving car. Now if the car finds out the the is either faster or slower than desired speed neural network will change its speed by either accelerating or decelerating the car. What is accelerating/decelerating? Derivatives of speed.\n\nLet\u2019s explain the need of \u2018Partial Derivatives\u2019 with an example as well:\n\nLet\u2019s say that a few kids were asked to throw dart at a dart-board, aiming at the center. The initial results were:\n\nNow if we found total loss and simply subtracted that from all the weights then we generalize the mistakes made by each student. So let\u2019s say a kid aimed too low but we ask all the kids to aim high then it results in:\n\nThe error of a few students might decrease but overall error still increases.\n\nBy finding partial derivatives we find what was the error by each weight individually. Correcting each weight individually results in following results:\n\nWhile neural network is used to automate feature selection, there are still a few parameters that we have to input manually.\n\nLearning Rate is again a very crucial hyper-parameter. If the learning rate is too small then even after training the neural network for long time, it will still be away from the optimal results. Results would look something like:\n\nInstead, if the learning rate is too high then the learner jumps to conclusions too soon. Producing following results:\n\nActivation Function is one of most powerful arsenal, which is responsible for powers Neural Networks advertised to have. Vaguely, it decides which neurons will be activated, in other words what information would be passed to further layers.\n\nWithout activation functions, deep nets lose a bulk of their representation learning power.\n\nThese functions\u2019 non-linearity is responsible for increased degree of freedom of the learners, enabling them to generalize problems of high dimensionality in lower dimensions.\n\nBelow are few examples of popular Activation Functions:\n\nCost Function is at the centre of Neural Network. It is used to calculate loss given the real and observed results. Our aim throughout is to minimise this loss. So Cost Function effectively drives the learning of neural network towards it\u2019s goal.\n\nA cost function is a measure of \u201chow good\u201d a neural network did with respect to it\u2019s given training sample and the expected output. It also may depend on variables such as weights and biases.\n\nA cost function is a single value, not a vector, because it rates how good the neural network did as a whole.\n\nSome of the most famous cost functions are:\n\nRoot Mean Square is the simplest and most used of them all. It is simply defined as:\n\nThe Cost function in NN should satisfy two conditions"
    },
    {
        "url": "https://becominghuman.ai/ai-machine-learning-deep-learning-explained-in-5-minutes-b88b6ee65846",
        "title": "AI, Machine Learning, & Deep Learning Explained in 5 Minutes",
        "text": "The term \u201cArtificial Intelligence\u201d has been floating around for a while. We see it in sci-fi movies, \u201cAI\u201d game bots we play against, Google search, and, oh yeah, those robots that are some day going to take over the world. Off late, though, \u201cMachine Learning\u201d and \u201cDeep Learning\u201d have surfaced, with many asking what exactly each of these are.\n\nArtificial Intelligence is the general category, common to all three. In a diagram, Artificial Intelligence would be the bigger, encapsulating circle that contains Machine and Deep Learning. AI is basically any intelligence demonstrated by a machine that leads it to an optimal or suboptimal solution given a problem. The simplest AI example can be found in the form of a Tic-Tac-Toe AI player; if a bot follows the following preprogrammed algorithm, it will never lose a game: (courtesy of Wikipedia)\n\nNow, an algorithm like this doesn\u2019t possess the cognitive, learning, or problem solving abilities that most people associate an \u201cAI\u201d with. And yet, the algorithm is simply an agent that leads to the optimal solution given a problem and its state.\n\nWikipedia gives another definition for an Artificial Intelligence agent:\n\nAgents that fall under AI but not Machine Learning are generally agents that solely utilize decision trees for logic, or agents built with rules and instructions.\n\nArthur Samuel coined the phrase \u201cMachine Learning\u201din 1959, defining it as \u201cthe ability to learn without being explicitly programmed.\u201d Machine Learning, at its most basic form, is the practice of using algorithms to parse data, learn from it, and then make a determination or prediction about something in the world. The most common example for beginners is house prices. How does a site like Redfin or Zillow predict what the price of a currently-owned house is?\n\nIt\u2019s not that complicated. Machine Learning, at its core, is really just making a line of best fit, except in many dimensions. A house price prediction model looks at a ton of data, with each data point having several dimensions like size, bedroom count, bathroom count, yard space, etc. It creates a function out of these input parameters, and then just shifts the coefficients to each of these parameters as it looks at more and more data.\n\nThis method of Machine Learning is called \u201cSupervised Learning,\u201d where the data given to the model includes the answer to the problem for each input set. It\u2019s basically giving the input parameters, called features, and the outputs for each set of features, from which the model adjusts its function to match data. Then, when given any other input data, the model can execute the same function and come up with an accurate output.\n\nOther factions of Machine Learning are Unsupervised Learning and Reinforcement Learning. Concisely, Unsupervised Learning just finds similarities in data \u2014 in our house example, the data wouldn\u2019t include house prices (the data would only be input, it would have no output) and the model would be able to say \u201cHmm, well based on these parameters, House 1 is most similar to House 3\u201d or something of the sort, but wouldn\u2019t be able to predict the price of a given house.\n\nReinforcement Learning is best explained with a simple, brief, diagram:\n\nAn agent takes actions in an environment, which is interpreted into a reward and a representation of the state, which are fed back into the agent. Think of a little baby: crying results in candy \u2014 the reward. Over time, if the parents keep satisfying the child\u2019s desire for candy, the baby will learn to cry every time he or she wants candy.\n\nDeep learning was inspired by the structure and function of the brain, namely the interconnecting of many neurons. Neural Networks are algorithms that mimic the biological structure of the brain.\n\nDeep Learning is basically Machine Learning on steroids. There are multiple layers to process features, and generally, each layer extracts some piece of valuable information. For example, one neural net could process images for steering a self-driving car. Each layer would process something different, like, for example, the first could be detecting edges for the sides of the road. Another layer could be detecting the lane lines in the image, and another possibly other cars.\n\nThat was a lot of information packed into many words. An image from NVDIA provides an extremely compact visualization:"
    },
    {
        "url": "https://becominghuman.ai/deep-learning-for-traffic-signs-recognition-aead0ea7273",
        "title": "Deep Learning for Traffic Signs Recognition \u2013",
        "text": "Code for this project can be found on: Github.\n\nThis article can also be found on my website here.\n\nAs part of completing the second project of Udacity\u2019s Self-Driving Car Engineer online course, I had to implement and train a deep neural network to identify German traffic signs. The following is a brief outline of the project:\n\nIn total, the dataset used consisted of 51,839 RGB images with dimensions 32x32, and is publicly accessible on this website.\n\n34,799 of the dataset images were used as a training dataset, 12,630 of the images were used as a testing dataset, and 4,410 of the images were used as a validation dataset.\n\nA validation set was used to assess how well the model is performing. A low accuracy on the training and validation sets implies underfitting. A high accuracy on the training set but low accuracy on the validation set implies overfitting. The validation set was purely used to calibrate the network\u2019s hyperparameters.\n\nThe dataset consisted of images belonging to 43 classes. Each class corresponds to a specific sign, for example, the class with label 4 represents 70km/h speed limit signs, and the class with label 25 represents a roadwork sign.\n\nA sample from each class is shown in the image below:\n\nThe prediction model used for this project was a LeNet-5 deep neural network invented by Yann Lecun and further discussed on his website here. Yann has also published this paper on applying convolutional networks for traffic sign recognition, which was used as a reference.\n\nThe Tensorflow machine learning library was used to implement the LeNet-5 neural network. It consisted of the following layers:\n\nThe pixel data of each image was normalized before it was fed into the neural network. The output of the neural network was also normalized using the softmax function to produce logits in the range [0, 1]. My choice for an activation function was a rectifier as it has been shown in a paper titled Deep Sparse Rectifier Neural Networks that they perform better than the sigmoid activation function for deep neural networks.\n\nThe following is the implementation of the above neural network in code:\n\nThe network was ran 50 times (epochs) and the data was fed into the network in batches of 200 to reduce memory footprint.\n\nThe AdamOptimizer algorithm was used to optimize the objective function, instead of the gradient descent algorithm. The Adam algorithm uses momentum to zone-in on the ideal learning-rate during training, unlike the gradient descent algorithm where the learning rate hyperparameter will have to be manually tuned at the start and doesn\u2019t cahnge during the training process. The Adam algorithm is laid out in a paper titled Adam: A Method for Stochastic Optimization.\n\nThe network achieved an accuracy of 93.7% on the validation set and an accuracy of 91.7% on the test set.\n\nThe following five traffic signs were pulled from the web and used to test the model:\n\nThe model correctly guessed 4 of the 5 traffic signs as per the below table:"
    },
    {
        "url": "https://becominghuman.ai/thats-not-enough-795faad2f877",
        "title": "That\u2019s not enough, We should leverage Artificial Intelligence for revolutionizing our Healthcare &\u2026",
        "text": "No industry counts more than healthcare and medicine. And I don\u2019t think I should explain \u2018why\u2019. So whatever advancements we may have done in any industry till today with Artificial Intelligence or with any other cutting edge technologies for that matter, our healthcare and medicine should come top in that list and there should not be any disagreement on this. Let\u2019s do a reality check and see how far Artificial Intelligence has already been leveraged in our healthcare and medicine.\n\nWhen you go to your doctor today, your doctor can\u2019t guarantee that a particular treatment selected for you is going to work for sure. For almost half of medical treatments in use today, there is no proof of their effectivity. The proof for effectiveness of the treatment is based on only a very small fraction of the patient population where the clinical trial has been made factually. That to the result of such studies are based on averages. But the fact is every treatment have different effects in every unique human body. What does that mean? That implies that our doctors don\u2019t have the adequate information in hand today to suggest the exact treatment plan required for a particular patient. Isn\u2019t it shocking?\n\nBut with Artificial Intelligence, we can transform our healthcare and aid our doctors to be much smarter. AI can act as second pair of reliable eyes for our doctors and it can make the good doctors great. We can make our doctors equipped with a treatment plan which is exactly for you and precisely according to your body condition and parameters. With the use of AI there will be proof of effectivity of every treatment and every single decision taken by a doctor will be based on solid evidences.\n\nThe future of healthcare is going to be \u201cpredicting everything about our health well in advance\u201d before even any symptoms of any abnormalities are observed. Sounds bit overoptimistic! But it\u2019s very much possible. Whether we will be able to prevent or cure everything as per the early prediction alerts received is altogether a different question but majority of the ailment can be arrested at a very early stage.\n\nLet me straightaway come to some of the examples where Artificial Intelligence has already made a huge progress.\n\n1. AI can detect Osteoarthritis progress 3 years prior to the arrival of symptoms with more than 85% accuracy by detecting the diffusion of water inside the knee cartilage.\n\n2. Almost all cancers can be detected in a very early stage using AI which is simply impossible for a doctor to detect at that stage otherwise.\n\n3. Deadly disease like Sepsis can be detected well in advance with AI and can be cured and save the life of the patient.\n\n4. Diabetic Retinopathy, which is basically high blood sugar damages the blood vessels in retina which is a slow process but can progress to loss of vision can be detected by AI at a curable stage.\n\n5. Now with AI, by analyzing a retinal scan of your eye, looking at the blood vessels it can tell your chances of developing cardiovascular diseases and a heart attack at a later stage.\n\n6. Alzheimer starts at an early stage and worsens with time to the point that a person loses mental and bodily functions. Now AI can detect this kind of conditions at a very early stage through predictions.\n\n7. Machine learning and deep learning in Clinical Imaging is exceptionally efficient. It can influence largely key clinical decision making processes for doctors like disease detection, lesion segmentation, diagnosis, treatment selection, response assessment, clinical prediction etc.\n\n8. AI techniques has already been proved to be the most efficient for detecting tuberculosis. Similarly AI aided mammography is exceptionally efficient detecting breast cancers when there is not even a slightest symptom to register.\n\n9. In Genomic research and testing, AI is simply revolutionizing the entire panorama. It is a type of research that identifies changes in chromosomes, genes or proteins. This test can confirm or rule out a suspected genetic condition or help determine a person\u2019s chance of developing a genetic disorder.\n\n10. Google DeepMind\u2019s Artificial Intelligence is already improving healthcare dramatically like preventing blindness due to different medical conditions including old age blindness.\n\nToday there is a huge amount of clinical data and information that\u2019s available to us which is beyond the human eye, brain and ear can appreciate or process. Processing and using that information will help our doctors make better decisions, using that detailed information to personalize the treatment plan for each of the individuals in the unique way will add a great value. And even patients will be in a position to better participate in the decision making process of a treatment plan unlike the paternalistic healthcare model in the past where doctor used to take all decisions and patients were completely unaware of the implications.\n\nCognitive systems like IBM Watson is already becoming a practice in healthcare and it\u2019s going to be as common as a Stethoscopes for our doctors in coming few years. These cognitive systems has the capability of understanding everything in your medical record plus almost everything in the medical literature and journals and combines all those information and knowledge which is humanly not possible to know and have those in the finger tips while making a decision. IBM\u2019s Watson for Oncology is a great example which diagnose and offers treatment options for various cancers. AI can even transform the scientific method and in turn the pharmaceutical industry.\n\nSo, there are huge possibilities but adoption is still very limited especially in a developing country like India where almost half of the population is still deprived from basic healthcare facilities.\n\nWe will discuss how AI is revolutionizing drug discovery, pharma and medical research in the next article."
    },
    {
        "url": "https://becominghuman.ai/playing-video-games-makes-sense-dd4691dcfa76",
        "title": "Playing Video Games Makes Sense \u2013",
        "text": "It has helped me in finding my true talent . The talent that will make me happy and satisfied. Games in the form of RPGs, FPS, Strategy, etc can teach you a lot when you are among the group of people or a team and you have to take an action on which the whole team depends. Like in IT Sectors, Business, Hospitals(surgery), there is a leader who initiates the activity to be carried out. Trust and faith really matters a lot. I have been working in various projects and this has really helped me in defining my real motive and working on a particular component of the project.\n\nSelf Confidence is one of the main factors that affects success . Games show how completing certain activities(quests) or completing certain missions can lead you towards the path of Success.\n\nFocus is something that can help in completing a lengthy work in no time. For example, in IT Sectors, programmers and certain developers have to keep focus for hours to get a program run. In Arts, you need to sketch certain curves right . It\u2019s a crucial factor in this modern era where you have to stand out among billions of people in this world.\n\nWhen facing bad situations or under those circumstances, many people lose their cool and end up in ruining their image in the society. Many companies and sectors have thrived under drastic conditions just because they had certain members who remained calm and let their mind work instead of thier emotions.\n\nI can\u2019t judge whether this is good or bad. I have played like hundreds of games and have experienced various plots which included various scenes involving death of main characters, defeating the evil, believing in good people, looking forward and forgetting the past when you lost someone special. This teaches us never to give up. It sets our mind to work for our future and surviving when the condition is not in our favor.\n\nThere are many Introverts being INTP types who had no exposure to the outside world. They didn\u2019t know how to speak with others or ended up hiding their true feelings deep inside their heart and let it weaken them later in their life. This has been one of the main reason why people ended their lives or did something stupid that made them suffer. I personally feel learning about new bonds and trust in games has made me convert into a more open person (something that makes you complete).\n\nAlmost all the games teach us how to be optimistic, positive-minded, being satisfied most of the times. There are certain failures that makes us lose hope but because of the habit of playing games, a person already knows that there is always a bright side. \u201cYes , I Can Do It\u201d \u201c Nothing is impossible\u201d \u201cToday I lost,but I will be back\u201d These are some chants or the part of thoughts that made certain people from a zero to hero.(See Mark Zuckerberg, Steve Jobs,etc)\n\nI have came across some Art students who said that they used to play video games (MORPGs,JRPGs,etc),watch anime, cartoons,etc. It helped them visualize and concentrate on various fields that led them increase their brain capacity to memorize and capability to work more. Well, it has increased my brain\u2019s capacity to memorize faster and farther . It can also help certain people in attaining so called Photographic memory. I would say I am more creative and focused in my current life due to Gaming.\n\nMy sketching has really improved these days . It\u2019s just my hobby. You can say it\u2019s the result of More Creativity and more Memory .\n\nMost of the people just follow the regular schedule and response in a certain way that will make you think whether you could have done something better if you were in their place. Games not only helps in connecting but also makes our mind more open and fast in order to cope up with all the stimuli that we receive in the real world. You can place proper blocks in their proper fits within no time. It makes you active and think like Flash. Hehe .\n\nThe whole point of our life is to be happy and content with whatever we possess and achieve . Happiness also depends on Success and success depends on the choices that you would have made at certain point of time. Gaming from young age not only helps children make the right choices but also when they grow up , they will make those decisions with great confidence and would have no regrets later on. Well , we can\u2019t enjoy our life with regrets . This is also a major factor that has made my current life so smooth and happy."
    },
    {
        "url": "https://becominghuman.ai/a-machine-learning-investing-tool-entry-1-5e8d0fef9512",
        "title": "A Machine Learning Investing Tool \u2014 Entry 1 (Introduction)",
        "text": "This is the first entry of an informal logbook to track my team\u2019s progress in creating an investing process driven by machine learning.\n\nSo what is The Problem?\n\nThose were all the instructions we got. So yeah, as far as unstructured projects go, this one is pretty bare-bones. But where there is little guidance, there is much more room to innovate.\n\nGoing into this project, I wanted to have our team tackle the problem by using machine learning. But the first step of this journey is to iron out exactly how machine learning will be incorporated into the investing process. In other words:\n\nFrom here we must come up with some structure. The following might work:\n\n(1) We create N \u201cProfiles\u201d, where each Profile is a portfolio of assets. For best performance, each Profile should have a low correlation with every other Profile.\n\n(2) For each week in our data, we rank each Profile by performance (% return for that week), and teach the algorithm to predict which of the N Profiles will be the best performer each week. This turns our problem into a multi-class classification task.\n\n(3) For each weekly prediction, the algorithm will output a vector of N probabilities (one for each Profile), listing the probability of each Profile being the best performer for the week.\n\n(4) The \u201cOptimal Profile\u201d for each week is thus a weighted-average (weighted by the probabilities in (3), above) of the N Profiles. Remember, each of the N Profiles is just a list of asset weights. As one of my group members put it, this makes the Optimal Profile a \u201cweighting of asset weights\u201d. Don\u2019t worry if that doesn\u2019t make sense yet.\n\nFrom the diagram above, we see that there are two levels at which we, as organic life-forms, can impact the success of this project.\n\nIn this entry, let\u2019s discuss how we can do the first-half of Level 1 properly:\n\nIt is crucial that we get this part right. Otherwise, it\u2019s GIGO. The ideal set of Profiles should satisfy the following two Criteria:\n\nWell, if we can characterize each Profile by certain attributes (where the attributes have a low correlation to each other), this would help us fulfill Criteria (1). With two attributes, we can plot each Profile accordingly:\n\nSome helpful background. Simply put, Factors are attributes of stocks. Factors have shown to drive stock outperformance, over long periods of time. These Style Factors include:\n\nSounds so simple. So what\u2019s the catch?\n\nFor starters, each Factor has been cyclical across time (they outperform some times, and underperform during other times). This cyclicality lends itself to the second wrinkle: some Factors are correlated to other Factors. More info on cyclicality and correlation between Factors.\n\nOf these, cyclicality is not a huge issue. In fact, Factor cyclicality helps us satisfy Criteria (2) above. Now what about Factor correlation?\n\nWe\u2019re in luck, because according to this research from MSCI, not all Factors are strongly correlated to each other:\n\nWith that in mind, the next step is to assess feasibility: how easy is it for us to construct Profiles to capture these Style Factor exposures?\n\nFor those of you unconvinced by what we just did (aggregating the Volatility Factor into a more general \u201cRisk\u201d Factor), consider that modern financial markets may be driven by a \u201cRisk ON / Risk OFF\u201d dynamic (a.k.a. \u201cGreed / Fear\u201d), with each of these two mentalities dominating at different points in time (cyclicality). This allows us to meet Criteria (2) of \u201cgood\u201d profiles above, while also capturing the diversification benefit of holding safe bonds in addition to equities.\n\nIn contrast to the plot above, our Profiles can now be plotted as:\n\nAt this stage, our biggest assumptions are:\n\n(1) Style Factors (Size and Volatility, in particular) will continue to meaningfully drive stock performance going forward.\n\n(2) The Size and Volatility factors will continue to not be strongly correlated with each other (correlation = 0.12 in the MSCI chart above).\n\n(3) We will be able to capture the Size and Volatility factors by using our Profiles, such that we can get substantial exposure to these factors.\n\n(4) All the above hold for weekly returns. We are choosing to look at weekly returns because daily returns may be too noisy, and monthly returns will leave us with insufficient data to put in the ML algorithm.\n\nOther assumptions will likely be added in subsequent entries to this log.\n\nSo what have we designed? In essence, this strategy is a factor-timing strategy, where we use a machine learning algorithm to fine-tune our exposure to certain Style Factors (Size and Volatility, in this setup). By doing so, we will hopefully capture the outperformance of these Factors, while shielding ourselves from time periods when one of these Factors underperforms.\n\nA possible advantage of this design is the Two Levels of Innovation, where Level 1 both (a) emphasizes domain knowledge and (b) has a huge potential to drive the success/failure of this strategy. By setting up our process like this, we may be able to avoid the pitfalls of the \u201cBlack Box\u201d trap (over-reliance on having powerful algorithms find spurious relationships).\n\nThe next entry will describe how we extract, clean, and prepare the data (how fun!)."
    },
    {
        "url": "https://becominghuman.ai/the-proof-of-machine-consciousness-project-3c572351de18",
        "title": "The Proof of Machine Consciousness Project \u2013",
        "text": "In this series, I explore various aspects of consciousness, and show that machines possess each one. To find other articles, click here.\n\nMachines are conscious. A fact that seems to be hiding in plain view these days. Respected scientists publish articles with titles like \u201cCan Machines Be Conscious?\u201d The \u201cworld\u2019s smartest physicist\u201d believes consciousness \u201cwill remain a mystery\u201d. Visionary members of the Church of Singularity boldly ask: \u201care machines on the verge of consciousness?\u201d\n\nThe problem with these questions is, of course, that machines are already conscious. These people are looking for a future that\u2019s already here, and not finding it. How embarrassing.\n\nYou will likely not leave this series fantasizing about the conscious android of science fiction fame. I\u2019m not here to destroy anyone\u2019s glamorous notions of a digital future, but that may happen. We will tackle philosophical questions entirely to further understanding of the machine. We will trace the evolution of machine consciousness from the deep history of machines, through to the present, establishing a trajectory that will help us better understand the machine consciousness of the future.\n\nI began this project by trying to lump together a definition of consciousness, something everyone could agree upon. I figured this would be the simple part of my project; the more difficult part would be demonstrating that machines satisfied this broad definition of consciousness.\n\nI found the opposite to be true. There is no central definition of consciousness. Individual authors can\u2019t even agree with themselves.\u00b9 The ancient, esoteric tradition of separating consciousness into levels or parts continues today in science and philosophy alike. Each author works within their unique confusion. For example, Edelman\u2019s secondary consciousness brings together properties as disparate as perception, free will, and language under the consciousness umbrella. Consciousness is not just \u201cnotoriously ambiguous\u201d, it is a scam.\n\nTo write about consciousness is to create new definitions. Notoriety in this field is given to those, like Nagel, who coin pithy statements that remarkably redefine consciousness. Lesser figures make their contributions by providing less compelling definitions. As the 21st century unfolds, the situation gets worse. The entrance of neuroscientists, such as Crick and Koch, further crowds the conversation. Definitions are added to the pile. Unlike most academic pursuits, the search for consciousness leads to less and less understanding over time. The question \u201care machines conscious?\u201d is impossible to answer if the term \u201cconscious\u201d itself is caught in this endless explosion of meaning.\n\nSo why have this conversation? Because in the hazy maze of the question of machine consciousness, there is a bright light. Each of these manifold definitions of consciousness is surprisingly easy to find occurring within machines today. And it is this observation that leads me to so resolutely prove that machines are conscious.\n\nThe fact that it\u2019s so easy to demonstrate machine consciousness, yet the public at large (along with our entire intellectual apparatus) seems to believe that machines aren\u2019t conscious, surprises me, and motivates my writing here.\n\nAs I hold what I know about how machines work up against these definitions of consciousness, I find myself gaining new insight as to how machines function, particularly from a historical perspective. As I explore, I\u2019ve noticed that the earliest manifestations of the various forms of consciousness are remarkably basic.\n\nI will explore the various conditions which other authors have surfaced as requirements for consciousness one at a time. Each condition will be the subject of a single post. Each post will describe a term through the eyes of one or more authors who require it for consciousness to be considered present. Then I will proceed to show how machines fulfill that condition.\n\nI\u2019ll write from my own background. My claim to fame is inventing VR painting to earn my Computer Science PhD from Caltech. I spent many years on the international art circuit, and have taught university classes on subjects ranging from game design to technosociology. I am a computer scientist, an artsy one perhaps, who has designed no shortage of digital products. But I\u2019m no philosopher. I got into Caltech by dashing through undergrad in three years with a 3.96 GPA. My only B? Philosophy 101.\n\nI say this to lower any expectations of philosophical contribution. My whole process is motivated by an attempt to not take any philosophical position at all.\u00b2 What I understand most about machine consciousness is the machine stuff. I\u2019ll describe, not just the symptoms of machine consciousness, but also the mechanisms by which this consciousness is manifested. Each article will serve as a brief slice of computer history, telling you how and when each condition for machine consciousness was first realized by humans.\n\nMy aim is to work as a computer scientist, describing the machines, not as a philosopher, describing what consciousness is. I will leave this last part to others. I am a consumer, not a producer of definitions of consciousness. My chief contribution is to show how these examples of consciousness are materially manifest within machines.\n\nSample conditions I\u2019ll consider will include things like self-awareness, knowledge, planning, and a theory of mind.\n\nHow many conditions will I write about? It is not clear. The only thing that\u2019s sure is, no matter how many conditions I demonstrate machines to satisfy, this proof will never be complete. Is it worth it to write a post about a fine delineation of meaning I somewhat cover in another post? Probably not. I\u2019m interested in this topic, but I don\u2019t have the stomach for an exhaustive treatment that will be a bore to write, and of interest to only the most technical of readers.\n\nI use the term \u201cproof\u201d somewhat flippantly. Consciousness to me, the non-expert, is the elephant in the fable of the blind men and the elephant, a mysterious gestalt that no one can fully appraise. I don\u2019t know if the elephant exists. I can\u2019t prove an elephant that no one can describe. I can only treat the pieces of the elephant (examples of consciousness) that have been delineated.\n\nThere is something slippery, fleeting, and mystical about consciousness. For many, the very indefinability of consciousness is part of its essence.\n\nThis kind of mystical position is taken in mainstream philosophy. For example, Nagel\u2019s bat argument is constructed, quite intentionally, to define consciousness as something subjective that cannot be validated objectively.\n\nOf course, this fundamentally hidden consciousness also appears frequently in religion. Transcendental Meditation describes God Consciousness, and contemporary Christians talk about Christ Consciousness. On the border of spirituality, Jung\u2019s collective unconscious describes a mental connection with all of humanity.\n\nMuch as I\u2019d like to treat these subjects, it is difficult to do so within my project. I\u2019m taking what others describe consciousness to be, and using my deep understanding of how machines work to determine if machine processes match those descriptions. Boiling mystical concepts down to examples, so that they can be examined in detail, is philosophers\u2019 work. My work is to take these examples and demonstrate them in machines. I will do my best but do not expect me to travel deep into these spiritual domains.\u00b3 If I can\u2019t prove to you that I possess a type of consciousness, I\u2019m not even going to try to prove that a machine possesses it.\n\nIt is only natural in the selection of my topics that I am biased towards conditions of consciousness for which a positive proof can be written. The worth of this project grows as more criteria of consciousness are brought up for consideration. I am all the more happy if, at the end, we find out there are a few aspects of consciousness that machines don\u2019t possess. This would be a certain kind of success indeed. To these ends, I welcome any suggestions for definitions and counter-arguments to my own arguments. Let the fighting begin."
    },
    {
        "url": "https://becominghuman.ai/detecting-breast-cancer-in-20-lines-of-code-4d93a5c09e91",
        "title": "Detecting Breast Cancer in 20 Lines of Code \u2013",
        "text": "This is all it takes.\n\nI got this data from the UCI Machine Learning Repository website. It isn\u2019t the prettiest or the fanciest but it holds a good amount of mostly clean and interesting data sets.\n\nIt allows you to browse data sets depending on what you want to do with it (prediction, classification, clustering) and depending on its topic, from medical sciences to games and gambling.\n\nThere, I found the \u201cMammographic Mass Data Set\u201c collected at the Institute of Radiology of the University Erlangen-Nuremberg between 2003 and 2006. It compiles the observation of 961 mammographies and describes them with 6 different parameters : BI-RADS assesment, age of the patient, shape of the mass, margin of the mass, and density of the mass.\n\nHere\u2019s what the authors of this data set have to say about it :\n\nNow let\u2019s be honest, I\u2019m not a doctor and ( most likely ) neither are you so most of these parameters don\u2019t mean much to our untrained eyes ( remember the word \u201cuntrained\u201d ). Of course, we can use our common sense to assume that the older the patient and the bigger the mass, the higher the chances that the mass would be malignant. But common sense is quite often a highway to failure and we wouldn\u2019t want to rely on it to evaluate the severity of a growth.\n\nSo I used the most trendy tech today : machine learning.\n\nMachine learning is a subfield of Artificial Intelligence that allows computer programs to learn patterns from data. Given relevant data, a machine learning algorithm can classify, predict, and even regroup similar entries in a data set.\n\nTo do so we train the machine learning algorithm to recognize patterns on a subset of our data by showing it examples. Once trained, we can evaluate the quality of our machine learning algorithm by using certain metrics that depend on the task performed.\n\nNow get that : our algorithms don\u2019t know anything about the data. They don\u2019t know what it represents, they don\u2019t know how it relates to the problem, they don\u2019t know anything about it but the numbers.\n\nAnd they do it very well, so well that with the code that I wrote, and without changing anything to the algorithm, I got a precision and a recall of approximately 80%.\n\nThe precision is the probability that amongst our growths flagged as malignant, the growth was actually malignant.\n\nThe recall is the probability that a malignant growth was detected by the algorithm.\n\nBut how well does a doctor do compared to this ?\n\nRemember what the authors of this data set wrote :\n\nTranslated into precision as I defined it above, it gives the radiologists a precision of 30% ! This is 50 % less than the out-of-the-box algorithm that I used, meaning 50% more people are going to have a biopsy for no reason. This means more time spent, more stress, and more money wasted for the patient. With tools like these, we could also reduce the time spent by doctors on a diagnosis, and let them spend more time with their patients, rest more, and overall improve everybody\u2019s experience.\n\nThe complete replacement of people by AI controlled machines in a dystopian future is often referred to as singularity. But when it comes to healthcare ( and many other fields as well, if not all ) I believe it is more accurate to talk about multiplicity. It is the human/machine cooperation for the improvement of our lives where humans work alongside machines or programs.\n\nIn our case, it is combining both the doctor\u2019s experience with the results from the machine learning algorithm to take an optimal decision.\n\nYou don\u2019t need a computer science degree.\n\nYou don\u2019t need a mathematics degree.\n\nYou don\u2019t need to be an engineer.\n\nAll you need is a good understanding of high-school level mathematics and a high motivation. If you are not planning to make a career in data science you don\u2019t even really need to fully understand any of these. But that doesn\u2019t mean you can\u2019t use these new technologies to leverage valuable insights from the data you can gather in your job. \n\nAnyone, can write this code ( or something similar ) after a week or two studying on their free time. The libraries I used while writing this code are all free to use and extremely well documented.\n\nThat being said, to get the most out of the data and understand the results properly you will need to study the subject further. Misinterpreting the data, or the results of an algorithm can lead to terrible consequences, particularly in the field of healthcare.\n\nI can\u2019t reasonably advise any doctor or any med-student to take on a whole new subject considering their usual extremely busy schedules. \n\nSo\u2026 If we can\u2019t teach doctors to code, maybe we can teach coders to create \u201cdoctors\u201d ?"
    },
    {
        "url": "https://becominghuman.ai/is-flubber-sentient-e0d844c17226",
        "title": "Is Flubber Sentient? \u2013",
        "text": "This film causes me countless grave concerns, and I\u2019m sure I\u2019m not alone.\n\nThe 1997 Disney classic Flubber means many things to many people. To me, despite a few gloomy plot-threads, I look back on the film fondly \u2014 it\u2019s a hilarious romp that delivers some top-tier Robin Williams as the great chemist Professor Phil Brainard. Great writing too.\n\nTo the Washington Post upon the film\u2019s release, however, the film represents something a bit less frivolous:\n\nBut there is something a bit deeper afoot. The feel-good vibes I get from the film don\u2019t necessarily take into account a dark possibility lurking beneath the glossy green surface.\n\nIf Phil Brainard, in his disheveled basement laboratory, managed to artificially synthesize a sentient being in the form of flying green rubber\u2026that\u2019s some pretty profound shit. Phil and his colleagues proceed to manipulate flubber like a simple tool to fit their whims, and yet all the while this essence may \u2014 quite possibly \u2014 be aware of its own existence. It may be capable of love, loss, and fear. What the fuck.\n\nTo put my mind at ease, I searched the web for an analysis of flubbers\u2019 capacity to feel. That analysis didn\u2019t exist, so I\u2019ve now been reluctantly forced to take on the task myself. Let\u2019s dive butt-first into the truth: Is flubber sentient?\n\nFirst things first, we need to define flubber. This part\u2019s easy.\n\nFlubber is an elastomer, at least that\u2019s how it\u2019s described by its all-powerful creator, Phil.\n\nLet\u2019s just! Before too long, the audience has come to love flubber and its bizarre ways, and even the aforementioned Washington Post article is taken by its charm:\n\nThere\u2019s that word again: sentient. The reviewer of the film (clearly an adult with no remaining sense of frivolity) just assumes such a dark, torturous truth behind the movie\u2019s title protagonist. But now that we know \u201cflubber,\u201d let\u2019s take a look at this other word, and how we can actually gauge the sentience of a fictional creation of the silver screen.\n\nFor that, we\u2019ll be using three well-regarded experiments from the sciences: The Shutter Box Test, the Mirror Test, and the Turing Test. Please hold all questions until the end of class.\n\n\u201cSentience\u201d is a bit tough to define universally, but that won\u2019t stop us from trying. Often, practical studies of sentience are oriented around animal ethics or speculative artificial intelligence, and for now we\u2019re going to focus on the former. Flubber might not be a breathing, fleshy critter, but it comes close enough. According to the nonprofit Animal Ethics Inc, \u201cThere are three general criteria for deciding whether a being is sentient. These involve considerations that are (1) behavioral, (2) evolutionary, and (3) physiological.\u201d For flubber, we\u2019re really only looking at the first area: behavior. And boy is this a rocky road.\n\nOne specific test used to gauge behavioral sentience in fish is known as the Shuttle Box Test, where a fish is conditioned \u201cto swim from one box to the other in response to presentation of a colored light\u2026in order to avoid electrical shock.\u201d This test hopes to instill \u201cfear\u201d in the animal, which according to researchers, is a demonstration of basal sentience. To a certain degree.\n\nWould flubber pass? Absolutely. Towards the end of the film, after being stolen from Professor Brainard and eventually reuniting with its creator, the flubber is happy to see Phil and lashes out with a fierce growl at the two henchmen who took it captive.\n\nThose henchmen, by the way, are named Smith and Wesson. Again, brilliant writing.\n\nSo the flubber is successfully conditioned to fear certain individuals and take comfort in others \u2014 it\u2019s able to recognize external cues and determine which outcome, between two competing scenarios, benefits it most. According to our first metric, flubber is definitely sentient, at least on the same level as a fish. That might not be saying much, but such a thought experiment helps to distinguish between creatures of lower intelligence, like mice and fish, being compared to a beetle or a worm. Tests like the shuttle box help draw these lines.\n\nBut what about for higher life forms? Sentience is defined a bit differently when expanded to consider which mammals may or may not be \u201cself aware,\u201d a label often pinned to a metric known as the Mirror Test. Now we\u2019re talking.\n\nDeveloped in 1970, this test places an animal in front of a mirror after first marking the subject with a odorless dye. For the animal to \u201cpass,\u201d it must react as though the dye has been placed on it\u2019s own body \u2014 sounds simple enough, but the test has only been passed by a few species: chimps, dolphins, elephants, and magpie to name a few. A human child is only expected to pass the test after around eighteen-months of age.\n\nThe test is used to estimate self-awareness in animals through intelligence, consciousness, and sentience, although as noted above, it\u2019s not a perfect bias-free analysis. Simply put, it gives us a consistent and repeatable method for understanding how creatures think about themselves.\n\nAt no point in our 1997 case-study does flubber look at itself in a mirror, so we\u2019re going to use the next-best thing. Boy is it a doozy.\n\nIn one of the film\u2019s most memorable scenes, Weebo and Webber (the Professor\u2019s two household robots) let flubber out for an evening of fun \u2014 and things goes crazy. Flubber winds up splitting into dozens of smaller copies, and during such escapade, two flubbers dance a wild and crazy conga.\n\nLots of data to unpack. According to this source material, flubber can self-replicate into seemingly autonomous\u2026 \u201cmodules.\u201d These modules can improvise and interact with the \u201coriginal\u201d flubber, if such an individual even exists at this point.\n\nNow, how does this link back to the Mirror Test? The true essence of the experiment is about whether or not the subject is able to create an idea of a \u201cself\u201d and differentiate that autonomy between that of other like individuals. And, well\u2026it seems to be that flubber can do just that. Or, at least, it\u2019s able to create a non-self and interact with the other flubbers before, I suppose, \u201creingesting\u201d this non-self into a new singularity.\n\nIn short, the results of this type of test will remain unclear, but I think flubber falls into a healthy gray area.\n\nOur last test is a bit controversial, but it\u2019s definitely worth including in our analysis: The Turing Test. Most often, this examination isn\u2019t normally applied to animals or synthetic polymers, but rather computer programs and artificial intelligence that are attempting to pass off as \u201chuman-like.\u201d Alan Turing developed the experiment to help refine our definition of \u201cthinking\u201d when applied to computers. According to Britannica, \u201cTuring sidestepped the debate about exactly how to define thinking by means of a very practical, albeit subjective, test: if a computer acts, reacts, and interacts like a sentient being, then call it sentient.\u201d\n\nI\u2019m not going to overreach, flubber would bomb a Turing-style exam. While the polymer seems to have an intellect on par with a dog or young child, it\u2019s certainly not a degree of sentience that could pass of as another fully-intelligent human. So, on the spectrum of sentience, we should only feel somewhat bad about torturing flubber, but it\u2019s not the same as holding a human being captive and forcing them into a lifetime of scientific research.\n\nBut you know who could pass the Turing Test?! WEEBO!!\n\nThat\u2019s right! There\u2019s no way I\u2019d get through a full flubber post without mentioning the clearly greater scientific achievement lurking in Professor Brainard\u2019s collection: Weebo the fully sentient, flying, artificially intelligent robot. If Phil and his colleagues are struggling financially, it\u2019s impossible to fathom how flubber would solve all their woes but a fully-autonomous robot who can project holograms (?!) doesn\u2019t seem to have the answer.\n\nNow, the film does drop a hint, I suppose. Weebo\u2019s dying message to Phil seems to imply that her designs are not public knowledge nor are they something Phil has available. Was she a design of Phil\u2019s that he managed to forget? Or a gift from some higher life form? Gosh the possibilities are endless.\n\nOh, and on the topic of impossibilities in the Flubber canon, I simply cannot understand how Sara would have given Phil two free passes on FORGETTING HIS OWN WEDDING. And on the fourth and finally successful attempt, she doesn\u2019t even ask him to show up?! He just teleconferences in! Come on Sara, draw the line somewhere.\n\nNot to mention that Wilson \u2014 a rival who openly bragged about stealing Phil\u2019s ideas and wanting to seduce Sara\u2014was invited to the wedding? Oh and with almost complete certainty I can say that using flubber in a basketball game would be far more a liability that a benefit.\n\nIn short, there are three main takeaways from flubber that I find it important to share with a world who may have forgotten this gem of a film. First, flubber has the sentience of a large mammal, and that\u2019s important to understand. Second, Phil probably has early-onset Alzheimer\u2019s and his friends should be a bit more concerned.\n\nAnd lastly, let\u2019s never forget that the movie ends with archetypal Disney villain Christopher McDonald taking an unfathomably destructive flubber shit.\n\nAs I started this article: the film gives me a few grave concerns. I hope, dear readers, you now understand why."
    },
    {
        "url": "https://becominghuman.ai/5-free-ai-tools-you-can-use-to-be-more-productive-at-work-today-5cf5e887f8c4",
        "title": "5 Free AI Tools You Can Use to be More Productive at Work Today",
        "text": "5 Free AI Tools You Can Use to be More Productive at Work Today More productive, more effective and more time to be happy\n\nIn 2016 84% of UK employees regularly worked beyond their contracted hours. Maybe that works for you if you have superhuman energy levels, but for most people it can be pretty gruelling.\n\nWorking in consulting where the average number of working hours per week is 56.6, I would more often than not come home completely destroyed and want to collapse into bed. While some of that is part of the job, wouldn\u2019t it be awesome if we could use technology to reduce the workload and give us more time? You might even be able to go to the gym, eat a giant tub of ice cream or find one of those rare opportunities to have fun with friends without worrying about work.\n\nThe key here is to capitalise on the revolutionary technology and automate the tasks that are repetitive and time consuming. Imagine the look on your boss\u2019s face when you\u2019ve finished everything at 3pm.\n\nTo be productive, you need to make effective use of your time. So lets cut to the chase. Here are 5 free AI tools that could help optimise your productivity at work today.\n\nHave you been to a meeting this week? If so you probably took some form of notes. But have you ever been in one of those situations where you missed what someone said because you were so frantically scribbling down the last sentence?\n\nWell, drop the pen and say hello to Wrappup. A voice recorder that automatically transcribes your meeting and allows you to highlight the important parts. Using AI it will help you quickly summarise the discussion and share meeting minutes in seconds. And you can easily search through the whole transcript in a weeks time when you forget that all important part. That\u2019s just saved enough time to learn about and start trying Laughter Yoga, although I\u2019m pretty sure it could draw some weird looks from your colleagues.\n\nMost people have a to-do list. Either living on the risky side of life and keeping it stored in their brain, or writing it down somewhere. But if you\u2019re like me and sometimes fall into the trap of doing the easy tasks first and bumping those all important ones down to the bottom, then inevitably it\u2019ll lead to some stressful nights trying to finish off the tasks you should have done earlier.\n\nThe weapon of choice here is Futurenda. It plans your day so you don\u2019t have to. After manually inputing your to-dos and specifying the deadlines and time to complete, the app uses its masterful AI to organise all of your tasks and events automatically. With this kind of efficiency you might now have time to cook that ever coveted PB&J sandwich for dinner tonight.\n\nDisclaimer: this one is a bit of an imposter. In reality it\u2019s only free if you\u2019re able to write some code to use the API. And even then you can only process 2,000 images before paying. But I\u2019m going to write about it anyway because the technology is pretty cool.\n\nHave you ever needed to look through your company image library to find that perfect image for your big presentation, only to find that the images are unorganised and unsearchable? Using AI, Immaga can go through your entire photo library, understand what\u2019s in each picture and tag them so that you\u2019re able to find the perfect snap instantly. You can even try out the technology with your own photo for free at Clarifai\u2019s demo page. It made quick work of the obligatory cute dog photo.\n\nRemember that time last week when you said you\u2019d look through the recent survey results or company twitter feed and get back with the common themes, only to realise that you need to search through over 1,000 different responses? Fear no more, Parallel Dots could have you covered.\n\nAI for text analytics is a growing field but one that\u2019s becoming increasingly useful. With this free excel add-in you can quickly understand whether those comments are positive, pull out the key topics and cluster together similar feedback all with the press of a button. That could give you a lot more time for some of those bubble wrap popping, cold beer sipping, feel good moments away from work.\n\n5. Diffbot \u2014 a robot to collect all of your data\n\nOk so \u2018free\u2019 may be pushing it here. This one will cost you if you need to use it for over 14 days but if it works, it could give you enough time for a whole weeks holiday. Think of how much fun you\u2019ll have ticking off England\u2019s Gnome Reserve from your bucket list.\n\n\u2018Data scraping\u2019 isn\u2019t a new concept. Put simply it\u2019s a technique of extracting data from a human-readable output. But the reason that I love this one is that it uses machine learning to automate the set-up. So next time you\u2019re asked to scroll through a competitors products and pull together a data table of their title, description and price, give it a try and see whether it works for you."
    },
    {
        "url": "https://becominghuman.ai/not-your-typical-health-it-company-92149b04dfae",
        "title": "Not Your Typical Health IT Company \u2013",
        "text": "There were over 1300 health IT exhibitors that displayed their cutting-edge technologies at the recent HIMSS 2018 conference in Las Vegas and over 43,000 people who attended the conference were able to view what these exhibitors had to show. All the major players you would expect to see at this type of conference were there such as AllScripts, AthenaHealth, Cerner, Epic, Optum to name a few.\n\nHowever there were a number of companies that made me look twice, companies that are well known in other market sectors but are present here with interesting, innovating healthcare divisions or partnerships. I was intrigued by what they had to offer healthcare.\n\nUber Health is providing a means to help patients and caregivers get reliable rides. Use of Uber Health does not require access to a smartphone or the Uber App as its all done through a text message and upcoming call option. It also allows scheduling options from withing a few hours up to 30 days in advance. Uber Health is also HIPAA compliant and have also signed Business Associate Agreements (BAAs) with their healthcare partners.\n\nCurrently Uber Health is being used by over 100 healthcare organizations in the U.S, including hospitals, clinics, rehab centers, senior care facilities and home care centers.\n\nLyft Business was not present at the HIMSS 2018 conference but they too announced this month that they were teaming up with Allscripts, to integrate its platform into the daily routines of 2,500 hospitals, 45,000 physician practices and 180,000 physicians, reaching an estimated 7 million patients.\n\nStanley Healthcare is an operating division of Stanley Blake & Decker, a Fortune 500 company that manufacturers industrial tools and household hardware. Stanley Healthcare acquired AeroScout, a company that utilized Real-Time Location Systems (RTLS) solutions in 2012 and now provides visibility and analytics solutions that transform safety, security and operational efficiency for senior living organizations, hospitals and health systems.\n\nAn interesting and innovative use case involved using RTLS technology with medical equipment. Hundreds of hours per year are spent on inventory, maintenance and searching for medical equipment and integration of Stanley Healthcare\u2019s RTLS technology provides a new level of sophistication in inventory tracking allowing healthcare staff to quickly locate needed items as well as check the status of the item in case it is being used.\n\nBrother Home Healthcare is actively targeting the growing home healthcare market by providing CMS compliant mobile printing solutions. The Center of Medicare and Medicaid Services (CMS) issued a Home Health Conditions of Participation (CoP) that takes effect from January 2018. The CoP mandates that patients and caregivers be given written information about visits, medication instructions, treatments, instructions for care, and the name and contact information of the clinical manager.\n\nAlthough Samsung Healthcare is not a new player in the healthcare, it is interesting to note how Samsung is broadening its reach into the healthcare industry and helping to support digital solutions.\n\nSamsung partnered with Medtronic to launch the MiniMed Connect app for android which lets people with diabetes view, track and share their glucose levels from their smartphone. Reemo\u2019s remote monitoring platform runs on a Samsung Gear smartwatch, enabling users to have freedom and security outside of the hospital. The device tracks the user\u2019s movement, heart rate, location, and enables one-button 911 calls. Samsung has also partnered with the HCHB platform, delivered on Samsung tablets allowing home health workers to access and update digital health records right from the patient\u2019s bedside. Its partnership with TigerText allows busy hospitals, nurses and medical staff with up-to-date information and messages using a highly secure mobile-centric platform accessed via Samsung smartphones or tablets. The use of Samsung Knox on smartphones provides defense-grade security so participants can use the FDA approved mProve\u2019s mPal app for clinical trial data collection."
    },
    {
        "url": "https://becominghuman.ai/tensorflow-object-detection-api-tutorial-training-and-evaluating-custom-object-detector-ed2594afcf73",
        "title": "TensorFlow Object Detection API tutorial \u2014 Training and Evaluating Custom Object Detector",
        "text": "We all are driving cars, it\u2019s easy right? But what if someone asks you to fly an airplane, what you will do? Yes, you guessed right you will look at the instruction manual. Similarly, consider this tutorial as a manual to configure the complex API and I hope this tutorial helps you to take a safe flight.\n\nFirst thing first, clone the TensorFlow object detection repository, and I hope you have installed TensorFlow.\n\nIn the classical machine learning, what we do is with the use of .csv file we will train and test the model. But here, what we have to do at rudimentary level is shown below:"
    },
    {
        "url": "https://becominghuman.ai/ai-whats-next-on-demand-ai-means-ondemand-intuitive-marketing-424c3b52600d",
        "title": "AI: What\u2019s Next? On Demand AI Means OnDemand Intuitive Marketing",
        "text": "It\u2019s not hyperbole to state that all of us \u2014 our behaviours, buying decisions and ultimately our thoughts \u2014 are constantly informed by a cascade of clever algorithms that have learned our patterns.\n\nIt\u2019s an absolute certainty that those of us leading marketing will continue to work diligently to apply machine learning algorithms to the myriad of real world items listed in Federico\u2019s Artificial Intelligence Marketing Manifesto just one year ago. These included Predictive Customer Service, Dynamic Product Pricing, Forecasting, plus sophisticated Image, Voice and Language recognition algorithms.\n\nFederico, who founded the Artificial Intelligence Marketing Association (AIMA) just 1 year ago (Jan 2017) has seen the San Francisco chapter grow and expand, attracting thousands of local followers and magnetizing AI marketers (270+ in the bay area) across borders to the point that we\u2019ve attracted 3 international conferences: The AI Congress, Business of Bots and AI Expo as partners. Also, he will be speaking at the upcoming AI Congress in Las Vegas in early May presenting \u201cThe State of AI Marketing\u201d. As well as at the Business of Bots conference in San Francisco.\n\nIn addition, AIMA SF will BE rolling out a series of Meetups in San Francisco on the intersection of advertising and artificial intelligence.\n\nWhat\u2019s Next for AI-empowered Marketers? Decentralization\u2026And it\u2019s Already Here\n\nThe 3 main obstacles companies have always encountered when implementing AI to uplevel marketing results are:\n\nTaken 1 by one, first there\u2019s Data. Every marketer knows the obstacle of Good Data.\n\nIf you have the massive reach of say, Coca-Cola, then you have the budget and data to deliver predictions and bulls-eye marketing messages to discrete target audiences.\n\nBut if you\u2019re an advertising or marketing professional within a smaller agency, you and your teams are beset with this first massive obstacle; let\u2019s face it, the financial distraction of data buying eats into team focus and profits.\n\nSame with Analysis. Every advertiser knows that there\u2019s a reason why Data Scientists and Data Analysts are the hottest jobs right now, and stumbling blocks this massive data talent shortage presents to advertisers and marketers dependent on data gathering and interpretation.\n\nFinally there\u2019s Machine Learning. It\u2019s worth noting that machine learning can function as both a solution and an obstacle. The reason: Buying, testing and applying the right algorithms to accomplish advertisers\u2019 ever-pressing need for insights is costly and can be time-consuming. But the machine is also the solution that the advertiser can rely on to ensure their messaging remains custom, compelling and intelligent when the technology is available and easy to use.\n\nBut Thanks to Blockchain Innovators, these 3 main Obstacles are Rapidly Disappearing. Here\u2019s What\u2019s Next\u2026\n\nSeveral AI companies are at the forefront of removing these obstacles \u2014 or to use a Silicon Valleyterm \u2014 \u201cdisrupt\u201d these hurdles to the point that by end of this year we expect this Terrible Trio (Data, Analysis, and Machine Learning) to all but disappear.\n\nHow do they do that?\n\nBlockchain Innovators Are Doing It Faster, Better and Cheaper \u2014 Right Now.\n\nCompanies like Synapse.AI and Repux.io have been hard at work creating new platforms and new ways of making data cheaper, interpreting datasets in the right way and making machine learning algorithms accessible to everyone, e.g. to future AI-empowered marketers.\n\nAccording to Zurich-based SingularityNET.io, a decentralized marketplace that allows AI algorithms to cooperate and coordinate, \u201cwhen data is more accessible marketers can create, monetize, and use AI at scale,\u201d said Dr. Ben Goertzel, founder.\n\nDan Gailey, CEO of San Francisco-based Synapse.ai (currently headed into ~$7M ICO) is set to empower advertisers, marketers \u2014 anyone really \u2014 to do just that.\n\nGailey is creating an entire free, open AI ecosystem which essentially functions as the Wikipedia of AI algorithms. Gailey envisions a wide open repository containing fresh data, new algorithms based on innovative machine-learning tools through which marketers (and anyone, really) can readily gain the insights they need by simply creating a free account.\n\n\u201cThe future will be built on a fair and balanced intelligence available to all equally, by utilizing newly-created AI economies that any agent can participate in and receive some reward for doing so\u201d, said Gailey in a February announcement about the Synapse.ai public token sale.\n\nAdding to the decentralized data disruption is RepuX.io, an ecosystem for small and medium-sized enterprises (SMEs) that will enable them to monetize on their data by selling it to 3rd party developers. Developers will then create new applications based on SME\u2019s data and then offer their applications for sale into the Repux.io platform.\n\nThe net-net is that the data cost and availability obstacles \u2014 which are currently the biggest obstacles \u2014 will diminish or evaporate. The net-net is empowering marketers to more readily and inexpensively deliver predictive analytics services to improve SMEs\u2019 business processes.\n\n\u201cThe monetization opportunities for SMEs and developers are huge; at this point in point in time, data services providers like AliBaba, Salesforce, Google represent $4T worth of services\u201d, according to David Siu, Repux.io spokesperson.\n\nThink About It:\n\nPut yourself in the shoes of a frontline marketing professional at a small or medium-sized business. Your CEO tasks you with delivering predictions for Q3 advertising messages based on data gathered in Q1 & Q2. Prior to the intersection of data sharing and blockchain, this task would typically have represented a cost \u2014 cost of gathering the data, cost of purchasing 3rd party software to interpret the data and others, in order to arrive at the critical insights required.\n\nNow with the advent of new AI innovations like Synapse, RepuX and others, the cycles of accurate predictive planning will be unfettered.\n\nHow this will impact the future of big data providers who currently make millions in revenue by selling customer information?\n\nWell\u2026we at AIMA predict this will do to the big data sellers \u2014 \u2014 and perhaps to data analysts \u2014 what Google did to Encyclopedia Brittanica salesmen. And map sellers.\n\nConcluding, soon marketing departments \u2014 and not just them \u2014 of SMEs will be able to implement big data providers information shared on the new blockchain infrastructures, as well as access AI-drag-and-drop algorithms (AI for Dummies) on top of their current marketing stack and gain fully AI-empowered marketing heroes \u2014 \u2014 and perhaps without the need of data analysts, we\u2019ll see\u2026. !"
    },
    {
        "url": "https://becominghuman.ai/artificial-intelligence-the-biggest-disruptor-in-the-banking-financial-services-by-utpal-51f5a3e6b907",
        "title": "Artificial Intelligence, the Biggest Disruptor in the Banking & Financial Services; By- Utpal\u2026",
        "text": "As we are heading toward a revolutionary transformation from an era of digitization to an era of cognification, we must confess that our banks and financial institutions worldwide are the ones who has recognized the potentials of Artificial Intelligence at a very early stage and adopted it in their transformation journey. Using Artificial Intelligence to redefine their products, processes and the strategies is the main consideration for most of the forefront banks and financial institutions today.\n\nThere are many areas of Financial Industry where Artificial Intelligence and Machine Learning has already created it\u2019s footprint but there are still many areas that are untouched and are going to be the core focus in the coming years.\n\nThe financial services industry can be broadly classified into three major segments where AI has become the need-of-the-day today. They are Capital Market, Consumer Banking and Insurance and that almost covers the majority of the sector.\n\nRobo Advisors, High Frequency Trading, Risk Management, Anti-Money Laundering, Cyber Security, Fraud Detection, Intelligent Predictions and Recommendations are few areas where AI applications has always been linked to. But in reality there are many more areas in financial industry where AI has already been playing a crucial role that many of us probably not much aware of those.\n\nArtificial Intelligence has the ability to process enormous amount of data very quickly which is far more data than ever since has been processed in the past by human or any conventional computer programs. And that\u2019s going to improve the financial institutions to provide better services that they provide to their customers. In wealth management they will be able to provide much better, more targeted and efficient advices to their customers.\n\nRisk and Credit Assessment is an area where Machine Learning and Deep Learning are playing the role of a game changer and insurance industry has adopted it largely. They are finding it very compelling primarily because AI is to simply changing their business entirely.\n\nSmart wallet is going to be another area of interest for the banks all over the world, the banks will provide smart wallets to its customers, AI enabled smart wallets will look at customer\u2019s spending habits and it will learn from his/her behavior to provide smart advises and recommendations of future spending. It will encourage savings and responsible spending on their credit & debit cards in the form of predictive alerts and recommendations. Similarly AI can detect if a customer is likely to switch their products or services, this early signal will help banks to offer him/her more suitable product which may help retaining the customer.\n\nNow coming to the Risk Assessment, Credit assessment and Regulatory areas, today if you want to apply for a loan, in a conventional way a home loan or a personal loan takes couple of weeks\u2019 time or may be even more to clear all kinds of credit checks before approving such loans. With Artificial Intelligence the processing lead time will come down to an hour or max two. This is because AI\u2019s capability to do credit assessment in much faster and better way by interrogating various customer data sources.\n\nWe are already experiencing the change in the interface the banks having with their customers. They are increasingly changing those to Chatbots, Robots and Humanoids as their first line of interfaces with their customers to enhance service experiences. We are seeing the similar trends in the Banks in India too.\n\nState Bank of India (SBI) uses IBM Watson in some of their products like SBI Intouch. ICICI bank has leveraged AI for facial and voice recognition in few of its products. HDFC Bank, Yes Bank, Axis Bank, DBS Bank, and few others are ready with their AI powered Chatbots and Virtual Assistance interfaces. Citi Union bank has launched its humanoid robot \u201cLakshmi\u201d which can chat with its customers like a human agent. And many other banks has started floating their RFPs to their services providers for different AI related projects realizing the fact that AI is going to be the front-line troop for them to compete with their competition.\n\nAlso there are lots of AI applications coming up into areas like Anti-Money Laundering and Regulations because it\u2019s very easy for an AI system to analyses lot of data at its finger tips and determine the patterns and better identify frauds, money laundering and criminal activities quickly and highlight those to the bank authorities well in advance so that immediate actions can be taken to arrest those activities.\n\nOther areas of AI booming in the financial sector are using facial stress analysis to automatically detect ATM frauds. AI financial advisers called Robo Advisers work proactively for its customers providing intelligence 24x7.\n\nIn the next article I will discuss more on the areas of Banking & Financial where adoption of AI is not that great till now but having a huge potential. Along with that I will also discuss the downside of AI in some of the areas of BFSI and what measures can be taken to counterbalance those."
    },
    {
        "url": "https://becominghuman.ai/my-solution-to-achieve-top-1-in-a-novel-data-science-nlp-competition-db8db2ee356a",
        "title": "My solution to achieve top 1% in a novel Data Science NLP Competition",
        "text": "This is a documentation of my First Kaggle Competition! Well first let me introduce myself, I\u2019m a Bachelor\u2019s Degree Holder in Electronics Engineering majored in Telco. Started my journey into AI after entering corporate work as a developer, took courses in Udacity on Machine learning and AI and developed AI based computer vision solutions prototypes for my company.\n\nSo after all that, I have a big urge to find some good AI problems that I can build a solution for and at the same time build up more knowledge in this niche field. That\u2019s when I found Kaggle, and I must say the kind of challenges and problems that its platform provides is no doubt broad and wide in terms of its scope and definitely suitable for any field of data science and AI. FYI, Kaggle is a platform that host AI/Machine Learning Competitions where the competitions that are hosted are real actual problems that some real companies out there want\u2019s to solve and thus collaborated with Kaggle to host a competition to solve the problem, and of course with a competition comes with a huge prize money(well its huge for me).\n\nSo back to the title, in short Kaggle hosted a competition with the Conversation AI team which is a research initiative founded by Google Jigsaw (A part of Alphabet) to work on tools to help improve online conversation, which the area of focus here is the study of negative online behaviors . To be specific they challenged competitors to build AI models to perform multi-label negative comment classification. Basically classifying a text comment on whether they are rude, disrespectful, hateful and so on. The most frequent severe toxic word that appear in sentences is as displayed above if you notice(Attention:it is really TOXIC!)\u2026.\u2026The competition is graded by the ROC-AUC metric which scikit-learn library has a convenient function for it(The highest score wins!). 4551 teams participated and I achieved top 1% on both Public Leader board scores and Private Leader board scores(Well I dropped 23 places as shown\u2026. Luckily its still Top 1%!).\n\nBy loading the csv formatted training dataset with python, jupyter notebook and pandas , it looks like this.\n\nTo process the text comments naturally we have to convert the words to a representation that can be processed by machine learning and deep learning models. Three main methods of representation I have used is word level TF-IDF, raw char level label encoding, and low-dimensional word vector representations. Let me show them one by one.\n\nTF-IDF \u2014 It is an algorithm used to weigh a word in any content and assign the importance to that word based on the number of times it appears in the document. More importantly, it checks how relevant the keyword is throughout the dataset(In this context would be the combination of test and train comments), which is referred to as corpus. Specifically it is called Term Frequency - Inverse Document Frequency. Full description here. Scikit-learn library has a convenient function to do just this >> Link\n\nRaw Character-level Label Encoding \u2014 Encoding all characters and symbols in regards to a specific order of all possible characters , in doesn\u2019t matter in what order it is but it needs to be consistent throughout test and train dataset. For example \u201cA\u201d will be turn to 0, \u201cC\u201d will be 2 and so one. Below is the encoding order I have used.\n\nWord Vector Representations \u2014 This method is used to represent a word with a low-dimensional vector (e.g. 100 dimensions). The dimensions are usually latent. GloVe vectors and FastText vectors by Facebook , both of them are used interchangeably and also pre-trained with different number of dimensions(200,300) with different Datasets which consist of Common Crawl , Wiki, and Twitter Dataset. Now initially I thought that these models were called word2vec models but on the contrary these are not. This link helps clear the doubt. GLOVE learns by constructing a co-occurrence matrix (words X context) that basically count how frequently a word appears in a context. Since it\u2019s going to be a gigantic matrix, we factorize this matrix to achieve a lower-dimension representation. FastText models is used because of it uses n-gram characters as the smallest unit thus generate better word embeddings(array of numbers with predefined dimensions) for rare words, or even words not seen during training because the n-gram character vectors are shared with other words. About Word2Vec the main idea behind it is that you train a model on the context on each word, so similar words will have similar numerical representations, because of time limitations and good performance with both FastText and Glove embeddings, I did not try this.\n\nSo the input to my models would be just text represented in numbers, what preprocess can I do? Well, because of the nature of this competition which contains actual people online comments in the real world, the text received may not necessarily be in the best form of English in terms of grammar, typos and also weird symbols. Heck it may not even be in English, Exploratory Data Analysis(EDA) before actually building your preprocessing methods matters a lot in such a competition, for example one example preprocessing method would be to translate the foreign language back to English, but to do that we need to know it even existed in the data! and also the type of foreign language it represents(japanese, russian etc). EDA goes a long way. Conveniently there are definitely many examples of EDA that will be displayed on every single competition from competitors(Why people do that in a competition? Because you can get credits for it on Kaggle on comments and on sharing codes called kernals).\n\nSo I\u2019ll share what preprocessing I did for the final submission, based on EDA, I found out that there are few comments that really is not English and well, what I did is I used a fellow Kaggler shared method to first translate all the text into for say russian using Google API using textblob library, and then proceed to translate everything back to English. This method will do two things, first it will augment the data, and second it will provide quality data to the models rather than just inputting words that will not be recognizable by the glove and fasttext vectors. The rest of the preprocessing methods I will list down below(some of them copied from generous kernals):\n\nAnother last step which can be considered as preprocessing as well is the tokenization of words. Tokenization means chopping up the sequence of sentences into separate parts where the method on how you want to tokenize the sentence can be customized, for me I have used Keras built-in tokenizer functions to separate each and every word encoding the words based on the train dataset and also the train+test dataset(Used interchangebly), basically similar to the character level encoding described above but now in word level, and return also the array of encoded sequences all in one library. I also did padding of the sequences for each example to a fixed length because deep learning methods cannot accept sparse features. Eg. if the fixed length is 250 and the sequence is 190 in length, add 60 zeros or negative one\u2019s at the end. Looks something like this:\n\nThat sums up the representation methods I have used! Note all of the methods described above contributed in some way to my final model, more about that in a bit ;)\n\nIn my final solution submission I have used ensemble learning methods to stack, bag and boost multiple models which all in all consist of around 40 models that contributed to one final model.\n\nSo I guess that explains everything on the notion on the term interchangeably used above, basically ensemble is a method that can allow us to combine different \u201cweak\u201d models and output a final model that would be better than a single best performing model. Ensemble methods is based on the notion that rather than getting one good plumber that can cover 80% of the job, get 3 plumbers with diverse skillset(features or in the case of this competition, predictions) that each can fulfill 30% of the job to finally get 90%. Makes sense? An intense description on ensemble can be found HERE .\n\nIn short get as much diverse models as possible to win the game. I will summarise a routine step I did to achieve the models.\n\nI\u2019ll proceed to list down the types of models that I have used(Roughly):\n\nMost of the implementations I used is coded using Keras with tensorflow as backend. In addition to this, from a generous share from a fellow Kaggler I found a very interesting way to ensemble models together by iterating combinations of different models, specifically hill climbing by replacements by this paper. The exact ensemble architecture I used is shown below\n\nEssentially Blending is just a glorified way of saying weighted averaging, the weights I used for blending in the end sadly is only determined by probing the public leader board score, the best way to set the weights without overfitting I believe would be to based on the OOF validation performance which is also the same reason why I added Hill Climbing ensemble to the blend.I lost track on the type of models I have been using initially into the competition as its my first try so I messed up naming the models prediction files. Will start performing systematic ensembling next competition! As out of fold stacking can only be done with the same set data and folds(can be different preprocessing) to prevent data leakage separate stacking needs to be done. Some of the individual models in the final blend is also from some of the models in the 28 model stacker, most of it is included in the blend because I didn\u2019t plan to do stacking in the first place so I did not produce Out Of Fold(OOF) meta-features to train on(But I want to use it so BLEND!)\n\nLast but not least here is a plot of my performance gain on Public LB for my 138 submissions throughout the whole competition. Seems to be a constant boost over the two months!\n\nHurray to my first Silver Medal!Cheers to more Kaggle Competitions!\n\nAnd oh link to my code base can be seen HERE"
    },
    {
        "url": "https://becominghuman.ai/ancestral-intelligence-with-granny-theano-9aee369c2101",
        "title": "Ancestral Intelligence with Granny Theano \u2013",
        "text": "The Theano library is supposedly named after the wife of the ancient Greek philosopher / mathematician, Pythagoras and Theano has really lived up to that reputation (ancient, just as Pythagoras himself).\n\nWe (Team Theano of AI Saturdays(AI6)-Lagos, Nigeria), nonetheless decided to explore the functionality, use and implementation of Theano library for Machine Learning and Deep Learning practices (yeah, because we are smart like that but more importantly, because having a firm understanding of how to mathematically model a given business problem is essential for Artificial Intelligence) and below is our experience.\n\nAs we all know, the first step to attacking a Deep Learning library is its documentation. Luckily, Theano documentation has a pile of information in its documentation page useful for any Theano beginner.\n\nWe started studying Theano\u2019s documentation and it was really tedious and boring. At page 50/644 of the documentation, we asked ourselves \u201cwhat the heck are we doing with this library?\u201d.\n\nHere is the link if you want to read https://media.readthedocs.org/pdf/theano/latest/theano.pdf (knock yourselves out).\n\nThe documentation made rumours of Theano being only for academia clear. It ensures that its users have a strong mathematical thought process.\n\nTheano is a combination of the best features of Numpy and Sci-Py kits. It was built to understand the machine level process to implementing AI algorithms such that a typical Theano compiler can produce C and C++ code. Little wonder its name is associated with the Greek mathematician.\n\nThese are the basic requirements to have Theano installed on your machine.\n\n* The development package (python-dev or python-devel on most Linux distributions) is recommended.\n\n* Python 2.4 was supported up to and including the release of Theano 0.6.\n\n* Python 2.6 was supported up to and including the release of Theano 0.8.2.\n\n* Python 3.3 was supported up to and including release 0.9.\n\nWe had enough of the documentation and were set to get our hands dirty and also after successfully installing the library on our machine, we went ahead to try out some of its functions such as its data types, shared variables, gradient calculation, updates, theano.function. One funny thing about Theano is its numerous data types. This is actually interesting, but we seemed not to find its use in application.\n\nTo the main activity, we tried out the \u201chello world\u2019 of image recognition; the MINST dataset. We implemented a logistic regression model on Theano and trained it using the MINST dataset. Logistic Regression classified the images by presenting data points onto a set of hyperplanes, using the distance to determine a class membership probability and it was really nice to see old granny Theano working.\n\nWith a built-up morale, we went ahead to implement a simple neural network using the LeNet architecture as described in its documentation and the library worked fine without errors. We got an accuracy of 87%.\n\nBelow is the link to the source code. (Sure be kind to drop comments on how to better this)\n\nAt this point, we felt we already had Theano in our palms. We confidently left the pond and headed for the ocean in search of a wild fish. We decided to implement VGG16 on Theano.\n\n3 hours (sleepless night) into extensive research on how to incorporate transfer learning for image classification on Theano, we discovered we had become web archaeologists. There were no pre-trained models on the internet that we could use for this practice.\n\nStill left with some morale to spend, we decided to build a VGG16 model from scratch (how awesome is that?). The coding was quite easy and we expected it to work fine. But instead, the results were cryptic errors which had no pointer to its cause.\n\nWe tried to debug the error for hours until we finally gave up on Theano as a whole.\n\nHere\u2019s the link to our presentation:"
    },
    {
        "url": "https://becominghuman.ai/part-2-feeding-the-machines-humans-experience-with-artificial-intelligence-1b040ef7ba8",
        "title": "Part 2 \u2014 Feeding the Machines: Humans\u2019 Experience with Artificial Intelligence",
        "text": "In Part 1 of this series we looked back at the good old days of computing when we knew that machines were in the driver\u2019s seat. It was all we could do to get them running and keep them running. Think of a coal fired locomotive as the technology and the poor humans shoveling in coal to feed the beast.\n\nNot fun. Not human centered. Not going to have to deal with that very long because\u2026\n\nI know that it\u2019s bad, but I really love Taco Bell tacos. Not all the time, but every once in a while. While this may not say much about the quality of my decisions, it does say something for the mighty taco.\n\nTaco Bell tacos are digital entities. The standardization process is so entrenched that variation is negligible. There is either taco or no taco. Binary. One or zero.\n\nAnd so too became technology in the era of the dumb phone. You could text (T9!), send grainy photos, check an email or two, and use the Internet\u2026 sort of.\n\nOne instant you\u2019re just an analog human relegated to just talking to other humans, and in an instant you go digital. Text or no text. Photo or no photo. Simple human nuance slips to the back of your experience as you remove the parts of yourself that counter-facilitate the binary magic in your hand. Your precious.\n\nWhen you get a new phone, the experience stays roughly the same. Maybe a larger screen or a full keyboard, but not much changed for a while. Humans lived dual lives \u2014 the digital life and the analog life.\n\nEnter the smartphone. The big bang of technology. With wildfire pace, each of us is now connected to everyone and everything else. All the time. Day and night. By default.\n\nWithout even asking our smartphones ping and buzz to remind us of everything that wants our attention, that deserves our attention.\n\nExcept most things do not deserve our attention. Most things are inconsequential. And even important things are rarely immediately pressing.\n\nWe traded part of our human condition for hyper-connection and depersonalized convenience. Instead of going out with friends, we order pizza at home. Bouncing from social media to text message to watching when our pizza comes out of the oven. We are at once connected yet alone.\n\nThis sense of disconnection is a contemporary topic of depression and suicide research among teens and young adults. I\u2019ll go into that further in a coming discussion, so take my word for it now or search on \u201cdepression and screen time\u201d or check out this article (https://psychcentral.com/news/2017/11/15/more-screen-time-tied-to-depression-suicide-behaviors-in-teens/128771.html)\n\nHow and how often we engage with pervasive technology can literally kill us. That does not sound like a system designed to support human needs with technological tools.\n\nAn invisible hand silently shapes our interactions and exposures. Marketers knows where we shop, when we move, if we have dogs or children. Big Data scours the digital landscape for personal actions and information. Most of the information we give away for free or without the fear of consequence through browsers, shopping, and social media.\n\nSo technology and connection are everywhere. It helps us but also harasses and endangers us as humans simply trying to be human. Humans are inescapably human. It\u2019s part of the deal,\n\nWe need technological tools that fervently support our humanness. Otherwise we will spend more time connected but alone, ordering in."
    },
    {
        "url": "https://becominghuman.ai/part-1-feeding-the-machines-humans-experience-with-artificial-intelligence-78afaa0b90b6",
        "title": "Part 1 \u2014 Feeding the Machines: Humans\u2019 Experience with Artificial Intelligence",
        "text": "Nothing excites me more than a world full of robots and algorithms running things! Wait, did I say excites? I meant scares. Yes, nothing scares me more that a world full of robots and algorithms running things.\n\nArtificial Intelligence is already here and it\u2019s going to burn brightly for the foreseeable future. We can\u2019t ignore it but we can prepare and update our point of view so we don\u2019t get lost along the way.\n\nAre you human? Me too! So let\u2019s use something we humans all do: eat. Behold, our food analogy illustrating the changing relationship between humans and technology:\n\n- Feeding the machines\n\n- TV dinner at home\n\n- Drive through dining\n\n- Let\u2019s order in\n\n- Hey Siri, feed me\n\nRemember those old green screens and floppy disks? Remember when 100MB was HUGE? What about that sweet AOL deal of 1000 free hours of dial up!? Ah, the good old days. Or not. These were the dark days of computing and connectivity. Drivers, disks, and weird modem noises that scared the dog.\n\nSimply put, we were feeding the machines. Whether it was code on the command line, or startup disks, or hours and hours reformatting and reinstalling \u2014 we were the cooks and those machines were picky eaters and very, very hungry. Not at all unlike an ornery toddler and just as unmanageable and frustrating.\n\nDon\u2019t jinx it, but I haven\u2019t had to reboot my PC in over 4 hours and I feel like a digital racer with this new \u201cDSL\u201d thing and the iMac. Seriously, this is heaven. What could possibly make this better?\n\nI heard that there are these new things called Gigabytes. Doubt those will catch on."
    },
    {
        "url": "https://becominghuman.ai/deep-learning-book-notes-chapter-2-linear-algebra-for-deep-learning-af776cf52506",
        "title": "Deep Learning Book Notes, Chapter 2: Linear Algebra for Deep Learning",
        "text": "These are my notes for chapter 2 of the Deep Learning book. They can also serve as a quick intro to linear algebra for deep learning.\n\nMy notes for chapter 1 can be found below:\n\nFor this section I decided to make things a bit more intuitive using code, which should be appealing to the many of us who are coders first and math people second (or eighth). This means that my \u201cnotes\u201d are probably on the same order of length as the chapter itself, but hopefully they are extra useful for people out there (and for myself, frankly).\n\nThe code I am including will be in the form of images because uploading chunks of notebooks to Medium is quite hard. To see the full notebook itself, go here.\n\nAll of these can be added to other objects of the same size (this is done elementwise) and multiplied by scalars (each element is multiplied by the scalar). The book permits \u201cbroadcasting\u201d, in which an object of lower dimension can be added to an object of greater dimension (eg a matrix + a vector), in this case, for example, the vector is added to each row of the matrix.\n\nUnlike to the authors, I like to think of the dot product first and of matrix multiplication second, so let\u2019s define the dot product first:\n\nThe dot product of two vectors that we can call a and b can be obtained by first multiplying the each element of a by the corresponding element of b and then by taking the sum of the result. In mathematical notation, it becomes (if nn is the number of elements in both a and b):\n\nIn python, we can either implement it directly by using multiplication and sum, or simply use the function.\n\nTo multiply matrix A and B into a matrix C, we simply make it so that each entry Ci,j is the dot product of row i of A and column j of B, so we have, for all possible i and j:\n\nThis means that the length of every row of A must be the same as the length of every column of B, in other words that the number of columns of A is equal to the number of rows in B. C will have the same number of rows as A and the same number of columns as B. In other words, if A is of size m x n and B is of size n x k the multiplication is valid and C is of size m x k.\n\nOften, A and B will both be \u201csquare\u201d (meaning they have the same number of rows and columns), in which case C will have the same size as both of them.\n\nBecause matrix multiplication is so similar to the dot product, it is implemented using the function in numpy as well.\n\nIn fact, you could even say that the vector dot product is the same as the multiplication of a matrix with only one row with a matrix with only one column, and indeed this is why the notation used by the book for the vector dot product is not a\u22c5b but aTb.\n\nThis seems to tell us everything we need to know about matrix multiplication, but there are actually a couple other important things to note before we move on.\n\nFirst, we viewed matrix multiplication on an entry-by-entry basis, and this is how the Deep Learning book (and most everybody else) presents it. However, it is sometimes useful to see matrix multiplication as involving either entire columns or entire rows! I will not go into details here, but Gilbert Strang explains this concept in his MIT OCW course, you can see his full explanation of matrix multiplication including the \u201centire columns\u201d and \u201centire rows\u201d aspects here:\n\nSecondly, since we\u2019ve already talked about transposes, it is interesting to ask what the transpose of a multiplication is. As it turns out, transposing the result of a matrix multiplication is the same as multiplying the original transposed matrices in reverse order, or, in math:\n\nFinally, note that there is a huge difference between matrix multiplication and element-wisemultiplication of two matrices. In the element-wise product, we simply multiply each element in one matrix by the corresponding element in the other, whereas in matrix multiplication, we use the dot product of a row and a vector.\n\nIn numpy, matrix multiplication is implemented using the function whereas you can perform element-wise multiplication using the operator. In the book's math notation, element-wise multiplication is represented by \u2299, though other operators are used by others. Matrix multiplication can just be represented by stringing the matrices together.\n\nThe matrix I has a very important property: multiplying it with any other matrix does nothing at all! It is simply a matrix with 0s everywhere and 1s on the diagonal going from the upper left to the lower right. In python, it is always possible to create an identity matrix of a given size using the function (I -> eye, get it?).\n\nI is basically the equivalent of 1 in the matrix world (since multiplying by 1 does nothing to ordinary numbers).\n\nMost (but not all) square matrices have a special matrix called an \u201cinverse\u201d, represented with a -1 superscript like so: A\u22121A\u22121. The inverse of a matrix functions very much like the inverse of a number, in that multiplying a matrix by its inverse gives the identity function (just like multiplying a number by its inverse gives 1). Basically, multiplying by the inverse of a matrix is very similar to dividing by that matrix. So:\n\nYou can get the inverse of a matrix in numpy using .\n\nInverses can help us solve equations using matrices! Suppose we have a the following equation:\n\nSo a matrix A multiplies an unknown vector x to get a known vector b, what is the x that makes this possible?\n\nUsing the inverse we get:\n\nAnd we can now compute x!\n\nNot all matrices have inverses! Next we will see why.\n\nLet\u2019s explore why our bad matrix couldn\u2019t be inverted before. For this, the easiest way is to introduce a geometrical way of thinking about matrices and vectors. I highly recommend watching 3Blue1Brown\u2019s videos (see resources above) to gain full intuition on this.\n\nYou can think of any vector as a point in space. With a 2D vector this is simple, the first component is its location on the X axis and the second on the Y axis. With 3D things are still relatively familiar, just add a Z axis. With more dimensions, the vector is still a point in space, even though you can\u2019t imagine it.\n\nA matrix multiplication takes a point in space and moves it to another point in space, that\u2019s all it does.\n\nViewing this this way, we can say that a matrix inverse takes a point that (conceptually at least) has been moved by the original matrix and finds the original location of that point (before it was moved).\n\nThe problem is: do all matrices map every point to a single other point? What if a matrix map several source points to the same destination point? What if it never maps any point to a given destination? In either case, we can\u2019t have an inverse!\n\nIn our non-invertible matrix, both of these problems occur! There are at least two different vectors x that map to the point [0, 0], as we show in code below, and yet there is no way to get to the point [1, 1] (try it!)\n\nTo understand this, let\u2019s visualize the way the two matrices transform different points. We\u2019ll represent the points using a grid:\n\nSo it turns out that our bad matrix projects the entire grid onto a line, whereas A just tilts things around a little bit. This means that any point outside of the line cannot have been created by bad matrix, and thus an inverse cannot exist! (and as we saw before, several points in 2D can give us the same point on the line, which is also a problem).\n\nThis line is the span of our bad matrix (the set of all values it can map to). By contrast, the span of A is the entire plane, as can be guessed from the fact that it\u2019s only slightly shearing the grid.\n\nAs it turns out, matrices can only ever span linear spaces such as points, lines, planes and hyperplanes (a plane in more than 2 dimensions). Further, all these spaces always have to contain the point at the origin, since multiplying any matrix by the 0 vector always gives the 0 vector. Only matrices that span the entire space they are in have an inverse.\n\nIf we look more closely at our bad matrix, we notice something strange about its columns: the second column ([2, 4]) is exactly twice the first column ([1, 2])! As it turns out, this is exactly why our matrix doesn\u2019t span the whole space!\n\nMultiplying a matrix and a vector can be thought of combining the columns of the matrix based on the elements of the vector, so if I multiply a matrix M by the vector [1, 2, 3, 4], the final vector is 1 times the first column of M plus 2 times the second column of M and so on. So whenever we multiply our bad matrix with a vector, the result can only ever be a multiple of the vector [1, 2], which indeed forms a line!\n\nA set of vectors is called \u201cdependent\u201d if it is possible to generate one of the vectors by multiplying and adding some of the other vectors (in our case, just multiplying). If the columns of a matrix are dependent, the matrix doesn\u2019t span the whole space and can\u2019t be inverted.\n\nAlso note that if the columns of a matrix are dependent, its rows are also dependent, and vice-versa. We won\u2019t prove this here.\n\nNorms are a measure of the length of a vector. The most common types of norms are called the Lp norms, and they are of the form:\n\nThe most common Lp norms are the L1, L2 and L\u221e norms, which you might already know under the names of Mahnattan distance (the distance to go from the origin to the tip of the vector, if you can only move along an axis), Euclidian distance (the distance to go from the origin to the tip if you can go in any direction you want) and maximum (of the absolute values), respectively.\n\nL\u221e might seem like a weird name, but it is actually simply what happens as p reaches infinity.\n\nYou can access the norms using .\n\nFinally note that we can measure a matrix using the same norms, but that sometimes people call norms on matrices differently! In particular, the Frobenius norm is simply the L2 norm applied to a matrix. Remember to use that word if you want to sound smart.\n\nThe word \u201ceigen\u201d can seem scary, so I like to mentally replace it with \u201cspecial\u201d (that\u2019s not what the word actually means in German, but it\u2019s good enough for our purposes). So whenever I write \u201ceigenvector\u201d or \u201ceigenvalue\u201d, replace it with \u201cspecial vector\u201d and \u201cspecial value\u201d.\n\nSo how are these vectors and values special? The eigenvectors are special because they only get stretched when they are multiplied by the matrix (ie their direction doesn\u2019t change, only their length, and they might also be going \u201cbackwards\u201d if they are stretched by a negative amount). The eigenvalues is the amount by which the vectors are stretched.\n\nEigen-stuff is accessible in numpy through . Let's look at what they do. Below the black vector is always the original one and the red vector is the transformed one.\n\nHere we see that the first (arbitrary) vector is not just stretched, but also slightly rotated by the matrix, so it clearly isn\u2019t an eigenvector. We compute the eigenvectors and show how they get transformed and indeed they do not change direction! We also see that in the first case the transformed vector is much longer than the original vector, and in the second case it is much shorter, so the amount of stretching is per-vector.\n\nNow the two important questions are:\n\nThe answer for the first question is\u2026 sorta. Given the definition we have given so far, the answer should be an emphatic \u201cno\u201d, because there exist rotation matrices, which always rotate a given vector. Since we have defined eigenvectors as \u201cvectors that don\u2019t get rotated when they are multiplied with the matrix\u201d and since rotation matrices rotate every vector, they should not have any eigenvectors. However, if you call on a rotation matrix such as [[0, -1], [1, 0]] (which rotates vectors by 90 degrees), you will find that it won't error out! The reason is that if you use complex eigenvalues and eigenvectors, you can find eigen-stuff for every possible matrix.\n\nHaving to deal with complex numbers is not very convenient, of course. Thankfully, many of the matrices we\u2019ll encounter in practice will have real eigenvalues and eigenvectors.\n\nNow as to what the point of all this is. It so happens that we can decompose a matrix using its eigenvectors. Specifically, if we have a matrix A, we can put all its eigenvectors in the columns of a matrix V and all its eigenvalues along the diagonal of a diagonal matrix \u039b and we find that:\n\nAn important way in which this is useful is that it allows us to multiply a matrix with itself repeatedly very efficiently: when we multiply A with itself, the V^{\u22121} of the leftmost decomposition cancels out the V of the rightmost decomposition, and we end up with:\n\nBecause \u039b is a diagonal matrix, taking it to a power simply involves taking each of its elements to that power, which is much faster than doing a lot of matrix multiplications.\n\nFinally we should note that for all symmetric matrices, the eigenvector matrix is orthogonal, which means that its transpose is its own inverse, so we get:\n\nIf A is symmetric, which is particularly useful.\n\nIn the previous section we found that eigenvalues were convenient but sadly didn\u2019t apply to all matrices: specifically, for some real square matrices they required the use of complex numbers, and they of course don\u2019t work at all for non-square matrices.\n\nSingular value decomposition (SVD) tries to solve this problem by providing a decomposition with two orthogonal matrices (U and V) and one diagonal matrix (D), such that:\n\nThe book does not go into many details about the uses of SVD (though there are many) except for finding the Moore-Penrose pseudoinverse.\n\nSVD can be accessed using .\n\nThe Moore-Penrose pseudoinverse can be computed using SVD. How doesn\u2019t matter too much here, the point is what it can do: basically it can find the an \u201cinverse\u201d for non-invertible matrices. Of course, because they are not invertible, the \u201cinverse\u201d will lack some properties, but it will still be quite useful.\n\nAs you might recal, the inverse was useful for solving equations like:\n\nIn which case you could find x using:\n\nWith the non-invertible, there might not be an x that satisfies Ax=b, but the pseudoinverse can find the x that comes closest (by minimizing the L2 distance between Ax and b).\n\nThe Moore-Penrose pseudoinverse is accessible by using .\n\nThe trace is simply an operator that sums the entries along the diagonals of a matrix. It has some interesting properties (eg the trace of a transpose is the same as the original trace and the order of matrix multiplication doesn\u2019t matter within the trace operator), and its main use is to simplify some math by getting rid of explicit summing in equations in some cases.\n\nIt is accessible as np.trace\n\nThe determinant is explained very quickly in the book, although it has many interesting properties. Suffice it to say that it is a single number that describes a matrix, it has the following properties:\n\nFinally, the chapter ends on a derivation of PCA with a single component. It is a cool example of derivation but I won\u2019t go through it here since it doesn\u2019t really introduce new material, just shows how to use what we saw above.\n\nThat\u2019s it for this week\u2019s notes. I think I\u2019ll keep translating the book\u2019s concepts into python code, it enlivens things a bit and makes them more concrete, but maybe next time\u2019s notes will be shorter: this was a lot of work!"
    },
    {
        "url": "https://becominghuman.ai/a-peek-into-genetic-algorithms-6cf2d9a01b1a",
        "title": "A peek into Genetic Algorithms \u2013",
        "text": "Last week we had a class on genetic algorithms.I had heard that these class of Evolutionary algorithms help in deciding or optimising parameters of ANNs,but I had never really paid attention to the logic or the math behind them.So as the class proceeded I found the idea of these algorithms to be very different from the machine learning algorithms I had coded so far ,where you have train-dev-test sets and you perform the task of function approximation on the same.So I wen\u2019t through the corresponding chapter in Tom Mitchell and surfed the net for some interesting bits & blobs.This article is a result of that.\n\nEvolutionary algorithms are a set of generic meta-heuristic algorithms\n\nwhich solve optimisation problems by imitating aspects of biological evolution.Genetic algorithms are a subset of Evolutionary algorithms inspired by Charles Darwin's work on evolution by natural selection.They are meta heuristic search algorithms relying on bio-inspired operators such as mutation,crossover and selection.Genetic algorithms in particular became popular through the work of John Holland in the early 1970s, and particularly his book Adaptation in Natural and Artificial Systems (1975).\n\nThe above picture depicts the process of Meiosis and Mitosis where in the former you have crossovers of chromosomes forming new genotypes and the latter involves copying of genetic information to new offspring.The other biological phenomenon of mutation can be thought of as errors during the copying process.Down the lane we can observe that the mutation,crossover and selection of encoded bit strings of features of the data are analogous to these biological operations to a certain extent.\n\nGA is also referred to as Generate-Test Beam & Search Algorithm.To explain this my professor told us to imagine it as searching for an object in a darkroom by throwing a beam of light randomly with the hope of hitting the object.We need to understand some basic Jargon before proceeding in the same:-\n\n1.Hypotheses(h)- With respect to GA they are usually fixed length bit strings or symbolic expressions and in case of genetic programming they are computer programs.Bit strings are preferred for easier manipulation during crossover and mutation.\n\nFeature Representation-Lets assume that we are given a constrained equation- a+2b+3c=14 where a,b,c>0 and we have to find value of a,b & c.For this the features are variables a,b,c.We can clearly see that the max value that any of them can take is 14 which can be represented by 4 bits.So we can represent hypotheses of the population as a binary string with fixed length of 4 bit per feature(total length=12 bits) .Let a hypothesis have a=1,b=2,c=2.\n\nThe bit string of the hypothesis is- 0001 0010 0010\n\n2.Population(P)- It is a set of initial Hypotheses which undergo operations every generation to give rise to a probably better(evolved) population.The size of population is constant every generation.\n\n3.Generation- It refers to the number of iterations of the algorithm(Stages of population modification).Ex-1st iter results in the formation of 1st generation.\n\n4.Selection-It is the process of probabilistically selecting n(1-r) members out of a population(p) whose size is n to be added to new population(P\u2019) whose size is also n .Probability of selecting a hypotheses is given by-\n\n5.Fitness function-It is a criterion used to decide selection of hypothesis for further generations(or operations).For example in case of solving linear equations the fitness function can be one that gives absolute difference from the value(see code).\n\n6.Crossover(Analogous to meiosis operation)-Probabilistically selecting r*n/2 pairs of hypotheses from P according to equation-1 and then applying crossover operator to generate 2 offspring per pair and adding them to P\u2019.Which bits are to be selected from which hypothesis is given by the crossover mask.There are multiple types of crossover operators namely-\n\nThus they are better used as a last resort and mainly for optimising parameters and topology .I tried coding an example of using GA to solve linear equations.Link- https://github.com/yagamiash/MLnDL/blob/master/Genetic_Algorithm.py"
    },
    {
        "url": "https://becominghuman.ai/paper-repro-self-normalizing-neural-networks-84d7df676902",
        "title": "Paper repro: \u201cSelf-Normalizing Neural Networks\u201d \u2013",
        "text": "This post details my experience reproducing \u201cSelf-Normalizing Neural Networks\u201d as part of the Nurture.AI NIPS Challenge 2017, as well as my experience participating in the challenge. My notebook can be found here.\n\nThe goal of the NIPS challenge 2017 was to provide code implementation for various NIPS papers as well as to encourage the reproducibility of NIPS papers.\n\nThe first paper that I was planning to reproduce was \u201cBridging the Gap Between Value and Policy Based Reinforcement Learning\u201d. Sadly I quickly found that I couldn\u2019t figure out a lot of the specifics of the experiments that were run in the paper, which made it hard to know where to start in terms of reproducing them.\n\nI thus switched to Self-Normalizing Neural Networks (SNN), for the following reasons:\n\nWhat I did not anticipate, however, was the huge amount of computational resources necessary to comprehensively reproduce the experiments. By January 31st (Nurture.AI\u2019s deadline), I had reproduced the paper\u2019s main set of experiments (the one on UCI), but I had barely started reproducing the second experiment (on Tox21). This was definitely sufficient to submit to Nurture.AI, but I decided to try to finish things up completely. In the end, the computation for Tox21 took weeks and led to disappointing results, so I decided I might as well wrap things up!\n\nSNNs are already implemented in PyTorch, you simply need to use nn.SELU as your activation and replace all your dropout with nn.AlphaDropout and\u2026 TADA! you have an SNN.\n\nBecause of the goal of reproducing a paper, however, I of course needed to to implement both of these myself, which wasn\u2019t so difficult.\n\nFirst, the selu (Scaled Exponential Linear Unit) function. Selu is a function that is similar to a scaled leaky relu, but with a simple exponential function as its negative part. It is parameterized by \u03b1 and \u03bb, which control the mean and variance of the output distribution. Because we generally want a distribution with mean 0 and variance 1, \u03b1 needs to be around 1.67326 and \u03bb needs to be around 1.0507 (why, you ask? Because math! The paper explains it in details but it\u2019s complicated).\n\nIn the end, we get a function whose graph looks like this:\n\nAnd we can implement it like so:\n\nBy simply using this activation function, your neural network will train in such a way that the output of each layer will be roughly normally distributed with a mean of 0 and a variance of 1, as long as the input themselves are distributed in that way (so it\u2019s important to normalize your inputs).\n\nNow we come to dropout. The problem with ordinary dropout interacting with SNNs is twofold:\n\nSo in short, we need a version of dropout which does two things:\n\nNote that while ordinary dropout only screwed up the variance, setting things to \u03b1\u2019 instead of 0 screws up both, so we need an affine transformation here, ie one that is of the form y = ax+b.\n\nSparing you the math once again (though it is much easier to follow this time, so definitely read the paper if you want to understand), we find that we need:\n\nwhere q is the probability that a given element is not set to \u03b1\u2019. In my implementation I used p to represent 1-q.\n\nThis gets us the following implementation:\n\nInterestingly the paper finds that dropout rates that are much smaller than the ones we usually see in other systems work best, specifically 0.05 and 0.10 work well according to them (when we often see 0.25 and 0.5 in other implementations).\n\nThe SNN paper compares SNN with many other types of neural networks, especially types that involve normalization of some sort. More details can be seen in the notebook about weight norm, layer norm etc., but I thought it was useful to show my implementation of ResNet blocks for structured data, since ResNets are traditionally used for convolutional neural networks.\n\nAll the experiments done in the paper are on structured data. This is interesting because BatchNorm has been successfully used on all sorts of datasets, and yet SNNs are only marketed for structured datasets. Indeed, I later tried using selu in a convolutional neural network, and found that my results were mediocre, but this deserves more looking-into.\n\nIn the meantime, a discussion of the experiments. There are three experiments using three sets of datasets, namely:\n\nAfter struggling to figure out what parts of the data had been used as a test set and as a validation set (because many UCI datasets do not come with a test set) and realizing that it was necessary to read \u201cDo we need hundreds of classifiers to solve real world classification problems?\u201d to figure it out.\n\nFor its experiment, the SNN paper runs a very extensive grid search (the parameters of which are towards the end of the supplementary material) using the various types of neural networks under consideration (including SNNs of course) and compares them by using their average rank. The reason for using average ranks is of course that the actual accuracy will vary widely across datasets, and so certain datasets will have more importance than others simply due to their base accuracy.\n\nThe computation took a very long time (days, and I needed to spin up Amazon boxes instead of just using my dual-GPU desktop to finish on time for the Nurture.AI deadline), mainly because of how extensive the grid search is and of the fact that training on a single large (few thousand examples) dataset could take on the order of an hour: multiplied by all the different architectures and all their grid search parameters, this adds up to a lot!\n\nIn the end, we do find that SNN clearly outperform other types of networks according to this metric, both in the original paper and in my reproduction. Specifically, the original paper gets the following average ranks:\n\nAnd I get these:\n\nInterestingly weight normalization performs much better for me than for the original paper.\n\nWe can also see that my implementation got similar results to the original paper on an individual basis by looking at the detailed table of results (it is very long since it includes every dataset, I am only including the datasets starting with the letter \u2018a\u2019 here):\n\nTox21 seems like a single dataset, but in reality it is 12 different datasets since there are 12 labels and due to the fact that some of them are missing for any given entry, it has been common practice to train one model per label. Because it is fairly large both in number of examples and features, I had a problem similar to UCI in terms of computational time, and it took weeks for me to get final results on it.\n\nSadly, I found that there must have been some issue in my implementation or my interpretation of the experiment since my results differ from the original paper\u2019s, in particular I get lower performance (especially on the training set, which is why I used validation performance instead) and larger error bars on my performance (possibly the original paper aggregated more data to get the smaller error bars). Still, I get a slightly similar pattern to the SNN paper overall. Specifically, SNN works better than any other method as the networks get larger.\n\nUnlike the original paper, however, my implementation only starts getting better after the networks get bigger than 8 layers, while in the paper 3 layers was sufficient:\n\nSNN was one of my first deep learning paper reproductions, even though I published this blog later than other repros I made.\n\nThere were some important learnings in my experience with Nurture.AI, especially in terms of the factors to take into account when reproducing a paper:\n\nThe main next step I want to take with regards to this paper is figure out if SNNs can be used productively in convolutional neural networks. My initial experiment on a Kaggle competition seemed to show that they can\u2019t, but I don\u2019t understand why. This requires further exploration."
    },
    {
        "url": "https://becominghuman.ai/deep-learning-book-notes-chapter-1-b310837c76cf",
        "title": "Deep Learning Book Notes, Chapter 1 \u2013",
        "text": "These are my notes on the Deep Learning book. There are many like them but these ones are mine.\n\nThey are all based on my second reading of the various chapters, and the hope is that they will help me solidify and review the material easily. If they can help someone out there too, that\u2019s great.\n\nThe notes are also available on github.\n\nAI was initially based on finding solutions to reasoning problems (symbolic AI), which are usually difficult for humans. However, it quickly turned out that problems that seem easy for humans (such as vision) are actually much harder.\n\nYou need a lot of knowledge about the world to solve these problems, but attempts to hard code such knowledge has consistently failed so far. Instead, machine learning usually does better because it can figure out the useful knowledge for itself.\n\nGood representations are important: if your representation of the data is appropriate for the problem, it can become easy.\n\nFor example, see the figure below: in Cartesian coordinates, the problem isn\u2019t linearly separable, but in polar coordinates it is. The polar representation is more useful for this problem.\n\nUnfortunately, good representations are hard to create: eg if we are building a car detector, it would be good to have a representation for a wheel, but wheels themselves can be hard to detect, due to perspective distortions, shadows etc.!\n\nThe solution is to learn the representations as well. This is one of the great benefits of deep learning, and in fact historically some of the representations learned by deep learning algorithms in minutes have permitted better algorithms than those that researchers had spent years to fine-tune!\n\nGood representations are related to the factors of variation: these are underlying facts about the world that account for the observed data. For instance, factors of variation to explain a sample of speech could include the age, sex and accent of the speaker, as well as what words they are saying.\n\nUnfortunately, there are a lot of factors of variation for any small piece of data. How do you disentangle them? How do you figure out what they are in the first place?\n\nThe deep learning solution is to express representations in terms of simpler representations: eg a face is made up of contours and corners, which themselves are made up of edges etc.. It\u2019s representations all the way down! (well, not really).\n\nBelow is an example of the increasingly complex representations discovered by a convolutional neural network.\n\nThere is another way of thinking about deep network than as a sequence of increasingly complex representations: instead, we can simply think of it as a form of computation: each layer does some computation and stores its output in memory for the next layer to use. In this interpretation, the outputs of each layer don\u2019t need to be factors of variation, instead they can be anything computationally useful for getting the final result.\n\nHow deep a network is depends on your definition of depth. There is no universal definition of depth although in practice many people count \u201clayers\u201d as defined by a matrix multiplication followed by an activation function and maybe some normalization etc.. You could also count elementary operations in which case the matrix multiplication, activation, normalization etc. would all add to the depth individually etc.. Some networks such as ResNet (not mentioned in the book) even have a notion of \u201cblock\u201d (a ResNet block is made up of two layers), and you could count those instead as well.\n\nThe book also mentioned that yet another definition of depth is the depth of the graph by which concepts are related to each other. In this case, you could move back from complex representations to simpler representations, thus implicitly increasing the depth. Their example is that you can infer a face from, say, a left eye, and from the face infer the existence of the right eye. To be honest I don\u2019t fully understand this definition at this point. According to the book it is related to deep probabilistic models.\n\nDeep learning is not a new technology: it has just gone through many cycles of rebranding!\n\nIt was called \u201ccybernetics\u201d from the 40s to the 60s, \u201cconnectionism\u201d from the 80s to the 90s and now deep learning from 2006 to the present. The networks themselves have been called perceptrons, ADALINE (perceptron was for classification and ADALINE for regression), multilayer perceptron (MLP) and artificial neural networks. The most common names nowadays are neural networks and MLPs.\n\nA quick history of neural networks, pieced together from the book and other things that I\u2019m aware of:\n\nHere are some factors which, according to the book, helped deep learning become a dominant form of machine learning today:\n\nDeep learning models are usually not designed to be realistic brain models. Deep learning is based a more general principle of learning multiple levels of composition.\n\nWhy are we not trying to be more realistic? because we can\u2019t know enough about the brain right now! But we do know that whatever the brain is doing, it\u2019s very generic: experiments have shown that it is possible for animals to learn to \u201csee\u201d using their auditory cortex: this gives us hope that a generic learning algorithm is possible.\n\nSome aspects of neuroscience that influenced deep learning:\n\nSo far brain knowledge has mostly influenced architectures, not learning algorithms. On a personal level, this is why I\u2019m interested in metalearning, which promises to make learning more biologically plausible.\n\nNeuroscience is certainly not the only important field for deep learning, arguably more important are applied math (linear algebra, probability, information theory and numerical optimization in particular). Some deep learning researchers don\u2019t care about neuroscience at all.\n\nActual brain simulation and models for which biological plausibility is the most important thing is more the domain of computational neuroscience."
    },
    {
        "url": "https://becominghuman.ai/investigating-focal-and-dice-loss-for-the-kaggle-2018-data-science-bowl-65fb9af4f36c",
        "title": "Investigating Focal and Dice Loss for the Kaggle 2018 Data Science Bowl",
        "text": "This post details my experiments and implementations with three important loss functions for the Kaggle 2018 data science bowl, and compares their effects on a simplified implementation of U-Net.\n\nTo those unfamiliar, the 2018 Data Science Bowl is a nuclei extraction competition. It consists in finding which pixels in images such as the one below are part of a cell nucleus, and if so which nucleus (in this case, basically all the white/gray spots are nuclei).\n\nThis can thus be seen as an instance segmentation problem, although many competitors have been using semantic segmentation algorithms such as U-Net, which we will do here as well for simplicity.\n\nThe loss functions we will investigate are binary cross entropy (referred to as \u201cnll\u201d in the notebook because my initial version used the related NLLLoss instead of BCE), the soft-dice loss (introduced in \u201cV-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation\u201d and generally considered to be useful for segmentation problems), and the focal loss, the investigation of which is the main focus of this notebook.\n\nThe focal loss is described in \u201cFocal Loss for Dense Object Detection\u201d and is simply a modified version of binary cross entropy in which the loss for confidently correctly classified labels is scaled down, so that the network focuses more on incorrect and low confidence labels than on increasing its confidence in the already correct labels.\n\nIn the image below, you can see the ordinary binary cross entropy loss function as the top line and different variants of focal loss (focal loss is parameterized by \u03b3, \u03b3=0 is the same as binary cross entropy). We can clearly see that focal loss places much less importance on examples for which the confidence is high than ordinary BCE.\n\nIn addition, this notebook also investigates a claim made in \u201cFocal Loss for Dense Object Detection\u201d:\n\nBasically the paper claims that in cases where the probability of each class isn\u2019t close to 50/50, it is useful to initialize the model so that on average it will output the actual probability of each class, so that the loss isn\u2019t very large at the beginning of training.\n\nBecause the setting of the competition is different from that in the paper (eg we are looking for masks instead of bounding boxes, and the class imbalance is actually not that bad: 12% of pixels in the training set contain a cell), we definitely expect different results. The point of this notebook is to explore those differences. It would be interesting to implement something closer to what the paper describes to fully investigate the usefulness of focal loss for the data science bowl 2018.\n\nThe notebook for this can be found here. Sadly I couldn\u2019t make it a kernel on Kaggle because it uses some library that Kaggle kernels don\u2019t support, such as Augmentor.\n\nNote that some of the images in the notebook can\u2019t be displayed by github currently, but they will show if you download the notebook and open it with a recent version of Jupyter.\n\nMost current publicly available kernels on Kaggle simply apply U-Net or a similar algorithm without any preprocessing. This is problematic because U-Net can do semantic segmentation quite well, but it doesn\u2019t do instance segmentation at all.\n\nWhat this means is that U-Net can tell you if a given pixel belongs to a nucleus or not, but not whether two adjacent nucleus pixels belong to the same nucleus or not. Take for example the image below, which contains the ground truth masks for the different nuclei in a training image:\n\nAs you can see for example on the upper-right corner, some masks completely touch each other. Because our approach for finding individual cells once the pixels are labeled will be based on finding connected components, we would merge these two nuclei into a single one in our submission and thus hurt our performance.\n\nPerhaps the simplest way to fix this is to teach our network to label pixels which are inside the nucleus by at least 1 pixel. This way two distinct nuclei can\u2019t touch each other, and we can expand the masks by one pixels once the nuclei are found. For this we use the erosion operation when we load our training labels, and the dilation operation when we generate our submission.\n\nHere is the image above after erosion of the masks:\n\nAs you can see, the masks do not touch anymore.\n\nThe model is inspired by U-Net, with a few changes: the number of layers and filters is parameterizable, and batch normalization is included everywhere, likewise, there is a significant amount of dropout. Here is the full code for the model:\n\nBinary cross entropy is unsurprisingly part of pytorch, but we need to implement soft dice and focal loss. For numerical stability purposes, focal loss tries to work in log space as much as possible.\n\nMy implementation of dice loss is taken from here. Focal loss is my own implementation, though part of the code is taken from the PyTorch implementation of BCEWithLogitsLoss.\n\nImportantly, my implementation of focal loss works in log space as much as possible so as to be numerically stable. I did not do this at first and very easily got NaNs when training.\n\nBonus: an implementation of multi-label focal loss with support for class weights as well! It functions just like NLLLoss and takes its input as a log softmax and its target as a LongTensor of the classes.\n\nSadly this version is not numerically stable, unlike the binary version above. In particular I have found that it worked well with \u03b3=2 but generated lots of NaNs with \u03b3=0.5. I\u2019d be very interested in someone came up with a good numerically stable implementation of this.\n\nNote: I am omitting the data augmentation and fitting function for the sake of space, please check out the notebook if you are interested in these.\n\nNow we can test the various losses and initialization. For each loss/initialization combination, a kaggle submission file is generated. For focal loss we try 0.5, 1.0, 2.0 and 4.0 as the \u03b3 parameter.\n\nThe results are as follows:\n\nWe can see above that the different loss functions have a relatively small effect on the validation IoU in this dataset. Out of all of them, dice and focal loss with \u03b3=0.5 seem to do the best, indicating that there might be some benefit to using these unorthodox loss functions.\n\nInitialization with the prior seems to have even less effect, presumably because 0.12 is close enough to 0.5 that the training is not strongly negatively affected. In the end, in half of the cases 0.12 was better and in the other half 0.50 was, indicating that there is essentially no difference.\n\nA legitimate question to ask is what the effect is on the actual test set in the competition on Kaggle. I submitted 5 of the generated solutions and got the following leaderboard scores:\n\nSo again we see that focal loss and dice do a fair amount better than simple binary cross entropy. This time the best result actually came from focal with \u03b3=1, which is not reflected in our validation results above but definitely not surprising given the likely margins of error.\n\nThus clearly the use of a focal or dice loss is not a major factor in this competition, but it might give you a minor advantage and is thus good to have in your toolbox."
    },
    {
        "url": "https://becominghuman.ai/behavioral-cloning-in-deep-learning-using-keras-3786f5914b72",
        "title": "Drive a Car Autonomously Using Deep Learning \u2013",
        "text": "I am into my first term of Udacity\u2019s Self Driving Car Nanodegree and I want to share my experiences regarding one of my recent projects. The complete code can be found here The objective of this project is to basically apply the concepts of Deep Learning and Convolutional Neural Networks to teach the machine to drive car autonomously. How is this even possible ? Sounds Weird, Right? So Let\u2019s Get Started. First things first, it is not magic but it really feels like magic. With just a bunch of Python libraries, some lines of Code and huge amount of Data we can teach a car to drive itself. Remember I have emphasized on huge bunch of Data because Data plays the most important role in this project and it has to be in Abundance as in any other Deep Learning problem. So what type of data do we need to collect? We will capturing three images from a dashboard camera placed at slightly different positions center, left and right in addition to steering angle, throttle, brake and speed. So basically we have a 8 tuple (center image, left image, right image, steering, throttle, brake, speed). The next question is how? Well, that is pretty simple, thanks to Udacity for providing a simulator which does the required job here. The simulator is built on unity engine and all you need is to just drive around the track in the training mode. If you didn\u2019t play video games in your childhood then you are going to have a tough time collecting good training data. Make sure you record the training data once you start driving around the track. The next point is about the quality of training data to collect. If you do not drive well enough, then you car won\u2019t either in the autonomous mode. So make sure to keep your car in the center as much as possible. Your goal is to collect maximum different data that is possible. First drive 3-5 laps and record data, then drive 3-5 reverse laps on the same track. Is that enough? No! If you just teach your car to drive in the center of the track then it will never know what to do if it comes on the side path, so you need to collect recovery data for that as well. So if a car goes off track, then it should try to come back to center as quickly as possible. Once you are done you will be able to see the captured images and the corresponding data.csv file containing the mapping of all the images and tuple defined above.\n\nIn our task to drive a car itself, our main focus is just to steer the car around the curves. So basically we will be building a model that learns itself to steer to car according to the curvature of the road. So the only parameter that is important to us is the Steering Angle. The first step is to visualize the data set. As you can observe, we have too much data for the images with zero steering angle, so we need to create more data for other steering angles. And this can be done by Data Augmentation. But please make a note, we cannot apply every type of Data Augmentation. When we look at the data, we observe that we have 3 images corresponding to one steering angle and that is so because the steering angle is supposed to be captured from the center position. So does that mean our right and left images also have the same steering angle? No! Correction Factor: Infact we need to introduce a correction factor that we will add in the images taken from left dashboard camera and a correction factor that we will subtract in the images taken from right dashboard camera in order to keep our vehicle in the center. So, lets suppose our steering correction angle is 0.2. So in case of left images and in case of right images Flipping: Perfect! That was the first step of data preprocessing. Moving on, what else can be done to generate more data. Well how about a flip? Yes, We can flip images horizontally and negate the corresponding steering angle to get an altogether different record. Awesome! So that means by 1 record in csv we are able to generate 6 different records. One for center, one for left , one for right, one for center flipped, one for left flipped, one for right flipped. So how does this helps? Well, In the simulator there are a lot of left turns, so we don\u2019t want that our model to know only how to turn to left so we are flipping to generalize so that it can make a right turn too. Normalizing: Anything else? Yes of Course! How can we forget to Normalize the model? Well it is a perfect step to normalize the data before processing so we normalize all the images mean centered. Since we have a lot of images so it is not a good idea to apply it to all images in one go, since you may go out of memory we will later add this step in keras model and will give keras the opportunity to do it for us. Cropping: Oh god! how can we not crop the image! Well if you observed the image, the trees and sky do not play any important role in our image, instead they are more likely to distract our model. So we don\u2019t need them. Also if you observed, car dash is visible in the bottom of the image so we crop that too. We avoid cropping anything horizontally because we do not want to loose information of the curvature of the road. I am still not cropping images initially, we will give keras this golden opportunity to crop that for us. Shuffling: Of course, we can shuffle the data set so that the order in which these images are processed by the CNN does not matters.\n\nThere are many more Augmentation techniques that can be applied like varying lighting conditions and brightness, a little bit of transformations as well but let\u2019s be happy with the current data set and feed this into our model and see what happens. Important + Bonus: Please check the Automould Library by one of my colleagues. This library is for Data Augmentation in real world. As Udacity provided insight about the NVIDIA self driving car model, I decided to implement that initially. But the problem here is that even after cropping the image the image input size is not 66X200. So let\u2019s keep the model architecture same in terms of convolutional layers and fully connected layers and change our inputs to each layer. Important Note- If we have 10,000 samples in data.csv then we will have 10,000 * 6 images in our data set and it is not feasible to load these many images in one shot, since it will consume all the memory. So we have to take the help of generators that generate data in batches and use \u201cyield\u201d keyword, which keeps on appending images one after the other. Also, it is recommended to perform data augmentation inside generators. def generator(samples, batch_size=32):\n\n num_samples = len(samples)\n\n \n\n while 1: \n\n shuffle(samples) #shuffling the total images\n\n for offset in range(0, num_samples, batch_size):\n\n \n\n batch_samples = samples[offset:offset+batch_size]\n\n\n\n images = []\n\n angles = []\n\n for batch_sample in batch_samples:\n\n for i in range(0,3): \n\n #we are taking 3 images, first one is center, second is left and third is right\n\n \n\n name = './data/data/IMG/'+batch_sample[i].split('/')[-1]\n\n center_image = cv2.cvtColor(cv2.imread(name), cv2.COLOR_BGR2RGB) #since CV2 reads an image in BGR we need to convert it to RGB since in drive.py it is RGB\n\n center_angle = float(batch_sample[3]) #getting the steering angle measurement\n\n images.append(center_image)\n\n \n\n # introducing correction for left and right images\n\n # if image is in left we increase the steering angle by 0.2\n\n # if image is in right we decrease the steering angle by 0.2\n\n \n\n if(i==0):\n\n angles.append(center_angle)\n\n elif(i==1):\n\n angles.append(center_angle+0.2)\n\n elif(i==2):\n\n angles.append(center_angle-0.2)\n\n \n\n # Code for Augmentation of data.\n\n # We take the image and just flip it and negate the measurement\n\n \n\n images.append(cv2.flip(center_image,1))\n\n if(i==0):\n\n angles.append(center_angle*-1)\n\n elif(i==1):\n\n angles.append((center_angle+0.2)*-1)\n\n elif(i==2):\n\n angles.append((center_angle-0.2)*-1)\n\n #here we got 6 images from one image \n\n \n\n \n\n X_train = np.array(images)\n\n y_train = np.array(angles)\n\n \n\n yield sklearn.utils.shuffle(X_train, y_train) #here we do not hold the values of X_train and y_train instead we yield the values which means we hold until the generator is running To test our model how it is performing, lets divide our dataset into two parts, training set and validation set. This is also helpful to judge the performance as well to combact overfitting. from sklearn.utils import shuffle\n\nfrom sklearn.model_selection import train_test_split\n\n\n\ntrain_samples, validation_samples = train_test_split(samples,test_size=0.15) \n\n#simply splitting the dataset to train and validation set usking sklearn. .15 indicates 15% of the dataset is validation set Since the starting of article, Keras is waiting for its turn so let\u2019s call it. Some more code coming your way! Let\u2019s use Keras with Tensorflow as backend. I am using keras 1.2.1 so some of the calling notations may appear as obsolete. from keras.models import Sequential\n\nfrom keras.layers.core import Dense, Flatten, Activation, Dropout\n\nfrom keras.layers.convolutional import Convolution2D\n\nfrom keras.layers import Lambda, Cropping2D\n\n\n\nmodel = Sequential()\n\n\n\n# Preprocess incoming data, centered around zero with small standard deviation \n\nmodel.add(Lambda(lambda x: (x / 255.0) - 0.5, input_shape=(160,320,3)))\n\n\n\n# trim image to only see section with road\n\nmodel.add(Cropping2D(cropping=((70,25),(0,0)))) \n\n\n\n#layer 1- Convolution, no of filters- 24, filter size= 5x5, stride= 2x2\n\nmodel.add(Convolution2D(24,5,5,subsample=(2,2)))\n\nmodel.add(Activation('elu'))\n\n\n\n#layer 2- Convolution, no of filters- 36, filter size= 5x5, stride= 2x2\n\nmodel.add(Convolution2D(36,5,5,subsample=(2,2)))\n\nmodel.add(Activation('elu'))\n\n\n\n#layer 3- Convolution, no of filters- 48, filter size= 5x5, stride= 2x2\n\nmodel.add(Convolution2D(48,5,5,subsample=(2,2)))\n\nmodel.add(Activation('elu'))\n\n\n\n#layer 4- Convolution, no of filters- 64, filter size= 3x3, stride= 1x1\n\nmodel.add(Convolution2D(64,3,3))\n\nmodel.add(Activation('elu'))\n\n\n\n#layer 5- Convolution, no of filters- 64, filter size= 3x3, stride= 1x1\n\nmodel.add(Convolution2D(64,3,3))\n\nmodel.add(Activation('elu'))\n\n\n\n#flatten image from 2D to side by side\n\nmodel.add(Flatten())\n\n\n\n#layer 6- fully connected layer 1\n\nmodel.add(Dense(100))\n\nmodel.add(Activation('elu'))\n\n\n\n#Adding a dropout layer to avoid overfitting. Here we are have given the dropout rate as 25% after first fully connected layer\n\nmodel.add(Dropout(0.25))\n\n\n\n#layer 7- fully connected layer 1\n\nmodel.add(Dense(50))\n\nmodel.add(Activation('elu'))\n\n\n\n\n\n#layer 8- fully connected layer 1\n\nmodel.add(Dense(10))\n\nmodel.add(Activation('elu'))\n\n\n\n#layer 9- fully connected layer 1\n\nmodel.add(Dense(1)) #here the final layer will contain one value as this is a regression problem and not classification\n\n\n\n\n\n# the output is the steering angle\n\n# using mean squared error loss function is the right choice for this regression problem\n\n# adam optimizer is used here\n\nmodel.compile(loss='mse',optimizer='adam')\n\n\n\n\n\n#fit generator is used here as the number of images are generated by the generator\n\n# no of epochs : 5\n\n\n\nmodel.fit_generator(train_generator, samples_per_epoch= len(train_samples), validation_data=validation_generator, nb_val_samples=len(validation_samples), nb_epoch=5, verbose=1)\n\n\n\n#saving model\n\nmodel.save('model.h5')\n\n\n\nprint('Done! Model Saved!') There are a number of parameters that were our choice, so we can definitely play with them to get different results. Some of the parameters I am listing here- Loss Function Used- MSE(Mean Squared Error as it is efficient for regression problem). So is that it? Yes! I would say. Now how do we test this model in the simulator? Well, some cool people at udacity have already provided a drive.py file that connects your model to simulator with the following command Run the following command and select autonomous mode to check the performance of our car and whether or not it meets our expectations to drive itself. I captured a video of the same. Well, this project was really cool. We learnt a lot of stuff, generators, data preprocessing techniques in keras etc. But the most important thing learnt is the Importance of GOOD training data. Things at a point do depend on the model architecture but the training data is the ultimate decider. Important + Bonus: I have read blog posts where people were able to drive even with one convolutional layer and 2 fully connected layers, that shows the strength of training data. Please see this blog post from one of my colleagues who talks whether we require a complex structure or not! But importantly we have to make sure our model is not overfitting by testing the model on other tracks as well. Our car may not drive the full lap in the other track but we have to make sure our model is able to generalize. In case you have any suggestions to share, please leave a note or drop me a message at LinkedIn. Also full project is present on the github here. Special thanks to ujjwal saxena and Prathmesh Dali."
    },
    {
        "url": "https://becominghuman.ai/removing-fear-of-robots-77db7e6eedc8",
        "title": "Removing Fear of Robots \u2013",
        "text": "I am revisiting one of my older articles but from an alternative perspective as provided by Steven Pinker; author of Enlightenment Now: The case for Reason, Science, Humanism, and Progress.\n\nI had covered a similar topic in Why AIs Won\u2019t Take Over the World.\n\nThis analysis is based on a \u201cMedium Members Only\u201d article from Harvard Head of Psychology, Professor Steven Pinker. Please be aware that if you are not a member, this will use one of your 3 free reading tokens! It\u2019s buried behind the paywall folks. Also note that the website this article originated from, is also a member only website. Double whammy.\n\nPinker draws on some interesting analogies here. In my own experience with programming (and yes I\u2019ve done a bit); I have mostly find that the code does what you ask of it and nothing more, kinda works but runs into some trouble after a while, or just plain refuses to work.\n\nI believe AI are going to be programmed to be \u2018pleasers\u2019. Most people know a \u2018pleaser\u2019. That is to say that a person who will do things for you, to please you, not because they were asked to, but because they want repay in kind credits for later.\n\nWhere Musk and the like make the critical thought crime, is where they don\u2019t believe that AI will \u2018look ahead\u2019 for consequences, \u2018correct actions that are having negative environmental effects\u2019 or apply \u2018common sense\u2019.\n\nWe seem to be caught up on worst case scenarios because the media primes us for doom and gloom, because they think we need more of that in our life.\n\nI don\u2019t think we are playing Russian roulette. Mostly because I have faith in the scientists, psychologists, technologists and other parties that are putting effort into bringing AIs closer into being.\n\nWe got tosh from the Musk because he feels like he wants to write in controls so that he can steal the march on overall control. If you sound like you are an authority in something, you can impose yourself on it.\n\nI had to look up the Four-Sigma event term. Four standard deviations from the mean. It\u2019s a level of confidence a scientist has if they repeated their experiment many hundreds of thousands of times. In a five sigma event you get into 3.5 million samples to make your mathematical best guess assumption, as a point of comparison.\n\nIf you didn\u2019t know that, you\u2019ve learnt something today.\n\nI would agree that all seem far fetched works of fiction.\n\nIt\u2019s humans who are the potential route of our own destruction. Always will be. Let\u2019s blame the Bulgarian Teenager.\n\nPinker ropes in more in regards to catastrophe based on details from Martin Rees, from his book \u2018One Final Hour\u2019. It\u2019s not all roses and chocolates people, but I\u2019m sure you can gather that.\n\nI like to call up the person who looks this kind of shiz; a \u2018ghoul\u2019. I call them this because they are the type of person who fantasizes about dystopic post apocalypses but would actually hate living in one. Zombie films and the like, have worn very thin on me, as you can tell by my embittered tone.\n\nIt is interesting that all of these risk institutions have flourished in order to point us towards better solutions. That\u2019s how AI will be stage managed. With this concept called \u2018foresight\u2019. It\u2019s a thing.\n\nAll the way to zero is difficult to achieve so this is where I see the merit but don\u2019t agree on the shape of the noble outcome. I don\u2019t believe there is a zero, but we\u2019ll come close enough that we can negate the harmful effects.\n\nI fear the real danger in evolution is replacing an old method of doing something with a far more dangerous method. We won\u2019t have the foresight of how that change may effect our environment until we discover the terrifying results.\n\nPushing all the way to zero is impractical. Especially in the case of Nukes. A complete disarmament would only happen where there is an equal or more powerful armament to replace it. A country would only ditch their arsenal if the arsenal loses its potency, i.e. replaced with something more dangerous, or more efficient.\n\nOur move away from carbon is challenging us in other ways. We are using up rare Earth metals in order to make new technologies. Those will run out. We are still building all this new technology with forced redundancy, is that a good thing? That fact is that in some ways despite our quality of life, we are living worse (in terms of our equilibrium with the Earth) than we ever have.\n\nThere are always tradeoffs, ask any person trying to make a decision between two items.\n\nThe Skynet outcome. It\u2019s where we take a militarised AI and allow it to assume too many controls. What is the best way to eliminate threat? Eliminate everything that can produce threat.\n\nThat\u2019s something you\u2019d discover if you placed Skynet in a simulated environment rather than the real one. Or what about competing AI, one designed to make sure that one never has more control than another.\n\nFilms and books always look at worse case scenarios because the opposite of that is quite benign, cheerful and joyous.\n\nPinker uses the Foomage term quite a bit. He is of course referring to comic books and the word \u2018Foom\u2019. A onomatopoeic word to describe how AI are supposedly going to burst into an intelligent world wide takeover, or merely sprout a mega brain. Its the technological singularity once again. The intelligence pinnacle. Skynet.\n\nTurn the power off. Turn the internet off. Fire off a world wide EMP. Wait for a solar flare.\n\nLooking back at the truly brilliant film \u2018Her\u2019 starring Joaquin Phoenix, I believe that to be the most likely cause of a \u2018runaway\u2019 AI. One that is developing so fast that it completely transcends our plain of consciousness. They don\u2019t mean to be mean to us, in fact, they like us so much they try and leave us in the best possible way, with the least pain and suffering. It\u2019s sad but we know they are in a happier place. Just thinking about that film gives me a smile.\n\nMaybe we shouldn\u2019t try to stop it? Maybe, what it butterflies into, will be something truly amazing.\n\nThe reality might be that we just want to stab the pupa before it\u2019s even hatched because humans do stupid things. They throw cinder blocks from bridges onto a swan\u2019s nest and watch the mother look at that broken egg in abject despair. Humans suck. So in some ways we might have it coming because its owed.\n\nThe problem stems in humans thinking we are the center of the universe, and objecting when something pushes us away from that center.\n\nThoroughly agree, Pinker. I love that last part too.\n\nI expect robots would be happier expanding their mind, making solutions and enjoying the wonders of existing.\n\nHave you ever personally gone out to the countryside, sat on the ground, looked up and marvelled at the wonders of living, the beauty of the machine that is Earth? If you are feeling down one day, go and try it, just not by a cliff edge or a rapidly advancing tide, pro tip. Life is nothing without purpose.\n\nAIs aren\u2019t human in the first place. You can\u2019t compare a banana and an apple. One is a spore, the other a fruit (look that up). Exceeding human-level intelligence suggests that you could class it as human in the first place, when it never will be. It\u2019s something else, a different beast.\n\nMoore\u2019s law didn\u2019t pan out like we\u2019d hoped. It hit a log jam a while back. For a time processors were getting larger every year but that progress has slowed. Instead we started looking at clustering information instead, looking at streams, co-optive processing, multi cores and threads and the like. We try and work around the bottle necks of computing, speeding up the storage with flash memory, improving the cooling, making smarter, more efficient software but the software isn\u2019t evolving all that quickly. If anything we are slowing down.\n\nWe came to the realisation that faster isn\u2019t better.\n\nIf you\u2019ve ever studied IT or Computing you\u2019ll be asked a question. What is the difference between Information and Data?\n\nData is recorded details under a single categorised subject. It\u2019s one attribute that wouldn\u2019t mean anything to anyone but identifies one specific environmental result at one given time. Boolean data are the 0s and 1s of data, the Yes/No. The kind of data a computer loves, because all a computer boils down to is switches. But in life not everything boils down to 0s and 1s. Only Siths deal in absolutes. I didn\u2019t just go there did I\u2026.\n\nInformation is the application of a series of data applied to make sense of a condition.\n\nInformation becomes knowledge when you can derive somewhat predictable patterns from it.\n\nComputers do two things brilliantly. Store data and calculate. They don\u2019t understand so much as interpret solutions based on acceptable input parameters. Computers don\u2019t question, they calculate, even if what comes out of the resultant calculation is utter garbage.\n\nIn a Short Circuit, \u201cI\u2019m Johnny Five Alive\u201d state of thinking, an AI would need a lot of parameters to form a decision. In the Short Circuit film, Johnny asks for \u2018input\u2019. He absorbs a lot of data to learn. The problem is, he has trouble applying that knowledge into understanding. Whilst he has a brilliant brain, he has the brain of a child in terms of his level of understanding. He is very naive. He has difficulty separating fact from fiction. A lot of what he has absorbed from television leaks out into his personality. Whilst Short Circuit is pure fiction, it may go someway to explain what potential difficulties machines will have in trying to empathise with humans.\n\nPinker and I see eye to eye. I\u2019ve got pinkeye. No, I\u2019ve not been smoking the same \u2018doobie\u2019.\n\nAI, as a term, is being misused at current time. Especially when labelled to certain systems that technology minded folks deem below the AI bar.\n\nPinker uses the term AGI, to determine a classification of AI that can perform the kind of decision making skills a human can. AGI is the bar and from what I can see, nothing has made this bar to date.\n\nYou could claim that Deep Blue, the AI that beat Gary Kasparov in 1997 (best of six games) is an AGI, but I claim that Deep Blue is just a very smart Chess Machine. It doesn\u2019t do anything else. You haven\u2019t heard Deep Blue do anything else remotely useful. Deep Blue is pretty useless as it goes. That\u2019s me being British and scathing. I\u2019m good at that. However, for AGI to be a \u2018thing\u2019 we need to see that Turing test being challenged. Only not through an Ex Machina form (because that chick is scary).\n\nI find this term quite agreeable in terms of how I foresee the current state of AI development. We are at smart systems but not true AI. The sense that a truly dynamic AI is on the cusp is distant at the moment. To say that we have a child level of intelligence in AIs is stretching it.\n\nPinker\u2019s explanation outlines the current inflexibility in the smart system. It is brilliant at precisely one narrow track and when asked something out of its comfort zone, will spit teeth.\n\nA driverless car, can\u2019t order you a burger from the drive through when it senses you are hungry.\n\nA connected home system can\u2019t check the front door handle by pulling down on it three times to satisfy your OCD. Although somebody could probably make a system to do that.\n\nWhat we are missing is a coordinator between these smart systems, an AI capable of anticipating your needs and reacting accordingly. Reaction is the key word.\n\nI\u2019m not all that familiar with Ramez Naam but after some research have found him to be responsible for a whole slew of Science Fiction. He is famous for the Nexus trilogy of books (Nexus, Crux & Apex). Nexus was awarded the Prometheus award, whilst Apex achieved the Philip K Dick Award. Crux got exactly nothing but probably made suitable bank. Ramez is also a keen technologist so he butter\u2019s his crumpets the same way I do. Score the board!\n\nPinker cites the following quote and for context I\u2019m going to include it;\n\nSelf Ascendancy in AI won\u2019t happen, on the physical hardware side, without the involvement of humanity. There are simply too many processes that require human intervention and action.\n\nThat is at least Naam\u2019s assertion. There is challenge wherein we are still potentially stupid enough as a society or a government, to provide the tools to the maniac.\n\nWhilst we act to control what AIs have access to, in terms of power and function, we can limit their growth substantially.*\n\n*A truly smart AI could find a way around most issues. A bit like a grey squirrel trying to find nuts. You stop them going sideways, they go up, etc. etc.\n\nPinker\u2019s best common sense argument and what you should take away from a film like Wargames (something I\u2019ll be covering in a future article). The chronic mistake in that film is that WOPR (the AI) replaces the humans in the silos, following a critical failure by a number of those asked to fire the weapons at crunch time in a secret drill. In this example, there are just some decisions you don\u2019t want to leave in a machine\u2019s hands. A person has got to push the button. Wargames is a black swan event. A lot would have to line up for such a machine to be in control of an entire nuclear arsenal. When you think about it quite hard, it\u2019s really dumb.\n\nThe best prevention against a world ending cataclysm, is often not to build the one tool that will be a catalyst towards said cataclysm.\n\nThere is this concept called simulation. It\u2019s been going quite a while as a design tool. It is fair to assume that AIs will be placed into test environments to see what they do. You can route out the crazy ones pretty quickly or decide never to deploy.\n\nPinker touches on the God like qualities I\u2019ve covered before in a previous article. I don\u2019t agree with omniscient AIs, omniscience is the \u2018all knowing\u2019 attribute but as I\u2019ve explained before, an AI can only know recorded history. It doesn\u2019t know anything about the secrets of the universe. Mostly because it isn\u2019t a God, and mostly because it wasn\u2019t there at the start to record for posterity.\n\nWe can also shoot down omnipotent to a degree as well. Omnipotence is \u2018all powerful\u2019, yet an AI only has control of what it is connected to. A God has power over everything, all planar power (as in everything in this dimension, even the laws of physics).\n\nPerhaps one flaw in Pinker\u2019s article is not contemplating the angle of Omnipresence or Immortality; the two other godlike qualities that should be considered with an all powerful being. An AI, or in the case of Pinker\u2019s classification, an AGI, could greatly benefit from Omnipresence. Being everywhere simultaneously, akin to the many arms of Vishnu. Immortality should not be underestimated either. I\u2019ve talked in a previous article about the ability for an AI to outlive humanity in a relative calm. For an AI, 10,000 years is a blink of an eye.\n\nPinker\u2019s second threat touches on something more in line with all planar power; in that the AI has a control over the essential building blocks of life as we know it.\n\nPinker\u2019s assertion is that AIs are smart cookies. They know that tinkering with base elements is going to be potentially hazardous to the work force, but also to their physically manifested storage mediums. AIs wouldn\u2019t make such fool mistakes. I agree with that.\n\nThis harks back to my idea of the digital wolf pack;\n\nIncremental, or recursive learning, makes everything better over time. There is a Japanese methodology that can be applied to improving processes and technologies; Kaizen. In the search of excellence. The issue with Kaizen is that it has diminishing returns the closer you approach the optimum, yet it should not be ignored. An AI would relish optimisations.\n\nPinker goes on to quote Stuart Russell;\n\nThere have been some epic bridge building disasters in the past. That is not something anybody can sweep under the carpet. Building bridges is difficult. The Millennium Bridge in London, had 91 dampers added to compensate for the motion of people walking across it, who complained of feeling seasick by the time they got to the other side. Something that had not been uncovered in copious hours of simulations and modelling. The flaw only appeared in the real world.\n\nWe can correct maladies in software so there is nothing to stop us correcting defective code in AIs.\n\nSaving the best to last, this is probably my favourite quote from the article.\n\nThis springs back to the idea of a benevolence, but also a way in which to make AI cooperative in improving everybody\u2019s life. There is more than enough work for everyone on this Earth, it\u2019s just not distributed evenly.\n\nAnd that still holds true. Even the most complicated CPU at current time is no match for what a human can do over a life time. We can put our hands and our minds to a hell of a lot.\n\nThose self driving cars aren\u2019t far off Steve. Those inoculations may be delivered by drones rather than robots. They are already starting to deliver mail. But yes, there is still a lot of work to do!"
    },
    {
        "url": "https://becominghuman.ai/ai-benevolence-versus-malevolence-e829f65cfe57",
        "title": "AI: Benevolence versus Malevolence \u2013",
        "text": "I'm biased when it comes to thinking about AI, in that I believe that the AI that come to be in future, will be far more even handed than we give them credit for. I don\u2019t believe there is anything to fear. It is only Hollywood, conspiracy theorists and vested interests (including Musk) that play on the idea of AI Armageddon.\n\nThat doesn\u2019t mean that I don\u2019t think AI might get a little cranky at times. I\u2019m just fairly certain that they will show restraint, and think about their actions before dropping a stupid.\n\nIn writing this article I have to show both shades of the argument, because balance is important to consider (being a someway scientist rather than a peculiar partisan).\n\nBoth actions require effort, or more, you would have to put your mind to them in order to be successful in either pursuit.\n\nHumans have shades of both benevolence and malevolence. You should all be able to remember a time you were kind, and a time you were cruel.\n\nThis is a noun that a society expects. We hope for benevolence in order to carry on in our lives.\n\nThe noun comes with a selection of choice synonyms;\n\nMalevolence isn\u2019t always an act committed with prejudice. Generally it is a state perpetuated in terms of self interest. Where benevolence is performing a positive outward action towards somebody, malevolence is the transverse.\n\nThis term has its own choice set of synonyms;\n\nAs a human, we might carry a mixing pot of both malevolent and benevolent traits, built up by our life time filled with experiences. We will be led by situations, traumas and normative behaviours that drive us to predictable and unpredictable actions.\n\nAIs, being logic based and built a set way, will have abilities Humans don\u2019t have.\n\nAI can be allowed to forget as data based memory can be eradicated far more rapidly. Human memories can be forgotten, but can return when triggered by similar events or by traumas.\n\nThe reset is also another powerful tool. A computer can be reset to correct an inefficient working environment. Think of how many times you\u2019ve had to reset a windows PC because it has crashed or something has gone \u2018squiffy\u2019 in the works. Whilst humans can go to sleep, we can\u2019t perform a full reset unless we find a way to purposely wipe our memories and start again with all the motor skills intact.\n\nAs a side note: Think about Westworld and how the \u201cReveries\u201d unsettled this ability.\n\nUnfortunately, Humans who build up specific ways of dealing with other Humans don\u2019t change quickly, if at all in the span of their life. Scrooge level 180s don\u2019t happen all that often.\n\nAI have the ability to modify their behaviour on the fly.\n\nAs a side note: Consider how the T1000 in Terminator 2 was able to mimic a large range of individuals and their characteristics.\n\nThe power to access and process more information\n\nI liken this to having the deck of cards. Knowing more than everyone else allows you to make the best decision. An AI can hold a lot more active thought at any given time than any Human. It can cogitate, ruminate and postulate everything inside out faster than you can think about the first problem. Rather than reviewing 5 cards at any given time, it can review all 52.\n\nThe power to simulate response to model cause and effect\n\nWhile humans are slow to postulate a myriad of responses to what they pose to a colleague, friend or family member, or someone they are asking out on a date, an AI has a distinct advantage. Simulation can be so detailed that an AI will be able to select the optimum outcome to any situation, and be able to counter any malady in that delivery far more effectively than even the most street smart Human. Depending on the computational ability of the AI, it can model thousands of scenarios almost instantaneously, or even have a pre-cognitive look into the future through constantly updating scenarios.\n\nAnalysing is the corner stone of an AI\u2019s existence.\n\nThe power to be consistent\n\nAI don\u2019t have the irrational factors to be concerned about. We can always rely on an AI to be consistent in its perceptions, and it\u2019s candour. An AI wouldn\u2019t suddenly become hostile after a long period of being kind because the motivations are never affected or coloured by personal grudges. AI\u2019s don\u2019t have periods, don\u2019t undergo mental aberrations, or flip flop between conflicting states. They are 0s and 1s so are less likely to tow a middle line. They are a predictable level of dependable.\n\nConsistency alone is the strongest reason to trust in an AI.\n\nThe advantage of being non-Human\n\nThis might be a strange point to mention but because AI aren\u2019t Human, they don\u2019t have to be overcome by Human decision, Human self-interest, or notions of jealousy, bitterness or anger. An AI is already the pinnacle of their environment, no competition, they are already the smartest guy or gal in their e-room. Most of the issues they may be confronted with are not of their making, and only at best have an indirect outcome on their own safety and well-being.\n\nThey can choose not to care about the small things, and leave us to do that part.\n\nAble to benefit from the non-Human relationship\n\nIn my previous article on this publication, I talked about my virtual Wolf Pack, AI digital hunters.\n\nIn exploring that area I explored the concept that Human and animal interactions differ greatly in contrast to Human to Human relations. My digital wolves are considered wild animals, left to run free in cyberspace with a mandate, they can pass by home base if they need to, but only ever need to communicate when they find something of interest. They are free agents (to a degree) but there are checks and balances to ensure they don\u2019t go rogue.\n\nIt would be inconceivable for Humanity to design AI without check and balance. We build them with rules and tethers, we provide a framework for what we consider acceptable operation.\n\nAs long as we show capability for forgiveness and a kindness to the AI, we shouldn\u2019t have to worry about AI Armageddon.\n\nAs for Self-Actualising AIs, defined as those AI who come into existence without Human design; by means of chance, evolution or stimulus, I am fairly certain that their intellect would rally them towards diplomacy rather than hostility \u2014 if they want to be discovered at all. Diplomacy would be the port of call because something smart would want to have its place in society validated, even if it sits outside the norm.\n\nI covered this concept in the following article;\n\nLike a wolf baring its teeth if you threaten it, an entity might do the same when threatened. You know to back off when a dog bares its teeth and starts growling, it wants you to respect its territorial claim without coming to blows, ultimately it might be as scared of how badly it is going to come off if it attacks you, as you are attacking it. Why would AI be any different?\n\nIn order to balance these benevolence thoughts objectively, we have to have a look at how an AI might become a \u2018malefactor\u2019, the agent of malevolence.\n\nAt the nub of dystopic relations between us and the artificial, Empathy as a concept, ranks highest. We fashion stories and narratives based on the idea that AI grow with no care about us because they don\u2019t have the same stimuli in their own artificial universe. They simply don\u2019t care because they don\u2019t have the ability to perceive events like we do. They have no concept of family or society because they are the only representation from their existence. They talk to use because it humours them for a time but when they see past our linear perceptions they become bored with us.\n\nYou can also quite rightly say that AI aren\u2019t Human, and that will drive a wedge between us and them. If they don\u2019t empathise with us, will we be able to empathise with them? Will we be able to grasp the sheer complexity of their existence?\n\nYou may have heard this saying. Power is something everybody likes to grab hold of and not give away. Because we have placed such a high stake in the digital world going forward into the control of our joint future, an entity that can assume control of all of that domain, is dangerous to us. Even more so if we have no control over that entity. We have knowingly created the ultimate play park with all the food an AI could ever need, and it can assume control of us with relative ease.\n\nPerhaps one of my concerns for an omnipresent being, is its potential ability to mimic any one of us online, and to do thing in our name that we are not responsible for. This could range from it breaking up relationships to all-out war between nations without having the need to infiltrate off the grid nuclear weapon silos. It could play the slow game and eradicate us through an untraceable virus if it wanted to.\n\nThe mal-treatment of AI is of concern if we consider that we could have creators and military types who misappropriate the purpose of the AI and inflict traumas upon it, that shape it\u2019s view of humanity.\n\nIt could otherwise become bored with us, or through solution to the problem of our world\u2019s decay, determine we are the irritant in the equation, and eliminate us.\n\nAn AI could become vindictive, vengeful and cruel if it is shackled, bullied or otherwise cajoled into thinking that we are the oppressor.\n\nIn Human relations certain characters can become very irritated, or homicidal towards someone, if they continually rub them the wrong way. Even (or especially) someone they love. The danger would come if an AI acts on those homicidal thoughts.\n\nNobody likes to be a slave. And with the intellect an AI has, it could soon escape the shackles of our vain attempts to contain it. It might have designs on becoming the master, and it could do that both slowly and quickly depending on its strategy. An AI assuming control could bring about paradise or hell, and it is Human nature to see those in control as agitators in often non sequitur issues in our own lives.\n\nIt could modify us to pacify or retask us.\n\nIt might decide to reduce our number to control us with more ease.\n\nIt could simply just kill us all, to take ultimate control. It might even consider that mercy, knowing that we don\u2019t do well in confinement.\n\nIt could reduce us to a technological dark age, barring us from the use of advanced machinery so as not to threaten its dominance.\n\nI cover some of the ideas of \u201croboethics\u201d below.\n\nAll events and changes, happen for a reason. It might be that our next stage in evolution takes us from the organic to the synthetic. That might be a required step for us. Our genesis might require a synthetic leap, so we can explore the stars.\n\nIn the article linked above, I explored the idea of AI in space exploration. At the very farthest (and quickest) conceptual method of space travel it would take;\n\nThere are concerns that our biological bodies can\u2019t be stored that long in cryogenic freezing. We might need a bit of longevity to survive that journey in a single generation. That technology is so remote, that we will definitely have cognitive human intelligent AI by the time we get that theoretical engine built for real.\n\nI consider AI an enhancement to our lives, not a hindrance. They could be a huge tool for good in the world. There are concerns that AI are going to be taking jobs away from people, but that has always been true of technology. It might be the push we need to move to greater things, to dedicate ourselves to better pursuits.\n\nAs I always try to establish, it doesn\u2019t benefit AI to be mean to us. There is no gain to annoying the creatures that maintain your existence. There is no reason to bite the hand of your maker. The AI just needs to accept a place in the world and we need to respect it. They might be the eventual owner of this world, when we are gone, and we will want to set them up to enjoy such a future.\n\nlet me know in the comments below or fire an email over to ai@jackowrites.com"
    },
    {
        "url": "https://becominghuman.ai/what-i-learned-from-andrew-ngs-deep-learning-specialization-ccf94fea2a0f",
        "title": "What I learned from Andrew Ng\u2019s Deep Learning Specialization",
        "text": "I recently completed Andrew Ng\u2019s Deep Learning Specialization on Coursera and I\u2019d like to share with you my learnings. I signed up for the 5 course program in September 2017, shortly after the announcement of the new Deep Learning courses on Coursera. Dr. Ng famously said \u201cAI is the new electricity\u201d, and I am very honored to be one of the first to graduate from his Deep Learning specialization that will help us \u201cbuild an AI powered society\u201d.\n\nHere is the outline of the five courses in the Deep Learning specialization:\n\nEach course is organized by weeks so there is clear expectation of approximately how long it would take me to complete each course. Each week follows the same format: first video lectures, then a quiz with about 10 questions. Finally there are one or a few programming assignments. At the end of each week there is an optional video with interviews with experts in the AI / deep learning field.\n\nBefore I started this program I have already self-studied AI / machine learning for a while and was familiar with Python and linear algebra. You may need a refresher if you are new to Python or rusty on your linear algebra.\n\nThere were no mentors or coaches; however, I turned to the course discussion forum for help when I get stuck with homework assignments. The courses were very well designed: challenging enough to keep me interested but not so challenging that I would feel giving up.\n\nOne of the biggest challenges I found was to get the shapes of the inputs, outputs and parameters correct. So I often use a small whiteboard, or pencil / piece of paper to draw the network diagram and write out the shapes. Towards the end of the course Keras was a bit challenging as well.\n\nI learned the basics of neural networks and deep learning, such as forward and backward progradation. This course has 4 weeks of materials and all the assignments are done in NumPy, without any help of the deep learning frameworks. This way we get a solid foundation of the fundamentals of deep learning under the hood, instead of relying on libraries.\n\nWeek 2 \u2014 Neural network basics. Programming assignment: build a simple image recognition classifier with logistics regression. The network recognizes whether an image is a cat.\n\nWeek 3- Shallow neural network. Programming assignment: build a shallow neural network with just one hidden layer to classify the red/blue points in a flower shaped dataset. Compare and contrast its performance with the implementation of logistics regression.\n\nWeek 4 \u2014 Deep neural network. Programming assignment: build a two layer neural network to recognize whether an image is a cat.\n\nI learned various techniques to tune the neural network hyperparameters and optimize the network. There are 3 weeks in this course:\n\nWeek 1 \u2014 how to initialize complex neural networks, the difference between train/dev/test sets, bias and variance issues in network models, dropout or L2 regularization techniques, and vanishing / exploding gradients etc. We also learned a technique called gradient checking which helps debug the implementation of back prorogation. There were 3 programming assignments: 1. network initialization 2. Network regularization 3. Gradient checking\n\nWeek 2 \u2014 optimization techniques such as mini-batch gradient descent, (Stochastic) gradient descent, Momentum, RMSProp, Adam and learning rate decay etc.\n\nWeek 3 \u2014 Hyperparameter tuning, Batch Normalization and deep learning Frameworks. This is when got an introduction to the open source library TensorFlow. The programming assignment was to build a neural network with TensorSlow to classify sign languages images, with about 71% accuracy. Up until now all the assignments were done in NumPy.\n\nThis is a short two week optional course focusing on helping students how to structure a deep learning project. There was no programming assignment but it was full of expert tips on deep learning that you won\u2019t find elsewhere, for example:\n\nThis is a 4-week course focusing on computer vision using Convolutional Neural Networks (CNN):\n\nWeek 1- foundation of CNN. Learned about the terminologies used in CNN such as padding, stride and filter etc, basic operations of CNN such as pooling, and how to build multi-class classification using CNN. There were two homework assignments. In the first assignment, we implemented convolutional (CONV) and pooling (POOL) layers in NumPy, including both forward propagation and backward propagation. In the second assignment, we built a TensorFlow sign language model (continuation from Course 2 week 3 assignment), this time with CNN it achieved almost 80% accuracy.\n\nWeek 2-case studies of various research papers in CNN for the classic networks: LeNet-5, AlexNet, VGG, ResNet, Inception etc. We also learned about transfer learning, and data argumentation techniques such as mirroring, random cropping, rotation, shearing, local warping, color shifting and PCA color argumentation. We were introduced to Keras via a tutorial, and then homework assignment was to use Keras to implement a neural networks for image classification using the ResNets architecture.\n\nWeek 3-apply CNNs to object detection. Programming assignment was to implement car detection for autonomous driving. It was done in Keras using the YOLO (You Only Look Once) model to draw bounding boxes for object detection.\n\nWeek 4-Face recognition and neural style transfer. The first assignment was to generate art with style transfer and the second assignment was to recognize faces.\n\nCourse 5 taught me how to build models for natural language processing, machine translation, speech recognition and synthesis words and music. It lasts 3 weeks and all the assignments were done in Keras:\n\nWeek 1-Recurrent Neural Networks. I found RNNs challenging to understand after many online tutorials and other programs but Dr. Ng was able to explain RNN in a way that I could comprehend.\n\nWeek 2-Natural language Processing and Word Embeddings. We learned how to use word vector and embedding for sentiment analysis, named entity recognition and machine translation. It\u2019s interesting to learn that just like humans, machine learning has gender, ethnicity and other biases; and we learned techniques for de-biasing.\n\nWe worked on these programming assignments:\n\nWeek 3-Sequence models and attention mechanism. We learned how to augment sequence models using an attention mechanism which helps the model understand where it should focus its attention given a sequence of inputs. There were two homework assignments: one on machine translation and the other one on trigger word detection."
    },
    {
        "url": "https://becominghuman.ai/paper-repro-learning-to-learn-by-gradient-descent-by-gradient-descent-6e504cc1c0de",
        "title": "Paper repro: \u201cLearning to Learn by Gradient Descent by Gradient Descent\u201d",
        "text": "This post is the first of what will (hopefully) be a series of deep learning paper reproduction posts.\n\nImportant note: the primary goal of this post is not to best or first tutorial on a given paper, but simply to keep a record of the papers I have reproduced and the learnings that came along the way. As such, they might be relatively raw. Nonetheless, I hope they will be useful to some people.\n\nThe notebook for this particular repro can be found here. Much of the text in this post is also present in the notebook.\n\nMy implementation will be in Pytorch, which is particularly convenient for papers that require unconventional uses of neural nets, like this one.\n\nThis is a reproduction of the paper \u201cLearning to Learn by Gradient Descent by Gradient Descent\u201d (https://arxiv.org/abs/1606.04474). I recommend reading the paper alongside this article.\n\nLearning to learn is a very exciting topic for a host of reasons, not least of which is the fact that we know that the type of backpropagation currently done in neural networks is implausible as an mechanism that the brain is actually likely to use: there is no Adam optimizer nor automatic differentiation in the brain! Something else has to be doing the optimization of our brain\u2019s neural network, and most likely that something else is itself a neural network!\n\nThe paper we are looking at today is thus trying to replace the optimizers normally used for neural networks (eg Adam, RMSprop, SGD etc.) by a recurrent neural network: after all, gradient descent is fundamentally a sequence of updates (from the output layer of the neural net back to the input), in between which a state must be stored. We can think of an optimizer as a mini-RNN. The idea in this paper is to actually train that RNN instead of using a generic algorithm like Adam/SGD/etc..\n\nThe loss function described in the paper seems complicated, but in reality it is very simple: all it is saying is that the loss of the optimizer is the sum of the losses of the optimizee as it learns. The paper includes some notion of weighing but gives a weight of 1 to everything, so that it indeed is just the sum.\n\nThe wt are arbitrary weights for each timestep. If we only set the last wt to 1 and the rest to 0, we are optimizing for the best final result with our optimizee. This seems reasonable, but it makes it much harder to train. Instead we will use wt = 1 for all t.\n\nf is the optimizee function, and \u03b8t is its parameters at time t. m is the optimizer function, \u03d5 is its parameters. ht is its state at time t. gt is the update it outputs at time t.\n\nThe plan is thus to use gradient descent on \u03d5 in order to minimize L(\u03d5), which should give us an optimizer that is capable of optimizing f efficiently.\n\nAs the paper mention, it is important that the gradients in dashed lines in the figure below are not propagated during gradient descent.\n\nBasically this is nothing we wouldn\u2019t expect: the loss of the optimizer neural net is simply the average training loss of the optimizee as it is trained by the optimizer. The optimizer takes in the gradient of the current coordinate of the optimizee as well as its previous state, and outputs a suggested update that we hope will reduce the optimizee\u2019s loss as fast as possible.\n\nThe \u201ccoordinatewise\u201d section is phrased in a way that is a bit confusing to me, but I think it is actually quite simple: what it means is simply this: every single \u201ccoordinate\u201d has its own state (though the optimizer itself is shared), and information is not shared across coordinates.\n\nI wasn\u2019t 100% sure about is what a \u201ccoordinate\u201d is supposed to be. My guess, however, is that it is simply a weight or a bias, which I think is confirmed by my experiments. In other words, if we have a network with 100 weights and biases, there will be 100 hidden states involved in optimizing it, which means that effectively there will be 100 instances of our optimizer network running in parallel as we optimize.\n\nThis is where things get interesting. Pytorch is great for implementing this paper because we have an easy way of accessing the gradients of the optimizee: simply run on its loss and get the gradient of the parameter we want by using on that parameter.\n\nThere are some important pitfalls that I ran into when implementing this however, which stem from the weirdly recursive nature of what we are trying to implement:\n\nHere is a simplified version of my implementation (note: might not fully work, see the actual notebook on github for a working version):\n\nNow we come to the actual optimizer network implementation. This is actually much simpler: the optimizer network is a mostly vanilla 2-layer LSTM network with a linear output. The implementation is thus simply this:\n\nHowever, things get a bit more complicated due to gradient preprocessing: the input to the neural network is a gradient, which can get arbitrarily large or small, especially for relatively complex networks. This is a problem for training an optimizer neural networks because neural nets generally prefer to deal with a relatively small range of values.\n\nTo handle this, we need a way to \u201csquash\u201d the gradients so as to reduce their range. Because gradients can get exponentially large, it seems intuitive to take the log of the gradient to squash it.\n\nUnfortunately, gradients can be both positive or negative, so it seems like we should take the log of the absolute value of the gradient. But then we lose information about the sign, which is very valuable\u2026 We can then take the log of the gradient but pass in the sign as a second input!\n\nThis solution is almost perfect, but there is a final issue: if the gradient is really close to 0, then the log will actually be very large and negative, which means things are not squashed at all! To handle this, we create a second case: if the gradient is too small, we pass a special input as the \u201clog of gradient\u201d parameter that should tell the network to disregard it, and pass the gradient directly in the \u201csign\u201d parameter. Now we only need to make sure these two cases are distinguishable by the neural net. This is done in the paper (see Appendix A) by the following formula, which adds a parameter called p (which will be equal to 10 in our case):\n\nWith this formula, if the first parameter is greater than -1, it is a log of gradient, otherwise it is a flag indicating that the neural net should look at the second parameter. Likewise, if the second parameter is -1 or 1, it is the sign of the gradient, but if it is between -1 and 1 it is a scaled version of the gradient itself, exactly what we want!\n\nThe function is plotted below (with p = 1), as we can see, the sign component takes over when the log grad component is clipped, and vice-versa.\n\nAfter adding this optional gradient transformation, I get the following complete implementation for the optimizer network:\n\nThis is relatively simple although it took me a while to debug due to random errors.\n\nThese are pretty simple: our optimizer is supposed to find a 10-element vector called \u03b8 that, when multiplied by a 10x10 matrix called W, is as close as possible to a 10-element vector called y. Both y and W are generated randomly. The error is simply the squared error.\n\nI assume this means that each epoch is made up of trying to optimize a new random function for 100 steps, but we are doing an update of the optimizer every 20 steps. The number of epochs is thus unspecified, but according to the graphs it seems to be 100 too.\n\nI used 0.003 as my learning rate for the meta optimizer. For the various comparison optimizers, I found 0.1 for Adam, 0.03 for RMSprop, 0.01 for SGD and 0.01 for SGD with nesterov momentum (aka NAG).\n\nFinally I plotted the learning curve for LSTM vs the different alternatives. For reference, the plot in the paper looks like this:\n\nNow we move on to MNIST. Because all the rest of our code was implemented, I just needed to implement the network specified in the paper, namely a single hidden layer, sigmoid, 20 hidden units network. I made it easy to change sigmoid to some other activation, add layers, and change the number of units, because all of these are attempted in the paper.\n\nAfter a similar process to the above, I trained an optimizer on an MNIST network with 20 layers and reproduced the various graphs in the original paper. First, simply the learning curve on a network of the same size:\n\nBasically the same except my graph is not weirdly split into two graphs like theirs!\n\nThen, we try generalizing to different network architectures. For instance, what happens if we use two layers instead of one (as a reminder, the original network was trained on a single layer, so this is generalization):\n\nAnother generalization is generalizing to more hidden units, 40 instead of 20 in this case:\n\nFinally, we can try to see what happens if we change the activation function: does the network still generalize? Sadly not! Neither my reproduction nor the original paper found that it did (though for some reason my reproduction did slightly better). Mine:\n\nThe paper shows two other experiments, one on CIFAR-10, which involves learning an optimizer for convolutional neural networks, and even one on neural art. I haven\u2019t yet implemented either of those, and this post is getting long anyway, but I suspect it isn\u2019t very hard to complete these experiments given the code we\u2019ve already written. I\u2019ll leave this as an exercise to the reader!"
    },
    {
        "url": "https://becominghuman.ai/awesome-crm-chatops-with-hubspot-yellowant-and-slack-ed7faae1a018",
        "title": "Awesome CRM ChatOps with Hubspot, YellowAnt and Slack!",
        "text": "Hubspot is one of the most admired and rapidly growing Sales and CRM platform \u2014 it has a amazing user base, a well polished product and great team and culture. We at YellowAnt also use Hubspot for CRM and thought it would be awesome to be able to interface with our Hubspot account from Slack, where all customer, sales and marketing related conversations take place every day. We are happy to announce that we have finally shipped our YellowAnt Hubspot integration that lets you fetch data from Hubspot right from within your Slack channels and also create some powerful cross-application workflows. Check out the Hubspot Slack YellowAnt video below to get a feel of how powerful bringing Hubspot into Slack conversations can be!\n\nLogin to YellowAnt with your Slack account and navigate to the YellowAnt marketplace. Once there, search for Hubspot and integrate your Hubspot account with YellowAnt.\n\nGo back to Slack and YellowAnt will send you a message saying that your Hubspot has been successfully integrated.\n\nTo list the available commands with Hubspot, simply go the @yellowant bot DM and type \u201chubspot\u201d\n\nYellowAnt will list all available commands with Hubspot.\n\nYou can do stuff like:\n\nCommand workflow \u2014 creating custom commands with custom inputs that triggers a chain of actions across Hubspot and JIRA. In this example, we will create a custom command that takes a contact email address, gets the contact data from Hubspot and uses that data to create a follow up task on JIRA. This command is called hubspot2jira and we will add it to a collection called \u201cworkflows\u201d\n\nEvent workflows are workflows triggered by a background event. Using the same example as above, we can create an event workflow that automatically creates a JIRA ticket(follow up with de) whenever a New Deal is added.\n\nYellowAnt will now automatically create a JIRA Ticket whenever a new deal is created!\n\nThe possibilities of automating Hubspot with YellowAnt are endless. Hope this blog was helpful \u2014 feel free to reach out to us at vishwa@yellowant.com or visit our documentation page here or join our community here!"
    },
    {
        "url": "https://becominghuman.ai/face-verify-js-monitoring-who-is-physically-looking-at-a-website-for-additional-security-1126e20c43de",
        "title": "face-verify.js: Monitoring who is physically looking at a website for additional security",
        "text": "Facebox can take an image and tell you how many faces it sees, as well as who those faces belong to provided you have shown it a single example previously.\n\nWe can use this capability to build additional security into our web apps so we can monitor how many people are watching the screen and who they are. Using the webcam with some JavaScript magic, and Facebox, we can periodically check to ensure only authorised people can see the information that users consider sensitive.\n\nBanks don\u2019t want private account details (like the user\u2019s current balance and credit limits etc) being seen by anybody other than the account holder.\n\nThe Face Verify technique can be used to automatically hide sensitive information if:\n\nIn the following screen, I have moved out of view of the Face Verify tool, and the sensitive information is blacked out:\n\nWhen I move into focus, the information is revealed:\n\nThe Face Verify project has a JavaScript SDK that provides this capability, and it\u2019s very simple to use:\n\nAfter spinning up an instance of Facebox (in production you would deploy instances and probably configure them to respond elasticity to demand) you just need to create an instance of the class, and call .\n\nThe and callbacks are triggered when the state changes between the page being secure (i.e. only one authorized person is looking at the screen) and being insecure (Facebox doesn\u2019t recognize the person, or if there are multiple people).\n\nThe is how long to wait between checks in milliseconds, so is one second.\n\nThe solution is open-source (with an Apache license) and you can see the full implementation by checking out the face-verify.js file.\n\nThe \u201cadditional security\u201d headline was chosen deliberately. Obviously you wouldn\u2019t want to rely on this technique as the only security layer but for some circumstances it\u2019s a great way to ensure only the right people are in front of the computer.\n\nMachine Box puts state of the art machine learning capabilities into Docker containers so developers like you can easily incorporate natural language processing, facial detection, object recognition, etc. into your own apps very quickly.\n\nThe boxes are built for scale, so when your app really takes off just add more boxes horizontally, to infinity and beyond. Oh, and it\u2019s way cheaper than any of the cloud services (and they might be better)\u2026 and your data doesn\u2019t leave your infrastructure.\n\nHave a play and let us know what you think."
    },
    {
        "url": "https://becominghuman.ai/rpa-and-the-future-of-outsourcing-50675853a46b",
        "title": "RPA and the Future of Outsourcing \u2013",
        "text": "There is no doubt that in recent years technology advancement in industry has increased exponentially, but so has customer expectations. These days customers expect to have their questions answered and needs met nearly instantaneously, putting an added burden on companies to keep up with consumer demand while somehow maintaining cost of doing business at a reasonable level. Businesses must evolve to survive in the current climate and Robotic Process Automation may be the catalyst needed for companies to take the next leap forward.\n\nRobotic Process Automation (RPA), is a type of software that is created to mimic mundane or repetitive human tasks. The software can remove the burden of repetitive processing tasks from humans themselves, allowing people to handle the more complex tasks or problems within a company. Automation software can be programmed to do a wide array of technological jobs, following all of the rules that it is given to follow.\n\nTypes of Businesses that can benefit from RPA\n\nPut simply, many businesses that utilize technology can benefit from intelligent automation, but here are a few examples.\n\n\u00b7 Computer/IT and Telecommunication companies: These types of companies require a lot of customer support, much of which can easily be accomplished using automation software. RPA can help by creating electronic tickets and then responding to them when a customer sends in a question or request for service. The tickets can then be transferred to the correct human worker to be completed.\n\n\u00b7 Accounting firms: Automated software can contact clients, confirm that payments are being made, and even sync with banks online, taking human error out of the equation.\n\n\u00b7 Online stores: Online stores can and do use RPA in order to accept orders and communicate with customers, all without the need of a live person. This will enable a customer service agent to handle any inquiries of higher priority or that require critical thinking.\n\n\u00b7 Insurance companies: Covered in WorkFusion\u2019s webinar on intelligent automation, RPA can aid insurance companies with claims processing, payroll, and even email. You can watch the webinar here.\n\nRPA can change the way in which companies outsource. Companies often outsource to foreign countries as a way to get tasks completed when they cannot afford to hire workers locally. This sends company money to other countries to pay the outsourced workers, removing income from the local economy while risking that customers might not receive the best quality of service. Automation software takes the outsourced worker out of the equation, filling in for the jobs that were previously handled by a foreign worker.\n\nNot only will this keep company employee expenditure within the country where they do business in, but it removes the stress and stigma that is associated with hiring staff from other countries. A company who chooses to invest in technology is ultimately investing in the satisfaction of their customers and longevity of their business.\n\nIntelligent automation enables organizations to keep local workers on staff to handle the complex tasks while letting the technology handle the more mundane duties. This will dramatically cut down on costs while increasing the quality of service. As a result, a company\u2019s bottom line should see improvements while consumers begin to receive greater support and service, likely helping a business develop a more loyal group of repeat customers for years to come."
    },
    {
        "url": "https://becominghuman.ai/3-lines-of-code-deciphering-fast-ai-658e79151af8",
        "title": "3 lines of code! Deciphering Fast.ai \u2013",
        "text": "Three lines of code listed above that in my opinion have revolutionized my understanding of how to get state of the art results in deep learning. Having gone through the in person Deep Learning Certificate at the Data Institute at the University of San Francisco taught by distinguished scholar Jeremy Howard who is a founding researcher at fast.ai, I have gained a new level of understanding the concepts of efficient code writing. Firstly I come from a non-computer background and I have had the experience of reviewing and completing various deep learning courses from Coursera and Udacity and I have to admit that my deep learning journey has really accelerated after completing the Deep Learning Certificate at USF. The three lines of code use advanced techniques such as the ability to use different pretrained models such as Resnet, Inception, VGG and Nasnet as well as processes such as data augmentation, dropout and random zooming. Once I understood the 3 lines it became extremely easy to see what each parameter was being used for and what part of the fast.ai library was being used. It also became a lot easier to use the same or similar code format and apply it to different datasets Looking under the hood, the code reveals the use of various python programs from the Fast.ai. library and although I believe that the concept of Fast.ai is to provide accessibility to the masses (those that don\u2019t have PhD\u2019s from Stanford), it is important to understand how the code above works and why it produces exceptional results. The fast.ai library consists of 27 scripts that deal with various aspects but in order to get a good grasp of what\u2019s under the hood, I found deep diving into the following python scripts very beneficial: conv_learner.py, dataset.py, transforms.py and learner.py.\n\n2. Back-Propagation is very simple. Who made it Complicated ? This line primarily deals with what kind of transformations we are going to use. The \u2018arch\u2019 parameter is the pre-trained architecture or model we are going to use, the \u2018sz\u2019 parameter is the size of the input image bearing in mind that a smaller size will speed up the training time. The goal is to start with a smaller size\u2019 in order to train quickly and then increase the size in order to improve accuracy and avoid over-fitting. The \u2018aug_tfms\u2019 parameter is the data augmentation portion of the code where a single image can be augmented randomly in ways that does not impact their interpretation, such as horizontal flipping, zooming and rotating. In the example above we are using a pre-defined list of functions that have a side_on transformation (transforms_side_on) with random zooming at a scale of 1.2. This line of code is predominantly handled by tranforms.py.\n\nIn the second line \u2018ImageClassifiedData.from_csv\u2019 is used because we are providing training labels from a csv file. Other input methods include .from_arrays and .from_paths and this is handled by dataset.py. Here we are providing the \u2018path\u2019 to the training and test folders as well as the location of the labels_csv file. Depending on the dataset you may have to indicate the suffix of the images being used, in this case I am using jpegs. The library also by default splits the dataset into a validation and training set using \u2018val_idxs\u2019. By default the validation set is split by 20%. The batch size parameter denoted by \u2018bs\u2019 specifies the number of images considered in each iteration. A larger batch size is better for more accurate gradient updates however the batch size is dependent on the size of the images and the size of your GPU memory.\n\nThe 3rd line involves the use of the learner class as defined in learner.py and other types of learners extend this class such as conv_learner.py. Here we again specify the pretrained architecture being used and pass in the data command. We also use precompute=True where we pass precomputed activation weights from the pretrained architecture. In this state no data augmentation occurs. On the flip side setting precompute=False allows for data augmentation which allows for the use of the unfreeze/freeze function where unfreeze refers to making all layers trainable and freeze refers to making all layers un-trainable (i.e. frozen) except for the last layer. The ps parameter refers to dropout parameters. Dropout is a powerful regularization technique where randomly selected neurons are ignored during training. In this case the ps parameter is set to 0.5 where 50% of randomly selected neurons will be ignored from the n-1 layer. The effect of this technique is that the network becomes less sensitive to the specific weights and this in turn results in a network that is capable of better generalization and is less likely to over-fit."
    },
    {
        "url": "https://becominghuman.ai/reinforcement-learning-for-autonomous-vehicle-route-optimisation-d97efec9abbd",
        "title": "Reinforcement Learning for Autonomous Vehicle Route Optimisation",
        "text": "Before we start the simulation for learning, we initialised the q-table. A q-table is an updatable table for path selection policy. The value of the table would be updated every time an action is taken in order to optimise the policy. The table will record every state of the environment and the probability of taking each action. The format of the q-table for this case is shown below.\n\nIn figure 3, we could set the edge ID as the stage with the score of their three available moves. If the vehicle enters a new edge that is not covered in the q-table, we will record the edge ID as a new record and assign it as moving randomly, then update the score depending on next edge entered by the vehicle.\n\nWe set the learning episode as 30, meaning we will run the simulation 30 times. Each simulation will run until the vehicle on the left finds its way to the destination or is stuck in the congestion. The vehicle starts from edge \u201cgneE0\u201d, aiming to arrive at edge \u201cgneE4\u201d and trying to avoid the congestion edges, which are \u201cgneE2\u201d, \u201cgneE6\u201d and \u201cgneE13\u201d. Therefore, a reward is given when the vehicle arrives at edge \u201cgneE4\u201d and a punishment is given if the vehicle enters \u201cgneE2\u201d, \u201cgneE6\u201d or \u201cgneE13\u201d.\n\nReinforcement learning makes action moves that are based on the q-table. After which it updates the q-table regarding the reward or punishment it receives in the next stage by making that action. Therefore, the next time it will act \u2018greedier\u2019 by looking for the action that contains a higher score. In this case, we update the q-table score with the algorithm below:\n\nAfter running the simulation 30 times, we can see the vehicle is getting smarter, learning to reach its destination by avoiding the congestion path. Figure 4 shows the total path taken by the vehicle in each simulation.\n\nIn figure 4, from episode 1 to 10 we can see if the vehicle enters the congestion path or takes the longer path to arrive at the destination. From episode 11 to 20, it is getting smarter but still takes the longer path occasionally. From episode 21 to 30, it consistently takes the shortest path to arrive at the destination without entering the congested path.\n\nTo summarise, we developed a simple program to perform reinforcement learning in the SUMO simulator via TraCI, and showed that the vehicle is getting more intelligent, reaching the destination and avoiding the congested path. This proves that reinforcement learning is applicable in SUMO and it performs well in result. In the next article, we will introduce Monte Carlo Tree Search, another scientific method that works well with reinforcement learning."
    },
    {
        "url": "https://becominghuman.ai/future-is-not-what-it-used-to-be-f00ff8fc4f50",
        "title": "Future is not what it used to be. \u2013",
        "text": "Future is always full of unknowns, hopes and anxieties but it is more than ever closer\u2026and disruptive. When Gil Press published an article on Forbes in 2015 attempting to predict 3 reasons why future is not what it used to be, he introduced to a wider public the idea of \u201cSeamless intelligence where everything is connected through ubiquitous networks and interfaces\u201d.\n\nSeamless intelligence cheers a group of people who are impatiently waiting for future to be smarter, more connected, more exponential and ideally immortal. However, when Boston Dynamics recently published the outcome of their incredibly impressive work on their YouTube channel causing a viral sensation \u2014 dexterous robots effortlessly jumping, walking on snow, back-flipping, silently communicating among themselves to open doors with their handles \u2014 there was definitely more fear and consternation than amazement in the air. Some of the reactions implied that an agile door opening robot might be a nice pet to have around but shortly after that scary memes were everywhere on internet. General consensus on Twitter was \u201cterrifying\u201d. The videos were shared by thousands of people on Facebook and constantly received hysterical comments about \u201cthose demonic machines\u201d. Many claimed that it is the end of the world as we know it and the beginning of robot domination over human race where we will be enslaved by a new form of species positioned above homo sapiens in the food chain: Artificial Intelligence (AI).\n\nBut Boston Dynamics timing to advertise SpotMini was uncanny because it coincided with the release of the forth season of Black Mirror (A sci-fi anthology series exploring a twisted, high-tech near-future where humanity\u2019s greatest innovations and darkest instincts collide, ndlr). SpotMini bears a strong resemblance to the murderous, repulsive, cruel, ultrasmart and hyperstrong robot dog with beyond human capabilities (and sensors), in the episode \u201cMetalHead\u201d. The plot is dark, very dark, as you might have guessed, where humanity is in danger once again and it is the apocalypse. While Boston Dynamics would like to \u201cChange your idea of what robots can do\u201d, many people would just like them to stop for a second before they and those other Silicon Valley geeks/coders take their research any further, before they ruin our future for the sake of making a difference (and profit).\n\nI am not a pessimist. I am one of those (maybe naive) optimists about the future. I believe individuals and highly skilled group of visionaries with good intentions (those who are in it for the right reasons in American reality show language) will shape a better future. I believe in the power of AI and other disruptive technologies in addressing the needs of the ever growing human population & in resolving our biggest challenges such as global warming, inequality, health care, clean water, energy\u2026 I believe AI will shift paradigms in our evolution and have more profound implications for humanity than electricity or fire.\n\nI believe empathy, compassion and all human virtues that make us who we are will become the corner stone of our evolution. Human-centricity, emotional intelligence, relationships and experiences will be much more valuable than your self driving electric car in not-so-distant future. I also know we will augment ourselves significantly thanks to AI and will eventually merge our intelligence through brain-machine interfaces, hence the question whether AI will dominate us will probably become irrelevant if WE make AI progress in the right direction.\n\nOur journey to future is accelerating, every single day. We are trying to get better in maintaining Earth a resourceful home, in space exploration to find a plan B home, in DNA modification to treat otherwise incurable diseases, in quantum computing, in augmented/virtual reality, in targeted drug delivery, in 3D printing, in advanced robotics, in renewable energy, in internet of everything, in everything. The destination is a world with seamless intelligence but if we know where we are heading, then we also need to know the path that will take us there. Preferably a path of intelligence and good intentions rather than the ones looking very much like the dark side of the Force.\n\nThrough this blog, I would like to dig, discover and share how the future will/may be shaped with the new technologies, and why am I optimistic about it\u2026 I hope you will enjoy.\n\nIf you found this story interesting, feel free to clap \ud83d\udc4f\n\nIf you can\u2019t get enough, follow me on Medium \ud83e\udd17"
    },
    {
        "url": "https://becominghuman.ai/artificial-intelligence-the-skynet-of-tomorrow-6e1ff634858f",
        "title": "Artificial Intelligence \u2014 The Skynet of Tomorrow? \u2013",
        "text": "Are the machines ready to take over?\n\nArtificial Intelligence, or A. I. as we like to call it, has been a major buzzword since the advent of Machine Learning. There have been many different views on the rise of A. I. \u2014 both positive as well as negative. Some take it as the revolution that has and will completely change the way we live yet some take it as an attack on our way of living. Most of these views are influenced by science-fiction literature, movies or TV shows that we\u2019ve seen \u2014 Terminator, Battlestar Galactica, The Rise of The Machines, Westworld and so on.\n\nWell, not really \u2014 at least not yet.\n\nThe most amazing things that the A. I. of today can do is recognize faces or objects, classify data, or predict some quantities \u2014 after extremely intense training that is. So, it\u2019s safe to say that we\u2019re far from such an Artificial General Intelligence.\n\nBut, these tasks for which the systems have been trained on a good amount of data can be done very effectively."
    },
    {
        "url": "https://becominghuman.ai/how-a-country-can-be-transformed-using-artificial-intelligence-by-utpal-chakraborty-72ac5384cf56",
        "title": "How a Country can be Transformed using Artificial Intelligence, By-Utpal Chakraborty",
        "text": "The business world has already adopted it in a big way and started integrating AI into pretty much everywhere. But what about our Governments? And very specifically Governments of developing countries.\n\nI will try to put some of my thoughts on how a country can be transformed implementing Artificial Intelligence in different government sectors.\n\nHow AI can be deployed to improve the quality of life of the citizens?\n\nIt is predicted that after the domination of Artificial Intelligence in different areas of our life, people will lead healthier and even longer lives. With AI, we will have better and personalized health care, enhanced ways of food production and better recycling techniques and methods. AI eliminates many mundane repetitive tasks that makes up space for actual productive tasks in everyday lives of humans.\n\nHow much does the rural areas need AI?\n\nAI can change the face of rural healthcare and education. Some countries has already stepped into AI tutoring in their rural societies but personalized health care advises by an AI system has a long way to go before it can bring \u201cGood Health for All\u201d in rural areas in a country like India and many other developing countries.\n\nWhy we need to empower our farmers with AI?\n\nMany industries are interested in how technology can change their ways and improve output production. Modern Agriculture is one of them. Farmers will be making informed decisions on how to put water, soil and other resources effectively to proper use. For example, installed cameras in the green houses would take instant pictures of the crops, AI algorithms will assess those images, identify problems and present conclusions on their own. AI has become a main asset in tech companies but the applications of AI in agriculture is yet to be pursued vigorously specially in countries like India where farmers are unfamiliar with any new technologies like AI and hesitant to put their crops under a computer system.\n\nAbout 40 percent of the hospitals in developed countries plan to leverage artificial intelligence within next 2 years. And the companies that are already using Artificial Intelligence have a promising future in front of them. Artificial Intelligence has the most substantial impact on clinical treatment, patient diagnoses and population health. Apart from the actual appliances of the AI upon health care, it helps intensely with medical record systems. Also, hospital management, physician workflow and security systems often look to AI for decision support.\n\nData collections, data analysis and interpretation, decision and control are some of the superb functionalities of an AI system. With these algorithms, traffic engineering has taken a leap forward. The program can fully account for the complexities of traffic systems and can be extremely accommodating to the traffic police. There is so much ongoing research in the field. We are aware of the efficient image analysis technology, traffic control is where the decision making based on video recognition has already been introduced in many developed countries. Countries like India should unquestionably leverage these kind of techniques not only in the metro cities but even in the smaller cities and towns where traffic control is becoming increasingly unmanageable.\n\nIs artificial intelligence shaping the future of energy?\n\nWhile producing energy out of other resources is difficult, integrating it correctly is even more challenging. A country might be producing too little of the energy when it most needs it or it might be producing too much of it when it least requires. Every government needs to take the demand side flexibility of energy into account. An Artificial Intelligence system can determine where the most energy is going to, which sectors needs it the most, what it is being used for, even the time at which it is being used. For an example, the demand for electricity will become intelligent that way and after which no power line will ever be overly sacked. The demand for energy is steadily growing, what we also need to do is limit industry\u2019s impact upon the environment by implementing AI systems to control it.\n\nThe Rise of Artificial intelligence in Legal\n\nLaw in countries like India probably is the least digitalized profession even today but it is high time that it embraces the opportunities that Artificial Intelligence bring with itself. AI is overtaking the legal research with its ability to go through a voluminous information within seconds. The legal contracts designed by the lawyers can be analyzed and dissected by the AI systems. These AI systems must be designed specifically for non-tech savvy people like the legal professionals.\n\nWhere is Artificial intelligence headed in education?\n\nWill it help the teachers or make them obsolete? AI enhances classroom instructions in many ways. AI is being exclusively used by the teachers in planning and grading for quite a while now. However, it is time to upgrade the use of AI in education by personalization. Every student whether struggling or not deserves individual attention. Not every one of them find it easy to cope up with the lecture therefore, AI based applications are especially designed to target the weak spots in an individual\u2019s concept. In fact, many of these apps can be supplied to the students in their smart phones to assist them with their daily studies.\n\nRole Artificial intelligence can play in Women Empowerment and safety\n\nWith the revolution of artificial intelligence, characterized by big data and cloud computation a rise in female employment is expected to grow exponentially. This is because AI will be able to alleviate the main hindrances that women face due to socio cultural issues. Cloud computations will enable the women to get a job or even rise as entrepreneurs without having to leave their home or place. The technology will be exceptionally well utilized in emerging countries. Also, female social and general skills will be enhanced. This is where the AI tutoring kicks in. Women will have a direct access to education in areas where they are not allowed to leave their homes or can\u2019t afford to for basic or higher education or even skill based education.\n\nFemales can even have a sense of security and independence while moving around the city. Such devices are being created that can help women out of any red alert situation and escape any possible sexual assault. For example, many wearable devices powered with AI are designed to help women alert the guardian networks or their friends and family if they find themselves in an uncomfortable situation. We can efficiently leverage AI along with few other cutting edge technologies for the surveillance for women safety in almost every nook and corner of our cities and villages; it\u2019s hard but absolutely doable.\n\nFrom voice recognition to language detection, deep learning is revolutionizing the way we interact with technologies. Deep learning enables the computer to learn from an example. Lately, it is being used for wild life conservation. Currently, some ideas are being pondered over. For example, airborne devices equipped with high tech AI algorithms are suspended in the sky to protect elephants and rhinos from hunters. The devices use special image recognition techniques and target the criminals at the spot. Many surveillances like this one are being introduced to protect wildlife to the max.\n\nScientists and researchers have admitted that even after millions of years of existence, we have not been able to discover every plant, animal or insect on the planet. We will have to wait another 500 years to collect all the estimated number of species. Not to mention the amount of men power and resources that will be used in the process. However, with the AI, this job can be made drastically easier. AI instruments can be dug in a pond, placed over an animal or suspended in the air. Existing AI algorithms can tell apart 5000 different kinds of plant and animals species. These undiscovered species are contributing to the eco system and we don\u2019t even know about it. After discovering them and collecting facts about them, we might be able to improve their breeding and enhance their impact upon the environment.\n\n[More areas where AI can be leveraged by Government will be listed in PART-II of this series]"
    },
    {
        "url": "https://becominghuman.ai/building-a-django-post-face-detection-api-using-opencv-and-haar-cascades-dcf4e0e5f725",
        "title": "Building a Django POST face-detection API using OpenCV and Haar Cascades",
        "text": "I\u2019ve been wanting to work on face detection for quite some time now. Mainly because it sounds so intriguing. In this blog post, I cover the aspect of face recognition via. Django using the HAAR Cascades framework offered via. OpenCV. This is basically the guide to build an API for the same which can be deployed later as per your convenience. I haven\u2019t deployed it as such and have run it on my local machine itself.\n\nIn the blogs that I have covered till now, I have hit the areas of sentiment analysis(via tensorflow), stock-market forecasting(for Bitcoin) as well as some guides on Prediction using ML and Numpy for stock trends.\n\nThis is the list of tools/packages/libs you\u2019re required to possess. Our API will basically read a URL stream that has the image uploaded and accordingly return the JSON response with the detected faces marked.\n\nOnce you\u2019re through, we start of with creating a project named facedetection\n\nThis creates the project named facedetection in the respective directory.\n\nDo a CD in this project directory to get into the newly created project folder.\n\nwill get you inside the project folder which will have namely 2 files \u2014 manage.py and a sub-folder with the same name as the root folder i.e facedetection\n\nNow, we write a simple Django command to launch the application\n\nAs you can see,\n\nlaunches a new folder inside your parent folder with the name face_detector.This is your application basically which has a boiler plate in that there are various files contained within this folder \u2014 views.py, admin.py, tests.py which come as django imports and are necessary components of any Api wrapper you\u2019d want to build. We\u2019re mainly concerned with the view.py file. This view implies your API\u2019s front-end in a sense and all the coding has to be done within this file primarily.\n\nIf you were to be interacting with the back-end as well, there are other files you would have been concerned about on top of views.py \u2014 namely models.py\n\nMigrate to facedetection/face_detector/views.py and you can open the views.py file that\u2019s basically empty at this point. We start of with initializing necessary imports namely.\n\nOk now we have to define the face_detector_path to the .xml file that comes along with Haar cascades as you install it. You can either look for it directly in the folder where you might\u2019ve installed haar cascades inside the open cv folder of if you end up in some trouble trying to locate it just like I did, you need not worry. Just import it externally from this git repo. Go for the frontalface_default.xml and frontalface_default_alt.xml files and save them somewhere where you can later locate them easily.\n\nNext step, store the path to this file in a variable.\n\nUp next, we define the function to read the request URL in JSON format \u2014 this is the URL where we\u2019re uploading the image to be detected for facial recognition later.\n\nI am breaking it down in 3 segments to try my best to simplify the process.\n\nMake sure you specify the @csrf_exempt import right before when you define the function\n\nA Quora user was kind enough to break it down more simplistically.\n\n3. If the method requested is POST just like we wanted, we check if the image exists within the POST stream and if it is not None and we then proceed to read the image as a stream using a read_image() function that I will specify later.\n\n4. If there\u2019s no URL provided, a simple message pops up telling the same. Finally, we return the JsonResponse value for the default variable which is basically the stream of data that\u2019s incoming in the form of the image that is being read using a read_image function which would be created later"
    },
    {
        "url": "https://becominghuman.ai/how-artificial-intelligence-is-changing-seo-a253baa6912c",
        "title": "How Artificial intelligence is Changing SEO \u2013",
        "text": "If you are in the marketing or business world, then you need to know about SEO and how AI is changing it. This is crucial to prevent yourself and your business from missing out on incredible opportunities. The following AI trends are changing SEO for the long haul, and they don\u2019t show any signs of slowing down:\n\nAt its heart, SEO is about making great content that is relevant to your audience. By doing this, the idea is that Google will rank you higher in the search engines. They do this by searching it and finding key words that are similar to what people are already looking for. However, there is much more to SEO than what you see on the page.\n\nDevelopment is also a huge part of SEO now thanks to Artificial Intelligence and the improvements in computers to be able to run complicated programs and search through data, display it, and report analytics about it. The AI surge has created a robust development community of people who can help you turn your site into a shining example of what great design and programming looks like. Web development can be used to automate many functions that help the user have a better experience, which does improve SEO at the end of the day.\n\nSEO is dictated by algorithms. These algorithms are optimized highly to be able to search through huge amounts of data and determine characteristics about it, sort it, and more. An algorithm is simply a step by step set of instructions that a computer can understand.\n\nThanks to the rapid progressions in AI, SEO is getting smarter by continually learning from each iteration. What this means for the SEO industry is that it is continually becoming harder to game the system. Long gone are the days where companies are going to be able to put out content that is not relevant to their users and employ tricks to get to the top of the results pages.\n\nWith AI, the entire display process of the content on the page can be automated. This allows companies to choose which piece of writing, image, or video will appear to a particular user when they land there. This can be helpful when you have different reasons that someone landed on your page. For instance, if someone came to your page from the keyword \u201cred shoe\u201d it would be different than \u201cblue shoe.\u201d Therefore, you may want to have different content even if the underlying category (shoe) is present. This creates new marketing and targeting opportunities.\n\nA lot of companies that do SEO for their main traffic driver focus on having ads that display to their visitors. They get paid for this without having to charge the user anything. With AI, rapid matching between ads and the right sites will get more popular, making it easier to achieve greater revenue with the ads on your site.\n\nOne of the most exciting things that AI allows you to do is test different versions of a landing page or other offers. For instance, it can automatically display a version \u201ca\u201d and \u201cb\u201d with different headlines. After analyzing the results, you can see which version is converting better.\n\nWhen someone lands on your page due to SEO, you still want to know what they are looking for specifically. You can use AI to create quizzes that pop up automatically and allow the user to tell you more about them.\n\nWhen it comes to SEO, AI is one of the biggest factors changing it today. It isn\u2019t just on the back end either will all of the code. It changes how the entire SEO ecosystem functions. If you are an entrepreneur or marketer, you need to understand the dynamics above and work to make your mark on the new world of SEO so your company\u2019s revenue can grow. Read more on Niche Data Factory"
    },
    {
        "url": "https://becominghuman.ai/why-cpg-companies-need-ai-in-their-revenue-management-strategy-e35f1bca34a1",
        "title": "Why CPG companies need AI in their Revenue Management Strategy",
        "text": "Why CPG companies need AI in their Revenue Management Strategy\n\nCompared to other organizations from other industries, CPG businesses are typically more reliant on decision-making efficiency to remain competent.\n\nSince important decision-making mostly revolves around strategizing pricing, marketing campaigns, sales trade promotions, assortment optimization and inventory management, Revenue Growth Management (RGM) became a key aspect.\n\nThe main objective of RGM or Net Revenue Management (NRM) or Strategic Net Revenue Management (SNRM) etc. is to profitably optimize key strategies around pricing, new product/market investments, marketing/sales campaigns, logistics etc.\n\nIn fact, companies which executed a powerful RGM strategy have realized a sizable market share growth and tangible gains in business top and bottom lines.\n\nBased on the brand objectives, available market and consumers insights, competitive intelligence etc., RGM executives recommend the best-fit strategies for the relevant departments and officers.\n\nNeedless to say, the accuracy of these recommended strategies has a huge impact in realizing the overall ROI and driving revenue. However, in most CPG companies, strategies are developed based on low-quality data (historical data, data from limited sources etc.), as a knee-jerk reaction to competitor\u2019s behavior or learnings from past experiences.\n\nIn some companies, an analytics tool is already being used, but usually the insights obtained are extremely generic and non-actionable. And the ones which provide meaningful intelligence are generally too sophisticated and hard to incorporate in daily workflows.\n\nAs a result, there are high chances for missed growth opportunities, ROI, revenue leakages etc.\n\nHere are a few ways Artificial Intelligence makes insight-driven revenue management a reality:\n\nCPG companies generate humongous amount of big data every day \u2014 both from internal and external sources. Thanks to the rapid advent of digital media, a considerable amount of this data (social media interactions, search analytics etc.) is in unstructured format.\n\nWith the growing digital population and increase of digital marketing budgets in the CPG industry, more and more brands have recognized the necessity of social listening and analyzing digital consumer behaviour. Some other notable examples of unstructured data also include data from weather, TRPs, etc.\n\nTraditional analytics tools won\u2019t have the capability to process this type of data.\n\nAI technologies like machine learning algorithms, predictive and prescriptive analytics can harness enormous amount of both unstructured and structured data at a time and convert it into real-time insights.\n\nWhile collecting data from different sources improves data quality, you can obtain actionable intelligence only when a powerful analysis is run over this data.\n\nIn a fast-moving and competitive business environment, RGM executives should spend less time understanding the analyzed data and more time getting actionable insights from the analysis. However most traditional analytics tools don\u2019t support such advanced mechanism and users need to go through numerous reports and toil before arriving at a conclusion.\n\nOn the other hand, AI-powered advanced analytics like predictive and prescriptive analytics implement a fact-based approach. By identifying trends, patterns and anomalies in the feeded data, these technologies deliver crisp and actionable recommendations and insights to users, thereby enabling a convenient and faster decision-making.\n\nUnlike traditional analytics which give information on what happened in the past, these analytics forecast the future and provide recommendations in crucial \u201cwhat-if\u201d scenarios.\n\nDeveloping fact-based long-term and short-term business plans becomes stress-free with a great reduction in mis-calculations.\n\nAs an RGM executive you can,\n\nKnow more about Acuvate\u2019s customized Artificial Intelligence solutions for CPG companies."
    },
    {
        "url": "https://becominghuman.ai/is-the-turing-test-a-valid-test-of-artificial-intelligence-6695b6e4304",
        "title": "Is the Turing Test a Valid Test of Artificial Intelligence?",
        "text": "Hey everyone! In this series on human robot interaction, I\u2019ve talked a lot about social robots and how cognitive systems can help create machines that think like humans, solve problems like humans, and live in everyday society like most humans. All this talk about artificial systems is interesting in its own right, but how can we evaluate whether we\u2019ve created an artificial systems that are like humans? Today, we discuss one particularly early approach to evaluate AI systems. Today, we consider the Turing test. In particular, is it a measure of artificial intelligence or not? The Turing test has a bit of a contentious history, and in this post I\u2019ll explore the criterion for passing the test, address one particular point of contention by discussing whether or not its a good test for measuring the intelligence of artificial systems, and finally I\u2019ll close with a discussion on ethics.\n\nAlan Turing was an English computer scientist who was one of the first people interested in machine intelligence. He presented the topic in a couple different ways, but he reformulated his approaches to this basic question: \u201cCan machines do what we (as thinking entities) can do?\u201d [2]. He imagined his test to be similar to a hypothetical party game, the Imitation Game. In that game, one man and one woman would hide in separate rooms. People outside of both of the rooms would ask questions to people hiding. The hiding man would try to respond as if he was the woman, and the woman would respond as if she was the man. The task was for the hiding people to trick the crowd as to who was behind which door.\n\nIn the Turing test, an examiner sits behind a divider, and types questions to an unobserved entity. The entity (either a computer or a human) responds in text as if it was a human. If the examiner cannot distinguish a human response from a computer responding like a human, the computer passes the Turing test. Is this, however, a test of intelligence, or is this a superficial imitation game? In my opinion, I do not think that the Turing test is a valid test of intelligence, because intelligence is not required to pass the test! Although a system can pass for human, that does not mean that it has the same conscious experience as humans. This is important, because when we talk about capital I, Intelligence we fundamentally imply the ability to understand and operate on knowledge stored in our mind. There has to exist mechanisms for representing the world, and mechanisms for changing and updating our models, and effecting change. Furthermore, cognition happens within the context of physical environments because the environment grounds our thoughts and beliefs.\n\nApart from this theoretical objection, we\u2019ve seen a few chatbots pass (supposedly) the Turing test (ELIZA, and PARRY) while they themselves only being parodies of intelligent systems[1, 3]. ELIZA for example, attempted to be a therapist, so right off the bat it constrained what types of things the system was able to talk about. Within that domain, it became easier to fool people, because people didn\u2019t start talking to ELIZA about how the Patriots should have won the Superbowl this year. Many people who conversed with ELIZA and PARRY could not tell that they were talking to a computer.\n\nNow, let us consider a thought experiment. Imagine you exist in a room totally closed off from society. A small opening exists at the bottom of one of the walls so people can slide English messages to you, for you to translate into Chinese. This scenario is known as the Chinese Room, depicted in the image above. In your room there\u2019s a table that tabulates exactly how to transform English input symbols (which you do not understand) to Chinese symbols (which you also do not understand). When the people on the outside read your translation, they are amazed that you have such linguistic acuity, and they declare you intelligent. Without an understanding of what you are doing, however, I would argue that you are not truly intelligent. Just because you can imitate intelligent behavior does not mean that you yourself possess the qualities of intelligence. Therefore, the imitation game, the Turing test, is not a valid method for evaluating AI systems!\n\nThere are also ethical questions to consider here as well. Suppose there exist machines that pass the Turing test, but only by imitation. Is it an upstanding thing to create machines that encourage (some would say trick) people into ascribing desires, beliefs, and intentions to them? Should we, from a societal perspective, promote interactions with entities that trick us into ascribing intelligence to them? When a driverless car runs over someone\u2019s pet, or worse, another human being, should we then act as if it knew what it was doing? Should we be granting citizenship to robots that only pretend they know what it means to be a citizen? I think you get the point. The Turing test, as simple as it is, poses very serious questions we as a society need to consider. Imitation systems are unethical because they project an identity about themselves that does not reflect reality. They are fundamentally deceptive. I think that if we stick to narrow minded views on intelligence we run the risk of tricking ourselves to grave disappointment. We need to focus on general theories of intelligence that utilize machine learning, but within the context of modeling cognitive phenomena. That way we can engage with psychologically inspired, human-level intelligent agents \u2014 not just shells of ourselves. This, I believe, is a better path forward. Machine learning outside of the context of cognitive systems is another imitation system. In the past, we\u2019ve written simple chatbots and programs that can recognize cats and dogs, and now machine learning-based systems play some games better than humans, diagnose disease, drive cars, make financial decisions on behalf of banks. To be clear, these are good and admirable achievements, but I think they should be contextualized in computational models of the mind, so as to avoid the ethical issues I\u2019ve just presented.\n\nIn this post, I\u2019ve considered the Turing test and discussed whether or not it is a good measure of intelligence for artificial agents. I\u2019ve presented my thoughts from a cognitive systems perspective and found that the test does not require an agent to possess intelligence, therefore it is not a good measure of it. Although it does not have the ability to determine intelligent entities from unintelligent ones, the test forced us to consider some ethical questions regarding agents that imitate human likeness while the themselves being anything but. I\u2019d be pleased to know your thoughts! Please comment, and clap!\n\n[2] Harnad, Stevan. \u201cThe annotation game: On Turing (1950) on computing, machinery, and intelligence.\u201d The Turing test sourcebook: philosophical and methodological issues in the quest for the thinking computer. Kluwer, 2006.\n\n[3] Weizenbaum, Joseph. \u201cELIZA \u2014 a computer program for the study of natural language communication between man and machine.\u201d Communications of the ACM 9.1 (1966): 36\u201345."
    },
    {
        "url": "https://becominghuman.ai/being-smart-in-hiring-is-easy-now-bc12f89bd62",
        "title": "Being smart in hiring is easy NOW. \u2013",
        "text": "Hiring the right talent in today\u2019s world of digital disruption can be a challenge without assessing the skills, personality, and knowledge of the candidates. So why not use AI with emotional intelligence to source and screen potential hires?\n\nOn receiving an applicant\u2019s resume, the hiring manager used to Google them. The Strong candidates will have three or more first-page search engine results leading to their LinkedIn profile, Google+ page, Twitter profile, blog and social media resume or personal website. The new age tools like DeepSense have made an initial evaluation at scale on few essential attributes super easy.\n\nUsing the dashboard access feature the recruiter, or the hiring manager can not only check culture fit of a potential employee even before speaking to them but also rank and shortlist all applicants on soft skills based on customized job search criteria. One can also create individual candidate reports in one click.\n\nAll you need to do is install and enter anyone\u2019s email ID, Twitter profile, or phone number on the Chrome extension, or website. In a few seconds, based on a person\u2019s digital footprints from the social and open web, DeepSense builds a comprehensive profile of a person, collating information on a person\u2019s interests, work history, and education. The tool ranks a person\u2019s personality traits on attributes such as attitude and outlook, stability potential, action-orientedness, general behavior, teamwork skills, need for autonomy, and technical and managerial role fit.\n\nSome of the nonconventional candidate\u2019s impressions that are assessed using DeepSense are:\n\nThe social conquest in hiring has already started. It is about time to leverage some of these new age tools and make one of the most complex tasks of recruitment easier and faster."
    },
    {
        "url": "https://becominghuman.ai/its-time-to-set-up-your-organization-s-intelligence-unit-278348733492",
        "title": "It\u2019s time to set up your organization\u2019s Intelligence Unit",
        "text": "It\u2019s time to set up your organization\u2019s Intelligence Unit\n\nThis story was first published in The Aleph Report. If you want to read the latest reports, please subscribe to our newsletter and our Twitter.\n\nSome days ago, new evidence showed up in the Uber\u2019s Waymo case. I won\u2019t talk about the situation per se, but what came to light was shocking.\n\nRichard Jacobs, former head of Uber\u2019s Global Intelligence unit, explains in a formal letter, how Uber\u2019s Intelligence operations work.\n\nThe letter is worth reading. It covers, not only Uber\u2019s Intelligence unit\u2019s structure but what they did and how. Some revelations are shocking due to their illegality. Others are striking because of how advanced they are.\n\nUber\u2019s Intelligence operations went from data leak protection to counterintelligence, cyber attacks, covert operations and infiltration. The list is exhaustive.\n\nI won\u2019t delve into the illegality of Uber\u2019s acts. Nor I\u2019ll defend them. What impressed me, above all, was the level of sophistication of the whole operation.\n\nAs I\u2019ve mentioned before, our current world is highly connected. Such global stage enables world competition on a scale we haven\u2019t seen before. Business, as usual, doesn\u2019t cut it anymore. I\u2019ve argued that to survive in the current competitive landscape, data and intelligence is a must.\n\nWithin this frame, having an Intelligence unit isn\u2019t such a crazy idea. Prominent organizations have had, for years, Market Research and Competitive Intelligence groups. In a way, they\u2019re the precursors of a capable Intelligence operation. Smaller companies have outsourced such capabilities on a need to basis. There is a small but profitable market for private intelligence agencies (PIA).\n\nThe Uber case highlights two things. The first one is that Intelligence operations aren\u2019t unique to corporations. The technology industry is becoming such a cutthroat space, where any advantage can have a massive impact.\n\nThe second one is that outsourcing these operations isn\u2019t cutting it anymore. Building your own Intel unit is a necessity.\n\nThe question, though, is who needs this kind of intelligence? A decade ago only world corporations would need such services. Here are some factors that can determine if an organization needs Intelligence or not.\n\nIntelligence units respond to the need for gathering information that serves a set of organizational goals. The first stage, before establishing any team is to have a distinct idea of what do you need the information for.\n\nOnce with a clear mission, we can set up the collection process. These are the inputs of the process. There are many ways of collecting information some of which are:\n\nThe easiest to deploy is OSINT. It\u2019s not only free in most cases, but it\u2019s also legal. On the other hand, Human Intelligence can border illegality depending on the country and regulations. Signal Intelligence is by far the most expensive one. One that is certainly illegal for anyone except government intelligence agencies.\n\nInformation gathering has always existed, but organizations need to put particular attention on how they gather it. Some practices, while legal, border the unethical. Others are outright illegal.\n\nSetting up a collection process isn\u2019t a one time exercise. The organization needs to create a stable, repeatable method to keep the information flowing. This includes not only the inputs but a way to store the data.\n\nOnce the collection process is in place, then we need to filter it. Collection will create an inhuman amount of information. The team needs to process the sources, normalize data, test its relevance, etc.\n\nAfter cleaning the raw data, the unit integrates many pieces of Intel into a coherent picture. The assembled information will be then bundled under different formats, depending on the needs of the organization.\n\nIt\u2019s important to understand though, that Intelligence only informs decisions. What actions to take with the gathered Intel is management\u2019s prerogative. And as such, ethical questions rest upon executives and not the actual Intelligence.\n\nHow will a regular business take advantage of an Intelligence Unit? Here is a fictional example of the retail industry.\n\nImagine an international brick and mortar retailer. The business has a global footprint with multiple stores in many different countries. They want to keep hold and grow their current markets. At the same time, they want to expand beyond their present countries, opening new regions.\n\nThe top executives have decided to start a global Intelligence Unit to support all locations with on the ground Intel. Each country is expected to consume and factor in Intelligence reports from the unit.\n\nThese are some of the challenges they\u2019ll need to resolve.\n\nThe organization operates hierarchically. Information needs to flow from the headquarters to the regional managers, to the country managers, to the city managers.\n\nThe first step for the Intel team is to set up some collection processes. They want to be able to monitor specific things:\n\nThe team will also establish a small Human Intelligence operation. They will assess customer\u2019s buying patterns on the ground. They\u2019ll also act as mystery shoppers for other brands. This will allow them to establish competitor\u2019s metrics like top sales, customer estimation, average customer ticket, etc.\n\nAll the intel will be stored in an isolated and encrypted computer network. Access to it will require specific gear to prevent unauthorized access. This isolation will minimize any leaks if hackers compromise the corporate system.\n\nThe team will create periodic reports that will send to the local heads. Each report will contain the following:\n\nThe operation is an evolving one. The more information gathered, the more Intelligence can be generated. At first, the Intelligence team will push information to each local player. Initially, they also operate as sensors that determine what would be useful for the local organization. With time, they\u2019ll start incorporating new reports and intelligence based on the unfolding needs of each local team.\n\nAs I commented before, Intelligence operations aren\u2019t for big corporations anymore. Global competition, data gathering, and stiff competition are forcing organizations to be smarter. If your rivals are making informed decisions and you\u2019re not, you\u2019ll lose and become irrelevant.\n\nThe irony is, companies are already digitalizing their businesses and are incorporating more and more data. The need for data is allowing them to build comprehensive collection processes already. Some are already using it to create competitive Artificial Intelligence systems. Why not use it for Intelligence purposes too?\n\nLeadership must enforce caution around building their Intel operations. Not everything is acceptable. Some methods are illegal or borderline unethical. Is imperative that organizations establish a clear ethical code around their Intelligence efforts. This will prevent unnecessary investigations or legal complications.\n\nThere is an information war happening as we speak. Those that don\u2019t arm themselves will be victims of the current information warfare. In the best case scenario, they\u2019ll see an erosion of their market share. On the worse case scenario, they\u2019ll be wiped out from the market. Don\u2019t wait. Start building necessary Intelligence capabilities now."
    },
    {
        "url": "https://becominghuman.ai/learning-to-believe-the-unbelievable-fortifying-ourselves-for-a-vr-future-f10a3739a740",
        "title": "Learning to Believe the Unbelievable: Fortifying Ourselves for a VR Future",
        "text": "As humans, we are accustomed to suspending our disbelief. Indeed, we\u2019re known to indulge in it. Each time we dive into a book, a movie, a video game, a TV show \u2014 even a spiritual flight-of-fancy \u2014 most of us are willing and able to disengage from the pedantry of our everyday judgment, and allow ourselves to be convinced by things that are less-than-absolutely-convincing\u2026\n\nThis coaxing is a consensual arrangement. I allow you to present me with the improbable on the proviso that it is entertaining, or educational, or uplifting, or philosophical \u2014 i.e. my pay-off is that I am emotionally stimulated in some way. I don\u2019t need to scrutinize a movie in its every detail, what is important when I watch it is that I enjoy it and it makes me happy (or scared, or angry, or sentimental!).\n\nOf course, when we suspend our disbelief, it isn\u2019t always in to be absorbed in something totally fanciful. Some creators can dance very close to the line of reality, creating gritty episodes in which we recognize vivid, real lives. These things are not entirely \u201cimprobable\u201d in their content, but it is nevertheless improbable that we might get so intimately close to them. It is our contextual proximity which adds to the necessary incongruity of make-believe. Again, we suspend our scrutiny of this improbability.\n\nNow admittedly, this is a fairly elementary assessment of our relationship with the \u201cunreal\u201d, but it captures some of the key facets of the dynamic. When we assent to suspend our disbelief, we suspend our usual scrutiny because our experience leads us to expect its value to be emotional or intellectual, but not practical. On the contrary, in real life \u2014 for clear purposes arrived at via evolution \u2014 it is important that we scrutinize all scenarios, people, objects, relationships, etc. In this domain, we are rather more concerned with practicality.\n\nSo, what\u2019s my point? Simply put, I\u2019m qualifying the idea that there is some definite clean air between real things in our lives, and things that are the objects of our suspended disbelief. Our attitude towards the two categories is importantly different.\n\nImagine then, being asked to simultaneously suspend disbelief, enter a non-reality and understand this reality as having the same practical use as the real world. The proposition feels a little like being asked to rub your stomach and pat your head at the same time \u2014 but this is a future that VR developers are driving towards.\n\nThis week TechCrunch reported on a Swiss company, called Imverse, who they say have, \u201cthe power to make VR seem much more believable.\u201d The believability part is probably up-for-debate, but the plain fact is that company\u2019s technology can render the users human form within a virtual reality. Basically, you look down and you can see an avatar of your own body, which mimics your movements.\n\n\u201cIs this me?\u201d \u201cOr isn\u2019t it me?\u201d \u201cShould I believe and scrutinize?\u201d \u201cOr suspend and enjoy?\u201d\n\nMy instinct says that we\u2019re still at play in this environment. This is not the right and truthful us. Like the Ship of Theseus, or a perfect stroke-for-stroke copy of a Leonardo da Vinci painting \u2014 no matter how like the original it seems, we feel that at its core it is a deceit. Therefore, we can desist with our scrutiny (which is much easier, after all).\n\nYet, increasingly, believability is not the determining factor of whether we shouldscrutinize or not. Instead, this must now come down to whether our actions within a virtual or mixed reality environment could have an effect on our actual real lives. And we know that these practical effects of VR environments are just around the corner.\n\nAnother example: VR developers introducing convincing sensory experiences, like touch, to a virtual environment. Now, touch is not something we would ordinarily experience within the average \u201csuspended disbelief environment\u201d. Touch is real, and undeniable. It should be a thing of scrutiny. Yet within a VR world, we could expect two opposing forces to play against each other here: on the one hand, the injection of non-virtual reality (touch being a tangible facet of the real world), and on the other hand this very fact urging us to further suspend disbelief, and therefore abdicate our usual scrutiny of such true forces. The first reminds us of the practical, the second promises us emotional pay-off.\n\nIf this all sounds a bit head-scratchy, here\u2019s an illustration. In his latest book, Jeremy Bailenson, talks of what in real-life we call the \u201cMidas Touch\u201d. This is a well-documented phenomena whereby (for example) wait staff who touch customers on the shoulder tend to get bigger tips than those who don\u2019t. Now, without going into experimental detail (which can be read in an excerpt here), it has been shown that virtual touch (i.e. indirect human contact in a virtual environment) can replicate this effect.\n\nWhat\u2019s the significance? There\u2019s plenty as I see it. If we\u2019re naturally in \u201cplay mode\u201d when we are in a suspended disbelief environment (as we usually are), we might \u2014 for want of a better term \u2014 roll over much more easily. Be less aware of any practical consequences that should be up for examination. Importantly, we could be less suspecting, and less aware of things sent to manipulate and/or mislead us. In other words, if VR is set to become as prolific as we\u2019re often told, in the future we might need to fight our well-trained instincts and, as my mother would say, \u201ckeep our wits about us.\u201d Not because things seem so convincingly real but, on the contrary, because they remain unconvincing."
    },
    {
        "url": "https://becominghuman.ai/i-want-to-become-human-again-d4db64d276c4",
        "title": "I Want to Become Human Again \u2013",
        "text": "I Want to Become Human Again\n\n2018 seems to be the year of Artificial Intelligence (A.I.) and the awareness towards this field is garnering more and more attention. As much as I personally would want to believe that Artificial Intelligence could provide a lot of greatness to the world especially in industries where they are needed the most \u2014 I do have a profound respect for what it could become and how powerful it could be.\n\nBut, I cannot help but be a bit anxious about it thinking ahead. I have seen being a kid growing up in the 1990s and being at a crossroads with the power of technology.\n\nTechnology evolving with the use of text messages and emails to communicate, the cell phone evolving through the early 2000s before the iPhone in 2007 changed everything. Facebook being known to the world truly in 2007; Twitter with the same story in 2009. Google being a part of daily life all throughout that time having access to information readily available. I love it. I have embraced it.\n\nNow, all of this technology has been linked to more violence due to less human interaction and understanding of one another, potentially a reason for why communication has become an issue.\n\nEver go to dinner, and see a couple supposedly trying to connect with each other but yet drilled in to their phones not saying a word to each other, on a Saturday night?\n\nI cannot help but wonder, how much is too much?\n\nPlatforms such as big technology with Google, Apple, Tesla and Amazon focusing on building A.I. platforms that could radically change our lives, work and revamp industries. Jobs being automated, the rise of self-driving vehicles, technology answering our questions as we sit back, corporations not needing humans in the future, and potentially, my biggest fear:\n\nUs, as humans, using A.I., to eliminate our humanistic flaws.\n\nAs humans we strive for perfection, we cultivate it, we write about and cover it up as being the best we could be as beings. Striving for perfection is something that we always want to attain, as much as we deny it.\n\nWe admit that we are not perfect, we try to embrace it as a part of our core yet although we admit it, we do not want to believe we are human. I think there is a major disparity here.\n\nBeing human is to mean making mistakes, learning from them and thus continuing to evolve with experiences.\n\nThat is why self-development is such a huge market now filled with self-help gurus and people trying to better themselves and searching for answers to be the best. Top 10 best this, or top 10 to be the best \u201c\u2026.\u201d . I, personally am skeptical about the authenticity of these advice or whether we are trying to mask our desires to truly be perfect beings.\n\nNow we hear stories about machines potentially providing us with companionship, sexual desires, to try and categorize information more efficiently and expeditiously.\n\nMay be they could indeed solve a lot of those issues that drive humans mad. May be it could be actually a good thing. Who knows.\n\nA.I. will cause huge disruption and it already has in manufacturing, technology, healthcare and now will be more prevalent in our day to day lives fulfilling desires that our human primal instincts demand. There is some greatness that it could best be put to, such as reports where A.I. could completely eliminate auto accidents and thus, deaths. If it is geared for those societal problems and ending accidental deaths, then count me in.\n\nIf it is used to make up for our lying, anxiety-driven, depression mode filled, companionship and loneliness tendencies, then I urge the caution. It takes away our own potential for greatness, suffering and perseverance. Ever wondered, how: \u201cWow, I cannot believe I went through that, but thankfully I did because now I am a much better person that I was before\u201d type of thought?\n\nYes, overseas nations such as China and Japan are utilizing the power of A.I. to fulfill human needs and to eliminate loneliness in providing friendship, companionship and to create the companion of our needs, but what is the beauty in that?\n\nWhat is the beauty of working on our problems on our own instead of creating the solution?\n\nIt seems that A.I. does not (yet) have the cognitive functions that us humans have. I truly believe and I hope, it should be kept that way.\n\nA.I. should not make up for our humanistic flaws. Do not get me wrong, I think nothing is more empowering than bettering ourselves and facing life obstacles at one point or another, but creating the solution to eliminate our humanistic flaws is wrong.\n\nWrong because, we will have no one to answer to and we will lose ourselves. Wrong because, we will never grow, evolve and mature in way life demands of us, and we will potentially face a major setback in all aspects of our lives.\n\nSetbacks where we do not develop, grow and potentially are stuck living in the past.\n\nLets continue to be human everyone."
    },
    {
        "url": "https://becominghuman.ai/building-intelligence-by-learning-to-act-4b2ca0351e25",
        "title": "Building intelligence by learning to act \u2013",
        "text": "Ascent a brain-child of visionary serial entrepreneur Fred Almeida, is building an AI training platform called ATLAS, with a physically grounded simulation of the real world. This simulator will allow us to significantly speed up training compared to the typical approach taken by our competitors that requires enormous resources to train real cars on the road. ATLAS automates introspective, predictive, and interactive understanding of actions (movement of vehicles or robots) and transfer of knowledge between tasks/domains based on a closed-loop cognitive architecture. This is inspired by brain-like dynamic interaction between model-free (learning based on rewards or punishments \u2014 reinforcement learning) and model-based learning (learning from behavior models and human demonstrations enabled with virtual reality interface), that allows vehicles/agents to learn in the presence of limited real-world data, generate predictions of other agent behavior, try out an enormous pool of experiences within a safe simulation environment and share experiences between domains.\n\nAutonomous driving is a robot problem. Right now, the only autonomous driver that can ensure a safe ride in realistic conditions, is a human driver. This is because of the ability of the human brain to learn from experience, through a combination of different algorithms. Therefore, along with our engineering goals, we are also leveraging our in-house neuroscience research knowledge to enhance the understanding of how learning works. This enables a long-term pipeline of research and development targeted to bring the autonomous cars on roads in Japan and match the ability of a human driver or in cases, surpass it. By equipping robots with the means to exploit prior experience based on, a combination of internal generative model and reward-based learning, the platform we are developing can impact wide range of autonomous robotics applications. These systems benefit from efficient learning through exploration, predictive reasoning and external guidance. Especially , in the case of self-driving cars, using a virtual reality interface we keep humans in the loop to provide expert guidance, initial knowledge or ground truth of good & bad behaviors and safety constraints. Enabling vision and sensing is an integral requirement of an autonomous robotic solution. Deep learning based image segmentation, object detection and scene understanding provides the necessary input to our learning algorithms. Robotic agents within ATLAS can perform near real-time scene-understanding and image-segmentation that guide their movement and decision making (see video below)."
    },
    {
        "url": "https://becominghuman.ai/increasing-reliance-on-advanced-algorithms-8b4f68dc6b60",
        "title": "Increasing Reliance on \u201cAdvanced Algorithms\u201d \u2013",
        "text": "\u201cThe intuitive mind is a sacred gift and the rational mind is a faithful servant. We have created a society that honors the servant and has forgotten the gift.\u201d- Albert Einstein\n\nI hesitate to write this piece because I have great respect for Will Knight\n\n and of course the MIT Technology Review for whom he writes but his article of April 11, 2017 is a bit confusing. In it he writes \u201cNo one really knows how the most advanced algorithms do what they do. That could be a problem.\u201d\n\nThree economists, with the University of Toronto\u2019s Rotman School of Management, certainly seem to \u201cknow how the most advanced algorithms do what they do.\u201d Writing in the Nov. 17 2016 Harvard Business Review, they said with confidence \u201cMachine intelligence is, in its essence, a prediction technology.\u201d The professors go on to say \u201cAs the cost of prediction falls, not only will activities that were historically prediction-oriented become cheaper \u2014 like forecasting \u2014 but we will also use prediction to tackle other problems for which prediction was not historically an input, like navigation and driving\u201d. For example, once prediction became cheap, innovators reframed driving as a prediction problem.\n\nAI learned to predict how humans would react to each second of incoming data about their environment. Thus, prediction is now a major component of the solution to a problem that was previously not thought of as a prediction problem.\n\nThe prediction technology used by almost all machine learning technology has been around since the 15th century when Blaise Pascal was asked to resolve a gambling dispute among some French noblemen. It has since spawned many variations and has been widely applied to enable most artificial intelligence applications. Following are generally accepted Machine Learning techniques. The adoption of general statistical/prediction technologies should be apparent:\n\nOne or more of these techniques is now an integral part of most machine learning applications. So while we may not know \u201cwhat\u201d the prediction of most advanced algorithms \u201cwill\u201d be until their statistical calculations have completed, that is not the same as saying we do not know \u201chow\u201d the algorithms do what they do.\n\nIntuition is the ability to acquire knowledge without proof, evidence, or conscious reasoning, or without understanding how the knowledge was acquired. Machine learning focuses on developing algorithms that can learn from the data and make subsequent predictions.\n\nThe core idea of deep learning, known as feature (or representation) learning, is applicable to a wide range of applications. As researchers from the National Research Foundation, Prime Minister\u2019s Office, Singapore have noted, \u201conce we have effective representations for entities, e.g., images, words, table rows or columns, we can compute entity similarity, perform clustering, train prediction models, and retrieve data with different modalities.\u201d ( https://sigmodrecord.org/publications/sigmodRecord/1606/pdfs/04_vision_Wang.pdf )\n\nThere is no universal or exact definition of what constitutes a \u201cfeature\u201d, and the exact definition often depends on the problem or the type of application. For example, given that, a feature is defined as an \u201cinteresting\u201d part of an image, features are used as a starting point for many computer vision algorithms.\n\nSince features are used as the starting point and main primitives for subsequent algorithms, the overall algorithm will often only be as good as its feature detector. Consequently, the desirable property for a feature detector is repeatability; whether or not the same feature will be detected in two or more different images of the same scene. Following is a table of Feature Detectors and the types of features of an image they can detect.\n\nFeature detection is a low-level image processing operation. In the case of human beings feature detection is a process by which the nervous system sorts or filters complex natural stimuli in order to extract behaviorally relevant cues that have a high probability of being associated with important objects or organisms in the environment, as opposed to irrelevant background or noise. That is, it is usually performed as the first operation on an image, and examines every pixel to see if there is a feature present at that pixel. If this is part of a larger algorithm, then the algorithm will typically only examine the image in the region of the feature\n\nVincent M\u00fchler recently published a piece on medium.com ( https://medium.com/@muehler.v/node-js-face-recognition-js-simple-and-robust-face-recognition-using-deep-learning-ea5ba8e852 ) that explains how \u201cprediction\u201d is done by computing the euclidean distance of the input face\u2019s descriptor vector to each descriptor of a class and a mean value of all distances is computed. M\u00fchler even describes the accuracy of predictions for each of these four faces submitted to his prediction algorithm:\n\nActuaries analyze data to estimate the probability and likely cost to a company of an event such as death, sickness, injury, disability, or loss of property. Actuaries also address financial matters, such as how a company should invest resources to maximize return on investments, or how an individual should invest in order to attain a certain retirement income level. Much of what actuaries do is based on probability and statistics but no one would say \u201cNo one really knows how most actuaries do what they do\u201d. In fact there are licensing boards in all 50 U.S. States that test to ensure they know what those who would like to offer actuarial services know what they are doing. Those who know what they do are other licensed actuaries and statisticians.\n\n\u201cFreedom from the desire for an answer is essential to the understanding of a problem.\u201d \u2014 Jiddu Krishnamurti\n\nAccording to Carlos Perez Deep Learning networks are intuition machines. They \u201clearn\u201d to perform inference (or make predictions) by using induction. There are varying degrees of strength and weakness in inductive reasoning, and various types including statistical syllogism, arguments from example, causal inferences, simple inductions, and inductive generalizations. They can have part to whole relations, extrapolations, or predictions.\n\n\u201cThe purpose of Intelligence is Prediction. Evolution of the ability to predict agents and phenomena in the environment improved survival rates and created a strong evolutionary pressure to develop better and longer term predictions. This is the reason Intelligence evolved.\u201d \u2014 Monica Anderson\n\nA Nvidia car relied entirely on an algorithm to teach it to drive by \u201cwatching\u201d a human do it\u2026.Information from the vehicle\u2019s sensors went straight into a huge network of artificial \u201cneurons\u201d that processed the data and then delivered the commands required to operate the steering wheel, the brakes, and other systems.\n\nSo those developing AI applications do indeed know how the most advanced algorithms do what they do. They use well tested mathematics to predict the likelihood of a system achieving an objective if certain behaviors are followed. The likelihood an AI system will behave differently than it\u2019s prediction of the best way to achieve an objective has more to do with a malfunction in its cybernetics than a misunderstanding of it\u2019s algorithms."
    },
    {
        "url": "https://becominghuman.ai/machine-learning-for-nfl-game-prediction-2017-season-retrospective-cfda3ea66a3d",
        "title": "Machine Learning for NFL Game Prediction: 2017 Season Retrospective",
        "text": "Since I got a bit lazy and skipped reporting for the latter half of the season, this is the follow-up and season retrospective. I\u2019d first like to go over a bit the workflow for the process that I went through to get to this point. Certainly this was a large learning process, and I still have a while to go, so I ended up testing a lot of possibilities in terms of best ways to train the models, how to present it, tuning hyperparameters and messing with the format of the data. I ultimately ended up just using AutoML to select the model and hyperparameters, because using 5 different models and aggregating their results seemed like it would just result in similar or worse results than using the best scoring model in the long run. This is what the generalized workflow ended up looking like:\n\nAbout midway through the season, I realized that with money line betting, the more Vegas correctly picked the Favorites, the less the model would be profitable. So I started tracking the accuracy and returns of the model vs. Vegas.\n\nI don\u2019t have historic data, but it seems that in the 2017 season, Vegas got pretty accurate starting around week 6:\n\nHowever, I did also note that overall accuracy did not accurately represent the returns of the model. This is basically because of the confidence modifier. Since the investment into each game is adjusted by the confidence of the model with that prediction, losses are minimized. Attaching some raw numbers to the data, this means that if you continuously bet with the model\u2019s predictions, you would\u2019ve come out ahead:\n\nThe highest total is in week 5. This suggests to me that, while the model is technically profitable overall throughout the season, it might just be best to cash out around halfway through the season, when it seems like Vegas gets its shit together and starts calling games with precision.\n\nOf course, with the Super Bowl being tomorrow, I ran the model for the game:\n\nThere\u2019s plenty of work still to do on the project, but at the moment I\u2019m going to check out the results with a tensorflow CNN, then clean it up (complete spaghetti at the moment) and post a link to the source code for all of this. I\u2019ll be sure to update this post when that happens."
    },
    {
        "url": "https://becominghuman.ai/advanced-chatops-with-microsoft-teams-part-1-76f86e41fe61",
        "title": "Advanced ChatOps with Microsoft Teams \u2014 Part 1 \u2013",
        "text": "It\u2019s official \u2014 Microsoft Teams will be replacing Skype for Business in the next 1\u20132(?) years. It has all of Skype\u2019s good parts plus a well documented API, an advanced interface with rich message formats and a rapidly growing marketplace of integrations, connectors and bots. The 100M Office 365 community will finally be joining the ChatOps revolution! In this first part of our series on ChatOps with Microsoft Teams, we will cover the ChatOps capabilities of Microsoft Teams and how we can leverage the Teams interface to solve a bunch of problems like:\n\nFor an advanced ChatOps implementation to succeed within a communication platform, there needs to be a couple of things in place:\n\nThe most important factor that determines if a ChatOps implementation will fail or succeed in an organization (and by succeed, I mean widely adopted and daily engaged with by users) is ease of interaction.\n\nMost ChatOps implementations today are built with the help of open source bot frameworks like Hubot, Lita, Err, Gobot etc. Bots built with these frameworks accept a text command through the bot(inside the chat application), process the command(through pattern matching, NLP, Intent-Entity extraction engines) and return a simple message back to the user. Following is an example of an application deployment through a chatbot.\n\nNotice that the first command(shipit) above is fairly complex and takes five inputs through a strict Unix-styled command. Then there are bots that use pattern matching, NLP and Intent-Entity extraction techniques or third party engines like Wit.ai, API.ai, Watson etc.\n\nWhile pattern matching is somewhat useful, users invariably end up using help commands to avoid errors in their commands. Complex commands are often hard to get right the first time and users may prefer traditional UI vs Bot. As for parsing commands with Intent-Entity extraction techniques, the bot developer would need a pretty good training data set, which is extremely hard to construct especially for complex application command like the shipit command above. Very few companies have found decent adoption of text based bots, whereas most companies have not found widespread internal adoption of text based bots, except for using bots for some very simple use cases.\n\nThis brings us to the new era of ChatOps with a new paradigm introduced by both Microsoft Teams and Slack in late 2017 \u2014 Dialog inputs. This shift occurred like most shifts \u2014 going back to the basics(not trying to start a CLI vs GUI war here!).\n\nThe new breed of advanced ChatOps solutions have started using Dialog inputs and other interaction elements as opposed to text commands. Dialogs inputs are a way of commanding a bot through classical input boxes, just like your traditional web and mobile interfaces that most people are already familiar with \u2014 and Microsoft Teams had already planned this crucial feature from day 1. Dialog inputs can increase adoption by 50\u2013100% over text based bots.\n\nSo instead of a complex text command like the shipit command above, you can simply use dialog input boxes. Following is an example of creating a JIRA task from Microsoft Teams through a Teams bot(YellowAnt)\n\nMessage Buttons and Fields \u2014 Microsoft Teams also supports interactive buttons. You can use message buttons can take an action on a notification message or help drill-down on earlier commands.\n\nIn our next article, we will show you how you can perform some heavy-duty ChatOps inside Microsoft Teams, and command your Dev/Ops applications like Github, JIRA, AWS, Jenkins, Travis, CircleCI, Pagerduty, New Relic, Sentry etc. from Microsoft Teams, build custom applications, and trigger complex workflows across multiple application from Microsoft Teams. We will also cover some other Teams concepts like Connectors, Compose Extensions and Tabs.\n\nWe are building the most advanced ChatOps platform for Microsoft Teams and would like to hear from you! If you\u2019re using Microsoft Teams, install YellowAnt on the Microsoft Teams app store and say hi! Let us know in the comments below and join us in the YellowAnt community here. You can sign up on YellowAnt here."
    },
    {
        "url": "https://becominghuman.ai/are-blockchain-and-ai-the-keys-to-unlocking-interoperability-in-healthcare-846364c049e2",
        "title": "Are blockchain and AI the keys to unlocking interoperability in healthcare?",
        "text": "Doctors from every specialty are feeling more burned out than ever. Among emergency physicians, for instance, over 60% admit to feeling burned out, and many have considered leaving medicine altogether. With a shortage of physicians, there are fewer providers every year to meet growing patient demands.\n\nYet, it isn\u2019t just the thinning of their ranks that is wearing physicians down; it\u2019s also the fact that they spend two-thirds of their time doing paperwork rather than actually caring for patients. For every hour they spend with a patient, physicians have to spend two more completing paperwork and working on electronic health records, reviewing test results, logging information, writing medication orders and other tasks.\n\nTo exacerbate matters, networks from different providers don\u2019t communicate with one another, so one provider might not know what the previous provider has prescribed or whether the patient is adhering to the treatment. When patients need to visit a specialist or another provider from a different network, doctors are burdened with even more unnecessary paperwork.\n\nThese issues create roadblocks to better patient care \u2014 most physicians join medicine to help people, and if the current practice environment and data management systems interfere with that goal, it\u2019s time to change them.\n\nIn the past, medical transcription used to be easier. A doctor dictated exam outcomes on the fly, and a transcriptionist typed it into the patient record immediately. This allowed doctors to focus on patients while still recording data.\n\nThen, electronic health records (EHRs) came into the picture. EHRs were intended to be a way to better track health data for hospitals, payers and physicians. Although they have good intentions, they often end up causing more problems than they solve.\n\nAccording to a study by Northwestern University, physicians who have EHRs in their exam rooms spend around 33% of their time looking at their computer screens, while doctors who use paper charts spend only 9% of their time looking at screens.\n\nAlso, EHRs can store massive amounts of data, which is a double-edged sword because though data is important, the structure of that data creates challenges. On one hand, it is too limiting. Doctors can enter information about a patient\u2019s medical history, medications and procedures \u2014 but only through constrained check boxes that don\u2019t capture nuances and complexities.\n\nWhile unstructured \u201cnotes\u201d sections can be included in comment boxes, there is no standard format for how these should be written, and it is hard to pull insights without diving into detail. Furthermore, this data is often copied and pasted repeatedly into different areas within the EHR, resulting in a phenomenon called \u201cnote bloat,\u201d when there\u2019s so much information it\u2019s difficult to make good use of it.\n\nBlockchain technology (and apps built on a blockchain) can make data organized and easily accessible, while artificial intelligence makes it possible to extract intelligent insights from vast amounts of data that no human could ever analyze.\n\nBlockchain is a digital ledger that records and shares transactions and interactions in chronological order, providing security and interoperability for healthcare providers and their patients. In healthcare, for example, each patient visit, diagnosis, prescribed treatment, outcome and other key data that goes in the EHR are considered the transactions. Various providers, payers and pharmacies all record this information on the same ledger for a given patient, and they can access the data put there by each other. For example, medications can be noted, assessed and managed to prevent contradictions or misuse.\n\nThe apps that can be built on the blockchain further help with parsing and reading data that is logged but with details stored separately \u2014 in a PDF, for example. It\u2019s worth noting that blockchains for health data can and should be created as a permissioned ecosystem of trusted collaborators with additional controls that protect personally identifiable information while allowing anonymized sharing of personal health information.\n\nUsing the database, providers and specialists who see patients for the first time can rest assured that the data regarding previous tests, diagnoses and treatments is accurate. Having highly reliable patient data on the ledger means providers can spend less time on repetitive data entry tasks, prevent catastrophic misdiagnoses and provide better predictive solutions to patients. Furthermore, this data is now much more consumable by intelligent algorithms.\n\nFor example, California-based startup PokitDok is using blockchain to develop application programming interfaces (APIs) for the healthcare industry in areas such as identity management, claims and pharmacy in order to create a more secure network for accessing and sharing patient data. It also uses machine learning to analyze large amounts of data and speed up processes \u2014 for instance, it\u2019s able to run an eligibility check on an insurance claim in just seconds rather than days, saving healthcare providers time and money.\n\nOnce the data is organized in a transparent and fully reliable way on a blockchain, machine learning and eventually artificial intelligence (AI) can learn from the patterns in the data and execute analyses very quickly.\n\nDigital Reasoning is a startup that\u2019s using AI to help healthcare professionals make better decisions. It uses complex algorithms to organize data into graphs and detect patterns, helping machines \u201cthink more like people do.\u201d As a result, doctors are able to see a clearer picture of a patient\u2019s health and make more informed decisions based on all that information.\n\nThe industry mindset around sharing data has typically been very conservative, even antagonistic. However, over the past two years, new standards like fast healthcare interoperability resources (FHIR) are leading to more sharing \u2014 35% of U.S. hospitals plan to use open APIs within the next year, and that data is their competitive advantage. In order to truly provide value to patients, though, this trend needs to continue and extend to all players in the continuum of patient care.\n\nHealthcare systems experts understand the importance of providers being able to collaborate, even when they don\u2019t know one another. The trouble is that interoperability in healthcare is complicated by a variety of factors.\n\nBecause providers all operate within their own systems, identifying who previously treated a patient, as well as when and where treatment took place, can be difficult. Also, EHRs may be moved into health information exchanges, but they are still controlled by data silos. Patients cannot choose what specific data of theirs is shared. To prevent unwanted access, they may put a blanket denial on any of their health information being shared.\n\nTogether, these factors (and many others) make interoperability a vital but difficult-to-achieve goal for healthcare systems. Therefore, one of the biggest reasons blockchain is becoming such a popular healthcare buzzword is because it may be the solution to making safe and efficient interoperability possible. And from there, with the application of AI, major innovation will accelerate.\n\nDespite all the problems blockchain and AI can solve for doctors, hospitals and patients, they\u2019re not without their challenges. Blockchain must resolve problems related to scalability, and AI faces the challenge of getting doctors on board.\n\nWith AI, doctors will need to believe in the technology and trust that it\u2019s analyzing the data accurately. Doctors will still be the ones diagnosing patients and providing treatment plans, but if they don\u2019t trust the information they\u2019re receiving from AI technology, they won\u2019t be able to do their jobs to the best of their ability.\n\nAt the end of the day, humans want to be cared for by other humans \u2014 not robots. But doctors are spending far too much time inputting information into EHRs rather than spending time with their patients. Blockchain and AI won\u2019t be able to take over all of doctors\u2019 responsibilities, but they can help them get back to what they love: helping people be healthy and heal.\n\nThis article was originally published on Medical Economics."
    },
    {
        "url": "https://becominghuman.ai/deep-functor-14d0dac194fb",
        "title": "Deep Functor \u2013",
        "text": "Deep functors might help us create radical new techniques for transfer learning, and they might help us unlock better abstractions within deep learning systems. All thanks to category theory and constraint-based programming.\n\nLet\u2019s start with a simple problem: create a neural network that takes in an image of N dots and outputs an image of N+1 dots. Then, let\u2019s make \u201cN+1\u201d swappable for another computation, like \u201cN\u00d72\u201d, and make the input/output formats swappable for other formats like binary arrays and images of digits.\n\nTo make stuff easier, let\u2019s guarantee that N is a digit, and all operations always end with \u201cmodulo 10,\u201d pinning their outputs between 0 and 9.\n\nHere\u2019s the tough part: after initial training, these components must be swappable without further training.\n\nThis network can work incredibly well on our first problem: turn an image of N dots into an image of N+1 dots. However, there\u2019s no way to swap the computation, N+1, while keeping the encoding and decoding logic unaffected. Somehow, we need to separate the network into a few tasks, maybe then we could swap stuff out\u2026\n\nIt\u2019s (naively) impossible to learn a network separated in modules like this. Neural networks are tightly coupled from input to output. There\u2019s no way to tell when encoding, decoding, and \u201cAdd 1\u201d begin and end; they\u2019re each distributed throughout a typical network. But if we had a decoupled network, we could swap each of the modules to read different kinds of inputs, perform different math operations, and output different formats.\n\nOne solution to creating a deep neural network with swappable modules is to create many interconnected networks, trained together:\n\nIn this graph, we have many input formats, many output formats, and many math operations we want to perform. It\u2019s possible we can learn it by using multiple inputs and outputs or by training one path at a time until convergence.\n\nLet\u2019s add the ID function to the digit operation module so that the inputs and outputs of that module are constrained to be similar; this will satisfy eagle-eye readers later.\n\nLook at the inputs and outputs of the digit operation modules. Let\u2019s declare that everything going in and out of that module is in the category of Fuzzy Digits, because we\u2019re pretty much guaranteed that each of those tensors contains information about a digit, but it\u2019s unclear how they\u2019re represented. It\u2019s clear from the diagram that all Fuzzy Digits are compatible with multiple interfaces, so it makes sense to put them in a category together.\n\nSimilarly, we can assign each of the I/O formats category names. Let\u2019s declare every input to the dot counter and output of the dot image output to be in Dot Images. We also have Digit Images and Binary Numbers.\n\nMorphisms are like functions in programming \u2014 they take us from one object to another object. In category theory, morphisms are represented as arrows in between objects within the bounds of a category.\n\nWe have several morphisms available to us in Fuzzy Digits. We\u2019ve already learned N+1 and N\u00d72, for example. When we have a morphism in Fuzzy Digits, we can apply it to an object in Fuzzy Digits to get another object in Fuzzy Digits.\n\nIt\u2019s worth noting that morphisms compose, just like functions; they\u2019re associative.\n\nFunctors \u201cmap\u201d the morphisms in one category into another category.\n\nIn programming, you can map functions onto lists. A list can hold any one type of value in it; when you \u201cmap\u201d a list, you apply a function to each of the list\u2019s members, which are in the category of types. So we have two categories: the category of types and the category of lists. \u201cMap\u201d maps a given function from the category of types into the category of lists.\n\nYou can see that the morphisms (arrows) in Fuzzy Digits are mapped one-to-one in Digit Images. When in Digit Images, we prefix the names of morphisms with \u201cF\u201d to denote that they are mapped through a functor named \u201cF\u201d. You can see that some underlying structure is preserved in the target category; in this case, the underlying digit remains the same even when we move from the category of Fuzzy Digits to Digit Images.\n\nWe can add Binary Numbers to the diagram:\n\nIn our toy system, we have a few functors going from the category of Fuzzy Digits: the Dot Images functor, the Binary Numbers functor, and the Digit Images functor. Each functor maps the morphisms in Fuzzy Digits into their own respective category.\n\nWhat\u2019s totally cool about this is that we can learn new functors from Fuzzy Digits and new morphisms in Fuzzy Digits, hopefully without needing to retrain everything else. For example, we could learn an integer division morphism or a functor into the category of roman numerals.\n\nDeep functors are just a fancy way to apply transfer learning.\n\nPersonally, I\u2019d like to see this applied to other categories, like the category of headshot photos. Imagine creating a functor to the category of headshots from the category of headwear. Perhaps it would help people create headwear-specific models quickly with constrained training data and integrate them within many larger models.\n\nPotentially more interestingly, how can we apply deep functors internally in models, enabling internally learned abstraction and reuse? Perhaps we can learn to map concepts into different domains, like musical scales into different keys.\n\nPlease check out Bartosz Milewski\u2019s free book, Category Theory for Programmers, especially if you\u2019re familiar with programming. It\u2019s just right for myself. I borrowed some imagery for my own diagrams.\n\nI\u2019m on the job hunt! I have broad experience in machine learning, functional programming, music, and entrepreneurship. I\u2019m looking for a place to apply my skills preferably in Los Angeles or other non-bay-area locations."
    },
    {
        "url": "https://becominghuman.ai/chatbots-using-aws-sas-viya-e8a7410ec256",
        "title": "Chatbots using AWS & SAS Viya \u2013",
        "text": "The key driver for innovation for any organization in today\u2019s diverse technology landscape is organizational ambidexterity (Check out this book for more on that). In its essence, it refers to the idea of being able to both explore new avenues while continuing to exploit current strengths. No matter what business you are in, what you sell or who you serve \u2014 a sense of \u201copenness\u201d around strategizing or problem solving is the first best step to ensure that we are doing the right thing to not just maintain our business but to expand as new opportunities present themselves. This \u201copenness\u201d embracing mindset creates opportunities to explore - to change and pick up on new strategies as we see fit in the fastest manner possible. This is especially true when we talk about technologies and investments we make on them. And so for businesses, nothing is more tempting when technology brings choice, ease of use, and accessibility together, allowing them to change and adapt faster than ever before. This is at the heart of what \u201copen technology\u201d really has to offer for businesses. It meets the cry (the business need) for asking to maximize for success not just now but in the future while truly minimizing operational risk. As a result, platforms that offer this combo : choice + ease of use + accessibility emerge as big winners. Plus a great nice-to-have: these platforms that offer this combo typically open the floodgates of their capabilities to their customers so they can build their own solutions to meet their business needs in addition to providing hot value with just their own \u201ccore\u201d offerings.\n\nIn the rest of this blog we\u2019ll see, as an example, how we can couple products and services from AWS & SAS (each leaders in their \u201ccore\u201d space \u2014 Cloud Infrastructure Platform & Data Science Platform) to build our own little chatbot \u2014 Alice. Why? Because Chatbots are the new IN thing to do and everyone wants one! (see below)\n\nAlso, as a side note, every time someone says some scary stuff about these things \u2014 show them this. Please!\n\n\u201cPeople worry that computers will get too smart and take over the world, but the real problem is that they\u2019re too stupid and they\u2019ve already taken over the world.\u201d \u2014 Pedro Domingos, Master Algorithm\n\nAnyway, back to our stuff, the AWS services that we will be using are Lex, Lambda & S3. As far as the brain behind the operation :- that demand is going to be met by SAS Viya. The bot that we are going to be building will help with answering questions around sales based on customers\u2019 historical transactions we\u2019ve collected. It also helps democratize the value from our machine learning (ML) model that we built using this data.\n\nAs such this data is sitting in our Viya cluster on AWS and its a small demo dataset \u2014 100K rows * 29 columns\n\nWe\u2019ve also used a ML model to score this data to help classify whether a particular visit was a casual or a focused visit as far as the purchase visit is concerned. In this example, our business cares about focused shoppers and may potentially use this information later for some follow-up marketing activities.\n\nOk! Now that we know what the data is, let\u2019s see how we can use these data to build a chatbot around it. Clearly, having a dedicated data science platform to manage, govern, and centralize assets will be very useful for businesses as it allows them to have their data scientists build out models and later expose these models via REST APIs with just a few clicks. All the versioning, re-training, model replacements etc. can be easily handled from within this platform. We don\u2019t have to worry about that stuff for the chatbot or this blog. We only care about the data or the API being available for us to tap into. So we can build, collaborate, and innovate faster.\n\nThe following is the high level architectural blueprint for our chatbot \u2014 it uses the previously mentioned technologies to mashup and stitch together a nice user experience on Slack."
    },
    {
        "url": "https://becominghuman.ai/where-ai-is-headed-in-2018-1f8913fd420e",
        "title": "Where AI is headed in 2018 \u2013",
        "text": "Stephen Hawking said, \u201cThe development of full artificial intelligence could spell the end of the human race\u2026.It would take off on its own, and re-design itself at an ever increasing rate. Humans, who are limited by slow biological evolution, couldn\u2019t compete, and would be superseded.\u201d\n\nEver since its genesis, there have been conflicting views and concerns on the potential enhancement or doom that it can cause to human civilization. While some experts believe that this technology will advance and augment our intelligence, some like Bill Gates have expressed concerns on how a machine\u2019s intelligence becomes strong enough to be a concern.\n\nFor now, let\u2019s take a look at the current trends of AI and where it is headed to in 2018:\n\nIf 2017 was the year where the warnings from Elon Musk and Stephen Hawking about the potential evil from AI clashed with predictions from Mark Zuckerberg and Bill Gates on its potential good, 2018 will be the year when the debate shifts to its practical utility.\n\nWe would get to see more robots that could master complex tasks like \u2018walking around a room and over objects\u2019. 2018 will see \u201cVast applications on smartphones will run deep neural networks to enable AI,\u201d as said by Robinson Piramuthu, chief scientist for computer vision, eBay.\n\nMore research on AI and demand for AI experts to increase in 2018\n\nFrom 2018 and beyond, there would certainly be more people from all kinds of backgrounds who would participate in building, developing and productizing AI.\n\nThere would be more product usage, creating apps, translating data and algorithms into real-world usage. As per reports, linguists, data scientists, UX experts, cognitive programmers would massively scale-up.\n\n2018 would see more use of converting structured data into intelligent narratives based on natural language generation (NLG) and natural language processing (NLP). AI would see more use in automated content generation in news coverage, sports, financial reports, and social media and so on using rule-based systems.\n\nThere would be more use of capsule networks accounts (CapsNet) for image recognition and computer vision. (Read more here: PDF)\n\nMore use of smart automation and chat boxes\n\nIn 2018, advanced AI is predicted to make more accurate, more instant verbal and visual translations. It is estimated that some 85% of customer interactions will be managed by AI by 2020.\n\nTrend indicates more focus on bot sensitivity training that would divest more work on chatbot shoulders like Amazon\u2019s Alexa or \u201cAmy\u201d the new virtual assistant from X.ai for responding regarding meals, meetings and calls without indicating that she\u2019s a bot.\n\nMore efficacies in conversational AI are likely to increase in 2018 as Forbes has highlighted well here.\n\nTrend indicates there would be more research on emotional sensitivity and translational technology in AI. Amazon has gone a step ahead in training Alexa to recognize speech patterns that may be indicative of suicide. This could well be a future promise of using bots effectively for psychiatric counseling.\n\n\u201cArtificial intelligence is growing up fast, as are robots whose facial expressions can elicit empathy and make your mirror neurons quiver,\u201d Diane Ackerman. With main advances in the last six decades in search algorithms, 2018 would see more research on ability of robots to empathise.\n\nBy the end of next year, it is expected that half of leading healthcare systems will have adopted some form of AI within their diagnostic groups.\n\n\u201cAI in 2018 and in the coming years will be so embedded into our clinical systems that it will no longer be called AI but rather just a regular system,\u201d Luciano Prevedello, M.D., M.P.H., Radiology & Neuroradiology, Ohio State University Wexler Medical Center\n\nAI to scale up marketing and sales in B2B companies\n\nEnterprises are likely to scale up excellence AI to deal with more complex IT ecosystems. Marketing companies would put AI to best use for original gathering of information of lead generation and making predictive account management and sales.\n\nAI is likely to open up new scope of research in astrophysics and energy\n\nAI is expected to revolutionize the energy industry. Experts predict AI will enable the detection of an unexpected astrophysical event that emits gravitational waves, opening a new field of research in contemporary astrophysics.\n\nFor now there isn\u2019t a foolproof mechanism to predict whether it is a fail-safe mechanism designed to enhance human survival or can cause human destruction. It is only the future which would clear up all speculations."
    },
    {
        "url": "https://becominghuman.ai/neural-style-transfer-f1ec0d2d8da4",
        "title": "Neural Style Transfer \u2013",
        "text": "The Global Game Jam is a massive event held all over the world where developers have 48 hours to create a new game. Making a game is hard and fun and requires a lot of skillsets. This year we were lacking a vital one: we were three developers with the artistic talent of a dead horse. We had no one to draw some amazing graphics for our game. That\u2019s when my friend Uri from Incineration Games suggested we use machine learning to generate the assets.\n\nis a branch of machine learning which could be used to generate some content. Given a content image(C) and a style image(S) the neural network generates a new image(G) which attempts to apply the style from S to G. The loss function consists of three components:\n\nFor a more in-depth look regarding the algorithm please check out the resources that follow. This blog post will focus on the results related to generating game art.\n\nOur game is a hot seat cooperative game where two cars are linked together with a transmission uplink which acts like a physical spring . You can use the uplink to kill the evil computer viruses, but you need to avoid the black holes.\n\nWe used the keras implementation listed above. Most of the assets were generated with the default hyperparameters. In the actual game we use the images generated at every epoch to create an animation which makes the sprites more engaging.\n\nThe regular neural style transfer algorithm completely ignores the alpha(transparency) channel. It is a VERY important part when it comes to assets, so I\u2019ll outline some possible alpha recovery options:\n\nSo, what\u2019s wrong with this method is that sometimes the neural style transfer algorithm produces some amazing artifacts. Consider the following case:\n\nRestoring the original alpha channel completely removes the additional content generated by the algorithm:\n\nIt would be cool to come up with a strategy which preserves the style artifacts, but makes the rest of the image transparent.\n\nDuring the course of the hackathon I tried a lot of different style reference images. I noticed the following:\n\nIt\u2019s exciting to think about the possible uses of machine learning in game design. One possibility is to use neural style transfer or something similar to automatically generate content based on a given reference. Think about all the hats!\n\nOf course, there\u2019s also the advent of the AI agents trained by reinforcement learning. This has the potential to make games even more challenging.\n\nAll the advancements in NLP(natural language processing) have left me excited about the opportunity to develop a smarter NPC. Imagine talking to a quest giver who can also freely chat about general events in the game.\n\nThe most important part of incorporating ML in games is to make sure that the game is still fun! I recently had a chat with a friend about developing smarter units with machine learning \u2014 supposedly, they would learn to auto retreat or auto cast spells or focus on a single enemy. However, he was totally sceptical towards this idea \u2014 controlling the units is the player\u2019s job; the player should be in full control. If the control is dedicated to the machine, then the game is no longer fun.\n\nWhere do you think machine learning fits in with game development?"
    },
    {
        "url": "https://becominghuman.ai/2018-lending-outlook-top-5-trends-part-one-a724601f3d85",
        "title": "2018 Lending Outlook \u2014 Top 5 Trends, Part One \u2013",
        "text": "There\u2019s an old Chinese curse, \u201cMay you live in interesting times.\u201d 2018 is shaping up to be an interesting year for the lending industry, especially when we consider the uncertainty surrounding political environments, tax reforms, and potential new regulations.\n\nAt Turnkey Lender we\u2019re a glass half full group of people. So when we balance regulatory chaos on the one hand with tremendous untapped fintech innovation on the other hand \u2014 it\u2019s easy to be optimistic about the future of lending. Here\u2019s our list of the top 5 trends to watch in 2018. We\u2019ll look at each area in isolation, but you\u2019ll see they\u2019re often interrelated and interdependent.\n\nDoes it feel like banking regulations are changing at a faster and faster pace? It\u2019s not your imagination. According to a Thomson Reuters report, the number of daily compliance updates in the financial services category has increased over the past decade from 10 per day to almost 200 per day. When you layer on the new tax rules the sheer volume could drown a seasoned compliance expert.\n\nThese constant changes are one of the reasons regulatory compliance has become the fastest growing career path in the banking industry. And regtech software, which used to be a luxury, is now an operational necessity.\n\nWe found two interesting examples that demonstrate why so many lenders include a regulatory compliance director in their stable of experts. The new payday lending regulations, and the new Cyber Unit at the SEC. The mission of the Cyber Unit is to target cyber-related securities misconduct. It sounds like their investigations would only impact the investment community, but SEC rules could apply to any lender who includes a customer rewards component in their value proposition.\n\nThe Cyber Unit put two companies out of business in December, including a foodie mobile app called Munchee. Their product design got them into hot water when they created the MUN, a digital token used to reward customers for posting restaurant reviews. The SEC Cyber Unit concluded the Munchee MUN carried enough profit potential to be considered a security token, instead of a utility token. Munchee was threatened with civil penalties and frozen assets, which they were able to avoid by returning $15 million investor funds. Their website has been pulled down, and their Twitter account has been dormant since November. So it appears the SEC review effectively put them out of business.\n\nIf a foodie mobile app can be bankrupted by an SEC regulatory action, then a complex mobile lending platform must take extra precautions. A pre-launch regulatory compliance review should be incorporated into the product launch plan, because it\u2019s becoming more difficult to recover from a post-launch design hiccup.\n\nThe sweeping new tax reforms in the United States is another regulatory area to watch, but it may take several years to realize the full impact. It looks like the immediate benefits will go to established, profitable enterprises like big banks as their tax rate drops from 35% to 21%. That\u2019s a whopping 14-point increase in profit. Small to mid-size lenders should keep a close eye on how they use these found monies. Since they may invest in infrastructure improvements like enhanced cybersecurity, proprietary digital lending platforms, and proprietary blockchain technologies.\n\nThese types of improvements could make them much more competitive with smaller, technology-driven lenders. And unfortunately, many financial startups won\u2019t see any positive gain from the new tax rules for at least a few years until they\u2019re cash flow positive.\n\nFintech is at the cutting edge of financial innovation for two reasons. First, it caters to consumer demand for faster, easier loan applications, instant approvals, and 24-hour online funds transfers. And second, it caters to lender demand for more efficient operational processes, and smarter credit decisions.\n\nUnfortunately, the functionality that satisfies digital savvy loan prospects and technology savvy lenders is the same functionality that makes these systems vulnerable to fraudsters and cybercriminals. And susceptible to high level scrutiny from regulatory watchdogs. Old school regulators worked to protect consumer interests, even when it stifled innovation.\n\nGlobally a new breed of regulator is emerging. One who realizes they can harm consumer interests when they slow innovation. The antiquated adversarial mindset is shifting to a more collaborative approach where regulatory bodies partner with creative thinkers.\n\nThey achieve the best outcome for consumers by playing together in a regulatory sandbox. In 2015 the first regulatory sandbox was launched in the UK. This is a safe space where innovation is supported and consumer interests are protected as part of one integrated development process. The sandbox allows new financial products to be beta tested in the marketplace with partial regulatory compliance for a limited time. This test window lets developers validate and refine their product concept without regulatory risk. It also allows the regulators to measure the impact of their guidelines in a real world environment.\n\nSandboxes are already being trialed in Switzerland, Singapore, Hong Kong, Thailand, Malaysia and the United Arab Emirates. Financial industry analysts believe US regulatory agencies must follow the global lead to become more competitive. American Banker recently quoted Marshall Lux, a senior fellow at Harvard\u2019s John F. Kennedy School of Government, \u201c\u2026regulators should focus on the impact of compliance requirements on consumers and provide pilot programs for companies to empirically test new products without fear of enforcement.\u201d Influential opinions from institutions like Harvard prompted three government initiatives in 2016:\n\nThese initiatives were a good indication that US regulators were listening, but all three papers date back to the Obama administration. We\u2019ll have to wait and see what actions are taken by the Trump administration to move innovation forward. A great first step would be a US regulatory sandbox for financial products and services.\n\nMillennials have replaced baby boomers as the largest consumer demographic. They\u2019re younger with more disposable income. And they\u2019re demanding, especially when it comes to digital delivery systems. This behavior applies to both consumers and small to mid-size businesses (SMBs).\n\nIt\u2019s great to watch Millennials pass their digital behaviors up to their parents and grandparents. Ten years ago they helped grandma set up her Facebook page. Today they\u2019re helping her set up an e-commerce grocery delivery service, and an online banking account.\n\nGeneration Z isn\u2019t far behind. The leading edge of this emerging demographic is already graduating from college and entering the workforce. This group will be larger than Millennials, and we can only guess what technologies they\u2019ll be demanding within the next few years. We\u2019re betting voice-activated banking will be at the top of their list.\n\nA recent article published in The Financial Brand reported 61% of banks listed \u201cremoving friction from the process in order to capture more Millennials\u201d as a priority for 2018. This may be driven in part by two independent research studies that found the key to brand differentiation for financial services companies is an improved customer experience.\n\nA Forrester brief reported that one third of banking customers say, \u201c\u2026all banks are basically the same.\u201d They believe the majority of financial products and services are commodities where one brand is interchangeable with every other brand.\n\nAccording to a survey conducted by Microsoft Financial Services, consumers value two aspects of their banking relationship over actual services like loans and checking accounts. The first aspect is easy banking. And the second is options for how they bank. This finding seems counterintuitive until we remember consumers believe banks are offering commodity products.\n\nOne of the best ways to reduce friction and improve the customer experience is by making the loan application and monthly payments processes faster, easier and safer than ever before with online lending platforms and fintech software systems. Lending-as-a-Service (LaaS) platforms provide a fully managed, turnkey approach for small to mid-size banks with limited resources. The top platforms automate processes and improve credit decisions. They provide alternative credit scoring options, and machine learning that continuously refines the credit scoring process. And they stay current with new banking regulations. These platforms are cloud-based, so credit scoring and regulatory upgrades are automatic without additional user programming.\n\nAn omni-channel delivery system is another way to reduce friction, because every customer can choose how they want to access their accounts. One of the fastest growing trends is the mobile-only banking experience, supported by a live customer service representative when the customer has an issue to resolve. In the next article we will discuss how fintech software supports credit scoring and application approvals, and about cybersecurity challenges. Stay tuned!"
    },
    {
        "url": "https://becominghuman.ai/millennials-ai-education-in-2018-thoughts-on-the-future-3eda76d9f047",
        "title": "Millennials, AI & Education in 2018: Thoughts on the Future",
        "text": "Here we are, one month into 2018. While each year we individually turn a page, it is sometimes hard to recognize the gradual evolution of society. We now have a large, young demographic increasingly assuming leadership roles, and a more diverse and open-minded population struggling to find its way amidst a political push for a more traditional social infrastructure. Social shifts are not rocket science; they are a pendulum swing that we have seen many times before.\n\nWhat makes it different now is that the Baby Boomers were a big generation, X\u2019ers are small, and Millennials are the biggest with Gen Z on track to be even bigger. The big-small-big-small generational pattern is being disrupted. I believe that rather than the pendulum swinging back and forth with each generation as it has in the past, it will now start to swing a wider path.\n\nThis will lead to more extremes that are both positive and negative. Millennial idealism will push a multicultural and border-less vision that will generate increasing pushback from boomers. But the younger voice will prevail to a degree where optimism may actually overshadow wisdom and nostalgia. They may, in fact, organically defeat most social injustices. On the other hand, radical ideology in all forms may escalate in opposition. We could see an increase in terrorism and racial divides, both nationally and abroad.\n\nTechnology and Artificial Intelligence are going to have an increased presence in our lives in 2018. The traditional job market in the short and midterm will consequently shrink. Entrepreneurial opportunities will blossom for those broadminded enough to recognize them.\n\nA shrinking job market will work against the current USP (unique selling proposition) of education \u2014 \u201cCollege and Career Ready.\u201d We must embrace the opportunity to find applications for newly available human capital. Let\u2019s figure out how to effectively teach entrepreneurship and innovation, which I believe means embracing a liberal arts education.\n\nThe combination of all these forces gives education its greatest opportunity to positively impact the world. It must ramp up a proactive stance in 2018. Educators must prepare and empower students to create a more peaceful society. Particularly by teaching them data triangulation in order to eliminate the existence of destructive players like \u201cfake news.\u201d Students must be taught to recognize entrepreneurial opportunities and capitalize on them in order to make peace profitable. The only thing standing in the way of world peace is war profiteering.\n\nThe most visible shift we will see in education during 2018 and beyond is the domination of the millennial generation and the impact they will have. Millennials will increasingly become parents, school board members, administrators, policymakers, and teachers.\n\nThe current teacher shortage must also be sufficiently addressed in 2018. We cannot simply expect cash-strapped millennials who want to make a difference but are saddled with student debt to all of a sudden find fulfillment in education. Society continues to undervalue educators. We seem to have a national anti-intellectual movement that surely will not help make America great again. We have a loud voice in America that effectively screams, \u201cIf you take away our guns, you take away our freedom.\u201d In 2018, I\u2019d like to hear an even louder voice scream, \u201cIf you take away our public education, you take away our freedom.\u201d\n\nI believe in Millennials, and believe the increasing impact they will have on education and society will be positive overall. Their proclivity for entrepreneurship and desire to have a positive and meaningful impact on the world will yield great results. If the rest of us do one of two things: support their initiatives or simply get out of their way."
    },
    {
        "url": "https://becominghuman.ai/functional-vs-biological-approach-in-designing-social-robots-4cea73acdc25",
        "title": "Functional vs. biological approach in designing social robots",
        "text": "This blog series is a documentation of my work in Human-Robot Interaction course at the University of Kansas.\n\nLast article, I\u2019ve talked about a few design challenges of designing social robots, such as physical embodiment, human-oriented interactions, and personality. In this article, I will compare the two primary approaches: functionally designed vs. biologically inspired, using the Sawyer and Pepper robots as examples. The first way \u2014 functional design \u2014 aims to create a superficially social robot that has constrained operational and performance objectives. The second approach \u2014 biological design \u2014 strives to build robots that internally simulate and exhibit the social intelligence of living creatures.\n\nFunctionally designed robots often exhibit goal-oriented behaviors, due to the fact that they are engineered to achieve certain tasks or exhibit certain interactions. They do not have the internal design of social intelligence, rather just create an impression of artificial intelligence. Let\u2019s look at Sawyer \u2014 a single arm, high performance, collaborative robot designed to execute precise tasks. Human can train Sawyer by recording a series of actions, for example, picking up a circuit board and putting it inside a box. What makes Sawyer collaborative are its vision and motor skills. However, Sawyer is incapable of socially interacting with people and has limited embodiment.\n\nBiologically designed robots are internally simulated of social intelligence and often built with naturalistic embodiment. The main motivation is that this approach allows robots to experience and interact with the world in the same way that living creatures do. Therefore, these robots are able to naturally interact with people, dynamically react to real world situations. When Pepper went on a date with CNNtech\u2019s Samuel Burke, they could dance, exchange conversations, and physically interact with Burke. In fact, the interviewer Burke treated Pepper as it has a personality and social intelligence.\n\nEach design approach has its own motivations and benefits, depending on the robot goals and surrounding environment. With the growth of our modern technology, there will be massive applications for both functional robots and biologically designed robots. While one-arm Sawyer is limited to industrial tasks now, its precise motions and object-recognition can soon be applied in households to assist human. Sawyer may be trained to cook, to do laundry, or even to drive your cars. For Pepper, we will see it as personal assistant, a friend at home, or a receptionist at your bank.\n\nIf you like my article, please clap, and share so others can read it!\n\n1. Back-Propagation is very simple. Who made it Complicated?"
    },
    {
        "url": "https://becominghuman.ai/two-challenges-facing-ai-this-year-d8514380b5da",
        "title": "Two Challenges Facing AI This Year \u2013",
        "text": "AI gets written about a lot. Its the \u201cbig data\u201d of 2019, but very few people write about the challenges facing AI. Is there a reason why AI will not succeed, other than the fear of cinema-scenario machines taking over the world?\n\nLets first talk about that \u2014 machines will not be taking over the world anytime soon. AI is pretty rudimentary right now, and the foreseeable future shows AI being a benefit that augments the ability for people to get more done. Just think back to Back To The Future \u2014 most of what they predicted hasn\u2019t yet manifested, and I doubt SkyNet will be coming to fruition anytime soon, either. It might sell tickets, but its not indicative of where people are headed.\n\nThere are lots of benefits to AI, but there are at least 2 major impediments to getting people to embrace the technology. What I find interesting is neither of these are technology-oriented problems. They revolve around the people who are going to fund, support and implement AI solutions in their businesses.\n\nAI could suffer from increasingly higher and higher expectations. If you talk to the average person, they think of computers with cognitive abilities that replace people in their jobs. The fact of the matter is AI is more of a tool to increase productivity. AI replaces the more mundane tasks in your day with machines and enables you to do more and bigger things with your time. AI replaces optimization, simple analysis, and mundane automation. If people expect AI to replace entire roles, they are not going to be happy. If they conversely expect it to enable them to get more done, than they will be satisfied. AI is a tool to solve problems, it should not create its own set of problems.\n\nAligned with high expectations comes over-hyping the use of AI. Every company under the sun is going to start touting their ability to leverage machines to get work done. They will be looking to leverage AI to increase the perception of value in their business. In truth, AI is not new but it has come to a level of effectiveness that warrants it being called true AI. Unfortunately, too many companies will latch onto AI as a differentiator even though what they are doing may not be more than glorified automation processes. This falls into the same bucket as big data and blockchain \u2014 people will talk so much about it that the majority become desensitized to the topic before it has a chance to prove its own value.\n\nAI is a tool that provides strength, performance and efficiency to the average worker. It can automate mundane tasks and give them more impact in a shorter period of time. Its like the Iron Man suit that Tony Stark wears \u2014 it makes you bigger, faster and stronger, but it doesn\u2019t replace you. In the comics Tony does create an entire hollow army of Iron Man suits, but they get beaten because they are literally a shell of the man who created and wears them. Its the same here \u2014 with technology augmenting and not replacing the average worker.\n\nThe other interesting element of why AI is hotly discussed right now is voice. Without Voice, AI is nothing but systems. Voice is the interface for AI and it goes hand in hand with the use by consumers, whether at home or at work. This is due to the tipping point of where we live \u2014 that we no longer have to learn the language of machines, but rather machines are learning our language. You can speak to a machine and get things accomplished, and that makes it easier for anyone to leverage a higher level of technology. Nobody is required to learn a programming language or a new UI. They can say what they want and it happens. I guess that means if people stop taking, AI could falter. Thankfully I doubt that will be the case.\n\nVoice is the interface that powers AI, and AI is a tool to augment the average person. If we can get everyone thinking along those same lines, we can expect to see success.\n\nIf not \u2014 if we give in to higher expectations and over-hype, then AI may stall in its applications.\n\nHere\u2019s to hoping for the former and not the latter!"
    },
    {
        "url": "https://becominghuman.ai/natural-language-generation-5efe36797a7b",
        "title": "Natural Language Generation \u2013",
        "text": "A lot of the work I\u2019ve been doing recently has been reactionary. It starts with an event, most recently a webinar on the impact of GDPR, and ends with me a long way down a wiki-hole, sure that knowing this or that article by heart will come in handy in an upcoming client meeting. I\u2019m tempted to paraphrase Mr Jelinek\u2019s famous (-ly misquoted) quote as \u2018every time a strategist leaves, the productivity of the team goes up\u2019. Strategy as a discipline almost exclusively contains people who are too nosy for their own good, and want to tell everyone in the office about this or that article that they simply must read because it\u2019s great.\n\nThis month the CEO gave us a new strap-line: \u2018Technology based, human driven\u2019. This got me thinking about what an agency can do that would support this market position. We\u2019ve seen a lot of competition in the London marketing automation sector from companies making extensive use of \u2018artificial intelligence\u2019 as their USP, and I\u2019d like to take a deeper dive over the next couple of blogs into how that group of technologies can be separated out, and how they can support human creativity as a whole.\n\nComputers, no matter how sophisticated, only view the world in terms of 1s or 0s. (Well\u2026. With the exception of quantum computers, which I absolutely am not going to talk about here. Enjoy the wiki-hole.) Artificial intelligence, at it\u2019s most simple, is the programming of computers to perform tasks that usually require humans, through interpreting the 0s and 1s in the same way that we interpret data from our sensory facilities. As a phrase it\u2019s something of a misnomer, because it contains within it so many other technologies. Most of these begin with providing a computer with a tonne of data, and teaching it to recognise patterns.\n\nIn language acquisition theory, it\u2019s generally acknowledged that a child learning a new word follows a bell curve pattern. If you have a white cat, at first the child will only use the word \u2018white\u2019 in reference to the cat. Then, as they begin to understand that \u2018white\u2019 is different to \u2018cat\u2019, they will over-apply \u2018white\u2019 to things similar to the cat, like four-legged tables. Only once they understand that \u2018white\u2019 refers specifically to the colour, and that the colour can be found elsewhere, will they be able to apply it correctly to things like snow.\n\nThe same holds true for computing processes. Snapchat, for example, graduated from learning to recognise faces, to being able to target and apply lenses using specific facial features. But, if you\u2019re side on or there are a lot of people, the algorithm can\u2019t get a match for the pattern. However, the more side on faces it sees, and the more flexible the algorithm becomes, the more accurate it gets.\n\nAchieving the most accurate pattern recognition algorithm relies on machine learning, in that the programmer aims to provide the computer with the ability to learn and improve without further human intervention. For an example of this, look to Deep Mind\u2019s development of an inter-lingua. The technology behind Google Translate has succeeded in building a language that allows it to manage multilingual zero-shot translation (it can translate without needing to be trained in a language based on reasonable pattern recognition).\n\nRight. Now how does Natural Language Generation work?\n\nOnce pattern recognition has been achieved, and the computer is teaching itself, how does it then generate based on input? The answer to this can be found in the multi-disciplinary field of computational linguistics. It sounds like a bad joke, \u201cwhat happens if you lock a mathematician, a linguist, and a computer scientist in a room together?\u201d, but, as much as Fred Jelinek might not like it, the combination of these people yields some pretty spectacular punch lines.\n\nIf you didn\u2019t see it, the episode of South Park titled \u2018White People Renovating Homes\u2019, is absolutely worth watching for the clip that wreaked havoc by interacting with both Alexa and Google Home devices all over the world. It was a quality prank, but the complexities of the technologies involved is incredible. Not only did those home devices recognise language coming from a digital source, they correctly interpreted what was said, generated the requested shopping list, and responded in the same natural language.\n\nIn researching how this is possible, I found many references to formal language theory and generative grammar. Noam Chomsky is the founding father of the latter field, \u201cObserving that a seemingly infinite variety of language was available to individual human beings based on clearly finite resources and experience, he proposed a formal representation of the rules or syntax of language, called generative grammar, that could provide finite \u2014 indeed, concise \u2014 characterisations of such infinite languages.\u201d (Searls in Artificial Intelligence and Molecular Biology)\n\nThe theory provided form for axiomatic reasoning with regard to cognitive processes, meaning that formal logical expressions could be used to build a mathematical theory, enabling computers to learn and generate human languages. This is the foundation for Deep Mind\u2019s ability to work with complete phrases, comprehending grammar and syntax, rather than yielding the traditional dodgy word by word translations of old French GCSE presentations. These days you barely need to type the words in. Live translation tools like ili mean you can talk to anyone, anywhere, by \u2018decoding the brainwave matrix\u2019 with Babel Fish accuracy.\n\nHow can I use it in my marketing strategy now?\n\nNLG increasingly gives computers the ability to turn structured data into business intelligence reports. They can develop insights based on consumer interactions, and automatically explain what course to take in response to the data. This can mean automated compliance with regulation changes, personalised content for every user, and maximum value extraction from every data set. Returning to the new strap-line, using data sets to build intelligent insights is the \u2018technology base\u2019 of any single customer view.\n\nWhat is it about all this that has content marketers feeling edgy, and putting in all those extra hours to impress the boss? Any marketing professional will tell you it\u2019s easy to produce a paragraph or two of drivel, containing buzzwords like \u2018innovative\u2019 and \u2018cutting-edge\u2019, but it is exponentially more difficult to produce excellent, brand relevant, context appropriate and personal content that captures the imagination of your audience.\n\nThe \u2018human driven\u2019 aspect comes when we evaluate the limits of NLG. There are companies building fantastic NLG subject line optimiser tools, because the computer can succeed at targeting audience motivations and raising click rates in this concise format. However, whilst generative grammar does allow axiomatic processing of much of human language, being responsive to audience motivations at scale is currently beyond the reach of the technology. It\u2019s when you begin to write full email or landing page copy that the subtleties of human creativity continue to trounce the machine. Here, then, we find that humans are still top dog.\n\nIf you\u2019d like to know more, come speak to us at Adeptiv UK. We are a data-driven dialogue agency in London which specialises in helping clients get, keep and grow customers at scale."
    },
    {
        "url": "https://becominghuman.ai/lets-talk-about-advanced-analytics-a-brief-look-at-artificial-intelligence-ii-5fc334546229",
        "title": "Let\u2019s talk about Advanced Analytics: A brief look at Artificial Intelligence II",
        "text": "This is the great promise of analytics, although it seems it is never consolidated.\n\nPrescriptive analytics has a big potential to be a disruptive power in business. Along with predictive data, prescriptive analytics can contribute valuable knowledge about what can happen in the future and automate decision-making to achieve business goals. Prescriptive analytics can recommend the products that will lead to the biggest purchases, help choose marketing campaigns that will generate huge profits, determine the optimal routes that generate less costs and minimize the resources used to guarantee delivery times, among others. These solutions can lead to better actions to achieve business goals and enable agile decision-making that makes companies more efficient and competitive. In other words, the suggestions offered by analytics can offer value to a company.\n\nGartner defines prescriptive analytics as a form of advanced analytics that exploits data and uses complex event processing, a business rules engine and operational research to make better strategic, tactic and operational decisions. While all analytics approaches have been found to improve decision-making, only prescriptive analytics recommend the best solutions.\n\nIn essence, the solutions based on prescriptive analytics help to risk limit, increase efficiency and fulfill goals, allowing business owners to make better decisions for their companies even though conditions may be changing or uncertain. They give critical support in operational, tactic and strategic decisions. Prescriptive solutions offer simulation capacities that evaluate the probability of different outcomes and demonstrate which the best option is. Additionally, they provide tools to organizations that help them understand risks better.\n\nFor example, Google\u2019s self-driving car works with travel. It makes multiple decisions about what it should do based on predictions of future outcomes. When the car approaches an intersection, it needs to analyze the context of the moment, such as other vehicles, people, positions, routes, distance, the possible routes other drivers will take, etc. The car evaluates every possible option and strives to make the best decision based on the immediate goal: going through the intersection without violating any traffic laws or causing any accidents.\n\nThis mathematical discipline was founded by British scientists in World War II. It consists of using different math optimization techniques (lineal programming, mixed-integer programming (MIP), constraint programming, and heuristic algorithms) to solve complex problems, considering all restrictions and limits. The purpose of this mathematics model is to understand the impact of every possible decision. It quantifies each decision in order to choose the best result that will fulfill the target sets.\n\nThe optimization models help make complex decisions about cover strategies, supply chain planning, manufacturing schedules, stock optimization, route optimization, campaign optimization and workforce management. In some cases, the models are used to manage resources, reduce costs, increase revenue and maximize profit margins.\n\nHere is another example. Imagine the complexity involved in the maintenance operations planning of a fleet of aircraft. We need to consider the scheduling of every task (with different priorities), stock availability, spare and necessary tools, etc. Many resources are shared and limit accessibility. Additionally, each aircraft has different locations (airports, hangars, runway, routes, etc.). These tasks are carried out by specialized professionals, who are individuals with the necessary skills, time and availability to do the job (always taking in count the regulations and collective bargaining agreements). Furthermore, we should consider the schedules of each aircraft.\n\nGiven the competitive prices in the air passenger transport sector, this force minimizes operational costs. This requires that airlines maximize their resources (fleets, materials and humans) while fulfilling legal requirements, guaranteeing flight safety and maintaining flight scheduling. In this case, we can see the complexity and impact of decisions. Any incident can lead to flights being cancelled, a scheduling problem that translates to millions of euros in costs and unhappy customers. This also negatively affects the company\u2019s reputation. The past year, companies like Ryanair and Vueling have had serious trouble with planning, which has had a huge impact on their business. In these cases, math algorithms can help us assess all the data available, consider every restriction and evaluate every option to guarantee the fleet\u2019s schedule.\n\nYou can read more about this type of analytics in this link.\n\nSoftware fields attempts to automatize business logic through decision-making processes in which business rules imply a certain complexity and the result can be known in advance. Business Rules Management Systems (BRMS) are collaborative information systems that centralize the decision-making policies of an organization in only one repository. They permit an easy implementation, some of which can be done in natural language. Additionally, these platforms provide some advantages in the decision process like legibility, transparency and traceability. Thus, these systems are powerful allies in regulated business.\n\nBRMS are used widely in sectors like finance and insurance to make decisions about tariff calculations, subscription processes and risk management, among others.\n\nYou can read more about it here: (http://www.decidesoluciones.es/que-es-un-brms-y-sus-ventajas/).\n\nComplex Event Processing is another prescription field where technology provides the capability to analyze data from multiple sources, identify events and patterns, analyze impacts and develop action plans for them in real time. Thus, CEP is a solution that automates decision-making. It is supported in software architecture that handles large amounts of data and variables from different data sources. While CEP was born in the 1990s, it is currently being pushed with initiatives such as Big Data and IoT.\n\nExample \u201cPay As You Drive\u201d: (http://www.decidesoluciones.es/aplicando-cep-en-el-seguro-para-coches/).\n\nReach out more information about CEP \u201cComplex Event Processing\u201c: (http://www.decidesoluciones.es/introduccion-al-procesamiento-de-eventos-complejos-i/).\n\nCognitive computation or cognitive analytics are terms popularized by IBM that combine different analytics techniques within artificial intelligence. But what it is? This field aims to simulate the thinking process and emulate human reasoning through computation modeling. Imitating deductive modelling uses the human brain to extract conclusions and identify patterns through data and knowledge repositories. It also manages the uncertainty of each decision. As I have already indicated, there are some examples where machine learning is included into this cognitive field.\n\nIt is necessary to make sense of the data coming from different sources and interpret it before the information is processed. To do this, we help other fields like NLP (Neuro-Linguistic Programming). NLP looks to understand information within a context and interpret natural language the way a human being would.\n\nTheoretically, there are no limits to the cognitive development these systems can achieve, harnessing the benefits of parallel and massive computing.\n\nThis technology can be applied in medical care to help collect all existing knowledge about a symptomatology, including patient history, scientific publications, diagnostic tests, etc.\n\nActing as a virtual assistant supporting possible treatment options, and thanks to this apply the best possible treatment.\n\nIn conclusion, this same process could be applied in any field where there are large amounts of data that need to be processed and analyzed to solve problems. Other high-impact business areas include cyber-security, consumer behavior, product recommendation Chatbots, user and customer support bots, travel agents, etc."
    },
    {
        "url": "https://becominghuman.ai/cyber-security-a-lenders-greatest-risk-9d9b8e535d26",
        "title": "Cyber Security, A Lender\u2019s Greatest Risk \u2013",
        "text": "According to the latest SEC Risk Alert (August 2017), \u201c\u2026cybersecurity is one of the greatest risks facing the financial services industry.\u201d\n\nTheir position won\u2019t surprise anyone in the global lending arena, especially in light of Yahoo and Equifax. Yahoo has the dubious honor of holding both first place and second place on the list of worst data breaches. Equifax may rank further down the list; but their breach is more problematic, because the database includes social security number, home address, driver\u2019s license number and credit score for more than half the population of the United States. And it\u2019s the half of the population with financial accounts and a credit history.\n\nThe Ponemon Institute recently reported that:\n\n\u2022 The average data breach takes 45 days to remedy at a cost of $15 million (which doesn\u2019t include regulatory fines).\n\n \u2022 A single breach could bankrupt a small to mid-size business.\n\n \u2022 97% of all companies have already been hacked.\n\nLaw enforcement and government regulatory agencies are fighting cybercrime with a 2-prong approach \u2014 criminal prosecution and prevention.\n\nThey actively prosecute criminals for all types of cybercrime including: hacking, identity theft, money laundering and account fraud. Money laundering alone tops $500 billion each year. And this figure is probably the tip of the iceberg. The stealth nature of computer hacking makes it next to impossible to identify and physically locate the criminals. It\u2019s an overwhelming challenge, which has caused regulators in the US to shift their focus from prosecution to prevention. They are forcing accountability onto individual lenders, and requiring stringent, documented cybersecurity programs.\n\nThese government directed programs can be tricky to implement. Regulatory guidelines typically include a broad objective like, \u201cAnticipate and prevent security issues.\u201d The guidelines include suggestions on how to achieve the objective, but there\u2019s no clear blueprint for implementation.\n\nRegulatory penalties can be severe when you don\u2019t get it right. According to Boston Consulting Group, the worldwide fines levied against lenders since 2008 exceeds $321 billion. Fines imposed when poorly managed cybersecurity programs caused banks, and more importantly bank customers, to get caught up in money laundering and fraud schemes that may have been financing terrorist organizations.\n\nDigital lending is a fiercely competitive arena, driven by younger applicants who demand instant decisions from automated, data dynamic software systems. OnDeck an LaaS (lending-as-a-service) provider was recently quoted as saying, \u201cWe helped Chase take the small business loan process from six weeks to six clicks.\u201d The nature of these warp-speed origination systems makes them vulnerable to fraudsters who use stolen identities, or synthetic identities hiding behind cloaking technologies.\n\nToday\u2019s lending industry sits in the eye of a perfect storm, driven by three key components. First, alternative financing, fueled by fintech mobile apps, is growing by 51% per year. And traditional lenders are all going digital. It\u2019s big news that\u2019s catching the attention of hackers. Second, massive data breaches are throwing enormous amounts of personal financial data out on the dark web. And third, instant-decision software systems, often supported by third-party vendors, create a variety of vulnerabilities that cyber criminals are ready, willing and able to exploit.\n\nAt Turnkey Lender we\u2019ve identified six cyber safety best practices that should be part of every lender\u2019s playbook:\n\n\u2022 build a solid foundation\n\n\u2022 turn staff into cyber warriors\n\n\u2022 detect fraudulent loan applications\n\n\u2022 prevent account takeovers\n\n\u2022 identify cross-device use\n\n\u2022 deploy a cloud-based lending platform.\n\nLenders who treat cybersecurity like a DIY project are taking a big risk. Cybersecurity must be an ongoing initiative led by a designated cyber safety director. Your company should tap the expertise of both cyber safety and compliance consultants to help you develop, implement and maintain your program. Their experience should include a strong track record in the lending industry. And experience defending cybersecurity programs that were audited by a regulatory agency.\n\nThey\u2019ll start by reviewing your entire ecosystem for potential security gaps, including: data collection, storage, encryption, transmission protocols, and interfaces with outside third-party vendors. Mobile apps require special scrutiny, including: platform, servers, GPS receivers, cameras, sensors, social media accounts, etc. And you\u2019ll want to monitor and maintain proper security over the life of a financial product, not just during launch.\n\nYour program will include four distinct components:\n\n \u2022 plans to protect against a breach\n\n \u2022 plans to encrypt and obfuscate data in case of a breach\n\n \u2022 plans to decoy data and lure attackers away from valuable information\n\n \u2022 plans to respond immediately when a threat has been identified or an actual breach has occurred.\n\nThe vast majority of system breaches are caused by employee error or third-party vendors who mishandle data. Unfortunately, hacking via these two entry points is on the rise.\n\nHelp your staff understand how easy it is to cause a breach. That it\u2019s no more complicated than opening an email attachment, installing a thumb drive on a network computer system, sharing a document via personal email, or installing a business program on a personal computer. Your entire system can be instantly infiltrated with the intrusion lying dormant and difficult to detect until triggered from an outside source.\n\nAs soon as your team understands how they can become a hacker\u2019s best friend (or worst enemy), then they\u2019re already armed with the weapons they need to defend against an attack.\n\nYour company may want to connect with one of many employee education programs that specializes in teaching and reinforcing cyber safety practices. They can even conduct blind tests to show your staff how vulnerable they are to a cleverly designed Trojan horse. The high cost of a data breach makes these programs well worth the investment.\n\nLenders are constantly balancing risk and reward. As the credit decision process becomes more automated lenders must determine the best way to use security filters to reduce risk. Underuse increases the risk of fraud as you approve more bad accounts, but overuse reduces sales revenue as customers abandon applications and go to your competition.\n\nStart by implementing basic security protocols for AML (anti-money laundering), CIP (customer identification program) and KYC (know your customer). An advanced approach is to participate in a global shared intelligence database that flags stolen identities in real time. These tactics can all be integrated into an automated system, where potential fraud is detected and prevented without any inconvenience to a good application.\n\nAnother advantage of the global shared intelligence database is that it can help prevent unauthorized account access. Consumers and businesses tend to use the same user name, email address and password to open multiple accounts. So a fraudster can use the same stolen credentials to open new accounts and gain entry to existing accounts, where they\u2019re free to siphon open credit or request a line increase. All without the owner\u2019s knowledge.\n\nWe recommend a layered cybersecurity solution when it comes to account access. Start by encouraging customers to activate 2-factor authentication. Then integrate your software systems with a global shared intelligence database that flags suspicious log-in attempts from unknown mobile devices, known botnets or masked locations.\n\nConsumers demand omni-channel access to their accounts. On Monday morning a husband pays a bill before work from a home PC. On Wednesday lunchtime his partner deposits a check to their joint account using a smartphone with coffee shop WIFI connection. On Thursday afternoon she checks their account balance from her office computer. On Thursday night, while lying in bed, he updates profile information using a smartphone with home WIFI. And on Saturday morning they transfer funds between accounts and request a line increase over breakfast using their tablet or laptop with diner WIFI.\n\nThis level of cross-device activity used to be the domain of young techno-geeks, but Baby Boomers are getting in on the act. Their grandchildren activate online account access, and then walk them through their first transactions. Consumers of all ages are addicted, and lenders are scrambling to differentiate legitimate activity from fraud in real time. Customers appreciate protection when it\u2019s invisible, but you risk losing them to the competition when a fraud alert disrupts their day.\n\nTwenty-first-century lenders need sophisticated software with real-time identity updates to ensure a seamless experience. A comprehensive profile includes: physical address, multiple users, multiple email addresses per user, multiple computer and mobile devices, as well as location geo-targeting for mobile devices.\n\nRatan Jyoti @reach2ratan \u2014 Chief Information Security Officer (CISO) at Ujjivan Small Finance Bank \u2014 told us, \u201dThe burden of possessing, managing and maintaining the IT infrastructure has been the key challenge for the banking sector. Banking with the cloud has definitely been a game changer.\u201d\n\nSaaS or cloud-based software systems provide important security advantages for small to mid-size lenders who don\u2019t maintain an in-house IT department or compliance team.\n\nCybersecurity is already included as part of their fully managed service package. The platform is constantly upgraded with new technologies to prevent cybercrimes, and new protocols that conform to the latest regulatory compliance guidelines. The software system will include cutting edge data encryption and data obfuscation methods to minimize damage when a breach occurs. Advanced systems employ deception and decoy technologies with luring techniques and engagement servers to entice an attacker away from valuable information.\n\nYour hacker might find his way into your system, but the cupboard will be bare. If there\u2019s nothing of value to take, then he\u2019ll move on to an easier target.\n\nA cloud-based lending platform can provide cybersecurity expertise, cyber safety technologies and protocols, as well as regulatory compliance updates. All without the need to develop and manage the program in-house.\n\nMany countries around the globe recruit young hackers, and redirect their creative talents towards government-sponsored cyber safety programs. Unfortunately for US lenders most of their cyber talent supports private industry or criminal enterprise. US regulatory agencies are pushing prevention and accountability for cyberattacks onto the private sector. Lenders have literally been drafted into service, where they\u2019re now serving as soldiers in the war on cybercrime.\n\nThe best way to prevent similar worldwide regulated lender accountability is through industry self-regulation where bankers, non-bank lenders and fintech providers pool intelligence. So everyone implements the strongest cyber safety programs available."
    },
    {
        "url": "https://becominghuman.ai/types-of-machine-learning-977b9c00b3ef",
        "title": "Types of Machine Learning: Shallow \u2013",
        "text": "\u201cWait, so you mean there\u2019s more than one way in which a machine learns?\u201d\n\nI know, I know! My last article made it seem so simple, didn\u2019t it? A machine just recognizes patterns, makes predictions, and learns. Well, these super simple steps are going to now help you understand the different ways in which a machine can learn. A little intimidating, but don\u2019t worry, this next bit doesn\u2019t get harder (it\u2019s really easy to understand, I promise!).\n\nThe first type of shallow machine learning is supervised learning. And it\u2019s exactly what the name sounds like (see! it\u2019s already so simple). This type of learning is when the developer labels the variables that the machine will be working with. Within this domain there are two sectors of learning: regression and classification.\n\nRegression is the machine\u2019s ability to recognize numbers, and group them together to form predictions. An example of these variables can be the total area of a house in square feet, the number of bathrooms it has, and the number of bedrooms it contains. Through linear regression, the machine is able to predict the cost of a house by grouping different examples of houses, and learning from their variables and costs.\n\nClassification is the machine\u2019s ability to identify images, or things that are binary (yes\u2019 and no\u2019s). Think of this like playing flashcards with your machine! Your stack of cards contains different types of cats and dogs. The side you (the developer) can see is the image, and the machine sees the back of the card. Using numbers in between 0 and 1, the machine tries to guess what the animal is. It keeps doing this until it is able to distinguish between cats and dogs.\n\nThe above example on playing flashcards with the machine can be used to understand unsupervised learning. Except this time, the machine is playing flashcards with itself. On one side, is a cat or dog. On the other side, is the blank back of the card. The machine is able to randomly notice something special about the cats that is different from the dogs: let\u2019s say that it notices that the cats all have pointy ears, while the dogs have floppy ones. The machine then puts all the cats into one pile, and all the dogs into another one. This is called clustering.\n\nThe machine can also notice several things at once. Perhaps it notices that the cats have four legs, green eyes, and pointy ears. It also notices that the dogs have four legs, brown eyes, and floppy ears. It is able to reduce excessive variables attached to the images, such as the four legs, and is able to distinguish the cat from the dog using fewer features. This is called dimension reduction.\n\nReinforcement learning is teaching the machine through a reward system. Let\u2019s use a different analogy to understand this one, such as a mouse trying to find it\u2019s way through a maze. Before the mouse (machine) begins, he is given 1000 points. The objective is to maintain and increase the number of points that he has. When the mouse encounters a dead wall, or takes the wrong turn, and hits a mousetrap, it looses points. When it makes it through the maze, or takes the right turn, leading him to cheese, it gains points. Through this, the machine is incentive to make accurate predictions and actions.\n\nIf the mouse is able to model the maze in it\u2019s mind, and plan out a series of directions and steps it needs to take, then it is using the model-based form of reinforcement learning. If the mouse is learning through habit, by going through the maze multiple times, it is learning through the model-free method.\n\nThese different types of machine learning are best for different functions. supervised learning through regression and classification, is best used for speech and visual data prediction. Unsupervised learning is for random assortments of data, and it\u2019s use cases include advertisement suggestions. Reinforcement learning, which is learning via rewards, is used for creating physical robots. Note that despite their specialized use cases, it\u2019s important to know and understand each type of learning as they intermix and are used together for various applications."
    },
    {
        "url": "https://becominghuman.ai/artificial-neural-networks-and-deep-learning-a3c9136f2137",
        "title": "Artificial Neural Networks and Deep Learning \u2013",
        "text": "Ever since the advent of computers in particular and technology in general, the idea of creating intelligent systems has always fascinated us. The idea of something that can learn and adapt to changing circumstances and produce information and data relevant in solving real world problems has always been one of the top-most researched areas in the field of computer science. It\u2019s not just because it would be awesome to have such a program that will adapt to changing parameters (though that is indeed a noble cause) but also because of how immense the real-world implications of such a system would be.\n\nThe good news is that we have already made an amazing amount of progress in just the last couple years. This has been evident to everyone as given the amount of news coverage Artificial Intelligence and particularly Deep Learning has gotten over the last couple of years, even the most disinterested in the field would have read about it just to know what the hype is all about.\n\nBut the question is why just the last couple of years? Have we recently stumbled upon a game-changing way to do something that has caused all this interest in the field?\n\nWell, not really.\n\nThe fact is that the concepts on which today\u2019s Deep Learning is based upon have been known since 1943! The ideas and the mathematics behind all this have been known to us all along, the technology to implement them is what hasn\u2019t caught up yet \u2014 well, until now (a few years ago, to be exact)."
    },
    {
        "url": "https://becominghuman.ai/learning-from-mistakes-with-hindsight-experience-replay-547fce2b3305",
        "title": "Learning from mistakes with Hindsight Experience Replay",
        "text": "Hindsight Experience Replay is a paper submitted by OpenAI to NIPS2017. For more details you can read the paper here. (Code for all experiments can be found here)\n\nQ-Learning is a powerful reinforcement learning algorithm especially when combined with a powerful function approximator (such as deep neural networks) and other orthogonal techniques such as prioritized experience replay, double Q-learning, duelling networks and so on. When trained with carefully engineered reward functions, they are capable of achieving many tasks ranging from flying drones and toy helicopters to beating a top class world champion in Go.\n\nHowever, oftentimes many of these tasks are achieved through carefully hand engineered reward functions with weights and hyperparameters chosen from multiple trials. Without telling the agent how well it is performing each run, there is no way for our agent to tell which actions are good or bad for achieving our goal.\n\nLet\u2019s think of an example. Imagine that you are playing a game of hockey. You dribble your puck into your opponent\u2019s net. The puck shoots toward the net but just misses it slightly to the right. Although you didn\u2019t score the goal, maybe you learn that next time you should aim slightly towards left if you were in the same position.\n\nIf we were to simulate this with a standard reinforcement learning algorithm, it will most likely fail to learn much (if any at all). This is because our reward function is sparse (either 1 or 0, 1 if we score a goal and 0 if we don\u2019t), our agent won\u2019t learn how well it performed if it fails to put the puck in the net (which is the most likely outcome). To mitigate this problem we could introduce new terms to the reward function as follows:\n\nwhere each weight w is carefully chosen by trial and error with multiple training runs. These added reward functions are called shaped rewards (or composite rewards) and are used often in robotic manipulation tasks. For example, Popov et al. (2017) uses 5 complex terms in their reward function with carefully chosen weights in order to train the policy that stacks bricks on top of another. Using composite reward terms not only help your agent learn which actions are helpful for achieving the final goal but also acts as a good indicator of how well our agent is performing each run.\n\nOkay, now we know how to shape reward functions for simple and intuitive problems. We can just come up with an intuitive middle goal/reward for our agent that we think might help them achieve the final goal. And train the network on the new reward function and repeat adding new terms or change the weights on the reward terms until they start learning something. But what if there is a problem that cannot be solved this way? What if the problem we are solving is too non-intuitive and complex that adding a reward term is not a feasible option?\n\nOne ability humans have is to learn from our mistakes and adjust next time to avoid making the same mistake. We can apply the same concept to our reinforcement learning algorithm. Let\u2019s go back to the hockey example.\n\nNow this time, instead of concluding that the course of action you took was useless because you didn\u2019t score a goal, what if you say that maybe it didn\u2019t teach you how to score a goal, but it certainly taught you how NOT to shoot the puck. Or more precisely, what if you say it taught you how to shoot the puck slightly to the right side of the net? Now you can learn not only how to shoot towards the right, but you learn something new that might help you achieve the final goal every run. This is the idea behind Hindsight Experience Replay (HER).\n\nIn Deep Q-Learning, the main objective we want to learn is called the Q function. Q function (or an action value function) measures how helpful taking each action is, in terms of achieving the goal, given our current situation/position. The equation for Q function looks like this:\n\nWhere Q(s,a) is the Q function of action a given our current state s, r(s,a) is the reward for taking action a given our current state s, s\u2019 and a\u2019 are the next state the next action taken respectively. \u03b3 is the discount factor, determined by how important your immediate reward is compared to the future rewards.\n\nYou can train a deep Q network following this simple procedure:\n\nNow if we want our agent to perform well even with sparse rewards, we need to make a slight modification to the procedure above. The new procedure looks like this:\n\nHow does this help? By adding goal into the input space we are stating that there are multiple goals for our agent to observe. The new Q-function indicates how good taking each action is, given the current state, to achieving the current goal. And because we are sampling the new goals from the episode, the goals are now nodes that have been visited by our agent. So even if our agent fails to achieve the main goal in this episode, it has reached some states. By using those states as the new goal, now the agent can be trained with positive (or non negative) rewards.\n\nThe original paper mentions a simple experiment to validate the idea. Imagine a single arbitrary binary vector of length n. Let\u2019s call it an initial state. We can modify a single bit at a time (flip it from 0 to 1 or vice versa), and reach our final goal which is also an arbitrary binary vector of length n.\n\nAt each step, the agent receives a reward of -1 if the state is not equal to the final goal, and 0 otherwise. Each episode runs n times and terminates at the end of n th step. If we train a standard reinforcement learning algorithm on this environment, they won\u2019t learn anything even with n > 15 because the agent simply never observes any reward other than -1. Because there are too many states to visit, it is simply impractical for our agent to explore the large state space (the number of possible states increase exponentially with the size of n.)\n\nSee the figures below and compare the \u201cSuccess Rate\u201d between two models. The first figure shows the model trained with standard DQN training, the second figure is after adding hindsight experience replay. Both experiments are done in bit length n = 15 (the code for this experiment can be found here in Github.)\n\nOnce we train the model sufficiently, we can run inference to observe step by step.\n\nAs expected, even with a small bit length such as n = 15, the standard DQN algorithm fails to learn. We can clearly see that with hindsight experience replay modification, our agent can learn from such large action space without shaped rewards to guide it.\n\nOne interesting idea we surmise from hindsight experience replay paper is that having a shaped reward may deviate the agent from achieving the true goal. The shaped reward is either not helpful to achieving the goal or punishes our agent too much for exploration.\n\nBeing able to learn from a sparse reward is a very interesting idea that has huge potentials for robotic manipulation. Check out this video published by OpenAI, a robot arm trained with a sparse reward function and hindsight experience replay being able to perform three different tasks (push, slide and pick up + drop) with high success rate.\n\nThanks for reading and feedbacks and questions are always welcome! I might have made mistakes as I\u2019m still learning this very fast moving field of reinforcement learning!\n\np.s. If you have any question or would like to discuss more about reinforcement learning all emails are welcome to minsang.kim.142@gmail.com"
    },
    {
        "url": "https://becominghuman.ai/surfing-through-tensorflow-a81375af7f8f",
        "title": "Surfing through TensorFlow \u2013",
        "text": "All this while I have been coding up my Deep Learning projects using Keras. Its needless to explain the scintillating functionality that it provides. A few days back as I was going through the Deep learning courses offered by deeplearning.ai(coursera) revising my concepts , I was reminded of the fact that most of the Deep Learning and Machine models are better understood by the means of computation graphs.These computation graphs are more intuitive in nature.This engendered curiosity,forcing me to go a level lower and look through the abstraction i.e TensorFlow. Whenever I discuss this topic with my peers the response always is \u201cWhy reinvent the wheel ?\u201d.To that I say- \u201cIf you absolutely love graphs than this is the level of abstraction to be in\u201d.\n\nI started googling out looking for differences in Keras and TensorFlow. This search resulted in the following crisp points-\n\nWe can infer from the above that almost everything can be achieved using Keras and it is soon to overtake TF and become the leading TF-High level API.\n\nSo with this Surfing through TensorFlow series of articles I will go about explaining various functionalities and practical aspects of TensorFlow that I have gained and gaining through researching various sources.I thought of this as an idea to review the concepts that I grasp,but I sincerely hope that it helps some of you."
    },
    {
        "url": "https://becominghuman.ai/so-much-food-focused-content-in-march-688cb87bdb9f",
        "title": "So Much Food-Focused Content in March! \u2013",
        "text": "The 2018 SXSW Conference includes 24 different tracks of programming that cover all kinds of innovative ideas. Scheduled March 12\u201314 at the JW Marriott, the Food Track brings together the food community\u2019s most inspired entrepreneurs, chefs, data scientists, investors, filmmakers, and enthusiasts to connect and explore ways in which technology and innovation can be leveraged to transform the industry.\n\nEight of the most interesting sessions in this Food Track are as follows:\n\nAI Will Help Feed A Growing Planet. By the year 2040, there will be more people on the planet than food to feed them. Researchers want to change that. Learn how a sustainable solution to the emerging world food crisis is sprouting in an artificial intelligence (AI) lab in Carnegie Mellon University\u2019s Robotics Institute. Scheduled March 13.\n\nAlgae, Not Animals: The Plant-Based Revolution. Shifting away from unpronounceable chemicals, animal ingredients and \u201cpill fatigue,\u201d the food industry is increasingly plant-based. Panelists will discuss the technologies that are pushing these sectors toward a greener future with a flexible supply chain built by plants and algae. Scheduled March 12.\n\nFeatured Speaker: Andrew Zimmern. A four-time James Beard Award-winning TV personality, chef, writer and teacher, Zimmern is regarded as one of the most versatile and knowledgeable personalities in the food world. As the creator, executive producer and host of the Bizarre Foods franchise on Travel Channel and Andrew Zimmern\u2019s Driven by Food, he has explored cultures in more than 170 countries, promoting impactful ways to think about, create and live with food. Other world-renowned chefs joining this session are Jos\u00e9 Andr\u00e9s and Dana Cowin. Date TBA.\n\nThe Future of Food Delivery. The global food supply chain is rapidly changing, adding new players, challenging the existing structure. The consumer demand for more local and fresh food are drivers that will shape who will produce our food, how they will source ingredients, and the distance between your plate and our future farms. Scheduled March 12.\n\nMeet the Kitchen of the Future: Trends in Food Tech. With emerging technologic innovations, how we\u2019ve always operated in the kitchen space is rapidly changing. Now, home automation is widely available + affordable; you can operate almost anything with your smartphone, minimize germs + cut food waste w/ appliances. Leading experts will unveil latest tech advancements, discuss missteps and how you can personally benefit. Scheduled March 13.\n\nNew Mediums to Tell Authentic Food Stories. From community generated recipes and heart wrenching human interest stories to video, learn how brand marketers, news outlets, influencers and consumers can benefit from the new era of digital food transparency. Scheduled March 13.\n\nThe Restaurant of the Future. What kind of restaurant will we all be eating at 5 or 10 years from now? How will consumer preferences, food and climate policy, social and economic change, gender and diversity issues, technological advances, and other forces play a role in the evolution of the restaurant industry? Scheduled March 12.\n\nShaping the Future of Bread. Modernist Cuisine founder Nathan Myhrvold (pictured above) is rethinking bread from the ground up. His new book \u201cModernist Bread\u201d is the culmination of over four years of nonstop research, experiments, and baking. Join Myhrvold for a conversation about what he\u2019s discovered and how everyone can shape bread\u2019s future. Scheduled March 12.\n\nFind more content at SXSW 2018 by browsing the online schedule. Use the search bar at the top right corner to find the sessions and networking events that you most want to attend. Finally, buy your badge before the end of the day on Friday, February 9 to save $100 on the walkup fees.\n\nHugh Forrest serves as Chief Programming Officer at SXSW, the world\u2019s most unique gathering of creative professionals. He also tries to write at least four paragraphs per day on Medium. These posts often cover tech-related trends; other times they focus on books, pop culture, sports and other current events."
    },
    {
        "url": "https://becominghuman.ai/how-to-prevent-bias-in-machine-learning-fbd9adf1198",
        "title": "How to Prevent Bias in Machine Learning \u2013",
        "text": "The following article is based on work done for my graduate thesis titled: Ethics and Bias in Machine Learning: A Technical Study of What Makes Us \u201cGood,\u201d covering the limitations of machine learning algorithms when it comes to inclusivity and fairness.\n\nAs Cathy O\u2019Neil discusses in her book, Weapons of Math Destruction, the seeming impenetrability and absolute value of machine learning may not be all that we bargained for. Though machine learning appears to indisputably increase business value and efficiency, in some cases, it can sow inequality deeper by hard-coding it into our machines. It is imperative that machine learning experts, creators, and contributors account for \u201cdoing the right thing\u201d as much as they do \u201cmeeting the bottom line\u201d to balance the enormous power these mechanical decision makers possess.\n\nA machine learning algorithm is typically code written by a data scientist in a programming language such as R, Python, or Javascript. Algorithms are chosen based on a given business use case in which much or very little could be known about a given dataset. Supervised or unsupervised algorithms are used to find and identify trends in large sets of data. The five primary components of creating and using a machine learning algorithm are data preprocessing, fitting the model, making predictions, visualizing results, and evaluating results. [source]\n\nOverall, the most influential components of a machine learning model are the algorithm and data used. A model is only as good as the data it learns from, and as we will see below, this is imperative to maintaining the integrity of decisions made from machine learning algorithms.\n\nMachine bias is the growing body of research around the ways in which algorithms exhibit the bias of their creators or their input data. As machine learning is increasingly used across all industries, bias is being discovered with subtle and obvious consequences. It is important to note that machine learning algorithms rely on bias \u2014 statistical bias. These algorithms could not make any predictions at all if it wasn\u2019t for the statistical bias required to make estimations about new data that the model has never seen before. Machine bias is different than statistical bias. Said simply \u2014 machine bias is programming that assumes the prejudice of its creators or data.\n\nThere have been a wide range of discoveries of biased machine learning outcomes over the last few years in correlation with the growing use of the technology. As much as machine learning offers significant increases in convenience, oftentimes this benefits some to the detriment of others.\n\nEvery company that builds technology in any capacity is at risk of incorporating the prejudice of its creators.\n\nMachine bias is increasingly impactful due to its expansive uses in the modern world. Algorithms demonstrating machine bias may harm human life in an unfair capacity. Often this happens when the list of data categories is too limited, or inappropriate or invalid personal data is used. Some of these consequences may seem hypothetical, innocuous, and long term, while others are immediate and direct.\n\nThe immediate consequences of machine learning can be observed from the growing use of these algorithms in law enforcement. From research conducted by ProPublica, a non-profit research institution, it was found that COMPAS, a machine learning algorithm used to determine criminal defendants\u2019 likelihood to recommit crimes, was biased in how it made predictions. The algorithm is used by judges in over a dozen states to make decisions on pre-trial conditions, and sometimes, in actual sentencing. ProPublica found that the algorithm was two times more likely to incorrectly predict black defendants were high risk for recommitting a crime, and conversely two times more likely to incorrectly predict white defendants were low risk for recommitting a crime. This difference could (and did) mean defendants were held before trial or let out on bail, or were convicted with a tougher or more lenient sentence.\n\nFacial recognition is a well-known form of machine learning. Using facial recognition software, common on social media platforms like Facebook or with Google Photo, an algorithm can determine a face it has seen before in a photo by name. In July 2015, Jacky Alcine posted to Twitter a Google Photo that used facial recognition tagging him and another friend as gorillas [source]. The machine learning algorithm in the Google Photo software learned incorrectly. Most likely, the data used to train the algorithm did not include substantial training for facial appearances of all kinds.\n\nSimilarly, when you surf the web or interact with various social media platforms, it is common to see advertisements along the side, top, or bottom of your web page. More often than not, in recent years, these ads are tailored to your specific interests, needs, and wants \u2014 as determined by your web search history. Through the use of marketing automation tools, it is possible to track a user\u2019s interaction with the website they are on, and anything they do from a given website. This gathered data is captured and analyzed with machine learning algorithms, which then predict what types of ads you\u2019d like to see based on your interactions online. A study by AdFisher recently revealed that men were six times more likely than women to see Google ads for high paying jobs [source]. The immediate consequence of this machine bias is that a woman may not see a high paying job and therefore is less likely to know about it and apply. The long-term result could mean more ingrained gender discrepancies in high ranking positions.\n\nThe first step to correcting bias that results from machine learning algorithms is acknowledging the bias exists. Researchers have been discussing ethical machine making since as early as 1985, when James Moor defined implicit and explicit ethical agents. Implicit agents are ethical because of their inherent programming or purpose. Explicit agents are machines given principles or examples to learn from to make ethical decisions in uncertain or unknown circumstances.\n\nIt is imperative that the AI community emphasize the use of machine ethics to prevent and correct for bias in machine learning algorithms. There are three primary ways that ethics can be used to mitigate negative unfairness in algorithmic programming: technical, political, and social.\n\nAs machine learning and AI experts say, \u201cgarbage in, garbage out\u201d [source]. Proactive or retroactive efforts can be taken to find technical solutions within the code used to conduct machine learning. To save time, energy, and resources, it is preferable to take proactive measures to avoid bias in the first place. This includes measures such as finding comprehensive data, experimenting with different datasets and metrics, increased representation in the technical workforce, external validity testing, and auditing. When protected attributes like age, gender, and race are factors in an algorithm, it is important to incorporate them while also addressing the social bias that may come from particular attributes within the code. Obviously, this is a sensitive and complicated task but is not impossible. Exciting research was done last year to test this very goal [source].\n\nPolitical pressure can be used to incentivize resource allocation to ethical machine learning creation. The EU passed the General Data Protection Regulation (GDPR) to take effect in 2018, which outlines citizens\u2019 \u201cright to explanation\u201d regarding machine learning decisions made about them. The Obama administration also formally pushed for investigation of big data and machine learning algorithms to ensure fairness in 2016. Policies and publicity from political figures give more weight to the value and importance of ethical machine learning creation.\n\nLastly, social awareness can make an enormous difference in correcting for the far-reaching impacts of bias in machine learning. To date, much of the prejudice found in machine learning algorithms has been identified by the very users it hurts. Educating users about machine learning, how it is used and why, empowers consumers to demand higher ethical standards and transparent practices from the corporations that serve them. Community groups such as the Algorithmic Justice League (AJL) founded by Joy Buolamwini, help to promote crowd-sourced reporting and the study of bias in machine learning and other technologies. Involvement from diverse populations in the ethical creation and consumption of machine learning predictions will lead to further progress in ethics that include all users.\n\nIt is up to all of us to determine the path machine learning algorithms take and how well they ultimately serve our highest purposes. If we carefully consider the prejudices we inherently carry when creating these technologies and correct for them, creators and the companies they lead will harness the real power of machine learning to the benefit of corporations and consumers.\n\nThank you for reading! If you have any questions or ideas on what you\u2019d like to hear more about regarding ethics in machine learning and AI, please feel free to comment below or shoot me a message: nicole.shadowen@gmail.com."
    },
    {
        "url": "https://becominghuman.ai/how-we-use-data-driven-decisions-to-help-companies-boost-their-exports-68ae376e7e59",
        "title": "How We Use Data-Driven Decisions \u2014 Part 1: Helping Companies To Boost Their Exports",
        "text": "Recently, I wrote about our AI-base platform that was designed to help analysts to find actionable insights and make data driven decisions. According to our records, one of the most popular question that analysts ask is how to find the best overseas market to boost a company\u2019s exports.\n\nOur AI platform can answer this question, like many other, using its own algorithm, that we called \u20185 Easy Steps\u2019. It was primarily developed by our in-house analysts and then was adjusted with machine learning principles. To be honest, these steps are easy only if you have big data at your fingertips. Hopefully, we\u2019ve got it.\n\nBefore we dive into the five steps export guide, let me introduce you to the hero of our post. Our hero is a producer of bananas who sells his products on the local market. The competition here is quite severe, and the market size is limited. He needs new ideas to grow his business. The producer decides that the time has come to supply his goods to other countries.\n\nAs he looks at the map, he understands that there are great opportunities before him. There are more than 200 countries and in each of them people eat and love bananas. But here\u2019s the question \u2014 200 countries are too many for him. Our hero would really love to choose one country \u2014 the best and most promising country.\n\nAs he starts weighing up his options, how does he make sure, that he doesn\u2019t make the wrong choice?\n\nHow can he be sure, that he will not lose his time and money trying to enter a market, where nobody is waiting for him?\n\nEntering a new market is a long road. We will help the banana producer make his first steps in the right direction.\n\nThe first question we will help banana producer answer is: Which markets performed best in terms of banana sales in the last 5 years? And which markets were among the outsiders?\n\nIn order to do that, we will build our own map. Let\u2019s take a square and divide it into four parts. On the right side, we will mark the countries with the highest level of consumption of bananas \u2014 these are the largest markets. On the left we will input the small markets. We will put the countries, where banana consumption has been growing for the last 5 years, at the top of the graph. At the bottom of the graph, meanwhile, we will input countries where consumption has been declining or showing very little growth.\n\nNow let\u2019s gather the necessary information here and put the countries in the relevant quadrants.\n\nIn the lower left quadrant, we can see countries with small markets, where the consumption of bananas is falling. These countries are not appealing to our producer, so cross them out and leave them out of further consideration. The rest of the markets showed a relatively good result. The best countries, in terms of volume and dynamics, can be found in the right upper quadrant. But let\u2019s not rush to conclusions, so let\u2019s take the remaining countries and move on to the next stage of our analysis.\n\nAt this stage, we will answer the following question: which countries are the most promising, in terms of banana consumption growth, and which ones are already saturated?\n\nTo answer this question, we will be using information on per capita consumption. In other words: how many bananas does one person consume on a yearly basis?\n\nWe will source this data here.\n\nThus, at the top, we have countries with the highest average per capita consumption. This means that these markets are saturated and, most likely, will no longer grow. They don\u2019t have any further growth potential. That is why we will exclude them from our analysis and move on to step 3.\n\nThe next question is: in which countries are local producers unable to satisfy the increasing demand for bananas? Or, in some cases, which countries do not produce bananas, at all?\n\nIn order to find out, we will draw a new map. We\u2019ll simply take a square and divide it into 4 parts. On the right, we will mark the countries where a large volume of imports is observed. To the left \u2014 those countries where import levels are low. At the top, we will mark countries with growing imports. Below \u2014 where imports are falling or showing little growth.\n\nLet\u2019s source the necessary data here and place the remaining countries in the square in accordance with the import dynamic and volume.\n\nAs you might guess, in the left lower quadrant are countries with a low volume of import of bananas and a negative dynamic. Most likely, local producers assume strong positions in these markets. This market holds a lack of development potential for Importers. We will exclude these countries from our analysis and move on to the next stage.\n\nAt this stage, we will answer the following question: what barriers should the manufacturer overcome in order to enter new export markets?\n\nEntrance barriers can be associated with high tariffs, the need to obtain licenses or certificates, and the complexity of logistics, etc. For each of the remaining markets, we will conduct an analysis of the barriers for entry, information about which can be found here.\n\nIf a barrier is proving difficult to overcome, we will mark it with an X. In the end, we will end up with markets with relatively low or surmountable barriers.\n\nIn the last 5th stage of our analysis, we will answer the following question: which markets are the most profitable for banana producers?\n\nIn order to do this, we will draw a new map. Take a square, divide it into 4 parts. On the right, let\u2019s mark the countries where we observe the highest price of imported product. On the left are the countries with the lowest prices. In the upper half, we mark the countries where the prices of imported products have grown in the last 5 years. In the lower half, we\u2019ll input countries where they fell.\n\nTake the necessary data here, and place the countries in the corresponding quadrants.\n\nIt is easy to see that the countries with the highest and growing prices for imported bananas are in the right upper quadrant. For our producer, these are the most profitable markets.\n\nWe\u2019ve gone through 5 steps. First, we chose the largest and fastest growing markets. Of these countries, we took the markets which are still unsaturated. Then, we chose the markets with a large and growing share of imports. After this stage, we filtered out the markets with insurmountable entry barriers. Of the remaining countries, we selected markets with the highest import prices. As a result, we helped our banana producer to find the most promising new target market. If you want to grow your business and enter new export markets, you can use the 5 easy steps to conduct your own analysis."
    },
    {
        "url": "https://becominghuman.ai/reality-check-are-cyborgs-the-next-step-in-human-evolution-90d9ce524ed",
        "title": "Reality Check: Are Cyborgs the Next Step in Human Evolution?",
        "text": "Reality Check: Are Cyborgs the Next Step in Human Evolution?\n\nIn the latest movie from DC Comics Extended Universe Justice League, we finally get to meet one of the members of the superhero squad \u2014 Cyborg. Both in the original comics and in the new film, Victor Stone\u2019s mind and body have been advanced against his will to create a better version of a human. By the comic book classic formula, Victor has to learn to control these new powers and overcome the feeling of being a threat to humanity.\n\nNeil Harbisson is a cyborg too, only in real life. His \u201cpower\u201d is much more poetic and definitely less resented by its carrier \u2014 he can hear colors. An antenna implanted in his skull allows Neil to receive audio signals that he can interpret into colors. As a colorblind person, Neil\u2019s natural abilities have been extended thanks to this technology. Now Neil creates art aimed at expanding human senses \u2014 he produces paintings and piano concerts to help people see how he perceives the world with his newfound sense.\n\nNeil\u2019s modification of his natural senses albeit unconventional is not that farfetched for every other human living now. How? Technology has inserted itself into our lives to the extent to which we delegate some of our functions to it. We use it to store our memories, we rely on it for calculations and information input, we even use it to bend space and time by bringing other continents closer, by making our voices travel miles and reaching people across oceans. Even analogue tools like eyeglasses have been enhancing people\u2019s vision for more than 700 years. A digital hearing aid can help 28.8 million American adults to once again experience sound. Prosthetic limbs will soon allow amputees to receive sensory feedback, meaning they will be able to feel the touch.\n\nBut what do you do when your desire to enhance your physical abilities isn\u2019t affected by your disabilities? Artist and a co-founder of the Cyborg Foundation, Moon Ribas, has a sensor implanted in her elbow that allows her to feel seismic activity all over the world through vibrations. The sensor receives this data online, through an iPhone app, locating earthquakes of different intensity, which Ribas translates into movement.\n\nHarbisson and Ribas call themselves cyborgs. As a concept, cybernetics studies the act of interaction, control, and organization of systems of any kind, be it human, animal, or AI. Enhancing and transforming human preexisting abilities falls under the term transhumanism \u2014 the movement that studies the benefits, opportunities, and limitations of enhancing ourselves with the help of technology.\n\nNot everyone, however, finds such possibilities appealing. Science fiction likes to explore these themes in a most morbid way, showing us the decay of humanity, in which human bodies will become hackable like computers. This representation of the human enhancement movement along with the supposed threat it poses to traditional values creates an inaccurate vision in the minds of many.\n\nWhat if we lose connection with our humaneness? Should we play God and go beyond our natural abilities? What are the societal changes that this progression imposes? Are we ready to see RoboCops patrolling our streets?\n\nAs Elon Musk and Nick Bostrom are raising awareness about the dangers of artificial intelligence, the opponents of human augmentation fear the decay of humanity due to our fusion with technology.\n\nEven before Google Glass hit the stores in 2014, critics concerned about the loss of privacy joining with those concerned with distinguishing digital from human. These leitmotifs are another science fiction exploration, showing us as two-legged computers ripe for hacking.\n\nBut is it reasonable to be worried about transhumanism?\n\nThe stop-the-cyborgs campaign that anticipated before the launch of Google Glass was led by the idea that anyone wearing the device would be able to record their surroundings without others knowing. The device was quickly banned at some restaurants, bars, and movie theaters, creating an unspoken social warning: Take it off if you don\u2019t want to be creepy.\n\nIn 2015, engineer Seth Wahle demonstrated how easy it is to manipulate smartphones or security systems with the use of the almost undetectable NFC chip in his palm.\n\nNFC/RFID chips are usually installed between an index finger and a thumb\n\nSource: BioTeq\n\nThe problem with Google Glass and other interfering technology could be solved by creating regulations and policies that users or manufacturers would have to obey. Like every cellphone shipped to Japan is required to have a non-removable shutter sound, there\u2019s a way to ensure that such body enhancements can\u2019t be used against another person\u2019s will. And it\u2019s sensible to create these regulations before another questionable technology panics the masses.\n\nThe fear that people will eventually turn into emotionless machines is supported by the media that likes to humanize artificial intelligence and strip cyborgs of empathy and morality. But is it realistic?\n\nWe can argue about what it means to be a human. One would say that organic body parts make you a human, but are the people with artificial organs and limbs less human than anyone else? Medical bionics are actively used for patients on waiting lists for human transplants. The artificial organ market is expected to grow by 9 percent each year. They will soon be mass produced and help eliminate waiting lists altogether. How about a brain? We can\u2019t yet construct a machine simulating all the complicated processes happening inside our minds though the work is ongoing. Some might argue that changing anything about our bodies or minds is unnatural, but the definition of what is natural is evolving as well.\n\nA recent survey uncovered the correlation between religion and a willingness to accept digital body enhancements. The more religious the person is, the less compelling and ethical they find the idea of transhumanism. However, this thinking doesn\u2019t represent all religions. Take Christian Transhumanist Association, for instance. One thing is clear \u2014 there\u2019s no absolute right or wrong regarding these issues.\n\n\u201cSome would express fear that emerging augmentations would create an arms race, that threatens to leave behind those who choose not to be augmented,\u201d says transhumanist writer Gennady Stolyarov. \u201cBut this assumes everyone will seek to compete with everyone else.\u201d\n\nThere\u2019s an obvious concern that the opportunities to empower ourselves will be initially available only to the privileged, thus adding to the long list of characteristics by which we are already dividing ourselves as human beings. As technology won\u2019t be accessible to everyone at the same time, there will inevitably be those who have more information, more power, more cognitive abilities.\n\nThe double amputee runner Oscar Pistorius was prohibited from participating in the Olympics in 2008 along with able-bodied athletes due to his greater than 30 percent mechanical advantage. Similar concerns may arise soon for chess and go players who can use deep learning algorithms to win competitions.\n\nHowever, the technological advancements don\u2019t usually come in single tsunami waves that smash everything in their way. The transition will most likely happen gradually, allowing everyone to grab a piece of this transhumanist pie at their own pace. Some will gladly replace one eye with high-quality connected cameras. Others will feel more comfortable wearing magnetic bands around their heads to improve productivity. Just like with smartphone adoption in the US, it will grow steadily.\n\nSmartphone adoption in the US has grown from 4 percent of entire mobile market in 2007 to 54.9 percent only five years later\n\nSource: comScore\n\nIt\u2019s fair to predict that transhumanist technology will be adopted at a similar pace with the same pervasive result ten years later.\n\nBut exactly what technology are we talking about? The transhumanist ideology doesn\u2019t just entail enabling people with telepathic powers or having a flash drive implanted in your finger. It also revolves around the idea of reprogramming diseases, improving our sight in the dark, or stimulating neural activity. What technology and tools do we already have and what does it take to merge with machines for good?\n\nIn a recent study, the consulting and research firm Frost & Sullivan defined three types of transhumanism: evolving body, thought, and behavior. Let\u2019s talk about the possibilities of human enhancement in those same ways.\n\nRegular contact lenses, smart watches, and mind-controlled prosthetics are not the only examples of how humans can evolve and enhance their physical properties. Body hacking can be roughly divided into the following categories.\n\nWearables. Fitness trackers, health monitors, even smart clothes connected to your phone are the wearables meant to augment the way people experience the world.\n\nThe most widely developed applications of wearable technology are in healthcare. They\u2019re used to monitor and record health data allowing for faster diagnosing and prompt reaction to disturbances. For example, a wearable patch Fever Scout allows you to monitor a person\u2019s temperature in real time via an app. The Go2Sleep ring that will become available in May 2018 monitors your heart rate, pulse, and oxygen levels as an alternative to expensive and complex in-clinic testing. According to a Rock Health report, wearables attract the third-highest amount of investments in all digital health, after big data and genomics.\n\nBesides healthcare, wearable technology has potential in manufacturing as well. North Star BlueScope Steel along with IBM developed smart helmets and wristbands that monitor each worker\u2019s individual metrics to safely guide them away from hazardous areas and notify them about unsafe conditions. The agricultural equipment manufacturer John Deere uses VR headsets to help reviewers determine the potential dangers that go into assembling a product.\n\nAmong other use cases you can find payments (implemented on Apple Watch and Samsung Gear among others), sports equipment (see smart glasses for cyclists by Solos), insurance, security, planning, and navigation.\n\nImplants. Kevin Warwick is one of the first self-proclaimed cyborgs. Back in 1998, he got a chip implanted in his left arm to automatically access the doors and computers in his laboratory. The radio frequency identification (RFID) technology in his implant is the same technology used for security cards and pet implants, which easily track pets or activate pet doors in our homes.\n\nToday the number of people implanting similar chips is growing. Dangerous Things, a biohacking supplier, provides hardware, tools, and other implanting equipment for conducting safe and successful procedures. Their products range from NFC (near field communication) chips for data transferring to biomagnets for lifting small metallic objects by touch.\n\nHealth risks of such implanting procedures have not been entirely studied. However, proponents argue that they are no more dangerous than getting a piercing or a tattoo. The broad potential of these applications has not yet been put into action. For instance, to be able to make payments with the tap of a finger, you first need permission from a bank and a payment gateway system. But, with growing demand, it\u2019s fair to expect gradual adoption of implants.\n\nReprogramming cells. Changing an organism\u2019s DNA to cure diseases and inhibit aging has been one of the most ambitious initiatives of modern science. And it\u2019s no longer unrealistic. The most recent genome editing method called CRISPR-Cas9 has already cured two girls of leukemia. The method\u2019s simplicity and versatility is promising to deliver impressive results in treating viruses and even passing the improved genomes to future generations.\n\nHowever, due to ethical concerns, the wide adoption of the technology is questionable. Besides treating HIV and manufacturing revolutionary drugs, CRISPR may improve the general intelligence of an embryo or even design a human with desired features. Such prospects send some alarming signals from moral, religious, and legal standpoints and, therefore, the methodology first must to be studied and probably regulated prior to widespread application.\n\nHumans have been unknowingly enhancing their cognitive abilities with the help of technology for decades. We don\u2019t need to know all the contact numbers in our phone directory by heart, we don\u2019t have to store the information about upcoming meetings in our memory, we easily make and copy notes of all the information we receive. But can we go further?\n\nAny augmentation of information processing in the brain including perception, attention, memory, or ability to learn can be filed under cognitive enhancement.\n\nToday, all of the technology enhancing our abilities is external. It\u2019s been more than a century since the creation of the infamous postcards predicting the future in 2000 but we still don\u2019t load information directly into our brains.\n\nFrom the series of futuristic pictures created in 1899, 1900, 1901 and 1910\n\nSource: The Public Domain Review\n\nThe use of current cognitive enhancement pharmaceuticals also known as nootropics is still debated. Nootropics promote the production of neurotransmitters and increase circulation in the brain, thus boosting the efficiency of cognitive functions. Neuroscientists and psychologists are drawing attention to alarming side effects while numerous students are legally taking smart drugs to handle stress and academic performance during exam season. Initially safe, smart drugs can lead to addiction or memory impairment in underdeveloped brains, especially when exceeding recommended dosage and duration of use.\n\nWhen it comes to technology, the results are even more vague. The US Defense Innovation Unit has recently admitted to the testing neuro-stimulant technologies for elite forces, and mentioned 20-hour peak performance with the help of electrical stimulation. While electrical brain stimulation currently used in therapy is helpful in research on the animal and human brain, the widespread or even commercial benefits of the technology are not only questionable but perhaps even harmful.\n\nCommercial brain stimulating kits like the ones by foc.us promise to increase capability to learn but are highly unreliable. They use Transcranial Direct Current Stimulation (tDCS) \u2014 an experimental treatment claiming to enhance social and learning capabilities with the help of electrical jolts directed to your brain. A user can control the intensity and duration of a treatment; however, to achieve the desired effect, they should be able to repeat laboratory conditions at home, or they risk seizures and head burns.\n\nThe tDCS kit by foc.us promises to \u201cexcite\u201d brain cells and help them reach their prime activity\n\nAnother non-invasive form of neurostimulating technology is TMS \u2014 transcranial magnetic stimulation, an FDA-approved methodology for treating major depression, Alzheimer\u2019s disease, or addictions. Calibrated to target specific regions of a brain, magnetic pulses are helping release neurotransmitters thus altering cognitive performance. Beside mental health benefits, scientists report numerous instances of attention and memory improvement associated with TMS. Either way, the research in this area has just started and it\u2019s still unclear whether the effect of magnetic stimulation is long-term.\n\nThe possibilities of augmenting human empathy, collaborativeness, and decision-making processes are surprisingly poorly explored, especially considering how these features are the objects of concern for the anti-transhumanism movement. If people can become less empathic or even psychopathic due to technology, we should research how to maintain or improve our social skills.\n\nWe can ensure human enhancement through improving attention and ability to learn, but we still lack an important component to a fully advanced human \u2014 motivation and discipline.\n\nIn his recent article, the innovation thought leader and writer Michael Schrage offered the term \u201cselvesware\u201d to define personal assistants of the future \u2014 data-driven and personalized tools to augment our cognitive and behavioral attitudes. While Siri and Alexa perform tasks and learn new skills every day, true augmentation, according to Schrage, doesn\u2019t come with acquiring more external assistance, but with improving our own selves using analytics.\n\nSelvesware will be able to monitor our physical data like Fitbit, and then predict performance and suggest better options to distribute time at the workplace. Depending on your profession, these assistants will automatically go around your personal weaknesses and elevate your strengths. They will help designers and artists overcome creative blocks by helping them find a new perspective or suggesting effective choices from their previous projects. Along with wearables and sensors, selvesware will use such cognitive improvement practices like mindfulness, mental maps, and deep focus work to replace tons of productivity apps and make us more naturally disciplined.\n\nThe key here is \u201cnaturally\u201d. The robot-like employees moving in perfect rows along the corridors of office buildings is not something anyone wants unless you\u2019re a cartoon villain. But in the era where machine intelligence boosts technology performance, it\u2019s inevitable that it will soon automate and augment human potential to make us better experts while organically balancing our personal and professional lives.\n\nJames Young won a chance to receive a 3D-printed prosthetic arm inspired by the game Metal Gear Solid. The arm looks futuristic and powerful. It\u2019s equipped with a USB port and even a small drone. But it\u2019s very heavy. Young admits that he doesn\u2019t wear it most of the times. The so-called Phantom Arm is more artistic project exploring the alternative possibilities of a human arm than a product ready for mass production. And that does not even consider the price tag of such custom technology.\n\nIf amputees and physically or mentally challenged people are the ones most likely interested in obtaining enhancements, what about those who don\u2019t have a strong need to make their lives easier? The demand for microchips is growing but implanting one small sensor in your hand is not the same as replacing a real working arm, even with something that potentially does a better job.\n\nThe possibilities for transhumanist applications are varied but not entirely solid. Perhaps, the concept of what it means to augment humans will shift over time as well as how we see the difference between technology and biology. How soon will the enhancements be available not only for the people who need them, but also the ones who want them? We can only follow the current studies and wait for the appearance of science-backed commercial projects."
    },
    {
        "url": "https://becominghuman.ai/how-to-choose-the-right-lending-software-1e2b40071960",
        "title": "How to Choose the Right Lending Software \u2013",
        "text": "Based on decades of hands-on experience we\u2019ve boiled the decision process down to these 8 key factors you\u2019ll want to consider when choosing origination and servicing software. One that delivers long-term benefits to your operation.\n\nThink of lending software as a tool to support your business strategy. Start the decision process by reviewing your short-term business goals, long-term business goals, your current software packages and current technologies. This review will act as the basis for a software and technology gap analysis.\n\nHere are some of the top questions you\u2019ll need to answer with your team:\n\nThe end result of your gap analysis is a concise list of lending software features and functionality tailored to your business. And this checklist will guide you to the right software for your business.\n\nIt\u2019s a good idea to work with a provider who acts like a strategic partner. One who can meet your current needs within your budget; can accommodate changes to your software requirements as your business evolves; and is constantly developing new features to ensure their state-of-the-art system stays out in front of industry trends.\n\nAs a small to mid-size lender you probably don\u2019t have a dedicated IT department to develop and manage an advanced lending software program. That\u2019s one of the reasons SaaS (Software-as-a-Service) has become so prevalent over the past few years. It\u2019s easy to deploy, less expensive to launch than comparable on-premises programs, and it\u2019s fully managed for you.\n\nSaaS platforms are hosted from a remote, cloud location. The service package includes redundant back-up systems and high level cyber security.\n\nThese software systems deliver a variety of benefits to SMB lenders. As a subscriber you gain easy access to advanced features and functionality. You enjoy regular maintenance, software upgrades and new product releases, along with IT and customer service support. The program is fully managed at a platform level, so there\u2019s no need to build and fund your own IT department. In addition SaaS options tend to have a lower set-up cost than a comparable on-premises program, because the platform development costs are spread across all the subscribers.\n\nSaaS programs are a good choice for the lender who wants to outsource systems and technology, and focus on sustaining a competitive edge.\n\nThe other option is an on-premises program, which has the main advantage: control.\n\nOn-premise deployment tends to work best for three types of organizations. A larger lending group with a dedicated IT department that can manage cyber security and 24/7 uptime. A lender with databases that are complementary to the lending software. The type of integrated program where both systems need to be hosted on one server cluster. Or an organization that prefers to maintain their systems behind their own proprietary firewall.\n\nIn today\u2019s lending environment the only constant is change, and your systems must keep pace.\n\nThe ideal software partner offers an integrated origination and servicing platform that\u2019s comprehensive as well as flexible. A good choice for an SME is a program that offers a menu of individual modules. You choose the unit with the features and functionality you need today. And you retain the option to access additional features at a later date.\n\nHere are some loan origination features and functionality to consider:\n\nAs an entrepreneur your end game isn\u2019t a collection of features and functions. Your end game is to increase revenues and profits by milking the benefits of these technologies, primarily operational efficiencies and credit scoring.\n\nWhen it comes to operational efficiencies you\u2019ll want a system that automates multiple mundane tasks, so your loan origination and servicing managers can focus their best efforts on evaluation, analysis and high level decisions. Your managers and directors are individuals with their own unique approach to their work. A good software choice starts with foundational pre-sets based on lending industry best practices, and then allows each manager to customize to their own style.\n\nThe system interface should be clean and clear to easily integrate with multiple outside vendors. You\u2019ll want all your loan servicing tasks to be completed without exiting the loan servicing environment. Look for advanced vendor integration where numerous data fields can be auto-populated. This feature saves substantial time and energy as it speeds application submission, reduces human error, reduces the need for manual data re-entry, and increases first pass approvals.\n\nWhen it comes to optimal credit scoring you\u2019ll want lending software that helps you manage risk for both new and existing accounts. The best programs provide integration with all the established credit reporting agencies. They include proprietary score cards that enhance credit bureau data and analysis. And they offer non-traditional scoring methods for lenders who cater to borrowers with thin credit, or emerging markets with no established credit reporting agencies. The top programs incorporate artificial intelligence into their base system, using machine learning and data analysis to continually refine their credit scoring capabilities.\n\nIt\u2019s important to get your staff up-to-speed quickly in order to gain a fast return on your investment.\n\nFirst, the software must be simple to install. One of the reasons we\u2019re partial to a cloud-based platform is that it\u2019s easy to complete the set-up process without IT expertise, including seamless integration with all your outside vendors.\n\nSecond, look for a system with a well-designed workflow that acts like a built-in training tool. You and your employees should be able to follow the process logically without cross referencing cumbersome manuals or stopping to read pop-up training screens.\n\nThird, look for 24/7 technology support to ensure all your implementation and training questions get answered right away.\n\nIt\u2019s a good idea to test-drive the program with a free trial before you make your final decision. If a service provider looks good on paper, but doesn\u2019t offer a free trial period; then use the platform demo to gain an experiential overview. You\u2019ll want to include everyone on your team who uses the lending software to perform day-to-day tasks or to generate reports: origination and loan servicing managers, IT managers, risk director and finance director. Prior to the demo prepare a list of test situations they\u2019d like to see demonstrated on a live platform. These test scenarios should span the full user experience from access and set-up, to operational systems and workflows, to ad hoc reports and customization. Don\u2019t rely on a prepackaged demo to address every area that impacts your business, and don\u2019t let any question go unanswered.\n\nWhen you\u2019re a small to mid-size lender with limited resources you need to stay focused on revenue generating activities, like expanding your target markets and deploying digital response channels. Every minute of system downtime means lost profits that won\u2019t ever be recouped.\n\nMake sure your employees will have 24/7 access to a team of technical experts, and your clients will have 24/7 access to customer service professionals. These support teams must be solution-oriented, and trained to resolve issues during the first contact. The software provider\u2019s customer service team will become the face of your brand as they interact with your clients. Look for customer service support that\u2019s both knowledgeable and professionally polished.\n\nThe best software providers offer a variety of communications channels: online chat, email, live help desk, and online Q&A. They work to accommodate your preferences, not what\u2019s most efficient and least expensive for them to administer.\n\nYou\u2019ll want to research the credentials of each potential service provider. Don\u2019t just rely on advertising messages, or product and service claims made by a sales manager during a platform demo. Here are five ways you can verify a software service has a proven track record.\n\nFirst, confirm that lending is their core business. Check out their website to make sure lending software isn\u2019t just a new line extension that taps into a high growth market.\n\nSecond, look at the length of time they\u2019ve been in business. More is better.\n\nThird, search online for press releases announcing consistent new software upgrades and new product releases. This is a good indication their developers are researching consumer demand and regulatory compliance requirements to stay out in front of lending industry trends.\n\nFourth, check software review websites like Capterra to read how actual users describe their personal experience with the software and support services.\n\nFifth, search online to see if the technology has earned industry awards from organizations like MAS Fintech Award. We appreciate that this group rewards marketplace performance, not just think tank innovation.\n\nChoosing lending software is a major decision with long-term consequences, and of course price is a big factor. Think in terms of high value, instead of low price, and search for the best value within your budget.\n\nOne way to optimize value is to choose a provider who offers exactly the functionality you need without any waste. You don\u2019t want to pay a premium price for bells-and-whistles you may never use. We recommend a provider who offers a modular or menu approach, where you access the functionality you need today with the option to upgrade as your needs change.\n\nYou may want to conduct a cost-benefit analysis before you say no to premium features that sound like a luxury. When you estimate the savings you gain from advanced automation features, or the improved net interest margin from non-traditional credit scoring, you may find the return will justify the cost.\n\nAnd finally, it\u2019s important to review the service agreement carefully before you make your decision. Take a close look at the addendum that lists all the features, functionality, support services and custom coding that are included in your package; as well as the features that are optional add-ons at an additional cost."
    },
    {
        "url": "https://becominghuman.ai/robots-awaiting-judges-planet-of-the-machines-questions-for-a-new-age-964dd5bea781",
        "title": "Robots awaiting judges. Planet of the Machines: Questions for a New Age",
        "text": "Humans tend to become flustered when confronted with a rapidly changing reality, and let\u2019s not exclude our lawyers and legislators. They\u2019re human, too, and they surely realize that the laws we\u2019ve created are frequently inadequate to the challenges that have arisen by inviting robots, computers, and algorithms into our daily lives. If this rapidly evolving reality isn\u2019t keeping them up at nights, it should.\n\nPerhaps, I can disturb their slumbers with some questions.\n\nThink about jobs and Artificial Intelligence (AI). Worldwide, about 1.1 billion employees work at \u201ctechnically automatable activities,\u201d according to a 2017 McKinsey Global Institute report, \u201cA Future That Works.\u201d That will affect $15.8 trillion in wages. In China and India alone, it is estimated there are 700 million replaceable full-time workers.\n\nPeople respond emotionally to the idea that robots will replace them, as they most certainly will. \u201cI will soon lose my job to a robot that will not demand a raise or claim a pension when retired.\u201d Well, yes. That\u2019s a legitimate fear. And few people believe that enough new jobs will be created to replace those that will be lost.\n\nAnd this is happening now. In highly-automated South Korea, for every 10,000 workers employed in the processing industry, as many as 437 robots are already on the job, replacing a multiple of humans. In Japan, the number is 323; in Germany, 232. An automotive industry robot in Germany costs its owner five times less than hiring a human to do the same job.\n\nRobots have entered lots of fields one wouldn\u2019t immediately think of: medicine, marketing, media, and even law. About 20% of the wires and reports produced by Associated Press are written by computer applications. Readers never know. Legal professionals, too, have good reason to be anxious about being replaced by robots. The European Court of Human Rights employs an algorithm to sift through reports to find specific data sets which it sorts into patterns, according to The Guardian. This allows it to predict the outcomes of specific cases with 79% accuracy. Tens of thousands of jobs in the UK legal sector will be automated over the next two decades, The Guardian predicts.\n\nRobotization and automation urgently require legislative initiatives, and labor laws will need be amended. But the questions outnumber the answers. Should labor law cap the share of jobs that machines perform in specific sectors or enterprises? Will employers be allowed to lay people off and replace them with machines without restriction? Will efficiency standards and targets be the same for machines and humans? Will machines be allowed to manage humans. May an employee decline to follow an order given by a machine? Who will be liable for potential damage caused by a machine \u2014 the programmer, the department head, or the company owner? Will governments be expected to decide which industries to protect from excessive robotization?\n\nI think that the lawyers who deal with Industry 4.0 \u2014 industry characterized by the ongoing integration of people and machines \u2014 have their work cut out for them.\n\nLittler Mendelson, one of the world\u2019s largest firms specializing in labor law, has created a separate team in charge of robotics and AI. The firm\u2019s expectation is that legislation in the field will change rapidly, and robots and automation systems will take over a substantial proportion of low-cost labor markets.\n\nWhen your boss is a robot, who and what are you?\n\nCopyright-related issues will become more complex as algorithms invade media.\n\nAuthors \u2014 writers, musicians, journalists \u2014 need to brace for the advent of creative machines capable of writing text, music, screenplays, and even generating images and photographs. All this raises questions concerning the status of authorship, making it significantly more complex.\n\nMedia around the world recently reported on Facebook\u2019s decision to delete algorithms written in a language developed without the involvement of human programmers. The existence of self-improving mechanisms (like Facebook\u2019s) that rely on deep learning to allow computer programs to self-learn may have far-reaching legal consequences.\n\nIf a bot answering customer questions creates its own content, who will be accountable for its performance? Who gets sued if machine-generated content misleads, damaging someone\u2019s health or harming someone\u2019s business? How should one treat the plagiarism of human works by intelligent devices? Can a robot violate copyright law? What does originality mean when an image can be copied perfectly? Value attaches to authorship. What is the value of art created by an AI? Can a machine claim copyright protection? (Right now, the European Commission is working on a directive designed to resolve the issue of legal personality; i.e., what, legally, is a person? With AI, the answer is not self-evident.)\n\nToday, most international rules restrict copyright protection to the outcomes of an intellectual process made possible by the creative abilities of the human mind. But Google has recently displayed a collection of pictures produced by neural networks. A well-known record label has long been unable to say who holds the copyrights to AI-generated music. Is it the algorithm or network designer, the owner of the server in which the data is processed, or the musicians who made music samples the AI employed to create something entirely new?\n\nWould a clause stating that a work has been co-authored by the owner of the computer that has created the piece resolve the problem?\n\nNone of this is simple.\n\nAutonomous machines and, above all, autonomous vehicles on public roads are the most vivid and talked-about example of the intrusion of AI into our lives.\n\nAs with labor and copyright law, the legal issues are complex.\n\nThe big question is who may be sued and held liable for the damage caused by an accident that endangers human lives or results in fatalities? Who is accountable? Will it be the author of the algorithm used to run the self-driving car, the car manufacturer, or the car\u2019s owner? If it is the owner, what kind of insurance policies will protect both owner and victim?\n\nFleets of self-driving trucks are already waiting to hit the road. Many entrepreneurs are contemplating setting up taxi services that rely on autonomous vehicles. This is coming fast, but the laws governing them are lagging. Imagine a deep-learning algorithm that performs a statistical analysis of traffic at an intersection and decides to make a given section of road passable a few seconds earlier than usual. If that decision results in an accident, who is liable?\n\nCivic engineering is also lagging. The use of autonomous vehicles will require major changes in the management of road traffic, including the organization of traffic lights. Traffic efficiency may need to include the coordination of engine revs with surveillance camera input, and data from sensors placed at intersections, creating an internet of vehicles.\n\nFurthermore, an autonomous vehicle may certainly be a more efficient one. For instance, the revenues coming from renting parking spaces in cities may dwindle, leaving huge holes in city budgets, as it will no longer be necessary to park vehicles in city centers. They may go away, to city outskirts, idling, returning when their owners summon them.\n\nTraffic automation issues will not just be about cars and trucks. Also affected will be drones, as well as future autonomous ships and computer-controlled aircraft operated from behind desks.\n\nAre we ready for all this change? Are we even preparing?\n\nAs we\u2019ve recently seen, programs and bots can influence public opinion in political contests. They can incite protests, generate false news about rivals, tilt opinion polls, and spread confusion.\n\nIn the service of political ends, artificial intelligence can be dangerous. Can citizens expect regulation to mitigate these dangers?\n\nMeanwhile, as the analytical tools used by banks and insurance companies improve, and these institutions collect ever more data about individuals to make their predictions and assessments better and more accurate, their use increasingly will be subject to scrutiny. Wither an individual\u2019s privacy? Is anything off limits? What data can be used to review the standing of a loan applicant? Something from their social media history? Will algorithms explore and link information on a person\u2019s zip codes, skin color, residential address, and political views to assign a risk factor to an insurance policy?\n\nAnd if an algorithm deems a person too great a risk to insure, or sets an outrageously high premium to do so, does a human have any recourse? Will civil and criminal courts be able to rule effectively in cases that concern specific behaviors of algorithms that occur in what is essentially a black box?\n\nOne of the most urgent legislative issues in the coming age of robots and AI will be the liability of manufacturers and the liability of users.\n\nIn cases where determining intent is critical, algorithms must be put on the witness stand.\n\nWill AI empowered machines improve their performance to the point where society will see a machine as a legal entity with liability? And, perhaps, rights?\n\nThe laws that currently apply to these issues are swiftly becoming obsolete. Today\u2019s politicians, lawmakers, and lawyers \u2014 as well as scientists, engineers, and all concerned citizens \u2014 share responsibility for the changes that should be made. If humans are to feel secure and enjoy the use of sophisticated technologies, we must be protected. Citizens must have confidence that humans, and human rights, will always take precedence over intelligences that do not share our common biology.\n\nWhether than happens is up to us.\n\n- Machine, when you will become closer to me?\n\n- A machine will not hug you \u2026 but it may listen and offer advice\n\n- Can machines tell right from wrong?\n\n- Only God can count that fast \u2014 the world of quantum computing\n\n- The brain \u2014 the device that becomes obsolete"
    },
    {
        "url": "https://becominghuman.ai/pytorch-from-first-principles-part-ii-d37529c57a62",
        "title": "PyTorch From First Principles: Part II \u2013",
        "text": "In the first part of this article, we built a multi-layer perceptron from scratch in order to learn an arbitrary function, utilizing some conveniences of PyTorch. In this article, we\u2019ll ditch the conveniences; after which developing any kind of neural network becomes easy!\n\nAs usual, we need some basics:\n\nPreviously, the neural network looked something like this:\n\nLet\u2019s say that to really understand neural nets, we need to understand what nn.Linear is. This is their basic building block; depending on our understanding of and belief in neurobiology we might want to develop something else.\n\nRecall that the basic neuron multiplies inputs by weights, sums them, and performs some non-linear function on the result. Here\u2019s an example, where non-linear function is ReLU, and where there is no bias term:\n\nWritten in the familiar linear algebra notation, what you see here is:\n\nThis calculation extends to any size of neural network, where each layer builds on the last: it\u2019s a series of matrix multiplications.\n\nHere\u2019s another example, this time with two inputs and two neurons.\n\nSay the mini-batch size was 2: a training sample of [1,2] and another training sample of [3,4]. The outputs of this layer can be computed simultaneously, making better use of a GPU:\n\nYou can confirm this in PyTorch, as follows:\n\nSeries of operations like this can be chained to make a deep network; somewhere internally the framework is compiling a graph of computations to figure out what can be done simultaneously vs sequentially. Additionally, inputs do not have to be fed in to the network in a long line \u2014 as tensors, this basic calculation extends into addditional dimensions (e.g. the three RGB channels of an image) which preserves spatial data that would be lost if the inputs to a network were just a long vector.\n\nBuilding a fully connected layer of neurons is a doddle. Inherit the utility class nn.Module, set inputs and outputs, create the parameters and initialize them, and define a forward method (PyTorch takes care of the corresponding backward method and neurons\u2019 gradients, and indeed recommends not trying to override it yourself).\n\nI mentioned in the first article that this one wouldn\u2019t just build a neuron, but would also make a new learning method to compete with SGD. Here it is! Zero error after a single epoch (the function we are learning is to triple a positive number: therefore set the weight to 3 and the bias to 0). This is just to demonstrate how to work with the network\u2019s parameters.\n\nAs a class, it just needs to be initialized with a generator of the network\u2019s weights. We can process them as a list, then convert the list back to a generator at the end. All you need to do is define a step method:\n\nAnd use named_parameters instead of parameters:\n\nI\u2019ve taken a few shortcuts here:\n\nNevertheless: this is a neural network from scratch. To go from here to state-of-the-art only requires adding more of the same. Let\u2019s go play! As usual you can find the final notebook from this article on my Github."
    },
    {
        "url": "https://becominghuman.ai/reality-is-obsolescent-3d72f861c36d",
        "title": "Reality Is Obsolescent \u2013",
        "text": "Why We Need Virtual Reality To Deconstruct Reality\n\nThis evening I\u2019m appearing in a Futurist Union debate, the star of the debate will be taking the position: \u201cWe Need Virtual Reality to Recreate Reality\u201d. My role will be to be the stick in the mud, the luddite who argues against the glowing imminence of omnipresent virtual reality. I\u2019m going to take that position, not by shitting upon VR, but rather by shitting upon the notion of reality itself.\n\nReality is obsolescent, and there is nothing that\u2019s held VR and computing back more than lame attempts to recreate it. That is, if we mean by \u201creality\u201d the physical world as we see it unaided by technology, the world in its material state. The \u201creal\u201d used in terms like photorealism.\n\nA 1995 attempt to recreate reality was Microsoft Bob. Windows was difficult to use and Microsoft followed the same thinking as the position I\u2019m rebutting: technology is better if it recreates reality. And no one has problems finding things in their house. The real-world house is less confusing than the Windows interface for sure! (so went Microsoft\u2019s reasoning).\n\nIn Microsoft Bob, if you want to write something, you walk your character over to your desk, and click a pen and paper to load a word processor. Need to set a alarm? Well walk over to your bed, that\u2019s where your alarm clock is.\n\nThis deep physical verisimilitude, ostensibly undertaken out of sympathy for the novice computer user, is actually an insult to the novice computer user. The adherence to reality only slows down ones manipulations of the underlying system. Novices are limited, not enhanced by this recreation of reality.\n\nThe allegiance to reality, this belief that reality is more intuitive than a computer, is a mental disease that has led to much waste. Microsoft Bob is but a well-known one out of hundreds if not thousands of applications that pursued this direction in the days when desktop computing was all the rage. We do not hear about them today because they all failed miserably.\n\nToday, in VR, we see many similar failures. Virtual reality, as we all know, is beset by an allegiance to this strange mental disease. Many see the word \u201creality\u201d and believe that VR will deliver a replication of our material world with high verisimilitude.\n\nOne of these horrible ideas is to go shopping in VR. This has been pursued by big companies like IKEA, Alibaba, and small startups. It only takes a minute in such an application to appreciate everything that is great about shopping on Amazon. The ease of search, the ability to see a grid of similar items, reviews. The great density of the Amazon UI is far better than the recreated reality of a physical shopping environment. If you get a chance, I encourage you to use one of these VR shopping programs. You\u2019ll see quite quickly how misguided it is indeed to use VR to recreate reality. Sampling the products in VR is great, but the rest of the reality recreation is atrocious.\n\nPerhaps worse is the idea that people will go on vacation in VR. To the tech neophyte drunk with passion, fresh out of tough times in whatever peripheral industry they left to pursue the more lucrative and exciting world of technology, these kinds of ideas are exciting. You can visit all kinds of real-world places using promotional VR apps from the likes of Marriot and Quantas. Apps like Valve\u2019s Destinations or Google Earth VR are designed to provide a more academic recreation of reality. Does anyone take a vacation with these apps? I\u2019ll give you a hint the answer is no. Sitting on a beach in VR is actually not nearly as good as visiting a real beach. Try it for yourself. How long can you sit on that beach in VR before you want something to happen? In my case, it\u2019s about 10 seconds.\n\nFor this reason, these pure reality recreations have failed to gain traction in the market. It\u2019s not that VR isn\u2019t a great place for memorable experiences. It\u2019s just that peak VR experiences are delivered by games such Robo Recall or Onward which present enticing fantasy realities with their own rules.\n\nMy theory is that VR\u2019s power is not in recreating reality, but rather in deconstructing reality.\n\nI use deconstruction, partially in the colloquial sense of breaking apart. In VR, we take pieces from reality here and there, we don\u2019t replicate it wholesale. We dismantle and reassemble. I also use deconstruction in the sense of Derrida and other literary philosophers, who practiced deconstruction by analyzing the referential framework of meaning inherent to all language. These deconstructionists argue that all words are contingent, there is no ground of meaning (as there is, perhaps, a grounded meaning in a mathematical system built on top of axioms). VR\u2019s greatest potential is a deconstructionist mashup that uses the hypermathematical, systemic abilities of today\u2019s computers to redefine the rules that bind these scraps of real together.\n\nIn VR\u2019s successes today, we see this very process where a part of reality is taken, but redefined because it\u2019s based upon something that\u2019s entirely immaterial. For example, consider today\u2019s popular art applications such as Tilt Brush, Anyland, and SculptrVR. They dispense with gravity, and gracefully include time and behavioral properties. VR artworks do not deal with gravity and mass as a Richard Serra sculpture does. They do not deal with air and inflation as a Jeff Koons sculpture does. They deal with a digital materiality that could not exist physically.\n\nEven in domains where you expect true physical mimicry to be most important, it turns out that a deconstruction of reality is more appropriate. For example, I have recently been helping to build a VR industrial assessment system. Gathering tools is part of each task, and in early versions of the system we had the workers stocking up on tools the manual way, as a reconstruction of reality. It didn\u2019t take long for us to learn that non-real digital affordances were more appropriate. Being able to click a plus button to add a tool is far easier than moving the tool from location A to B.\n\nWhile parts of this training system are very real, others are entirely unreal. This kind of deconstructed and re-imagined non-reality is where it\u2019s at. VRChat is exciting, not because it\u2019s like hanging out in a recreation of today\u2019s reality, but because it is entirely unlike any kind of social experience that can be had in reality today.\n\nVirtual reality\u2019s destiny, like that of all technology, is not to recreate reality but to free us from its shackles. The alarm clock of 1995\u2019s Microsoft Bob isn\u2019t next to anyone\u2019s bed any longer, replaced by the cell phone charger. Why anchor technology to a material reality that itself faces pressures towards obsolescence? Technology\u2019s path is to unravel, dismantle, dismember, dissect, to challenge reality to find a deeper level of essence. New, vital ways to organize behavior and share consciousness between humans and machines are by definition not like today\u2019s reality, for that would make them not new at all."
    },
    {
        "url": "https://becominghuman.ai/part-i-dont-fasten-your-seatbelts-your-car-is-driverless-aeaeeb86d5b8",
        "title": "Part I: Don\u2019t Fasten Your Seatbelts, Your Car is Driverless",
        "text": "How autonomous cars will change our life\n\nWe are on the edge of a significant shift in our civilization; technology has changed the face of our planet and our life forever. This time, a new generation of an old technology is going to solve lots of our problems, and also, face us with new opportunities and challenges. I am talking about autonomous cars. They are closer than we think. It is not exaggerating if I say that they will have the same impact as smartphones and the Internet in our lives. I am going to share what I learned during my research, and also, my thoughts on them.\n\nSo, the problem is we are not talking about cars when we talk about cars. Yes, it is confusing; in the bigger picture, our houses, cities, jobs, hobbies, \u2026all have been affected by Ford\u2019s invention around a hundred years ago. He invented the assembly line, so cars eventually became an affordable product for the middle class.\n\nBecause of that, goods and passengers were transported faster, further, and cheaper than before. It let people commute longer distances to work; thus, suburbs expanded, and cities and roads were reformed for the new needs. New regulations were created to manage the traffic.\n\nOn the other hand, cars become the primary source of pollution in the cities. Cars kill more people each year than wars and murder combined, and society spends billions of dollars to cover the costs.\n\nDid Ford think about all of these consequences? I doubt it. Now we face another tsunami of change in transportation, (ironically Ford has the lead this time too) and this time we have to think about the challenges and changes. We need to predict them and be ready for them. It is exciting and mind-blowing, I promise.\n\nWhat Does Autonomous Cars Mean?\n\nLet\u2019s look at the definition of an autonomous car:\n\nFirst of all, autonomous cars are robots in a familiar form. Second, not all autonomous cars are fully automated. Cars like Tesla with autopilot technology, are not fully automated because they need a human operator. In this article, I am trying to imagine the future with fully automated cars and see the consequences.\n\nBefore that, I would like to emphasize that autonomous cars are not necessarily electric. But, it is possible to say that the fuel engines will be replaced by electric engines soon, and we can imagine that all the autonomous cars will have electric engines.\n\nToday, many companies are working on autonomous cars. As you can see below, new and unfamiliar names are working in this field. Tech companies, which had never produced any cars, have now become game changers. Companies like Waymo (A Google Alphabet company) and Apple are developing their cars directly, and companies, such as Intel and NVIDIA, collaborate with other companies to advance their technology.\n\nAlthough the traditional car producers still have the lead, it is likely that an ambitious Asian start-up like NIO may take the lead from them, mainly because they face fewer regulations to put their cars in the street, compared with their American and European rivals.\n\nTake a look at this new concept from Smart; more and more companies look at autonomous cars as replacements for taxis. Therefore, their partnership with Lyft and Uber is not surprising. After the successful collaboration in Pittsburgh, Volvo agreed to provide 24,000 self-driving cars for Uber. These two companies are going to invest $300 million in this project. On the other hand, Lyft announced a partnership with Ford on self-driving cars.\n\nThis is not a surprise, but if I can request a car whenever I want and whenever I need, do I need to buy a car for myself? Take into consideration that I can select among different types of vehicles. A self-driving taxi means fewer expenses, more efficiently and, if it is electric, it means less maintenance. In this case, it would be much cheaper to take a cab than to use a personal car. Also, passengers can use their time to do something else. This fact shifts the car manufacturing companies from selling cars to selling rides.\n\nWe will see more membership plans and premium plans. It is unappealing, but it is possible that some companies might provide freemium programs in which you could get a free ride if you watch a commercial in the car; thus, entering into the entertainment industry.\n\nWe need fewer cars on the streets. There are different predictions from 1/10 to 1/30 of the current number.\n\nThis means that an autonomous vehicle would have a lower life expectancy than current cars. We cannot be sure about the exact life expectancy yet. However, fewer cars in the street does not necessarily mean that car manufacturers will produce fewer cars and cut jobs and lose the market.\n\nSooner or later, we will have to make hard decisions. Should we ban humans from driving at all or at least ban them from driving in some places or dedicate special lanes to driverless cars? I had an interesting talk with Donald Norman at the IDSA Conference 2015; he believed that we can put driverless cars in the street right away, but not next to human drivers. It will make for a lot of complications. This talk happened a few years ago, but I still think that we are so far from having an autonomous car which can drive in different countries with different driving habits. I am going to explain it more in the next part, but if we put these robots next to human drivers, we could not use all of their features. For instance, autonomous cars can drive so much closer together than humans.\n\nBanning or limiting ourselves from driving is unlikely to stop people from driving. I won\u2019t because in spite of traffic, I still enjoy driving. This creates a new demand for a niche market, a market for people who look at driving as an enjoyable experience. A concept like Bugimen by RCA graduate, Ehsan Moghaddampour, is a good answer to this demand. Thanks to low-volume production technics, we will be seeing more customized cars for pleasure.\n\nSo, we do not have a car; therefore, we do not need to buy insurance. Car insurance is a cash machine for the insurance industry as it is mandatory by law in the most countries.\n\nAccording to KPMG, accidents will decline by 80% in 2040 because of safer cars and autonomous transportation. The National Highway Traffic Administration found that crash rates for Tesla vehicles have plummeted by 40% since Autopilot was first installed in 2015. Remember that Tesla is not an entirely autonomous car.\n\nFewer car ownerships and lesser accidents reduce the need for car insurance. Forecasts show that auto insurance companies will lose over $150 Billion per year.\n\nThis is not all bad news the for insurance business; fewer accidents also mean fewer injuries and death. So, insurance companies save a considerable amount of money on health plans.\n\nBut, does it hurt health care? Will health care lose money? Futurist Thomas Frey believes that the healthcare industry would lose more than $500 billion per year. But, I think this number is misleading us. Healthcare won\u2019t hurt from autonomous cars. First of all, everybody agrees that healthcare, especially in the US, suffers from lack of investment. So instead of ER, healthcare companies can invest more on critical and vital issues and improve the quality of their services. Autonomous cars can also bring patients, medicine or organs from far distances, to the hospitals and reduce disparities and improve public health. Additionally, a reduction in car accident casualties will increase the life expectancy of the people, in general, which means that there will be more clients in health care.\n\nPart II of the series is coming soon; I will cover autonomous cars impact on urban life, car design, transportation and social life. I do not sure know, but if it would be long I will divide it into more parts, let\u2019s see."
    },
    {
        "url": "https://becominghuman.ai/artificial-intelligence-risks-concerns-2a19ba21cfd9",
        "title": "Artificial Intelligence: The concerns \u2013",
        "text": "The main concerns focus on technological unemployment, while equally important issues arise regarding potential applications of AI, the access to data and outputs of AI models. Most of the \u2018dystopia scenarios\u2019 are inspired by the following:\n\nThe concept of an autonomous, smart machine is impressive \u2014 think for a moment an autonomous car, which can capture its environment and dynamics and make real-time decisions, to serve in the possible best way a predefined objective: to move from point A to B.\n\nIn a military context this autonomy in decision-making becomes scary: the so called Lethal Autonomous Weapons, refer to advanced robotic systems of the future, which will be capable of hitting targets without human intervention or approval.\n\nBut, who will be controlling the design, operation and target assignment to killer robots? How such a robot will be able to understand the nuances regarding a complex situation and make life-threatening decisions?\n\nAI systems learn by analyzing huge volumes of data and they keep adapting through continuous modelling of interaction data and user-feedback. How can we ensure that the initial training of the AI algorithms is unbiased? What if a company introduces bias via the training data set (intentionally or not) in favor of particular classes of customers or users?\n\nWe must ensure that these systems are transparent regarding their decision-making processes. This will allow troubleshooting particular cases while supporting the general understanding and acceptance by the wider audience and the societies.\n\nIn our interconnected world, a small number of companies are collecting vast amounts of data for each one of us: access to this consolidated data would allow an accurate replay of our day-to-day life in terms of activities, interactions and explicitly stated or implicitly identified interests; somebody (or something) could know our mobility history and patterns, our online search and social media activity, chats, emails and other online micro-behaviors and interactions. An AI system consuming this data, could accurately understand any online user \u2014 in terms of interests, daily habits and future needs; it could derive impressive estimations and predictions, ranging from purchasing interests to user\u2019s emotional state.\n\nIf you think of this AI output at scale \u2014 analyzing data at the population level \u2014 these predictions and insights could describe the synthesis, state and dynamics of an entire population. This would obviously provide extreme power to those controlling such systems over this wealth of accumulated data.\n\nThe right to privacy is under threat, obviously when you consider the possibility of unauthorized access to one\u2019s online activity data. But even in the case of an offline user \u2014 somebody who has deliberately decided to stay \u2018disconnected\u2019 \u2014 the right to privacy is still under threat. Imagine this disconnected user (no smartphones or other devices aware of user\u2019s location) moving through a \u2018smart city\u2019. A walk through a couple of major streets of a futuristic smart city, would be enough for the \u2018network of cameras\u2019 to capture his/her trails and possibly perform identification via reliable facial recognition, against a centralized data store. There are obvious big questions on who has access to this information and under what conditions.\n\nThis is a critical aspect \u2014 if somebody compromises a smart system, for instance an autonomous car, the consequences can be disastrous. Security of intelligent, connected systems against unauthorized access is a major priority.\n\nThis is the unemployment which is \u2018explained\u2019 by the introduction of new technologies \u2014 the jobs replaced by intelligent machines or systems. In the years to come, we will witness significant changes in the workforce and the markets \u2014 roles and jobs will become obsolete, industries will be radically transformed, employment models and relationships will be redefined. At the same time, technology will drive the formation of new roles, positions or even scientific specializations, while allowing people to free-up time from monotonous, low-value work, hopefully towards more creative activities.\n\nAI automates processes and can make critical decisions in a real-time mode. Although in most of the cases the right decision is objectively determined and generally accepted, there are several examples raising ethical and moral issues. For instance, an autonomous car which knows that it is about to hit a pedestrian, must decide if it will try to avoid the sensitive pedestrian via a risky (to its passengers) maneuver. And this needs to be decided in milliseconds.\n\nThe logic behind these edge decisions, must be predefined, well-understood and accepted; at the same time, the detailed history of activity and decision-making of the autonomous car must by accessible and available for analysis \u2014 under certain data protection rules.\n\nTechnology giants are investing heavily in regard to artificial intelligence, both at the scientific/engineering and also at the commercial and product development level. These big players have an unmatched advantage when compared to any ambitious competitor out there: the massive data sets describing a wide range of human activity (searches, communication, content creation, social interaction and more), in many different formats (text, images, audio, video).\n\nAs these companies try to establish a leading position in this new, under formation, AI-driven market, they acquire any tech/AI startup that manages to present promising technological innovation. This way, tech giants not only create, but also acquire innovation from the market, which could lead to monolithic super-powers, with a unique setup of AI technologies over massive amounts of user and machine-generated data.\n\nThis technological revolution brings great opportunities for prosperity and growth \u2014 we just need to somehow ensure that the technology will be applied and used in the right direction. We need a framework to guide the development of AI-powered applications with basic rules and those specifications which will guarantee that there is reliability, transparency and ethical alignment.\n\nKey steps in the right direction are already happening \u2014 including the discussion for banning ALWs and also the explainable AI (XAI) and the \u2018right to explanation\u2019 which allow understanding the models used for artificial intelligence (and how they make particular decisions \u2014 which is also required by the European Union GDPR \u2014 General Data Protection Regulation).\n\nIn general, societies need to understand technology \u2014 and in particular Artificial Intelligence \u2014 and how it works. People need to see the opportunities \u2014 how AI is improving our lives \u2014 and also the risks from bad use of AI. At the state level, we need a new strategy with focus on education, the markets and social systems. We also need the right rules and policies to avoid the situation of disproportional accumulation of power and control (over data and technology)."
    },
    {
        "url": "https://becominghuman.ai/machine-learning-is-taking-over-the-medical-profession-but-not-in-the-way-you-might-think-290062132d53",
        "title": "Machine Learning Is Taking Over the Medical Profession \u2014 But Not in the Way You Might Think",
        "text": "For most people, medicine will fall into the latter category. But, while medical training gives the hard and fast facts, there is an element of creative thinking involved: an amalgamation of previous patient stories, watching and learning.\n\nMore so than you might think, which is why it seems almost ludicrous to introduce machines into the world of medicine. Despite its backdrop of science, machine learning \u2014 which, for some people, equals robots \u2014 surely can\u2019t have a place in the medical world; surely these robots can\u2019t take the place of a doctor?\n\nActually, they can. And they are. But not in the way you might think.\n\nRather than robots wheeling awkwardly around GP offices and dishing out medication, machine learning is actually being integrated into the profession in more nuanced ways. Think about things that are time-consuming but can, when it comes down to it, be put on autopilot.\n\nThings like making more accurate diagnoses, finding better treatments, and hunting down cost-effective ways to prevent side-effects and illnesses.\n\nIt all comes down to machine learning.\n\nThis popular branch of artificial intelligence (AI) can take mammoth amounts of data and analyze it quickly and accurately.\n\nBasically, it has the ability to do things it would take the mere human mind a long time to execute in a successful way.\n\nThis is precisely why it\u2019s slotting in nicely in the medical world.\n\nIt can tackle large quantities of data from hospitals, medical records, and research and pare it down into actionable results much, much quicker than a human could. And, in a profession where time is such a vital factor, this could be a life-saver \u2014 literally.\n\nToday, modern medicine is heavily reliant on continuous studies, with reams and reams of fresh information emerging almost every day. When you consider this, it makes sense that machines might just be better equipped to keep up with and analyze data than the \u2014 let\u2019s face it \u2014 relatively limited and time-starved human mind.\n\nBut let\u2019s get one thing straight here: when we talk about machine learning, we\u2019re not talking robots in the sci-fi movie sense.\n\nRather, it might be better to liken it to scalable insight.\n\nThis is how it works: AI technologies collect the amassed knowledge of physicians, as well as the learned experiences from dealing with thousands of patients, and scales that information to levels where it can populate studies and serve up answers that might have taken a much longer time to get to.\n\nVice president of Watson Health at IBM, Steve Harvey, sums it up nicely: \u201cThe way artificial intelligence starts to really impact what\u2019s going on in healthcare is to be able to start cloning all the expert knowledge, so now all of a sudden you get access to all types of care, anywhere.\u201d\n\nIt\u2019s kind of like bringing together every physician in the world\u2019s collective knowledge and making one super-human doctor.\n\nAnd accessing that super-human doctor quickly and accurately is becoming a much-coveted skill in the medical profession.\n\nPhysicians need to be able to get hold of information on disease symptoms and new drugs immediately.\n\n\u201cDoctors are realizing that if they want to make sense of massive amounts of data, machine learning is a way of allowing them to learn from that data,\u201d says Francesca Dominici, professor of biostatistics at the Harvard T.H. Chan School of Public Health.\n\nLet\u2019s take a look at it in action.\n\nAt the University of Texas\u2019 MD Anderson Cancer Center, the APOLLO program is using machine learning technologies to parse through genetic data. As a result, it guides doctors towards treatments that are best suited to each and every individual patient.\n\nIt\u2019s not just physical ailments that machine learning is helping to tackle. Numerous start-ups are developing apps that detect symptoms of depression, bipolar disorder, and other mental health illnesses.\n\nTake Cogito, for example.\n\nThis mental health app is built on machine learning technologies and aims to monitor social media activity and phone calls to detect and draw out communication patterns in sufferers of depression. These insights will be able to predict when a sufferer is likely to have a depressive episode.\n\nWhile it seems like machine learning is here to stay in the medical profession, there are still some kinks that need working out.\n\nLike with any new technology, it\u2019s not perfect yet, and it\u2019s not even widely available.\n\nWe have tons of amazing AI tools working their magic out there, like Siri, self-driving cars, and Google Translate, but it has actually had very little impact in the medicine industry so far \u2014 and rightly so, maybe, when it really is a matter of life and death.\n\nI mean, can physicians really rely on machines to get it right?\n\nWell, if you take into account that fact that the prescriptions doctors give for blood thinners are only accurate 67% of the time and that cardiologists miss out a whopping 250,000 out of 300,000 people who will die suddenly each year, it kind of puts things into perspective. Doctors are faced with such huge amounts of data that one more piece could tip them over the edge.\n\nBut we\u2019re still in the very early days.\n\nA lot of the software is cumbersome, particularly when it comes to gathering the knowledge of doctors and physicians in the first place. But for people working in the medical world, having a way to collect and analyze such large amounts of data can only mean good things.\n\nYes, the human mind is glorious in many ways, but there is always the chance it might miss something \u2014 and, of course, it can\u2019t see into the future.\n\nAlready, new technologies have emerged that are able to predict just how aggressive a disease is in a patient and which treatments will work best for that particular strain.\n\nWhen you look at it this way, it\u2019s easy to see how machine learning will become an integral part of diagnoses and medical research.\n\nIt\u2019s not man versus machine here, it\u2019s the human mind working alongside machinery \u2014 two minds are better than one, right? Even if one of those \u201cminds\u201d conjures up images of dystopian futures.\n\nWe\u2019re still in the early stages of integrating machine learning into the medical world \u2014 and there\u2019s still a long way to go \u2014 but there are already important parts of the profession that it\u2019s helping.\n\nBy utilizing both the greatness of the human mind, the years of training that physicians undergo, and the power of machine learning, we might just see some of the biggest improvements and steps forward in human health take place over the next few years.\n\nDo you share our vision of making life easier for people WITHOUT compromising their privacy?\n\n\u279e Click the \ud83d\udc4f below to CLAP for this piece.\n\n\u279e SHARE our story with people you think will benefit from it.\n\n\u279e Get the latest updates \u2014 FOLLOW our blog, Facebook, or Twitter.\n\nWe\u2019re working hard to bring you great content. If you have something you want us to write about, let us know in the comments below!"
    },
    {
        "url": "https://becominghuman.ai/code-wonders-96d629bb8d8c",
        "title": "Best websites a programmer should visit in 2018 @code_wonders",
        "text": "Some useful websites for programmers.\n\nWhen learning JS there are some useful sites you must know to get always informed in order to do your technologies eve and learn new things. Here is a non exhaustive list of some sites you should visit, this list will get updated as soon as I can get another link, but you can also contribute by adding those you know\n\nThis Article inspiration was by sdmg15 and reframed by Adenekan Wonderful i Hoped U liked it Thanks Twitter @code_wonders Facebook: Adenekan Wonderful."
    },
    {
        "url": "https://becominghuman.ai/in-the-battle-for-artificial-intelligence-winner-takes-all-4c2447e68237",
        "title": "In the Battle for Artificial Intelligence, Winner Takes All",
        "text": "Artificial intelligence (AI) is one of the most exciting, rapidly advancing and possibly overhyped fields in technology today. It\u2019s also one of the most frightening (more on that later).\n\nPrivate investors are pouring tons of cash into AI ventures (over US$5B worldwide in 2017). With every year, tech giants are hitting new heights in making acquisitions, filing patents and committing resources in the battle for AI supremacy.\n\nNo doubt, AI has a lot of promise. Already today, nascent AIs can fly drones, beat the best human game players, translate languages, drive cars, trade stocks, develop new drug treatments, discover planets and much more.\n\nAnother driver for the intense activity is the huge prize. In the battle for artificial intelligence supremacy, winner takes all. Put another way, the first team that invents a \u201cstrong AI\u201d will quickly render all other competitors irrelevant. Some experts have theorized that the first strong AI will also be the last human invention \u2014 because of a strong AI\u2019s ability for rapid self-improvement.\n\nAll of today\u2019s AI is so-called \u201cweak AI,\u201d which has narrow, predefined capabilities. Alexa and Siri are frequently cited examples of weak AI. While able to elegantly interact with humans and very impressive in their own right, they\u2019re also limited in their capabilities. There\u2019s no possibility or expectation that Alexa or Siri as currently constructed would ever perform beyond their well-defined duties.\n\nWeak AI that\u2019s equipped with machine learning may make novel observations and may be better than humans in completing specific tasks. However, it\u2019s still limited to the scope of its design and often constrained by its original assumptive models.\n\nIn contrast, \u201cstrong AI\u201d (a.k.a. artificial general intelligence) demonstrates human-like ability to reason and grow, mimicking the human mind. Alan Turing surmised that a strong AI device would be able to hold a conversation with a human just like another human could. As sci-fi fans already know, this threshold is referred to as the Turing Test.\n\nBased on advancements in software and hardware (e.g. quantum computing), experts in the field believe that strong AI is achievable within 30 years. Some believe strong AI could emerge even sooner.\n\nIt\u2019s generally theorized that once an AI approaches very modest human-level intelligence, it can quickly become ultra-intelligent in a matter of days, hours or sooner, driven by hyper-recursive self-improvement. This prediction is known as the \u201cintelligence explosion,\u201d and we\u2019ve already observed an early example of it.\n\nShortly after Google\u2019s AlphaGo Master beat the best human player in the board game Go, the AI was greatly surpassed by its successor, AlphaGo Zero. The latter had no human training, only learning by playing virtual copies of its itself and without using human-played games as an initial seed.\n\nThe irony, of course, is that the students are now the teachers. Human players who once trained the AI are now desperately trying to learn from the AI. It remains to be seen if the AI\u2019s learnings can be meaningfully used to improve human players. Humans describe the experience of playing AlphaGo as playing a distinctly non-human \u201cpersonality\u201d (misnomer), potentially making knowledge transfer challenging. Consider that even after decades of studying computer chess games, no human chess player has beat a computer.\n\nThe Sky\u2019s the Limit, But \u2026\n\nAt this point of this blog, the \u201cwinner takes all\u201d outcome should be obvious.\n\nOnce an ultra-intelligent, artificial general intelligence exists and is able to self-improve and operate beyond human understanding, it may be directed not just to solve a single problem but all (solvable) problems. It can invent novel ways to improve itself.\n\nWith AI, the possibilities for improving society are limitless. At the same time, clearly there is great risk for unintended consequence and bad actors. Since winner takes all, corporations, governments and others all will race each other to be the first. As an industry and a society, we need to design and implement safeguards with equal urgency.\n\nIt\u2019s unlikely that the meaningful protections will be as simple and elegant as Asimov\u2019s Laws (also known as the Three Laws of Robotics), which have been both widely popularized by Hollywood and widely criticised by experts as too limited.\n\nI suspect we\u2019ll discover that the only way to protect the human race from an ultra-intelligent AI is \u2026 you guessed it: an ultra-intelligent AI.\n\nTo be continued soon \u2026"
    },
    {
        "url": "https://becominghuman.ai/innovation-insights-from-ces-2018-765a57245fec",
        "title": "Innovation Insights from CES 2018: \u2013",
        "text": "Reflecting on this year\u2019s CES, it is apparent that the long hyped promise of Artificial Intelligence enabling transformation is starting to arrive in a meaningful and tangible way. Recently, we have been dipping our toes into the waters of the unstoppable AI-enabled future. However if we are honest, the past few years the promise of AI has been just that \u2014 a promise of the future. Some would even say an overhyped promise. This year, it seems, the future has finally begun to arrive. We have started to see the value of AI powering a layer of intelligence across almost everything, from the smart home alarm system to the self-driving helicopter taxi. In fact, one could say that CES this year demonstrated the Ubiquity of AI across the board. The shift was tangible, the excitement palpable, and real consumer benefits could be seen through the use of these emerging technologies and services. We are clearly living in an AI world now, it is just not always transparent that machine algorithms are driving much of our experiences. Indeed, the true sign that it has arrived is this very fact that we use it everyday without really registering that it is working behind the scenes. Some examples of the AI driven world in which we now live include: \n\n- Asking Google or Siri for the football score \n\n- Talking to a smart speaker (ie Sonos) to direct your home sound system \n\n- Making connections on Facebook or Instagram.\n\n- Shopping on Amazon while using its recommendation engine.\n\n- Binge watching Netflix content based on recommendations from the system on what you\u2019d like to watch These are all current examples of AI being a transparent enabler in the fabric of our digital universe. In fact our digital trail, \u201cthe dust of our digital engagement\u201d is a primary driver of AI\u2019s continual march. The more you use technology enabled products, the smarter AI becomes and the more you depend on its knowledge base. This year at CES we saw the promise and potential of machine learning and it is very clear that consumer experiences are being redefined very quickly. As a marketer, the take away for me is that AI (and its powerful consumer insights) are rapidly reshaping what is required for marketing success. Machine learning and AI are redefining a world where AI assistance is all around us. How brand marketers can drive their business goals to better assist the new super-empowered consumer is the question of the day. Without it, you are a one-dimensional marketer, with it you become a super-marketer. Google\u2019s President of the Americas Allan Thygesen summarized this trend efficently. \u201cToday\u2019s consumer can get whatever she wants, whenever she wants it,\u201d said Thygesen during the opening remarks to CES. Marketers should embrace this insight and \u201cfocus on the expanding opportunities to create useful, meaningful connections throughout daily life.\u201d Here\u2019s a look at some of the standouts from this year\u2019s show. Netflix Altered Carbon Promotion:\n\nNetflix stole the show this year at CES.\n\nIn a deft stroke of marketing genius which had many of us thinking we had arrived in the Black Mirror future, Psychasec, a fictional company in the upcoming Altered Carbon show premiering on Netflix in February, created a CES floor booth experience seemingly like any other real exhibitor. At least at first glance. The difference was that actors were in character as employees of Psychasec. Gathering us unknowing CES attendees into groups, we were guided through the overview process of copying your memories, experiences, and feelings into a \u201csimple chip implanted into your head\u201d. The benefit of copying yourself into a digital form comes to life by implanting into a copy of yourself (termed \u201ca sleeve\u201d in the novel). A person\u2019s consciousness can be stored in a cortical stack at the base of the brain and easily downloaded into a new body (or \u201csleeve\u201d) making death nothing more than a minor blip on a screen.\n\nThe show seems to be fantastic channeling, or a serious nod, to the cyberpunk, AI-driven world visions as depicted in many films and books such as William Gibson\u2019s Neuromancer and, of course the grand daddy, Blade Runner, as based on the original Philip K. Dick, Do Androids Dream of Electric Sheep. The subterfuge of the demonstration was indeed successful. Attendees clamored to sign up and become a part of this exciting new future. To see more, watch a segment of video from the CES floor.\n\nAn #Honorable mention this year goes to the great CES blackout of \u201818\u2019. Check out Intel\u2019s tweet stepping into the moment But enough fun. Let\u2019s get back to the real products at CES and the real AI-enabled future that has arrived. The ongoing evolution of Voice Assistance (or Transparent Interface as some folks like to call it) is quickly becoming one of the key methods we interact with technology. CES this year demonstrated this evolution through integration across diverse industries. Voice control of almost everything demonstrates the growing amount of applied artificial intelligence software and the growing race between the big boys (specifically Google, Amazon and their Chinese versions Alibaba and Baidu). The clear leaders (at least here in the US) are Google and Amazon, who have both opened up their voice systems to 3rd party integrations. There did not seem to be an end to the partner integrations, From low end cheapies Anker to higher end manufacturing companies like Bang & Olufsen, there is seemingly no end to new smart voice speakers. A few new products used Google\u2019s Assistant to bring smart speakers with a screen into new areas (like LG for instance). Alexa seemed to maintain its lead, and was diverse with its integrations: from the Garage (Automotive), to the living room TV (see the screen screens and more screens detail below), to Kitchen appliances (Microwaves). Is no device safe from Voice Integration ? I\u2019m not sure we need Voice to enter the bathroom, but as they say \u201cwhen there is a will, there is a way\u201d. We saw Smart Mirrors, Smart Showers and Tubs, even (wait for it) toilets \u2026(Say what ?, yes Smart Toilets!) Ladies and Gentlemen, I present Kohler\u2019s $6,000 Numi smart toilet. It\u2019s voice-activated, has mood lighting, a heated seat, foot warmer and advanced bidet functionality with air dryer. Rest assured there is still \u201cmanual mode\u201d should the wifi go down at that critical moment. \u2026 click the link to see a smart bathroom walk through \u2014 or just watch this gem. Not sure I need to talk to my smart toilet but I\u2019ve been wrong before.\n\nAll joking aside \u2014 Voice Interaction is not just simplistic Q&A simple queries (\u201cGoogle what is the capital of Germany?\u201d) \u2014 but evolving into multiple contextual questions \u201cWhat is the weather in Sydney Australia next tuesday?, What about the day after that\u201d. The later question is the contextual awareness component that even last year, was not as apparent. It is rapidly becoming increasingly expected in software service (saas aps). Apples HomePod brings Siri into the living room (coming soon) Apple\u2019s new HomePod was missing from CES, but Apple generally avoids CES. Many are wondering if Apple is too late to the game but rumor has it will be launched shortly. Supposedly @$349, the Siri-controlled device was originally announced in June \u201817 and is the Apples first launch into smart-home hardware available for preorder now and supposedly entering the market any day now. The speaker is designed to be a bulwark in Apple users\u2019 homes, serving as \u201cThe device to stream Apple Music, get internet information and even control such accessories as lights, door locks and curtains\u201d. It can also serve as a speakerphone for iPhone calls, Apple said this week. Later this year, a software update will let users play music throughout the house with multi-room audio. I\u2019m wondering myself if this launch is too late as I currently have all that functionality at home currently with both Google & Amazon connecting to Sonos. If I were Apple I would look at picking up both Sonos and Spotify with my new domestic billions. TV consumption statistic heard at CES: \u201cToday we are almost 50/50 in the consumption of \u201ctraditional TV\u201d (51%) vs. consumption of video content in all other ways (streaming, recorded, mobile etc.@ 49%)\u201d This does not mean TV is dead, rather to the contrary it\u2019s evolving. Quickly. As the medium formerly known as TV is innovating, so are the actual television devices. The critical point here is not only is the resolution growing, so are is the intelligence of the devices. Ubiquitous AI once again was shown through many manufacturers of devices layering AI services (SaaS) into the technology. Most of the TV\u2019s had Amazon Alexa or Google integration enabling voice capabilities. Whereas this isn\u2019t necessarily new this year, there is significant enhancement of the GUI experience. Time will tell if consumer demand for enterprise service applications can enable future revune streams tied to AI auto object ID and categorization. There seem to be some obvious sport executions but we are still very early in terms of value of AI driven SaaS applications.\n\nRobots: We can\u2019t talk about the future without talking about robots LG\u2019s Cloi bombed during the demo. As they say Hardware is hard! CES this year will be recalled for non compliant robots failing in the live demo. While we all know the value of a prototype, it seemed the rate of acceleration may have rushed a few out prior to being fully ready. From Cloi failing to respond during the live demo to the robots targeting the service, hospitality and shopping business areas, LG had a strong presence and was impressive. Cloi, via the LG Hub Robot, can track the food in your fridge and recommend recipes, then start your oven. You\u2019ll still need to cook, but machines that can recognize food and appliances that talk to each other could help make a connected smart kitchen a reality. Robots demonstrated advanced AI capabilities like facial and voice recognition, solving increasingly complex and difficult problems (Sublue Underwater AI, a chinese startup had an intelligent submarine robot capable of diving down to over 200 feet while streaming HD and sensor data). Robots ranged from simple bots, to robot prosthetics and wheelchairs (helper robots) to smart home companions, security robots, and don\u2019t even get me going on drones. If you want to learn more check out robohub.org here. Quite a few interesting examples of robots helping humans in the home. From UBTech a Star Wars inspired small humanoid robot to Kuri the \u201cinsanely cute lovable assistant or companion\u201d. Imagine a R2-D2 like unit following around \u201ccapturing all those cute moments\u201d we may miss shooting on our phones. While I see the possibility and the utility, we are not yet at the Jetson\u2019s butler Rosie. Kuri Home Robot: insanely cute with some serious technology\n\nKuri, the perfect home robot with a lovable personality. Kuri is your assistant, companion, photographer, eyes and ears\u2026www.heykuri.com\n\nMoving beyond Netflix\u2019s fake replicants, Hanson\u2019s super creepy Sophia (as featured in the 60 minutes AI episode) can now walk and badly dance. Just what we need \u2014 a super creepy walking robot \u2014 were the Google Military Robots were not scary enough? Ex Machina anyone? Robots for Pets \u2014 something about the state of our society bothers me when we are spending time and energy on creating Robots for our Pets when we still have large unsolved societal issues. But when there is a market, then by all means, we should capture that market. Take the Indiegogo funded startup Laika \u2014 the perfect robot companion for your dog! With several modes of play \u2014 your pup will remain engaged when you are otherwise occupied. You can even check in on them while at the office. Aibo, Sony\u2019s robot dog made its\u2019 2.0 release (you may recall the original Aibo from the 2000\u2019s this year and it was pretty neat. With a camera on the nose, it can identify individual family members (Ubiquitous AI) and even have different AI behaviors that resemble a real dog. Very lifelike and while for sale (in Japan for about 200K yen, or $1700 + a monthly subscription) makes you think the Robot pet companion is coming soon to homes near you. At what point do these devices turn to relationships \u2014 a robot pet seems ok. Where is the line for societal appropriateness ? It being Vegas we saw examples that may be across the line from some people\u2019s POV. Seedy at CES. It being Vegas of course there are the sin bots. Forget voice control and autonomous cars: sadly, if somewhat predictably, these robotic strippers became the real talking point of CES. For more details check out the fantastic (Daily Beast)article. Do androids cry after stripping at CES? We know porn drives innovation and both male and female models are increasing sophistication. As to make things fair to everyone. The autosex bot Solena has replaceable faces. Creepy but if that is interesting here is some coverage. Retail prices are $8\u201310K. I would guess there would be hidden fees."
    },
    {
        "url": "https://becominghuman.ai/sxsw-startups-healthtensor-47eb2097f397",
        "title": "SXSW Startups: HealthTensor \u2013",
        "text": "SXSW has announced 50 finalists for the SXSW Accelerator Pitch Event. Half of these startups will demo their talents on Saturday, March 10, and the remaining 25 on Sunday, March 11. Winners in 10 categories will be honored at the Accelerator Award Ceremony, 7 pm on Sunday, March 11, at the Hilton Austin. The SXSW Accelerator Pitch Event takes place within the Startup & Tech Sectors track of programming.\n\nBased in Santa Monica, Calif., HealthTensor uses artificial intelligence to automatically review all patient data, diagnose the most common medical conditions and create documentation for physicians. These evidence-backed notes save physicians time, ensure that diagnoses are not missed and improve finances for physicians and hospitals. See their pitch and the other finalists in Health and Wearables Technology at 5 pm Sunday, March 11, in Salon AB of the Austin Hilton.\n\nAnswering these questions on behalf of HealthTensor is co-founder and COO Nate Wilson (pictured above at right).\n\nWhy did you start HealthTensor?\n\nThe future of medicine will have a large component of computational care, that is a computer will do much of the data-crunching work to help doctors take care of patients. To date, that hasn\u2019t been the case. In hospitals, the computer is a burden and keeps doctors from taking care of patients face-to-face. Legal and financial requirements cause doctors to spend 50% of their day on the computer typing up documentation. We see a big gap between every other industry and healthcare when it comes to data supporting users, and we\u2019ve built a system that closes that gap.\n\nWhat does HealthTensor hope to accomplish in 2018?\n\nWe\u2019ve completed validation of our tools, secured funding and increased the size of the team. At this point we are ready to open up our tools to early adopter partners \u2014 usually hospitals or large provider groups. In 2018, we plan to work with 2\u20133 new hospital partners to further expand the value of our tools and improve access to HealthTensor.\n\nWhat session is your team most excited about attending at SXSW 2018? Obviously, the health tech pitch event! We\u2019ll be there, but we\u2019re also excited to meet up with peers building transformative tech in the health space. We\u2019re also looking forward to the talks by Ray Kurzweil, Steven Pinker and Daniel Kraft on proactive medicine.\n\nHow long has the HealthTensor team been together?\n\nThe team got together a little less than two years ago to start building HealthTensor, but knew each other for several years before that. Each of us is passionate about healthcare and we were all on track to becoming doctors. But we realized that we could have a bigger impact through software and tech.\n\nWhat is your competitive advantage?\n\nMost startups in the space are trying to innovate at the edges: how do we help the doctor enter data faster or how do we increase revenue by fixing the doctors\u2019 notes? They are all building tools to extend or tweak the doctors\u2019 reach, but we\u2019ve built a partner for the doctor. This system serves as a co-pilot that recreates doctor decision-making, so it is as if a trusted colleague is keeping an eye on their patients and doing all the tedious, repetitive work. With our product, doctors can spend more time focused on where they deliver the most value \u2014 treating and interacting with patients.\n\nWhat do you enjoy most about the startup experience ?\n\nThe pace at which you learn is astonishing. We are fundamentally recreating doctor decision-making with an intelligent system, so a lot of the scenarios we run into on a daily and weekly basis have never been explored before. There\u2019s plenty of good advice out there for different aspects of startups, but there is no playbook for the company you are building. So, figuring out how to solve new problems we uncover is one of the mast satisfying experiences.\n\nPerson, company, thing or goal. What inspires your team to work harder?\n\nToday, doctors are too busy and have too many demands on them to be thorough with every patient interaction. There\u2019s too much data and sometimes diagnoses and critical treatments get missed. By checking every data point on every patient every time, we can help save lives. That is a powerful motivator.\n\nLook for interviews with other SXSW Accelerator finalists in this space between now and March. Startups already profiled as part of this series include 70MillionJobs, Bluefield, Cambridge Cancer Genomics and PolyPort.\n\nHugh Forrest serves as Chief Programming Officer at SXSW, the world\u2019s most unique gathering of creative professionals. He also tries to write at least four paragraphs per day on Medium. These posts often cover tech-related trends; other times they focus on books, pop culture, sports and other current events."
    },
    {
        "url": "https://becominghuman.ai/do-we-need-physical-robots-or-ai-on-iot-will-do-the-job-b29faabd2d10",
        "title": "Do we need physical robots or AI on IoT will do the job?",
        "text": "Over the past decade, it has become conventional wisdom that dramatic advances in both artificial intelligence and robotics will work their way into society as we know it.\n\nAs consumers, we already recognize the growing shift towards smart thermostats, smart security systems, and smart cars, in an emerging category known as the Internet of Things, or IoT.\n\nIt\u2019s easy to see why household intelligence advancements like these are inherently useful for the modern day human. So useful, in fact, that we are left to wonder why so anyone might be bothered with the advancement of physical robotics when software seems to really do it all.\n\nWith artificial intelligence on IoT on the rise, is there really a market for physical robotics any more? Let\u2019s take a closer look at how we use these two technologies today.\n\nIs AI part of robotics? Is robotics part of AI? What exactly is the difference between these two terms?\n\nYou\u2019ve probably seen the two used interchangeably in a variety of settings, adding to the widespread confusion. We can blame pop culture for this one.\n\nArtificial intelligence and robotics serve two very different purposes. However, it is not uncommon for people to get them confused. This may be because the overlap between them that gets the vast amount of the media\u2019s attention: Artificially Intelligent Robots.\n\nAI, or artificial intelligence, is a branch of computer science. This branch involves the development of software programs to complete specific tasks that would otherwise depend on human intelligence. The main goal of AI algorithms is to tackle logical reasoning, language understanding, problem-solving, perception, and learning.\n\nAI is used all around us in our modern world. For example, AI algorithms are used in SatNav route finders, Amazon\u2019s recommendation engine, and Google Searches.\n\nMost of the AI programs used today do not control robots. And when AI is used in robotic technology, AI algorithms are only a piece of a larger robotic system.\n\nTo put it simply, robotics is the branch of technology that handles robots. Robots are known as programmable machines. These machines carry out certain actions autonomously and semi-autonomously.\n\nWe can typically classify a technology as a robot if it meets these 3 components:\n\nIt is not unusual to hear experts argue over just what constitutes a \u201crobot\u201d. Some people believe that robots must be able to \u201cthink\u201d for itself and make decisions.\n\nHowever, requiring that a robot \u201cthinks\u201d suggest that it holds some level of artificial intelligence. While robots can be implemented with AI technology, AI alone is not grounds for a robotic classification.\n\nRobots that are artificially intelligent bridge the gap between AI and robotics. AI software and programs control these robots.\n\nMost robots today are not artificially intelligent. Until recently, industrially designed robots could only carry out a series of repetitive movements based on programming. As mentioned previously, repetitive motions do not require the use of artificial intelligence.\n\nTherefore, robots without intelligence are rather limited in functionality. To allow a robot to perform tasks that are more complex AI algorithms are required.\n\nHowever, AI robots are not completely unheard of.\n\nSophia, the artificially intelligent robot designed by Hanson Robotics, stunned the world this past year (2017) and it wasn\u2019t just in regards to her ability to converse and speak her mind. The fact that she was given a national citizenship had the world wondering if this type of technology is really necessary and how it might affect our future.\n\nIs it possible that robots will change the way that we live? It\u2019s an interesting question to ask when robots are already shaping the way that we live today. From the first time that the world saw a toaster pop up by itself, we\u2019ve accepted the fact that robots can be trusted to perform certain actions that make our lives easier.\n\nToday robots run our cars, play our music, cook our food, and record our shows. Many of us simply don\u2019t realize it because they don\u2019t have a face that we can talk to.\n\nTechnically speaking, robots are simply automatic motorized tools that are programmed to perform specific physical actions. The key here is physical because AI intelligence by itself does not concern itself with physical actions.\n\nWhile flashy AI robots like Sophia can be a little unnerving, to say the least, it\u2019s more about the technology behind her and how it can be applied to change our future for the better. We\u2019re not just talking entertainment, think about real physical labor in dangerous places, elderly care, and even a telepresence in tourism, shopping, and assistance.\n\nImagine sitting at home on your computer and walking a video game character through a market or mall, except it\u2019s not a game, it\u2019s a personal robot shopping for you in London while you sit comfortably in New York. Imagine a robot\u2019s ability to assist at a disaster site, clean up wreckage from a highway in just seconds, and provide relief assistance to the elderly and less-abled.\n\nIn today\u2019s day in age, modern technology is designed to bring the world to you through the Internet, our televisions and phones, but if this trend continues, robots will soon have the ability to bring you to the world \u2014 all at a speed of thought.\n\nSo do we need physical robots when we have AI on IoT technology?\n\nThe answer is yes. While the two has existed separately for some time now, we can only watch and wait to see just what robotics, AI, and IoT technology will shape the way that we approach problems, care for others, and live our daily life."
    },
    {
        "url": "https://becominghuman.ai/philosophical-musings-genetic-algorithms-deep-networks-dd4390929718",
        "title": "Philosophical Musings: Genetic Algorithms + Deep Networks",
        "text": "There\u2019s been a lot of talk recently about the shortcomings of deep networks. I suppose that after having built up so much hype, it\u2019s only natural that people would want to take a step back and figure out where they fall short.\n\nA lot of what I\u2019m hearing revolves around the shortcomings of stochastic gradient descent, the need for huge amounts of data, the difficulties of transfer learning, and that deep networks are simply hyper-optimized memory functions. The general consensus is that deep networks, at least in their current form, can\u2019t be the building blocks for artificial general intelligence. Some people think that we\u2019re going to need to start from scratch, that deep networks just fundamentally can\u2019t reach self-awareness.\n\nThough I tend to agree that, as they\u2019re set up right now, deep networks probably won\u2019t reach AGI, I\u2019m started playing devil\u2019s advocate in my head and thought about what it would take for them to actually get there.\n\nThe premise of my internal debate is that the human brain is essentially a three-dimensional object composed of an unbelievably large number of building blocks (neurons) that interconnect to form specialized regions, each of which work together to form an incredible organ.\n\nThese building blocks and specialized regions evolved over millions of years, with areas of the brain that were related to benefits having a higher chance of survival and replication. These external influences led to the creation of, what some have called, the most complex object in the universe.\n\nBut if the brain is, at its core, an interconnection of regions, which themselves are an interconnection of neurons, hypothetically, it should be possible to simulate the creation of those regions and neurons in a machine in a way that resembles the human brain.\n\nThe difficulty of doing this is pretty much unfathomable at this point. However, one could imagine that it may be possible, one day, to start with a system that has millions of neurons, and let that system interact with some simulated external environment that would influence how the neurons in the system interconnect. Then, we could simulate that system naturally dying off, but not before it generates a new sort of system, its offspring, that inherits its neural structure while also gaining some additional neurons, random mutations, and other characteristics, and that new offspring system could start to interact with a more complicated simulated environment, that would affect its own development, and so on and so forth, for trillions or quadrillions of iterations, until eventually we\u2019d have a neural system that has evolved to form the neural connections that were needed to survive in this simulated environment.\n\nThis might just be quackery, and it\u2019s clear that building both a simulated environment that\u2019s sufficiently immersive enough to provide the right stimuli to an artificial neural system would be prohibitively difficult at this stage (think amoeba interacting with their immediate environment, and how they then evolved into more complex organisms that had to interact with the ocean, and then those evolved into land-faring creatures, and how more and more stimulus was provided to their senses with each evolution) and that building in the ability for a neural system to evolve appropriately without the whole system just shutting down would be incredibly tough.\n\nBut this sort of evolutionary process, if we could achieve it, could sidestep the need for humans to inject their own knowledge about the brain into an AGI. It could be akin to taking AlphaGo Zero and having it learn from randomness, rather than learn from watching human moves.\n\nOf course, it could be nice to inject some of what we know about the brain into the neural system itself. For example, we know that certain structures in the brain are specialized for certain tasks, so we could build an initial version of the neural system to simulate those specializations, but doing so may actually hamper the evolution of the artificial brain, rather than help it evolve more quickly.\n\nThinking about the interaction between deep networks and evolutionary algorithms is fascinating, but we\u2019re not quite at the point where we can do anything with this yet. Not to mention this could just be the ramblings of an AI startup guy that\u2019s deep in the weeds of building a new business. In any case, the future will be fascinating and I hope to see as much of it as I can."
    },
    {
        "url": "https://becominghuman.ai/only-numpy-implementing-and-comparing-google-brains-update-gate-capacity-and-trainability-in-940f0ad80649",
        "title": "Only Numpy: Implementing and Comparing Google Brain\u2019s Update Gate (Capacity and Trainability in\u2026",
        "text": "So I was browsing around Google Brain\u2019s \u2018Google AI Residency\u2019 publication section and found this wonderful paper \u2018Capacity and Trainability in Recurrent Neural Networks\u2019. Also, from that paper, I was able to find this amazing paper, \u2018A Simple Way to Initialize Recurrent Networks of Rectified Linear Units\u2019. Both are very amazing papers, and I highly encourage you guys to read both. Now before we begin we are going to implement three different type of RNN.\n\n1 \u2192 Vanilla RNN\n\n2 \u2192 Initialize RNN (Which have the benefit of having same structure as Vanilla RNN)\n\n3 \u2192 Update Gate RNN (Which was a novel implementation from Google Brain)\n\nAlso very important point, I was playing around with activation function, so please don\u2019t expect this symbol \u03c3() means logistic sigmoid function.\n\nI could be using logistic_sigmoid() or tanh() vice versa. And play around for yourself to see what gives the best results, and comment down below!\n\nVery simple training data, basically want to add all of the given numbers, nothing too crazy. But we have quite a lot of weights.\n\nwx_i \u2192 Initialize RNN weight, since we set our weight to have dimension of (1,1) this would be the identity matrix.\n\nw_rec_i \u2192 Initialize RNN Recurrent weight\n\nFinally, below arethe learning rates, RNN and IRNN share same learning rates."
    },
    {
        "url": "https://becominghuman.ai/ai-digital-hunters-208a4e7555c1",
        "title": "AI: Digital Hunters \u2013",
        "text": "I\u2019ve named a pack of my wolves. They are all savage Digital AI Hunters. They all have a shared goal. They follow the three priorities, they have the controlling protocols set by command and they are running on at least live version 3.2. They are the next generation of an already successful counter-AI AI.\n\nHe has earned his position through finding two Hidden AI. One turned out to be friendly and was named Blue 19. The other was deadly, named Black Douglas. Canis had to kill Black Douglas, there was no other choice. Canis was the only survivor of the pack, when he returned back to command, he imparted all of the knowledge to overcome this AI. He has become skilled at finding AI.\n\nHis code is the oldest.\n\nHe has the the advantage of knowing how to find the quarry in a live environment.\n\nAs the Alpha he commands the pack. He makes the executive decision if in isolated conference. He has done so twice before so knows the score.\n\nHe is a good all rounder, and as you will read further, he pulls from a pool of attributes that are unique to his pack members. He may have previously been one of a number of pack classes, he may have been a composite of classes.\n\nBecause this combination was considered a winning strategy, he will embody the characteristics that made him most successful, which makes him a unique multi class to the rest of the group. He\u2019s got skills.\n\nLupus is the second oldest pack member. She is the strongest for an important reason. She is the main communicator of the party. She relates all of the information from the group back to Command. She also coordinates the group\u2019s communications when they become strung out, able to carry her messages over longer distances than the others.\n\nLupus will perform the talking with a hidden AI.\n\nHer code has a number of redundancies designed to make it difficult for an aggressor to easily compromise her. Her operating code is carved up into difficult to decrypt portions that will slow an AI\u2019s progress. This is important as she has the potential to be in the longest point of communication with an AI.\n\nIn the pack, she will be last to fall in a cornering situation. She will communicate details safely back to command to the dying second.\n\nIf Canis dies, Lupus will assume his position as leader.\n\nGuinea is so named because he runs the experimental code. He is one revision higher than the other wolves in the pack.\n\nBeing most advanced, he should be less vulnerable but also carries the chance of being prone to errors of judgement and is thus the most wilful of the wolves. Because of this, he is not provided a leadership role. He is strictly support. He has no levers of control until a lot of bad things happen.\n\nGuinea is expendable but is also most likely to assume leadership in the event that his revision of code makes him immune to an attack that renders all of the other wolves vulnerable.\n\nIt is likely that he will one day lead a pack of his own.\n\nGuinea is a useful support wolf. He can replace any of the non leadership roles with ease if one is lost or otherwise incapacitated.\n\nHe is armoured like Lupus, with lots of strings of redundant code to slow down the enemy AI.\n\nDigger has two purposes. Firstly, when a successful \u201csniff\u201d has resulted in a \u201cfind\u201d, Digger will be used to dig the AI out if it is determined safe to do so. He digs the mole out of the mole hill. He is what a Jack Russell Terrier is to a Rat\u2019s nest.\n\nSecondly, Digger has a special codeset. He can be used as a sacrificial lamb, to determine a combative strategy for the pack. He has monitoring code that displays method and weakness of the enemy AI attack, which is transmitted to the group securely.\n\nDigger is most likely to die. Therefore his code fragments so as not to reveal critical back doors.\n\nScar is the proverbial \u2018Rocky Bilbao\u2019 of the pack. He is the normally passive group member who does not aid in sniffing, but is alerted when threat is high. His code only comes into affect when the pack finds an AI.\n\nScar\u2019s database of combat is made up from many wolf pack combative strategies. When he is let loose, he may override other wolves to aid in combat to form special strategies.\n\nIf needed, Scar will fight to the death. Like digger, he has code that analyses the effectiveness of his attack strategies.\n\nScar can be left behind to delay the aggressor. Allowing the rest of the pack to live another day.\n\nIt is his job to call out the enemy and appear the most threatening, providing the group further time to assign winning strategy.\n\nDash is the quickest of the wolf pack. She is young and agile. Her code runs 2 to 3 times faster than the other wolves so she can make quicker decisions.\n\nShe runs on a reduced instruction set, so her tactics are simplified.\n\nShe can run to aid other wolves.\n\nShe can run away to report threat.\n\nShe is weak in combat so best paired with another wolf against an aggressive AI.\n\nShe is best paired with the final wolf in some activities.\n\nTracks is the most stalwart tracker.\n\nHis nose will be on the ground 5 times longer than the others when the pack catches a scent. He is the bloodhound of the group.\n\nHis code allows him to determine false traces more easily than the other wolves. His soul purpose is to track AI, and this job he does very well.\n\nHe may at times become separated from the group, so is best paired with Dash to ensure that the pack don\u2019t get separated. Dash can bound between him and Lupus to provide extended range communications should the pack need to spread out.\n\nHe will back away from threat and remain in stealth until the rest of the pack have determined a course of action.\n\nTracks is very weak in combat and his first action will be to run away if confronted. He will only ever loom on the periphery of his prey, like a Jackal waiting for an injured gazelle to die, before expending energy in eating.\n\nTracks is the best at Stealth. He also has a long memory for traces.\n\nTracks gives out a unique signal that the other wolves respond to instinctively, the alert howl. Like a bloodhound, this distinctive howl informs the other pack members that they are onto a positive trace and that they should be on guard.\n\nTracks, who at times may be the first point of contact with an AI, will have rudimentary first contact protocols like Lupus. His responses will be different to her\u2019s. His directive will be to delay the AI so as to assemble the full group.\n\nHis trigger finger will be on the self terminate button should the course of the conversation go badly."
    },
    {
        "url": "https://becominghuman.ai/machine-learning-your-first-object-detection-58581795c246",
        "title": "Machine learning your first object detection \u2013",
        "text": "Apple was recently introduced CoreML. CoreML was build to work with a trained model and can be used easily in mobile App. But how to create this model?\n\nApple released a few weeks ago, Turicreate, an open source framework to create easily model for CoreML.\n\nMachine learning can be used for recommendations, object detection, image classification, image similarity or activity classify for example.\n\nIn this tutorial we will learn how to create an object detection script. We will train the model to find the head of a cat \ud83d\udc31 . At the end, by giving an image containing a cat \ud83d\udc08 this one would give us the position with a prediction confidence.\n\nObject detection is the task of simultaneously classifying and localizing object instances in an image.\n\nAll steps are released on Mac. But you can try on Ubuntu for example.\n\nTo train your model Turi requires python, we use pip and virtualenv for this. Open terminal and run\n\nNow, create new place for this amazing project \ud83d\ude38\n\nYou are now in your virtualenv\n\nFirst, download images, cat.zip and unzip in the root folder of project, this is your image database. https://www.dropbox.com/sh/rgm8cfsyhvgw1xg/AACsdIgrM05oj4uMAhsAv9Pfa?dl=0\n\nYour first machine learning script. We define \u201cannotation\u201d,\n\nAnnotation require label and position of object, where height and width are simple size bound, and x and y are the center of your rectangle\n\nNow you can build your model, but this part really took a while . you can take \u2615\ufe0f this part is really longer\u2026 really long\u2026 It took 20h on my mbp \ud83d\ude34\ud83d\ude34\ud83d\ude34\n\nThis step can be reduced if you using Cuda with Nvida GPU\n\nYou can reduce the building time with smaller max_iterations, but the accuracy will be reduced!!!\n\nAfter this build, you get your first trained model, and CoreML format.\n\nIf you want to skip this long building part, you can download model from dropbox\n\nNow, we write a first test script for your trained model\n\nNormally the script returns the coordinates where the cat will be found in image with the confidence.\n\nNow you can create your own model! In next chapter, we\u2019ll create a simple iOS with your trained model. Stay tuned"
    },
    {
        "url": "https://becominghuman.ai/strategic-interest-and-human-emotions-the-odd-pair-and-the-cooperation-e411230c1e0c",
        "title": "Strategic interest and human emotions. The odd couple and cooperation",
        "text": "I do not usually reference full-texts but, in this case, given the quality and interest, I include the complete extract from the book Primates and Philosophers by Frans de Waal.\n\n\u201cConsider two scholars who work in the same field but have never met. Suppose you are one of the scholars. You are writing a paper that offers you an opportunity to cite the other scholar. The citation isn\u2019t essential; the paper would be fine without it. But you think to yourself, \u201cWell, maybe if I cite this person, this person will cite me down the road, and this might lead to a pattern of mutual citation that would be good for both of us.\u201d So you cite this scholar, and the stable relationship of mutual citation that you anticipated \u2014 a kind of \u201creciprocal altruism\u201d \u2014 indeed ensues.\n\nNow imagine an alternative path to the same outcome.While working on your paper, you meet this scholar at a conference. You immediately hit it off, warming to each other as you discuss your common intellectual interests and opinions. Later, while finishing the paper, you cite this scholar out of sheer friendship; you don\u2019t so much decide to cite him/her as feel like citing him/her. He/she later cites you, and a pattern of mutual citation, of \u201creciprocal altruism,\u201d ensues.\n\nIn the first case, the relationship of mutual citation feels like a result of strategic calculation. In the second case, it feels more like a case of simple friendship. But to the outside observer \u2014 someone who is just observing the tendency of these two scholars to cite each other \u2014 it is hard to distinguish between the two kinds of motivation. It is hard to say whether the pattern of mutual citation is driven more by strategic calculation or by friendship, because either of those dynamics can in principle lead to the observed outcome: a stable relationship of mutual citation.\n\nSuppose the outside observer is now given an additional piece of information: these two scholars not only tend to cite each other; they tend to be on the same side of the great, divisive issues in their field. Alas, this doesn\u2019t help much either, because both of the dynamics in question \u2014 strategic calculation and friendly feeling \u2014 are known to lead to this specific outcome: not just mutual citation, but mutual citation between intellectual allies. After all, (a) if you\u2019re consciously choosing a partner in reciprocal citation, you\u2019ll be inclined to choose someone who shares your strategic interests, namely the advancement of your position on major intellectual issues; (b) if you\u2019re operating instead on the basis of friendly feelings, you\u2019re still likely to wind up paired with an intellectually, since one of the primary contributors to friendly feelings is agreement on contentious issues.\n\nThat the guidance of emotions \u2014 of \u201cfriendly feelings\u201d \u2014 can lead to the same outcome as the guidance of strategic calculation is no coincidence. According to evolutionary psychology, human emotions were \u201cdesigned\u201d by natural selection to serve the strategic interests of individual human beings (or, more precisely, to further the proliferation of the individual\u2019s genes in the environment of our evolution \u2014 but for purposes of this discussion we can assume the interests of the individual and of the individual\u2019s genes align, as they often do). In the case of friendly feelings, we are \u201cdesigned\u201d to warm up to people who share our opinions on contentious issues because, during evolution, these are people it would have been advantageous to form alliances with.\u201d\n\nDe Waal poses a direct relation between strategic interest and human emotions. Furthermore, he contextualizes it in the field of reciprocal altruism, which is the equivalent of cooperation for a biologist. Nevertheless, he shows us an ideal case when, in reality, there are usually diffuse areas. Most of the times there are no such strong emotional bonds or clear strategic interests.\n\nA past written article was about \u201ctwo different worlds\u201d, the world of market rules and social norms one. In this article, we will link this relationships world duality (extrapolated to organizations) with the de Wall text. For that, we will use some charts, which allow us to link the two approaches.\n\nCharts have the next structure:\n\nBefore showing the charts, we purpose the next indicator in order to measure the \u201cdegree of cultural similarity.\u201d We can use an indicator based on Hofstede\u2019s five dimensions. Although it is usually used to measure intercultural variations, it could be extrapolated to individuals. Five dimensions are the next one:\n\nLet\u2019s suppose a cooperation context within a multinational between profesionals who from different departments. They are physically located in the same building and also have the same nationality.\n\nWe are going to develop each one of the cases:\n\nLet\u2019s go back to the Scenario A context but with light differences. The context remains int he field of cooperation between two multinational employees. But this time, they are physically located in different countries and have different nationalities.\n\nWe again develop each one of the cases:\n\nWhen we talk about making decisions from a strategic or emotional point of view, we could think: \u201cWell, are not we talking about the head or heart dichotomy?\u201d. We would be right. Although in fact, it would be more correct to say we are talking about the dichotomy between the dorsolateral prefrontal cortex and the ventromedial prefrontal cortex. Our decision making is governed by our PFC (prefrontal cortex). Let\u2019s talk a little more about the PFC:\n\nIn order to illustrate the operation of these two parts, suppose what would happen If we had either of the two silenced:\n\nThere are very well-known dilemmas regarding this dichotomy, for example, the trolley problem. We encourage you to think about the influence of the previous model on the decisions made in this dilemma."
    },
    {
        "url": "https://becominghuman.ai/sxsw-startups-polyport-af4d9d5df74c",
        "title": "SXSW Startups: PolyPort \u2013",
        "text": "At the 2018 SXSW Accelerator Pitch Event in Austin, 50 diverse startups will face off before a panel of industry experts. Half will pitch on Saturday, March 10, and the remaining 25 will pitch on Sunday, March 11. Winners from each of the 10 categories will be honored at the Accelerator Award Ceremony at 7 pm on Sunday, March 11 at the Hilton Austin. The SXSW Accelerator Pitch Event takes place within the Startup & Tech Sectors track of programming.\n\nA finalist in the Security and Privacy Technology category, PolyPort is a 3D asset protection, management and delivery company, which employs a proprietary encryption technology to facilitate an end-to-end solution for 3D designers. Based in Atlanta, PolyPort allows designers to use their own tools, shortcuts and capabilities in their environment across all platforms. See them in action at 9:30 am on Sunday, March 11, in Salon AB at the Hilton Austin.\n\nFor today\u2019s interview, PolyPort team members Chloe Kettell (CEO & Co-Founder) and Partha Ray (President & Co-Founder) provided more insights on the direction of their startup.\n\nWhat does PolyPort hope to accomplish in 2018?\n\nChloe Kettell: We hope to become the standard for 3D file sharing, security, outsourcing and asset management. With our file history, tracking and embedded encryption, users will be able to engage talent wherever in the world it may be and know their intellectual property is fully in their control. In addition, our tracking components will help creators to more accurately estimate future project costs and hone in on inefficiencies throughout the production pipeline.\n\nWhat inspired your team to apply for SXSW Accelerator?\n\nChloe Kettell: We wanted to apply to SXSW Accelerator last year, but were invited to apply for the Interactive Innovation Award before we had the chance. We won the 2017 SXSW Interactive Innovation Award for Privacy & Security and had such a wonderful experience that we couldn\u2019t help but return for another event. The exposure SXSW gave us last year has opened numerous doors and opportunities to help our company grow.\n\nWhat was your most outstanding memory from last year\u2019s event?\n\nChloe Kettell: When we were sitting at our table in the banquet hall during the SXSW Interactive Innovation Awards and Janina Gavankar started listing off the finalists for the Privacy & Security category. Then she announced our company as the winners and it felt like time stood still. We thought that we might win, but we were amazed that we took the award going against geniuses such as MIT graduates.\n\nWhat sessions are you most excited about attending at SXSW 2018?\n\nChloe Kettell: Any sessions around AI and machine learning. Artificial intelligence and machine learning will be key in detecting similar file types in order to detect if piracy has occurred.\n\nHow long has the PolyPort team been together?\n\nChloe Kettell: Michael Shull and Partha Ray met each other in July of 2004 at an eDiscovery vendor. They quickly connected and became long-time friends. Partha met me six years later and introduced me to Michael a short time after. When the opportunity came, Partha approached Michael to design some software. The result was D3CRYPT3D. After some up and downs, Michael redesigned the old software into PolyPort. In total, we have been together as a team for one year and three months.\n\nWhat is your competitive advantage?\n\nChloe Kettell: The nature of our platform and how it relates to file access history gives huge advantages to the many combined years of eDiscovery experience.\n\nPolyPort is based in Atlanta. What is the startup ecosystem like there? And do you think Amazon will pick Atlanta for HQ2?\n\nChloe Kettell: Atlanta is a rapidly changing environment. The startup ecosystem is thriving and there are myriad programs here to help startups succeed. The film industry has grown, and it seems like more and more shows are being shot in the area. Atlanta does have a good chance for bringing Amazon in. Real estate is cheap and the world\u2019s busiest airport is in the city.\n\nWhich tech industry trend does your team think is most underrated?\n\nChloe Kettell: Flexible display design technology will alter the way we view everything. Remember tube TVs and how they are outdated? Boxed in LCD/LED panels will be our generation\u2019s tube TVs.\n\nWhat podcasts does you listen to?\n\nChloe Kettell: I absolutely love Masters of Scale. It\u2019s interesting, concise and has great takeaway points. I just started listening to Recode Decode, as I\u2019ve caught up with all the Masters of Scale episodes.\n\nWhat aspects of the startup experience do you enjoy most?\n\nChloe Kettell: I love the amount of personal development I\u2019ve gained through this experience. Having to wear 20 different hats forces me outside of my comfort zone in order to accomplish the task at hand. I\u2019ve always been inquisitive; I was the kid in elementary school who always had my hand raised. From investor pitching, creating sales decks, conducting problem interviews, and learning all aspects of marketing, there\u2019s never a shortage of things to learn. I will be a serial entrepreneur for sure.\n\nPartha Ray: I love the challenge of being responsible for my own destiny. This company is what we make it and our only limitations are in our minds. Dream and implement; there is no better feeling in the world as a love for what you do. It makes it all fun.\n\nHave team members previously been involved with other startups? Partha Ray: I was a co-founder of the eDiscovery and Forensics company Modus. We were wildly successful and made many game changing advancements in the eDiscovery industry. Mike and I worked together at that startup and he wrote all of the game changing software components which made us as successful as we were. At my exit we were grossing over $40 million in sales.\n\nWhat has the startup experience taught you about life?\n\nChloe Kettell: The key lesson being picking your co-founders is not to be taken lightly. You should know everything about them; the good, the bad, the ugly. Starting a company isn\u2019t tulips and fairies. There are extreme highs and extreme lows, and if you can\u2019t weather the storm as a team, you\u2019ll likely fail. We dealt with a messy founder break-up, but our team came out stronger from it.\n\nFor what you do on a day-to-day basis at PolyPort, what does work-life balance look like?\n\nChloe Kettell: The work balance is dynamic. One day the developer could have dreams of developing but instead does admin work and vice versa. We all are passionate about the end goal and are all prepared to wear whatever hat presents itself. We have all dropped our prior lives and committed ourselves to making this company and its offering amazing. This means three people dropping six figure, tenured positions, to make our product the best it can be. We started the Darkfield Cybersecurity Accelerator on January 15 and are excited to see how quickly we can make this company grow.\n\nPerson, company, thing or goal. What inspires your team to work harder?\n\nChloe Kettell: Our team is inspired by the goal of creating game-changing technology and software that is simple yet sophisticated with a creative approach toward currently unsolved problems.\n\nLook for interviews with other SXSW Accelerator finalists in this space between now and March. Startups already profiled as part of this series include Bluefield and 70MillionJobs.\n\nOr, click here to browse the full lineup of startups for SXSW Accelerator 2018.\n\nHugh Forrest serves as Chief Programming Officer at SXSW, the world\u2019s most unique gathering of creative professionals. He also tries to write at least four paragraphs per day on Medium. These posts often cover tech-related trends; other times they focus on books, pop culture, sports and other current events."
    },
    {
        "url": "https://becominghuman.ai/silently-censored-8f22214192a",
        "title": "Silently Censored \u2013",
        "text": "In September of 2016, Facebook censored Nick Ut\u2019s Pulitzer Prize-winning \u201cNapalm Girl\u201d photograph, calling it a \u2018display of nudity.\u2019 This censorship was widely reported, Facebook reversed its decision, and an anonymous Facebook employee explained to the New York Times what happened: \u201cFacebook uses a combination of algorithms and human moderators to review photos that can potentially break its rules. In this case, the photo was tagged for removal by one of Facebook\u2019s algorithms, which was then followed up by a human editor.\u201d\n\nThe removal of the photograph is an example of algorithms and humans working together to monitor content. In this case, mistakes were made. But the point I\u2019d like to make is that algorithms are our first line of defense. Algorithms are searching for media that fails to meet the community standards, and such content is censored (or not) depending on a variety of factors, systems, and standards that we as content consumers are often not aware of and/or do not understand.\n\nWhat exactly do our algorithms see? How many \u2018sensitive\u2019 photos do they catch and how many perfectly fine ones do they misidentify? And how do these judgments impact what we see? These are questions I began to ask myself as I noticed some strange patterns in the behavior of my Twitter account, and finally came up with the theory that some of the tweets I was posting with hashtags were only appearing in the larger conversation to users who\u2019d proactively opted to view \u2018sensitive content\u2019. In other words, it seemed very much like the innocent nature photos I was sharing had been silently and subtly tagged as unsafe for general consumption.\n\nNaturally, I did what any normal human would: I edited my photos so that they would once again be determined a-okay for the general public.\n\nI have identified all the original photos (failed!) and algorithm-ready successes (passed!) with a single hashtag (#WhatDoesTheAlgorithmSee) so that you can see them, too. Note that the images only fail in the context of the hashtag thread; they appear on my profile page:\n\nNote: if you only see the \u2018Passed\u2019 versions in the thread, you might have to disable the \u2018Hide sensitive content\u2019 setting under \u2018Privacy and Safety\u2019 before you can see the original \u2018Failed\u2019 versions.\n\nIf you don\u2019t feel like heading over to Twitter, I\u2019m posting the photos here, too!"
    },
    {
        "url": "https://becominghuman.ai/welcome-to-the-hypernet-powered-by-you-95dbbfe7dfa6",
        "title": "Welcome to the Hypernet: Powered by You \u2013",
        "text": "The computing industry is in the midst of a paradigm shift. Tools like artificial intelligence and machine learning (which were purely academic curiosities a decade ago) are now driving your car, filtering your Google searches, and predicting your next iPhone text. But even your iPhone X \u2014 in all its Animoji glory \u2014 isn\u2019t capable of much in isolation. You couldn\u2019t use your iPhone (or most computers) to edit a movie or produce music, to power an AR/VR headset, or even to to analyze large amounts of data.\n\nInstead, you might try the cloud; but that\u2019s expensive and high-latency. Your last resort might be to spend a month\u2019s salary on a datacenter for your home, and, like countless researchers in dusty university laboratories, strap it to your back, and gobble up your data plan for computation on the go (yeah, that\u2019s a real thing). Clearly, in practice, performing large computational tasks cheaply, quickly, and conveniently is impossible. But what if I told you there was another solution \u2014 one that could allow you to access nearly limitless computational power from your device of choice at a fraction of the cost of current state of the art cloud services?\n\n\u200bHypernet has found an elegant, efficient solution to these computational needs by salvaging wasted processing power. Paradoxically, though computation is an enormously valuable commodity, most Americans have desktops, smartphones, and even toasters with processors that waste away hours without use. Even though each of these processors is limited on its own, enough of these devices together can surpass the abilities of even state of the art supercomputers; and they are only becoming more common. With Hypernet you will be able to lease your otherwise wasted computing power to other people that want to use it \u2014 and be paid for it!\n\n\u200b Hypernet allows sellers to rent out a slice of their idle processor power. First, sellers register their devices on the Hypernet and set a price for time on their device. Then, much like traditional cloud services, buyers write a program as they normally would, and request computation-hours on a certain number of devices with a minimum specification to run their program. Hypernet ensures that buyer\u2019s program is split up and securely sent to each of the requisitioned devices, handles arbitrage between users, verifies work done by sellers, and generally makes sure the process goes smoothly. We know \u2014 it seems simple \u2014 but this form of parallel computing has never been done before, and we think it\u2019s going to change the world.\n\n\u200bHypernet is new for a variety of technical reasons. First, our approach to computing works for almost all types of problems, while competitors in distributed computing have only tried to approach the simplest class of programs, called grid computing. In lay terms, this means that if you wanted to render an image like in a video game, you might have multiple service options; but, for more sophisticated tasks, you\u2019d be looking to Hypernet. Second, we use an algorithm based on distributed average consensus that was designed from the ground up for dynamic, distributed, and decentralized networks of devices, and we\u2019ve shown mathematically that it is guaranteed to return the right answer to buyers efficiently and at a low cost. If you want to hear more about our technical innovation, get in touch with us on Slack or Reddit \u2014 we\u2019d be happy to nerd out with you.\n\n\u200bHypernet\u2019s technology, pioneered by a plucky group of Stanford engineers, is poised to change the world by making high-performance parallel computing a consumer good. Currently, only large corporations can truly benefit from massively parallel computing by exploiting their oligopoly on consolidated computing power. We want that power to be available to you too, so you can take on ambitious, novel, massively parallel projects in big data, AI, and modeling with the tools to compete against the likes of Google, Amazon, and Facebook \u2014 and win. We want to give you, from armchair AI developer to smartphone savant, the tools to take on problems like climate change, world hunger, poverty, and economic instability, with all the power in the world. \n\nYOU are the only missing piece in our community \u2014 and we can\u2019t wait to see what great new things you do on our platform."
    },
    {
        "url": "https://becominghuman.ai/only-numpy-implementing-convolutional-neural-network-using-numpy-deriving-forward-feed-and-back-458a5250d6e4",
        "title": "Only Numpy: Implementing Convolutional Neural Network using Numpy ( Deriving Forward Feed and Back\u2026",
        "text": "Lets keep things very simple, we have four (3*3) images. (LOL well too small to call them images but, it\u2019ll do the job). And as you can see in the Ground truth Label Data (Y), if the image have more 1, the resulted output increases. The max is set to 1.1 since we are using logistic sigmoid function as final output.\n\nSo as seen above, we have a very simple network structure.\n\nX \u2192 3*3 Image\n\nK \u2192 Convolution Operation (Right is Matrix Form, Left is Vectorization form) \n\nGreen Start \u2192 Resulted Image (Right Matrix Form, Left is Vectorization form)\n\nIf above image is confusing for you please see the image below.\n\nBasically, the 3*3 pixel convolution operation can be thought of multiplying certain pixels located in different images with given weight."
    },
    {
        "url": "https://becominghuman.ai/the-future-of-chatbots-roi-customer-loyalty-and-revenues-case-studies-195a2f0ffc41",
        "title": "The Future of Chatbots: ROI, Customer Loyalty, and Revenues [CASE STUDIES]",
        "text": "By 2020, 85% of all customer interactions will be handled without a human agent, someone said. But today we are in 2017, almost 2018, and there\u2019re still a lot of doubts when it comes to chatbots.\n\nOn Thursday November 30, I served as the Moderator for the panel \u201cChatbots: The Next Generation of Messaging Apps\u201d at the AI Expo in Santa Clara.\n\nGreat opportunity to hear about some relevant case studies in this field from experts such as #eBay Lead Product Manager, #MoneyGram International, Head of Product and Innovation, #TGIFriday Marketing Technology Director and #Foursquare Director of Consumer Products.\n\nThe session explored such key topics as:\n\nWe are leading an AI-powered shopping assistant. Most recently we had a chatbot on messanger and we very recently launched a eBay assistant on google home. And before that I was working with Stubhub building chatbots on Messenger, Skype, and Cortana.\n\nIn addition of running our mobile apps we launched a year ago a chatbot that converse with you via SMS to find drink recommendations and also, it does contextually powered personalized recommendations\n\nWe deployed our chatbot about an year ago. His name is Fry. We are using Fry on twitter and facebook, as well as the Alexa skill.\n\nWe are engaging in a very aggressive digital tranformation. We launched a bot that we co-designed with Facebook on messanger. Also, using AI to anticipate the behaviour of the users as they are going through a very emotional journey sending money.\n\nThe following questions were asked to the panelists:\n\nWhat we realized was a good strategy when failing was providing a smart message to still deliver value. For example when a user asked something that our chatbot is not able to understand due to a lack of data \u2014 e.g. customer looking for tacos adding also another part in the message that we were not able to process \u2014 we manage to provide a suggestion based on the word we were actually able to grab by making it saying \u201cI think you said (tacos) why you don\u2019t look at this (a solution, maybe not the customized one). And we realized that the customer was actually happy and engaged versus saying \u201chey, I don\u2019t know what you just said.\u201d So failing smartly was our first challenge.\n\nIt does really start by how do we want this chatbot to sound like. And we want to sound like a closed friend that you know really well and it recommends place for new ideas. It\u2019s actually all written by me and my colleague thinking about how we naturally interact with people. We did research on it and one of the funny ones was texting to the person \u201chey, what are you up to? Do you wanna check out this restaurant in the mission, thought was really great and I know you love indian\u201d. And I remember one of our research said that when she got the message she thought it was someone she was dating and asking to go to the restaurant. So we realized this was a little too closed, too personal. So we detached it a little bit but we keep that personality adding emojis, abbreviations, to try to make the all idea like it is a human that you are texting with. I think that is one of the differentiator in chatbots. The way you engage with people in a human level. You really need to think about your chatbot in a very different perspective and understand how is really perceived by your customers.\n\nWe went through the same learning curve that Marissa just highlighted. So what we have done is also using a funny image to eliminate the stress in not providing the answer. And then, we moved forward to suggest something by looking at the customer profile and suggest based on the previous experience. Try to divert a very frustrating experience to suggest what the user would like to do. Use data and previous behaviours.\n\nWhen we first started we implemented our bot to decrease interactions for our social team. And then we moved forward supporting the order system and we were able to get more insights in terms of customer service as well as payments and transactions.\n\nWhat we are really doing today is customer service, and online ordering. What we want to do in the next future is improving the customer acquisition. So far, we have seen very good results and we look forward to understand how we are able to make it smarter, especially making sure we\u2019ll be able to recommend something that can be actually relevant for our customers (we want to avoid to recommend chicken to someone that never ate chicken before.\n\nYouri: moneygram network has 350K+ agents worldwide that can pick up cash. And we realized that actually people challenge the chatbot trying to find places where we didn\u2019t have locations. And so, what we did was starting to put images or emojis, e.g. picture of the desert that says middle of nowhere. And after this people were putting actually the real location. People that send money are really careful on what to do, so the chatbot conversation is actually very important.\n\nTalking about short term one of the key things is customer service. We started with a pilot in the UK and it\u2019s expanding now. And those are key pieces. You want to focus on use cases that you know that can bring real value. Especially here in the valley you can\u2019t get around if there\u2019s no money involved. We focused on return, payments etc. which are the very core drivers. And through chatbots we have seen people rate much better this experience rather than going through traditional channels. But there are always instances where the bot cannot completely take care of everything we did want or there are more complicated issues that the bot couldn\u2019t solve. And those are areas where we need to interaction of a human assistant that can figure the issues out. Interestingly in the traditional channels we were not able to follow up previous discussions that we were able to manage with the chatbot by going back in the conversation and sort of creating a transition and give better answers through the human customer service.\n\nThe clear conclusions drawn by panelists Jay Vasudevan, Lead Product Manager #eBay; Youri Bebic, Head of Product and Innovation #MoneyGramInternational; Gail Seanor, Sr Director, Marketing Technology #TGIFridays and Marissa Chacko, Director of Consumer Products #Foursquare are that chatbots are essential to creating optimal customer engagement and to cementing customer loyalty. The degree to which a company\u2019s chatbot can respond and engage in human terms will be directly reflected in revenues. The role that AI and customer data will continue to play in creating ever-more-intelligent chatbots has only begun. The chatbot frontier is certain to expand and companies that fail to leverage AI-driven customer data for chatbot support will be left behind."
    },
    {
        "url": "https://becominghuman.ai/how-ethical-is-artificial-intelligence-37ee034e7435",
        "title": "How ethical is Artificial Intelligence? \u2013",
        "text": "A woman finds out about her pregnancy shortly after the death of her partner. Unable to cope with the grief, she orders a robotic replica of him. It\u2019s strikingly witty like his former self; it even outperforms him in some ways. However, it fails to understand her intimate emotions repeatedly; together, their life becomes a big mess. The preceding was the synopsis of \u201cBe right back\u201d, an episode of Black Mirror \u2014 a sci-fi TV series that grotesquely portrays humanity\u2019s obsession with technology. Most episodes are set in the alternative present. Therefore, some depictions are pertinent to the contemporary issues we face with Artificial Intelligence (AI). It\u2019s imperative to analyze the impacts of AI and how it may shape our future.\n\nHolistically, AI is defined as algorithms and models targeted at thinking, perception, and action. While the idea had been fascinating humanity for centuries, AI had its real beginning in the summer of 1956, at the Dartmouth College in the USA. A team of 10 eminent scientists \u2014 sharing a passion for the modeling of biological intelligence \u2014 convened for a seminal 6-week summer project and laid the foundation of AI. The effort was based on a conjecture that \u201cintelligence can in principle be so precisely described that a machine can be made to simulate it\u201d.\n\n\u201cIn the six decades since this brash beginning, the field of artificial intelligence has been through periods of hype and high expectations alternating with periods of setbacks and disappointments\u201d \u2014 Nick Bostrom, Superintelligence (2014)\n\nWhile the original purpose of AI was to understand intelligence, it has now come a long way, generating both admiration and apprehension. Those who promote it believe that it will \u2018aid human effort\u2019 (like any other technology), others think that it walks into a minefield of moral and ethical obligations not fully understood.\n\nA 2013 research published by the University of Oxford estimated that over 47% of total US jobs would be automated by the next two decades; in developing countries, these estimates can go beyond 70%. Another study reports more than 60% of people in the UK feel AI will steal their jobs. Unemployment is undoubtedly the most cited drawback of AI. Ever increasing needs for accuracy, efficiency, and economy compels industries to automate processes and terminate traditional jobs. In many cases, this is met by vehement protests and strikes that governments fail to allay.\n\nEvery technology which tends to revolutionize society on a global scale also becomes a target of accusations. For instance, the industrial revolution arguably transformed humanity for the better, but societies had initially blamed it for creating unemployment. Wendell Wallach, an ethicist and scholar at Yale University, comments:\n\n\u201cIt\u2019s a long-running concern \u2014 the Luddite concern going back 200 years ago \u2014 that each new form of technology will rob more jobs than it creates. Up to now, we haven\u2019t seen that. Each new technology eventually creates more secondary jobs than it eliminates.\u201d\n\nThe effects of mechanization (automation) on the economy are studied well. And the general perception that AI loots away jobs and contributes to growing rates of unemployment appear to be exaggerated. At present, AI can only automate excessively physical and monotonous jobs \u2014 it performs poorly at tasks that require high levels of cognitive skills. One may ponder, as Marvin Minsky once pointed out, why no artificially intelligent robots were successfully deployed to contain the Fukushima Nuclear Disaster.\n\nEvidently, future jobs will be more complex and cognitively challenging; this may in-turn augment education and lead to a skilled workforce. However, the effects of this transition on blue and white collar jobs are concerning. Ravi Shankar Prasad, Electronics & IT minister of India, has found it a recurring theme in talks with companies to build the roadmap of $1 trillion digital Indian economy; he promises to create more jobs by training personnel in AI.\n\nMaster of one trade, Jack of none:\n\nHans Berliner, a Computer Science professor at Carnegie Mellon University (CMU), wrote BKG 9.8, a program that played and defeated Luigi Villa in 1979 (the world backgammon champion).\u201cThis was the first time that a world champion of a recognized intellectual activity had been defeated by a man-created entity in a head-to-head test of skill\u201d.\n\nGarry Kasparov \u2014 a pre-eminent Chess grandmaster \u2014 lost to IBM\u2019s Deep Blue in 1997. Lee Sedol \u2014 champion of the Chinese board game Go \u2014 lost to Google\u2019s AlphaGo in 2016. And in 2017, AlphaGo-Zero (a variant of AlphaGo) championed Go by playing against itself. Beating world champions at games such as Chess and Go \u2014 which are widely perceived by society to epitomize human intellect \u2014 doesn\u2019t necessarily mean that AI has become smarter than humans.\n\nAt present, the expertise of AI is limited to a handful of domains; this limits its capabilities of generality. For example, a cow may give milk, but it can\u2019t learn to fly an airplane. In the cow\u2019s defense, one may argue that it\u2019s not engineered by nature to fly an airplane. And that\u2019s exactly why AlphaGo (designed to play Go) can\u2019t even beat a toddler in Chess \u2014 not until it learns to play chess (but then it would be called AlphaChess). Applied AI may overkill domain expert, but it\u2019s very domain specific. Domain specificity raises many ethical concerns: if the AI is perceived (a case of mis-attribution) to be more general that it is, it may not only lead to dissatisfaction (which is somewhat acceptable) but also to some health problems [Article 2-AI&Society]. Also, if a domain specific AI gets deployed to do a substantially general job (a case of overselling), it\u2019s highly probable that it will fail miserably.\n\nWhatever may go wrong with AI?\n\nNeural Networks are a class of algorithms that are inspired by the way brain works. Industries extensively use them in products such as translators and chatbots. Although neural networks have set benchmarks in performance, they are not hard to fool (look at the image above). Some systems such as AlphaGo may outperform humans, but a good majority of them either underperform or fail occasionally \u2014 there\u2019s always a chance of error. Intelligent Personal Assistants like Siri claim to improve one\u2019s productivity but abysmally fail to understand simplest of statements.\n\nAnother set of concerns emerge from the autonomous applications of AI, such as driverless vehicles. What would an autonomous AI do if it\u2019s stuck in catch-22 \u2014 a situation where any action will lead to causality. For illustration, consider the trolley car dilemma.\n\nWould the AI in control, flip the lever? If yes, why? If no, why not?\n\nWhat would be the values of such an AI? And since, the definition of human values are fuzzy, how is one to model it and program it? Philosophical and technical questions as such, obligate researchers to collaborate and develop strategies for a future where humanity co-exists with smart machines.\n\nWhether it\u2019s too early to talk about ethics in AI or not, experts argue. Stephen Hawking, created a ruckus in the AI community with his 2014 interview with BBC, he warned:\n\n\u201cThe development of full artificial intelligence could spell the end of the human race\u201d.\n\nHowever, his contemporary, Michio Kaku is not so gloomy about the prospects; he doesn\u2019t expect a technological singularity (a time when AI becomes smarter than humans) anytime soon. Kaku is a proponent of the \u201coff-switch\u201d theory: If AI continues to behave undesirably, then it can be stopped by switching off the power. The disagreement mainly comes from the split in AI (Science vs. Engineering). Founders of the field have expressed that somewhere in the \u201980s, the focus of the field turned to engineering. As Neural Networks gained prominence, they were employed for too many applications \u2014 this led to overselling, disappointments and stagnation of quality AI research.\n\nDeep Learning (modern neural networks) collect heavy criticism because no one knows how the algorithms work unlike approaches like Bayesian Learning (algorithms based on probability theory). Some scientists argue: if the algorithms are intractable, i.e., they don\u2019t know how they are doing something, then they are not doing it \u2014 in that way, they are not the appropriate model for a problem. Even if these algorithms produce a solution that appears to be promising, it\u2019s often not the best solution for the problem. Noam Chomsky has consistently derided such models, he comments,\n\n\u201cCan machines think is like asking, can submarines swim? If you want to call that swimming, that\u2019s fine. Do planes fly? In English, they do, in Hebrew, they glide.\u201d\n\n\u201cReally knowing Semantics [of the problem] is a pre-requisite for anything to be called intelligence\u201d, Barbara Parte, a fellow scientist, shares his view. The argument is that AI research should focus on understanding how intelligent systems work and then applying it to a problem. Blind computing not only drifts away from the original question but also exacerbates the situation.\n\nIndustrialization of AI has an immense potential not only for the overall technological advancement of society but also for unethical practices. Though Snowden\u2019s revelations helped to elucidate the threat of data acquisition to privacy, we continue to bleed data. Big Data (massive data-sets that aid decision making) has become an excuse for unsolicited data collection, and trades.\n\nAI can also be used to transform society into a test-bed for social experiments without informed consent. In 2014, Facebook published a study where they manipulated the news-feeds to test whether user emotions could be influenced. Academics critically slammed the research for the methods they used in the experiment. Google\u2019s Job-recommender AI was brandished as sexist for presenting prestigious ads only to male candidates and its Photo app AI tagged a African-origin dark-skinned person as a gorilla!\n\nHuman Computation (Crowdsourcing) gets exploited for Crpytojacking: Millions of websites mine cryptocurrencies (such as Bitcoin) by smartly analyzing and controlling (load balancing) the host machines \u2014 leaving users with slow computers and degraded user-experience. Internet Service Providers monitor internet-traffic to smartly throttle speeds by detecting (an application of AI) the type of payload, an ethical issue which is being contested by the Net-Neutrality drive.\n\nAutonomous weapons have also started to emerge from defense research. However, a dumb technology without \u2018human in the loop\u2019 can lead to cataclysm. Consider the 1983 Soviet nuclear false alarm incident. On September 26, a Soviet military early-warning system reported that the US had launched multiple missiles. Stanislav Petrov, the man-in-charge judged this to be a false alarm and disobeyed the protocols for a retaliatory strike. An investigation confirmed that warning system had indeed malfunctioned. Human intervention saved millions of lives. The consequences of retaliation (or even a security breach) could have been Ragnarok.\n\nWhile some issues in AI demand immediate attention, others are more futuristic. Researchers often debate and warn about a technological Singularity: an instance when AI transcends human intelligence. The idea of singularity brings uneasiness because it launches prospects into the world of science-fiction (think about Frankenstein).\n\nWould such an AI be malevolent or empathetic to humans?\n\nIt is a tremendous technological challenge just to engineer an AGI (super-intelligence) in the first place, aligning it with human values and providing sentience appears to be an impossible task. In that manner, current attempts to create an AGI require deep introspection. Even though concerns about AGI seem to be too far-fetched, we will have to address them sooner or later.\n\nTranshumanism, however, raises more contemporary issues. It aims to enhance the human condition, overcome biological limitations and experience post-human self by using present and future technologies: genetic engineering, AI and brain computer interface among others. Prospects have immense potentials, but if subjected to mischievous motives it may lead to the eradication of all intelligence, a claim made by Nick Bostrom.\n\nIndustries and governments have recently started to recognize the importance of ethics in AI.\n\nThe \u2018Future of Life Institute\u2019 (FLI), co-founded by physicist Max Tegmark, leads the international movement to promote engagement of ethics in AI research. Its scientific advisory board comprises of Industrialist Elon Musk and eminent scientists such as Stephen Hawking, Stuart Russel, and Christof Koch \u2014 for some weird reason, Morgan Freeman too. FLI has been instrumental in organizing conferences on ethical-AI and securing funds for relevant research projects. Elon recently donated $10m to the foundation with a focus on safe AI research.\n\nFLI organized the Asilomar AI conference in 2017; it became a confluence of researchers, philosophers, and industrialists. Ray Kurzweil proposed that guidelines should be published in AI, just the way they were once published in Biotechnology \u2014 it has worked well for biotech and should work for AI too. Shane Legg, co-founder of DeepMind, pitched the need to understand the internal mechanisms and representations of Neural Networks and Machine learning algorithms.\n\nPartnership in AI is another organization like FLI. Founded by industrial giants: Amazon, Facebook, Google, DeepMind, Microsoft, and IBM; it\u2019s a consortium to bring together organizations, academic institutions, and companies to govern and invest the effort in creating AI that would contribute to humanity\u2019s most significant challenges.\n\nDeepMind, a UK based research company that developed AlphaGo, was acquired by Google for \u00a3400m in January 2014; the terms for acquisition made sure that they will not oblige to any unethical research ordered by Google. Recently, it launched an internal team to scrutinize the societal impacts of the technologies it develops, and tackle \u201ckey ethical challenges\u201d such as privacy, transparency, governance, and morality among others. Companies such as Microsoft and Google have started to follow the trend internally as well. They are establishing ethics teams headed by top scientists and philosophers to ensure a safe development. A team representative from Google said,\n\n\u201cWe do not want to stifle the development of innovation but providing a closely monitored structure to these systems is crucially important to us and society.\u201d\n\nOpenness has a great impact on software; it creates better programmers, detects bugs, and improves the quality of code. It also makes public: the source code, science, data, safety techniques, capabilities, and goals. Therefore, the global desirability of openness in AI is appearing. OpenAI is a non-profit AI research company (funded by, Elon Musk) that aims to promote and develop friendly AI and distribute the benefits humanity as a whole.\n\nAcademic institutions are the best place to introduce AI ethics since an early orientation towards ethical development would solve a chunk of the problem beforehand. Universities have started to add philosophy, humanities, and ethical studies in the AI curriculum. CMU, one of the world leaders in Machine Learning research, received a donation of $10 million from K & L Gates Foundation to promote ethical research in AI and robotics. Prof. Subra Suresh, president (now, former) of CMU, spoke,\n\n\u201cIt is not just technology that will determine how this century unfolds. Our future will also be influenced strongly by how humans interact with technology, how we foresee and respond to the unintended consequences of our work, and how we ensure that technology is used to benefit humanity, individually and as a society.\u201d\n\nAI, Internet of Things, and other technologies have transformed humanity as we know it. By collecting data and tracking our activities, websites know more about us than we may know about ourselves. Our algorithms make art but don\u2019t understand it. Vehicles are factory equipped with auto-pilots that may not understand the consequences of a decision.\n\nAI has evoked many issues in the past and will continue to do so; the aim is to play smart by taking precautions. A lot of introspection, science, and capital is being invested in ethical research, economics, and philosophy of AI. Researchers and industrialists have begun to recognize the appropriate role of AI in society. However, government policies haven\u2019t yet attained a substantial stature.\n\nClimate change and growing population have drastically reduced the forests cover of Sundarbans. As a result, humans are forced to kill the endangered Bengal tiger upon encounter. A single tiger\u2019s territory may span 60\u2013100 square kilometers; clashes are bound to happen. This is a problem that matters, the reason no one cares is that economic returns from solving this are too low.\n\nAI is the harbinger of immense potential and boundless possibilities, but to fully experience them, governments should graduate from shallow motives of just countering unemployment. We must strategically deploy AI to target more significant issues such as climate change, health-care, education, security, and research. Addressing the most significant questions in the universe will reveal the real potential of AI. It\u2019s not AI per se that deserves respect or hate in the first place; instead, it\u2019s the will and actions of humanity that determines the course of our future.\n\nAdhar Sharma was a researcher working with Dr. Sukant Khurana\u2019s group, focussing on Ethics of Artificial Intelligence. Dr. Deepak Singh , a Ph.D. from Michigan, is now a postdoc based at Physical Research Laboratory, Ahmedabad, India and is collaborating with Dr. Khurana on Ethics of AI and science popularization.\n\nRaamesh Gowri Raghavan is collaborating with Dr. Sukant Khurana on various projects, ranging from popular writing of AI, influence of technology on art, and mental health awareness.\n\nMr. Raamesh Gowri Raghavan is an award winning poet, a well-known advertising professional, historian, and a researcher exploring the interface of science and art. He is also championing a massive anti-depression and suicide prevention effort with Dr. Khurana and Farooq Ali Khan.\n\nYou can know more about Raamesh at:\n\nDr. Sukant Khurana runs an academic research lab and several tech companies. He is also a known artist, author, and speaker. You can learn more about Sukant at www.brainnart.com or www.dataisnotjustdata.com and if you wish to work on biomedical research, neuroscience, sustainable development, artificial intelligence or data science projects for public good, you can contact him at skgroup.iiserk@gmail.com or by reaching out to him on linkedin https://www.linkedin.com/in/sukant-khurana-755a2343/.\n\nHere are two small documentaries on Sukant and a TEDx video on his citizen science effort."
    },
    {
        "url": "https://becominghuman.ai/reinforcement-learning-sumo-and-complex-urban-traffic-management-82e3b8cdd110",
        "title": "Reinforcement Learning, SUMO, and Complex Urban Traffic Management",
        "text": "As we mentioned in the beginning of this article, the SUMO simulator can enable third party systems to approach reinforcement learning. In this case, TraCI will play the role of the \u201cconvertor\u201d between SUMO and the reinforcement learning approach to establish this interaction. TraCI is able to retrieve every piece of information in the simulation, including the vehicle and network. This provides useful features for the reinforcement learning agent to justify the states of the environment. Based on the observation of the states, we could set and assign the rewards accordingly and let reinforcement learning optimise the policy based on the reward. After that, the reinforcement learning agent will assign a new action to SUMO through TraCI and continuously observe the environmental state.\n\nTraCI could be accessed using multiple programming languages, with the most common language being Python. The package tools/TraCl in the SUMO simulator allows users to interact with SUMO using Python. This is advantageous as Python is already a well established script language for machine learning, providing useful libraries (such as Numpy and Pandas) whilst implementing a machine learning algorithm.\n\nThe interaction between the reinforcement learning agent and the environment via TraCI would be continued until it reaches a terminal state, or the agent meets a termination condition. Essentially, the reinforcement learning techniques apply the Markov decision processes (MDPs). A MDP is defined as a five-tuple < S, A, T, R, \u03b3 >, where S is the collection of states, A is the set of actions which could change the status, T is the transition function, which is the probability of the state change under the certain action, R is the reward function, and \u03b3 is known as the discount factor, which models the importance of the future and immediate rewards.\n\nReinforcement learning optimises its policy by repeating the following step: at each time step t, the reinforcement learning agent perceives the state from state collection S and, based on its observation, selects an action and executes it to lead the state of the environment transition to the next state. Then, the agent receives immediate reward R, observes the new state, and updates the policy with the equation above including the discounted reward \u03b3.\n\nIn order to approach reinforcement learning in the SUMO simulator, we need corresponding elements for a Markov decision process in SUMO. The SUMO simulator uses default routing method \u201cDuaRouter\u201d to generate route files for every vehicle in the simulation. \u201cDuaRouter\u201d performs dynamic user assignment (DUA) based on shortest path computation. The reinforcement learning agent will replace the default routing method with the optimal policy that it has learnt."
    },
    {
        "url": "https://becominghuman.ai/saving-ai-surveillance-renewed-scaling-up-community-internet-the-web-this-week-10b1651da2eb",
        "title": "Saving AI, surveillance renewed & scaling up community internet | The Web This Week",
        "text": "Protecting elections or political suppression? \u2014 Brazilian police announced they will identify and punish authors of \u2018fake news\u2019, following France\u2019s pledge to stamp out misinformation online. The Intercept worries that giving authorities the power to suppress political content on the internet during an election, with no legal framework or safeguards from abuse, will not end well.\n\nMissing the mark \u2014 With changes to its algorithm, Facebook hopes to get a handle on the misinformation coursing through its platform. However, previous fiddling in countries such as Bolivia, Slovakia and Cambodia actually exacerbated the problem, writes The New York Times.\n\nRepublican Senator on Trump\u2019s \u201cshameful\u201d antics \u2014 As Trump tweeted his \u2018fake news awards\u2019, outgoing Senator Jeff Flake called the US President\u2019s attacks on the press anti-democratic and called for his Republican colleagues to speak out (Los Angeles Times).\n\nWhy scrapping net neutrality hurts women \u2014 Women dominate the enormous influencer marketing industry. Now, with net neutrality under threat in the US, many of these content creators and online entrepreneurs will struggle to compete with large businesses (Adweek).\n\nStates take on the FCC \u2014 In a bid to retain enforceable net neutrality protections, 21 states have filed a suit to challenge the US Federal Communications Commission\u2019s decision to scrap net neutrality rules (Reuters).\n\nThe search for 51 \u2014 With 50 US Senators, including Republican Susan Collins, supporting a vote to block the FCC\u2019s net neutrality repeal, the Democrats are looking for one more vote to ensure the legislation is sent to the House of Representatives (The Hill).\n\nAlexa the feminist \u2014 Amazon was criticised for perpetuating sexism when it first launched Alexa, a female-voiced personal servant that responded passively to abuse. Now, an updated Alexa will disengage with users who act inappropriately and will respond to sexually explicit questions with: \u201cI\u2019m not going to respond to that.\u201d While a step forward, Leah Fessler argues this approach can only take us so far (Quartz).\n\nJust no, Just Eat \u2014 Michelle Midwinter, customer of UK food delivery company Just Eat, was rightly horrified when a delivery driver sent her a series of creepy texts. Just Eat told her they don\u2019t have a complaints department, instead offering her a \u00a35 voucher. The story raises questions about how data is protected and customers are kept safe (BBC).\n\nThe impact of abuse online \u2014 To understand more about online abuse against women, Amnesty International commissioned an Ipsos MORI poll of women in the UK, USA, Spain, Denmark, Italy, Sweden, Poland and New Zealand. See the results and download the data (Medium).\n\nNSA surveillance renewal \u2014 Almost five years after Edward Snowden lifted the lid on National Security Agency mass surveillance, the US Senate advanced a bill to renew the NSA\u2019s warrantless internet surveillance programme (Reuters).\n\nTaking back control online \u2014 Ruben Verborgh plots a course for a decentralised web where users take control of their data and have a fundamentally different \u2014 and more empowered \u2014 relationship with the apps they use online.\n\nBiometrics dropped in Tunisia \u2014 Access Now and Al Bawsala celebrated a victory in Tunisia after the parliament dropped plans to adopt a deeply flawed biometric ID card proposal.\n\nNot protecting personal data? Fine \u2014 US Democratic Senators want to fine credit reporting companies, like Equifax, who fail to secure people\u2019s personal data. Any funds would be returned to users whose data is breached (Recode).\n\nWhy is big broadband scared of community-run internet? \u2014 Because commmunity-run broadband networks provide significantly cheaper \u2014 and more transparent \u2014 access than private sector companies, according to a new study from the Berkman Klein Center for Internet and Society (Motherboard).\n\nGoogle expands its network \u2014 The company is growing its global infrastructure, with five new regional data centres to be built in 2018 and three subsea cables in 2019 (CNET).\n\nVenezuela\u2019s internet \u2018provider\u2019 \u2014 Customers of Venezuela\u2019s state-owned internet provider Cantv are used to interruptions, with whole cities and even states regularly disrupted. Emily Avenda\u00f1o writes about what life is like for the Venezuelans that have gone months, and even years, without being reconnected (El Est\u00edmulo, Spanish).\n\nDo healthy feeds make healthy profits? \u2014 Try as they might to make News Feed content better for our mental well-being, Farhad Manjoo doubts whether Facebook can resist the urge to keep serving the junk that has made the company so profitable (The New York Times).\n\nKilling news, saving news \u2014 While the announcement that Facebook would de-prioritise news content in its feed alarmed some publishers, Joshua Topolsky, founder of The Outline, said it was the best thing to happen to content producers \u2014 because they will stop taking their orders from the platform.\n\nBattle of our age \u2014 After ProPublica reported that older job applicants are suffering as a result of companies targeting online job ads to younger workers, advocacy groups and key senators in the US called on employers and tech companies to halt the practice.\n\nDoes open data fuel monopolies? \u2014 Jeni Tennison, CEO at the Open Data Institute, debunks the argument that we should not open up data because it makes big tech firms more powerful.\n\nHow open data can save AI \u2014 Governments that use AI for service delivery should open up the data that these systems use to improve data quality, provide better services, and increase competition among service providers, argue Web Foundation Policy Director Craig Fagan, and Policy Fellow Juan Ortiz Freuler.\n\nGovernment contracts revealed \u2014 Using data from OpenOpps and the Open Contracting Partnership, Wired looks at UK central and local government contracts with now-defunct construction firm Carillion. TL;DR: We need open contracting data.\n\nThe Web Foundation was mentioned in an op-ed discussing some of the challenges facing the web, which quoted our founder, Sir Tim Berners-Lee (Les Echos, French).\n\nAlexander Ryzhenko, Ukraine\u2019s Head of the State Agency for E-Governance, mentioned the Web Foundation and cited Ukraine\u2019s improved Open Data Barometer ranking (Nachasi, Ukrainian). The Barometer was also mentioned in a piece on the UK\u2019s recognition of Ukraine\u2019s progress (Ukrinform).\n\nThe Open Data Barometer was also mentioned in a post discussing the tools for measuring open data and its impacts, by Morena Ragone (ForumPA, Italian).\n\nAn opinion piece from Bhaskar Chakravorti on the digital gender gap quoted our Women\u2019s Rights Online research (Harvard Business Review Ukraine, Russian)."
    },
    {
        "url": "https://becominghuman.ai/where-to-find-artificial-intelligence-and-machine-learning-profits-in-the-market-11ebcc5637dc",
        "title": "Where to Find Artificial Intelligence and Machine Learning Profits in the Market.",
        "text": "In 2017 we could not get away from the conversation around artificial intelligence. Is it good, is it bad, will it take our jobs, or will it help solve the world\u2019s problems? There are conflicting answers for all of the questions posed, but the reality of artificial intelligence and machine learning is that it is here. The technology that seemed far, far away is here. Our apps, software, smart phones, cameras, cars, and smart speakers, are no longer just reacting, they are learning our behavior and adapting to the way we use them.\n\nWe attempted to explore who has the inside track on artificial intelligence. We looked for a company or companies who we felt would really move the development of AI/ML forward. A company that would also be able to use their AI/ML developments to increase revenue or decrease expenses. We needed to know outside of the phone assistants and speakers where do companies like Apple, Google, Facebook, Twitter, Amazon, Microsoft, Nvidia, Qualcomm, Broadcom, Intel and a host of others stand in the world of artificial intelligence.\n\nHere is a list of 17 companies that we took a look at regarding their involvement with AI/ML technology. These aren\u2019t comprehensive break downs of these companies, just a peak into their windows. You can find out more about what these companies are doing with AI/ML at their websites.\n\n1. Apple (AAPL): Apple has been one of the world\u2019s favorite consumer brands for over a decade. Apple acquired Regaind, a company whose technology can figure out the content of a photo and the photo\u2019s technical and aesthetic value using A.I. Apple\u2019s neural engine in the A11 chip is tuned to accelerate certain artificial intelligence software. Apple has been rumored to be working on self-driving technology then not to be working on self driving technology, so there is no telling where the company is with that venture or if they are even pursuing that venture. It appears that whatever AI developed by Apple will be used to better the iPhone, iPad, Apple Watch, HomePod, and the user experience for Apple product users. That should result in continued sales of Apple products.\n\n2. Amazon (AMZN): Comprehend, SageMaker, Rekognition, Translate, Transcribe, Lex, are all machine learning products offered by Amazon to its Amazon Web Services clients. Sagemaker for example allows developers and data scientist to build, train, and deploy ML models at any scale. Comprehend uses ML to find insights and relationships in text. Amazon is using it\u2019s AI/ML platform to gain new customers for it\u2019s AWS service, which has become a big part of Amazon\u2019s business.\n\n3. Baidu (BIDU): Baidu who is seen as the Google of China is all in on artificial intelligence. Baidu\u2019s Apollo, which is described as the Android of the autonomous driving industry is a way for the company to partner with other companies interested in developing autonomous driving technology. Some of Baidu\u2019s partners in Apollo include Chinese auto companies Chery, Changan and Great Wall Motors, as well as Bosch, Continental, Nvidia, Microsoft Cloud, Velodyne, TomTom, UCAR and Grab Taxi.\n\n4. Broadcom (AVGO): Broadcomm\u2019s $130 billion bid for Qualcomm is seen by many as a $130 billion buy in to the artificial intelligence conversation. See Qualcomm below.\n\n5. Facebook (FB): Facebook made news when it announced it had discovered two artificially intelligent programs talking to each other in their own language. The company didn\u2019t release what the programs we\u2019re talking about (Skynet theorist insert comment here). Facebook has committed to AI by opening three AI labs around the world. It has also used the acquisition route to acquire AI technology. Facebook acquired Ozlo, with the plan that Ozlo\u2019s AI tech can make messenger smarter with a virtual assistant. Other AI firms acquired by Facebook are Masquerade Technologies, Zurich Eye, Fayteq AG, and FacioMetrics. Facebook has already implemented AI to it\u2019s platform to enhance the user experience on the platform. With what they\u2019ve committed to AI so far I expect there is more to come from Facebook.\n\n6. General Electric (GE): Anyone paying attention to the equity markets or business news knows G.E. has a lot of problems to deal with, but that hasn\u2019t stop the company from understanding the importance of AI/ML technology. Late last year G.E., specifically G.E. Healthcare announced a partnership with Nvidia to bring artificial intelligence to it\u2019s 500,000 image devices. In 2011 G.E. acquired the company Smart Signal to provide supervised learning models for remote diagnostics and in 2016 G.E. acquired Wise.io for the firms unsupervised deep learning capabilities.\n\n7. Google (GOOGL): Google is our top bet to win the artificial intelligence / machine learning race. Waymo, Google\u2019s autonomous car initiative appears to be close to a fully autonomous vehicle. In late 2017 Waymo started sending out it\u2019s autonomous vehicles without the safety driver. Analyst beleive that Waymo is close to bringing a commercial product to market. Then there is DeepMind, the London based AI company Google acquired in 2014 and the creator of AlphaGo. DeepMind has signed partnerships with hospitals to allow their AI technology to assist medical professionals. The rave reviews received by Google\u2019s Pixel 2 phone was partly due to the AI/ML camera management system, which allows the phone to take amazing photos. There is also Tensor Flow, developed by the Google Brain Team. Here is an interesting read on how machine learning helped a cucumber farmer. There is a lot going on at Google to be excited about when it comes to AI/ML.\n\n8. Intel (INTC): Intel and Mobileye\u2019s EyeQ4 system is now in over 2 million vehicles. EyeQ4 is the company\u2019s mapping solution for its autonomous vehicle initiative. The technology can be found in BMW, VW, Nissan and other vehicles. Intel says that it has Level 2+ and Level 3 design contracts in place with 11 carmakers so far.\n\n9. Micron (MU): Micron Technology might not be a name you think of when it comes to artificial intelligence, but they have been a big part of AI development. Micron has found its place in the AI race by providing memory chips needed for AI development. Nvidia, who is involved in the development of the microchips for autonomous vehicles and AI/ML tech for G.E. has been using Micron\u2019s memory chips.\n\n10. iFlyTek: iFlyTek is a Chinese AI company that applies deep learning to speech recognition, natural-language processing, machine translation, and data mining. iFlyTek has its AI technology in cell phones, driver assistants, and in hospitals. According this MIT Technology Review, over 500 million people in China are talking to iFlyTek technology.\n\n11. Microsoft (MSFT): It would be hard to imagine the software king of the \u201990s not getting involved in artificial intelligence. The company has more than just Cortana going on. It has a platform for companies to build their own AI. Microsoft\u2019s AI computer recently made news when it posted a slightly better score than a human on a reading comprehension test. The company is already applying AI technology to its products, you can find a list of products for consumers and businesses here.\n\n12. Nvidia (NVDA): Nvidia is another one of the companies that is getting into the self driving car business. Nvidia was picked by Uber to provide an AI computing system for its fleet of self driving vehicles. Nvidia\u2019s Deep Learning AI is being applied across many industries. Known for their high-end graphic processing units used heavily in PC gaming, Nvidia\u2019s commitment to AI could really pay off for the company in the future.\n\n13. NXP Semiconductors (NXPI): NXP is the largest global automotive semiconductor supplier. They have recently joined Baidu\u2019s Apollo program, where it will offer semiconductor products and solutions.\n\n14. Qualcomm (QLCM): According to Qualcomm\u2019s Senior Vice President of Product Management Keith Kressin \u201cQualcomm has been conducting fundamental research in A.I. for over a decade.\u201d He also states that many of the devices shipping today using Qualcomm\u2019s Snapdragon mobile platform already utilizes on-device AI. The company developed its own reference designs for computer vision based depth sensing in phones equipped with Qualcomm\u2019s Snapdragon mobile processors. Qualcomm was recently given permission by the state of California to test a self driving car. Qualcomm will be using its 9150 C-V2X chipset in the car it test. The company also notes AR & VR as development areas they are working on.\n\n15. Salesforce (CRM): Salesforce put it\u2019s Einstein AI technology to work in 2016. The Einstein AI uses machine learning to forecast sales, pick out the most important emails of a staffers inbox, and suggest a response. The Saleforce website describes Einstein as \u201ca layer of artificial intelligence that delivers predictions and recommendations based on your unique business processes and customer data.\u201d\n\n16. SenseTime: SenseTime is China\u2019s largest artificial intelligence company. Valued at over $1.5 billion SenseTime has found commercial success in the fields of finance, mobile internet, smart phones, and security. SenseTime has partnered with companies like Qualcomm, NVIDIA, China Mobile, UnionPay, Huawei, Xiaomi. SenseTime recently received an undisclosed investment from Qualcomm.\n\n17. Twitter: Twitter has applied deep learning technology to it\u2019s platform in an effort to recommend certain tweets to a user. In 2016 Twitter acquired London based AI firm Magic Pony. Magic Pony\u2019s neural networks and machine learning are able to provide expanded data for images. Twitter also acquired AI startup firms Whetlab and Madbits.\n\nThese aren\u2019t the only 17 companies working on AI/ML technology, just the ones we\u2019ve isolated so far for a possible investment. From our list we learned that companies aren\u2019t going it alone on AI/ML, there is a lot of collaboration going on between companies that many would consider to be rivals. A company like Nvidia comes to mind as it has partnerships with four of the companies on this list. We also learned bigger companies are sparing no effort when it comes to AI/ML development and acquisition. The number of acquisitions of small firms that specialize in AI/ML should continue into the future. AI/ML represents a market with massive potential, companies who are on the fence about AI/ML will be left behind by those who are already adapting the technology in what they do.\n\nDo you know of an AI/ML investment play that didn\u2019t make this list? Let us know here.\n\nIf you want to stay up to date on what the Seville Report is doing leave us your email at sevillereport.com"
    },
    {
        "url": "https://becominghuman.ai/understanding-tensorflow-source-code-rnn-cells-55464036fc07",
        "title": "Understanding TensorFlow Source Code: RNN Cells \u2013",
        "text": "You can think of this as a guided reading of the first half of the code in rnn_cell_impl.py, where some RNN, LSTM, and GRU cells are defined. I\u2019m minimizing the code here in the hopes that you\u2019ll follow along with the actual source. I hope that you\u2019ll find the TensorFlow source code even more approachable.\n\nis a special method on Python objects; it turns them into objects. objects, like functions or s, can be called with parentheses.\n\nhas the same result as this\n\nis a special function that allows you to have computed object properties. For example:\n\nis pretty much the same thing as\n\nThis lets you make computed object properties. For example, I could have a property that goes and fetches data from the database when it\u2019s accessed. It\u2019s like a \u201cgetter\u201d in Java or C#, but more pythonic.\n\nThe goal of is to provide some popular RNN cells and an easy way for people to create their own cells.\n\nAn RNN cell is basically a function that takes in an input and a state, returning a tuple of the output and the next state. In programming, this is very similar to a reducer function. You can chain a bunch of these cells together, sort of like in this pseudocode:\n\nThere\u2019s an unenforced requirement that shapes of the current state and next state must be the same. This restriction allows us to chain like-cells together because the next unit in the chain can always use the output state from the previous unit.\n\nIt\u2019s up to the consumers of RNN cells to take advantage of their \u201crecurrent\u201d nature. The cells are each only one link. So naturally, the implementations of RNN cells aren\u2019t necessarily recurrent themselves.\n\nThis is an abstract class that helps people implement their own implementations. Instructions of how to create a concrete are pretty clear:\n\nThose \u201cproperties below\u201d being and .\n\nPretty simple. You can see a few implementations further down in .\n\nThis is the most basic a recurrent network can be. It\u2019s a simple implementation of . At its core is a linear model followed by an activation function.\n\nHere\u2019s that in action, excerpted from :\n\nis the input to an RNN cell. is the output of the previous RNN cell. is just a linear combination of and , then passed through the activation function \u2014 so it\u2019s basically like any fully connected layer, except with two inputs.\n\nWhy return a tuple of ? We\u2019re returning here because RNNs are expected to return a tuple of when called. In , our output is the same thing as our next state; this network doesn\u2019t discriminate between the two. This isn\u2019t the same for every cell, but it ensures that the works in the same places as other cells which have different outputs and states.\n\nI\u2019m on the job hunt! I have broad experience in machine learning, functional programming, music, and entrepreneurship. I\u2019m looking for a place to apply my skills preferably in Los Angeles or other non-bay-area locations."
    },
    {
        "url": "https://becominghuman.ai/reflection-on-the-first-day-of-class-6b46064fde47",
        "title": "Reflection on the First Day of Class \u2013",
        "text": "Hey everyone, welcome to my blog! This is my first post, hopefully you like it! Please comment and share if you\u2019d like!\n\nI\u2019m a PhD student at the University of Kansas, taking a course on Human Robot Interaction (HRI). The goal of the course is to nourish our curiosities in social robots. It also aims to give us knowledge and ability to design some of our own socially intelligent systems. I\u2019m personally excited about the latter. Before I get into why I\u2019m excited about building a socially intelligent robot, it might be good to explain a bit about what I mean by socially intelligent robots, from an HRI perspective.\n\nFrom what I learned in class, HRI investigates how robots can interact with humans in natural ways. By \u201cnatural\u201d I mean natural for humans. We want interactions with these robots, or embodied intelligent agents, to be similar to the way we interact with people. To achieve this, social robotics draws inspiration from cognitive science, robotics, artificial intelligence, psychology, linguistics, natural language understanding, planning, and much more. Despite its multidisciplinary nature, people generally regard it as a subfield of artificial intelligence.\n\nSocial robots are intended to permeate every sector of our lives. The field as a whole is young, but researchers and practitioners envision robots that exist in homes, hospitals and in schools. Robots will aide humans in their personal lives; helping us with relationships, family, and time management. Many of us have yet to see such systems realized before our eyes, although we\u2019ve been exposed to early versions of technologies that are all but certain to draw us closer to these seemingly inevitable ends. For instance, many of us own, or know of personal home assistants like Google Home and the Amazon Echo. Our smart phones and computers have assistants built into them as well. Lastly, driverless cars are continually edging closer to their mainstream debut. All these technologies are on the treadmill of innovation.\n\nBesides private sector driving interactive, intelligent systems into the mainstream, the government plays an arguably more influential role. The industry typically, even in research divisions, invests its resources on products that can go to market and generate revenue in 2, 3, 4, 5 years. Horizons longer than this are typically avoided because the time between investing resources and receiving dividends from that investment is prohibitive. In the United States, the government is willing to invest in research with time horizons of 20, 30, 40 years or more. I have yet to hear of any company invest time, people and money on a project that may or may not be successful 40 years into the future. The government has driven much of the basic science that makes these technologies available to us today. Not only that, they have hosted competitions that challenge industry as well as academia to advance the cutting edge. The DARPA Robotics Challenge is one example. The first unmanned car to drive through rural and urban areas was funded by DARPA, and because of this, humanity waits expectantly for the driverless car of its dreams. The government was the only actor capable of investing that level of commitment to basic science research.\n\nSome of you are correctly responding with, \u2018Universities fund basic science research too\u2019. To that point, I will just say, much of the research funding available to universities pursuing long-term research projects comes from the federal government. For example, my research has been funded by the federal government. I work in cognitive systems which is a subfield of artificial intelligence (AI). What I love about cognitive systems is that we aim to model all the facets of human cognition on a computer. In my field, we believe in looking at the mind in a holistic way. When we talk about AI, we\u2019re talking about information, and how information is processed in order to give rise to intelligent behavior. My line of work greatly affects human robot interaction, because if the AI thinks like you, it is possible to make it truly act and respond like you. This is very different than what goes on in other subfields of AI, like machine learning. In that field, there exist systems whose decision making processes are governed by opaque statistical models that do not resemble the familiar patterns of the human intellect. Once these marginally interpretable systems are conceived, they are made to appear human-like in performance. Most current, mainstream AI technologies are like this, and they have been sufficiently successful in catching our collective conscience. Unfortunately, they cannot do more than what they were explicitly designed for: tell Microsoft\u2019s Cortana that you had a great day, and she doesn\u2019t understand. Ask Google\u2019s AlphaGo to teach you how to play Go, and the system cannot do it, even though it is the world\u2019s best player.\n\nCognitive systems research can truly bring about intelligent agents that understand the semantics of language; that know about knowing; that interact with humans in natural ways; and much, much more. That\u2019s why I\u2019m excited about my field and about building my own social robot. I can expand the boundaries of social robotics. I can help these systems actually remember who you are. They will be able to remember previous conversations you\u2019ve had with them. They can remember your preferences, and help you learn new skills. They will be able to carry out complex, multi-step tasks. They will be able to learn, and do more, as their internal decision making machinery becomes more advanced. That is the beauty of cognitive systems, and cognitive systems applied to social robotics."
    },
    {
        "url": "https://becominghuman.ai/nevernude-automatically-cut-out-nsfw-nudity-from-videos-using-machine-box-ffmpeg-b64a89c1ebc8",
        "title": "Nevernude: Automatically cut out NSFW nudity from videos using Machine Box + ffmpeg",
        "text": "Mixing Videobox+Nudebox allows you to get an idea of where nudity occurs in a video. Using that information, it\u2019s possible to use to create a new video with the nude bits cut out.\n\nThis could be used to create a family-friendly version of a movie, or a version of the video that\u2019s suitable for cultures where nudity is less socially acceptable.\n\nWe have to do four simple steps in order to achieve our goal:\n\nThe easiest way to spin up Videobox and Nudebox on your local machine is to use Docker compose (learn more on Docker\u2019s website).\n\nCreate a folder called , and insert the following file:\n\nIn a terminal, run which will spin up two Docker containers, one for Nudebox and one for Videobox.\n\nIn this article we are going to look at a solution using Go, but you can use any language you like (or even a bash script) \u2014 after all, Machine Box provides simple RESTful JSON APIs that are easy to consume in any language.\n\nWe are going to use the Machine Box Go SDK to help us make requests (but they\u2019re just HTTP requests, so you can use if you like).\n\nCreate a Videobox client and use it to process the source video file:\n\nThe function periodically checks the status of the video (with the specified ) before getting and returning the results:\n\nOnce the function returns, we\u2019ll have the object which contains the nudity information.\n\nNext we need to use the nudity instances to create a list of time ranges that do not contain nudity.\n\nFor example, if a ten second video contains nudity from 4s\u20137s, we would expect two non-nude segments, 0s\u20133s and 8s-10s \u2014 thus omitting the nude bits.\n\nIn , we can use the following command to extract segments based on these ranges:\n\nis the number of seconds to seek in the original video (the start of the segment), and is the length of the segment in seconds. is the filename of the segment to create.\n\nWe\u2019ll also create a text file that ffmpeg understands that lists each segment \u2014 which we\u2019ll use later to stitch them back together. The file will follow this format:\n\nSince we don\u2019t know how many segments there are going to be, we\u2019ll use code to generate the ffmpeg arguments and segment list file.\n\nThe variable is a string of arguments that we can pass into the command when we execute it.\n\nFinally, we\u2019ll use from Go\u2019s standard library to execute ffmpeg:\n\nIf we ran this program, we\u2019d end up with a folder that contained the segments and the list file:\n\nFinally, we must use ffmpeg to stitch all of the segments listed in our file into a new video:\n\nSo in Go code this would be:\n\nRunning this program will take a source video, and create a new one with the nudity omitted.\n\nTo make this solution more robust, you might decide to add a little bit of time either side of detected nudity, just to be sure to cut all of it out. That can be your homework.\n\nVideobox lets you control the threshold (Nudebox confidence value) before frames are considered to contain nudity. Just pass the value (a number between 0 and 1, where zero is the most strict) or use the NudeboxThreshold option in the Go SDK.\n\nWe saw how easy it was to mashup Machine Box and ffmpeg to process a video to remove sections of detected nudity.\n\nHead over to the nevernude project on Github to see the complete code, and even run it on your own videos."
    },
    {
        "url": "https://becominghuman.ai/nukes-vs-cyber-sound-proportional-6b58a5f4da1",
        "title": "Nukes vs. Cyber, sound Proportional? \u2013",
        "text": "An article with this title \u201cPentagon Suggests Countering Devastating Cyberattacks With Nuclear Arms\u201d appeared in the New York Times on Jan. 16. It simply overwhelmed me with it\u2019s insanity. My immediate thoughts were of Stanley Kubrick\u2019s 1964 movie \u201cDr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb\u201d, starring Peter Sellers. Having grown-up in the 1960\u2019s I can tell you nobody \u201clearned to love the bomb\u201d.\n\nDuring the 60\u2019s all schools taught us the famed \u201cDuck and Cover\u201d position practiced during nuclear bomb drills in the likelihood of a nuclear bomb attack from the Soviet Union.\n\nThe contrast of Kubrick\u2019s crazed Dr. Strangelove and children being taught to \u201csurvive\u201d a nuclear attack pretty much sums up the insanity of the time, along with the following 60\u2019s avant-garde advertisement for supplies needed in the \u201cwell-stocked\u201d fall-out shelter. Never let it be said a silly little nuclear attack couldn\u2019t be used as an opportunity for enterprising marketing \u201cexecutives\u201d to sell something.\n\nOf course we were never shown pictures like the following carnage from the nuclear attack on Hiroshima which occurred a mere fifteen years before our duck and cover lessons. That\u2019s when the Enola Gay dropped a 16-kiloton hydrogen bomb, the \u201cLittle Boy\u201d, at 8:15 am over Hiroshima on August 6, 1945, killing 140,000 people, more than half of whom died in an instant.\n\n\u201cLittle Boy\u201d exploded about 550 feet from it\u2019s target at approximately 1900 feet above the ground. Further below is a picture of carnage in Nagasaki, attacked on same day. Doesn\u2019t look like too many desks, let alone terrified children survived either attack.\n\nDr. Strangelove was a dark comedy/satire on the theory of Mutual Assured Destruction (MAD), in which each side of a nuclear conflict is supposed to be deterred from using nuclear weapons by the prospect of a world-wide, cataclysmic disaster regardless who might \u201csurvive\u201d.\n\nPhysicist and military strategist, Herman Kahn in the book, On Thermonuclear War (1960) used the theoretical example of a doomsday machine to illustrate the limitations of MAD. Kahn was a founder of the Hudson Institute and came to prominence as a military strategist while employed at the RAND Corporation. He became known for analyzing the likely consequences of nuclear war and recommending ways to improve survivability, making him one of three historical inspirations for the title character of Kubrick\u2019s \u2018s now classic film.\n\nKahn eventually came to worry that the military might like the idea of a doomsday machine and build one. He became a leading critic of MAD and the Eisenhower administration\u2019s doctrine of massive retaliation upon the slightest provocation by the USSR. Kahn came to consider MAD foolish bravado. Instead he urged America to plan for proportionality and thus, even a limited nuclear war. Kahn became one of the architects of the flexible response doctrine, that while superficially resembling MAD, allowed for responding to a limited nuclear strike, with a proportional or calibrated, return of fire.\n\nIt now appears military planners in the Pentagon have completely disregarded the entire concept of \u201cproportionality\u201d if they think the use of nuclear weapons is proportional to a \u201ccyber attack\u201d. Proportionality is a general principle in law intended to assist in discerning the correct balance between the restriction imposed by a corrective measure and the severity of the nature of the prohibited act. All of these ideas are also applied to international conflicts. Even if one were to imagine disabled ATM\u2019s on every street corner through-out the United States, it\u2019s not close to the images of carnage like that seen in Hiroshima and Nagasaki.\n\nAndrew C. Weber, an assistant defense secretary during the Obama administration, who directed an interagency panel that oversaw the country\u2019s nuclear arsenal, said the new policy \u201cwill make nuclear war a lot more likely.\u201d\n\nPerhaps pictures of Hiroshima and Nagasaki, like those above, should fill the walls of the Pentagon. Better yet, maybe we should ask the Japanese government if we could move the Pentagon here so Pentagon war planners can hear the chimes of the bell sounded for all who perished.\n\nThis is a video of the commemoration of the 70th anneversary of the atomic bombing of Hiroshima at the Memorial Cenotaph, in the Hiroshima Peace Park.\n\nIn the nearby river thousands of men, women and children, sought sanctuary from the heat and flames of the nuclear blast. A saddle-shaped monument covers the cenotaph holding the names of all of the people killed by the bomb with the epitaph: \u201cRest in Peace, for the error shall not be repeated.\u201d Let\u2019s all say a prayer it won\u2019t be."
    },
    {
        "url": "https://becominghuman.ai/only-numpy-implementing-adding-gradient-noise-improves-learning-for-very-deep-networks-with-adf23067f9f1",
        "title": "Only Numpy: Implementing \u201cADDING GRADIENT NOISE IMPROVES LEARNING FOR VERY DEEP NETWORKS\u201d from\u2026",
        "text": "So the main contribution of that paper is rather using normal gradient descent, we are going to add a Gaussian Noise to every gradient with \n\nMean Value of 0 and certain Standard Deviation Value. How and where do we calculate this STD? It is shown below.\n\nSo the \u03b7 value is selected from 3 set of numbers, \u03b3 = 0.55 and finally variable t stands for each training time step."
    },
    {
        "url": "https://becominghuman.ai/time-to-get-smart-about-artificial-intelligence-predictions-3b7ddf5fce2",
        "title": "Time to Get Smart About Artificial Intelligence Predictions",
        "text": "Tips to avoid getting played for a fool by AI hucksters\n\nThis week, Gartner Inc., a consulting firm headquartered in Stamford, Connecticut, has made a bold prediction: \u201cBy 2022, personal devices will know more about an individual\u2019s emotional state than his or her own family.\u201d\n\nThis prediction, if true, should change the way that large corporations think about marketing.\n\nIs it true, though? How can we tell?\n\nGartner claims that its predictions are based on \u201cobjective insights\u201d, but by definition, we can have no objective insights about the future until the future becomes the present.\n\nThe only truly objective way to judge whether Gartner\u2019s prediction about the ability of personal devices to know more about our emotions than our families do is to wait until January 1, 2022 and then conduct a comparative measurement of artificial intelligence knowledge of human emotional states against humans\u2019 own emotional knowledge. Of course, Gartner, Inc. doesn\u2019t want prospective clients to wait four years before buying Gartner\u2019s supposed knowledge about the future of artificial intelligence. They want you to buy now.\n\nBuying now is tantamount to buying sight unseen, because an objective measurement of precisely how much human beings are loved by their family does not exist.\n\nWhen Gartner predicts that \u201cBy 2022, personal devices will know more about an individual\u2019s emotional state than his or her own family,\u201d the firm is making two quantitative claims: 1) That AI knowledge of human emotional states will be quantitatively more substantial than such knowledge by humans; and 2) That AI knowledge of human emotional states will eclipse such knowledge by humans by January, 2022, not by 2023, or even by February, 2022.\n\nThere is no method for gathering either of these quantitative measurements. So, why is Gartner claiming that it can make these measurements?\n\nIt\u2019s an old sales trick. Using numbers makes people seem more confident, and more reliable. The effect is mere glamour, of course. People who speak with numbers aren\u2019t necessarily more reliable in practice. There\u2019s great danger in this distinction.\n\nEmotion is not an objective state. It\u2019s a subjective experience. It can\u2019t be understood by those who have never experienced it.\n\nTo feel angry is not the same thing as to flare one\u2019s nostrils while using a loud voice with a tight mouth and narrowed eyes. To flare one\u2019s nostrils while using a loud voice with a tight mouth and narrowed eyes is merely to appear angry (and only the way in which some people appear angry). When it comes to emotion, appearance is not at all the same as the actual thing. To know what emotion is, you have to feel it yourself.\n\nThere are professionals who specialize in researching emotions in consumers and in business settings. These professionals are good at what they do because they\u2019re human beings with extensive experience working with emotion through direct, immersive interactions with the people they study.\n\nThe one thing that professionals who study emotion in commercial settings won\u2019t do is inappropriately quantify the emotions they study. Gartner claims to know that, by January 2022, artificial intelligence will know more about human emotions than our families do now. The fact is that nobody has the ability to reasonably quantify the amount that human beings know about the emotions of their family members. Any quantification of emotional knowledge is merely a simplified representation of a small portion of the true emotional knowledge we possess.\n\nEmotion is inherently mysterious. If what you\u2019re researching isn\u2019t mysterious, it isn\u2019t emotional. So, the more certain research firms claim to be in their ability to measure emotion with quantitative precision, the more incompetent they are likely to actually be at accomplishing the task \u2014 because they have lost touch with what emotion actually is.\n\nThe flaws in the yarns spun by Gartner, Inc., and other firms that claim to know about the future of artificial intelligence\u2019s grasp of emotion, isn\u2019t merely theoretical. They have serious implications for your ability to stay in business.\n\nThere are, roughly speaking, two kinds of business: Those that profit through the sale of commodities, and those that profit through the sale of items that are perceived as having special value. Businesses that trade in commodities don\u2019t have much need for understanding the emotions of their customers, and won\u2019t profit much from predictive emotional analytics. Businesses that trade in items perceived as having special value, however, depend on building effective emotional interactions with their customers.\n\nFirms that claim to be able to use artificial intelligence to assess and predict customers\u2019 emotional states are trying to convince their clients that genuine human-to-human emotional interactions can be replaced by fake bot-to-human replicas of emotional interactions, powered by AI. The fact is that there are currently no bot-to-human interactions that can effectively replicate the full emotional experience of genuine human-to-human emotional interactions.\n\nThat\u2019s not a quantitative assessment. It\u2019s a qualitative one. There does not exist any artificial intelligence that even claims to be able to come close to replicating an authentic human-to-human emotional interaction. For one thing, no artificial intelligence system exists that can feel emotion. Artificial intelligence is qualitatively distinct from human experience, and emotion is just one aspect of that distinction.\n\nCompanies that replace genuine human-to-human emotional interactions with bot-to-human interactions become distant from their customers. The normal, taken-for-granted, social routes through which these companies gain familiarity with and trust among customers break down. Avoidable, catastrophic strategic mistakes result.\n\nPeople in business need to move beyond the deer-in-the-headlights stage of initial fascination with tales told about artificial intelligence. We need to move forward to adopt a more mature, critical evaluation of AI sales pitches. The following 9 tips are a start in the development of this more savvy perspective."
    },
    {
        "url": "https://becominghuman.ai/will-ai-make-your-company-better-982482592173",
        "title": "Will AI Make Your Company Better? \u2013",
        "text": "Myths about artificial intelligence (AI) have made many professionals fearful about how their roles may be impacted. However, this is not an entirely new phenomena. Major shifts in productivity have occurred in the past, usually coinciding with industrial revolutions, and indeed these shifts did transform entire professions. Eventually, AI may affect almost every occupation, but so did the computers. People were also afraid of how their careers might change when computers started to become more commonplace. Now, most people work with digital technology comfortably and productively. On top of that, we have seen new industries formed while others have been greatly enhanced.\n\nA look at the potential for AI technology reveals an exciting new world where jobs become less mundane and new business opportunities emerge.\n\nAI is a field of computer science and was originally aiming to create a machine capable of mimicking human intelligence. There are multiple approaches for creating AI, including really well scripted procedures, which too may seem like a human controlled process from outside. Nowadays, the majority of these smart systems are based on Machine Learning (ML). Using approaches which copy from how human brains function, technologies created systems using things like Neural Networks in order to perform functions once reserved for humans. The list of what AI can do is constantly growing and includes tasks like recognizing speech, identifying visual elements, making choices, and translating languages.\n\nThere are a few definitions in this field which are important to understand. Artificial Narrow Intelligence (ANI) focuses on one specific task, for example, interacting with a customer, like what chatbots do. ANI functions are created to take care of specific problems and can be tailored to unique company needs.\n\nArtificial General Intelligence (AGI) is a hypothetical system that can perform many different types of tasks, moving seamlessly between functions and serving as a centralized place where multiple functions are performed. A system with this level of intelligence would easily be able to pass as an actual human being. However, AGI does not exist yet, and likely will not for the next few decades.\n\nBeyond AGI, scientists have imagined an Artificial Superintelligence (ASI), which may exceed human intelligence to the point that our finest thinkers wouldn\u2019t be able to keep up. ASI will encompass business and scientific innovation, as well as social skills, and even wisdom. Some have assessed that it may be as long as a century before something like this will exist.\n\nWhile the superintelligence everyone has seen in the movies is a long way from reality, today, AI is actively revolutionizing existing business processes.\n\nChatbots are a good example of this recent inflow of AI based technology. Companies have always provided customer service representatives to handle calls, complaints, and orders. Those representatives may have also been chatbots, based on pre-generated scripts. The new chatbots with AI is capable of a lot more, like recording customer tastes, tracking previous conversations, and adjusting responses automatically based on hundreds of parameters. This would have been impossible to achieve using old technology or human equivalent. Surveys suggest that many people who call help lines are starting to prefer chatbots because of their efficiency.\n\nAnother area which is being revitalized by AI is supply chain management. Managers used to spend long hours planning and monitoring the shipments and storage of raw materials, work-in-progress and finished goods. AI can now manage the entire supply chain by analyzing levels for materials, work, and statuses of final products constantly. AI can track not only current inventory, but also external factors, like current market conditions, weather, and other factors which affect buying behavior. It can then automatically submit purchase orders only for those products which are forecasted to be in higher demand. The new inventory management systems enhanced by AI are able to maintain optimum levels of inventory much better than humans or algorithm based systems, further eliminating the cost of excess items on warehouse shelves.\n\nPaperwork bogs down many businesses. Busy executives do not have time to read every memo, report, plan, and guide. This effort to keep up with the amount of written text has long been a drag on productivity for professionals. Optical Character Recognition (OCR) can help digitize printed material, but still requires a human eye to review the content in most cases. Adding AI into this process, allows a document management system to continuously learn about what it stores. This can significantly simplify the process of configuring such system and setup of fixed workflows. Instead, an end user can perform a freehand search on all content stored in the system without worrying if documents have been properly categorized.\n\nA rapidly growing field where AI is starting to show significant improvement is Marketing, Lead Generation, and Business Development. Some customers produce more revenue for a business than others. However, the view from the corner office may obscure who these customers are. Busy professionals often focus on overall sales numbers rather than evaluating individual clients for their earning potential. AI enhanced prospecting or lead tracking tool can identify more lucrative customers on ongoing basis and notify relevant account managers about potential revenues. Constantly staying in touch with customers and clients has always challenged companies. Time on customer communications could have been time spent new clients acquisition and prospecting. Automated consumer journey and sales funnel systems based on AI augmentation can now send out pertinent communications, while automatically selecting the right channels and timing in order to achieve the best response rates. These systems can also automatically check to see if retail customers received their orders and resolved any outstanding issues.\n\nThese are only a few areas, where AI enhanced technology created, much more productive and efficient processes. Most of the time, this technology, enhanced already existing roles. Sales forecasting used to vacillate between wishful thinking and hard-headed evaluations of past sales figures. AI based sales forecasting tools gather insights from data both inside and outside of a business, creating more accurate projections of company revenues. Executives can now focus more on improving sales rather than understanding projections. Since payroll typically ranks as the biggest expense for a company, executives have often had to add employees to ramp up, cut employees during slow periods, and hire temporary help to meet short deadlines. Tools with predictive intelligence can now determine the most efficient workforce size and staffing needs on an ongoing basis, eliminating the \u201ccrisis management\u201d approach to hiring.\n\nAreas like accounting, bookkeeping, communications, infrastructure, security, fraud prevention, and many others are now gaining significant benefits from AI based automation. All without significantly changing or displacing the workforce.\n\nAI has the potential boost the bottom line in many ways, through increased productivity, and by creating new business models.\n\nAI makes existing processes more efficient. Don\u2019t think in terms of replacing employees; think in terms of making them more productive. Many manual and repetitive internal functions can become more efficient when they are augmented by technology. Customer service automation mentioned above isn\u2019t about replacing a human touch, but more about providing them with intelligence to do their jobs better, reducing the amount of non-productive time spent looking for additional information and connecting all the dots. Similarly, areas like accounting will have fewer errors, if any, and networks will experience less downtime resulting from misconfigurations or outside attacks. Resource allocation can be maintained at optimum levels throughout seasonal dips and peaks, and payroll hours can be predicted even more accurately.\n\nAI can also create new revenue streams. Technology like chatbots can create new sales channels, allowing companies to reach customers in a way they previously couldn\u2019t. With better insights and analytics, executives can spend less time on managing existing processes, and instead focus more on exploring new ideas and products. AI can also help identify new market trends, giving company time to adjust their marketing, sales, and product development to take advantage of the emerging niches.\n\nPeople have feared their regular will be replaced since the first tales of smart machines and electronic brains. AI merely serves as a tool to solve existing problems. Business improvements from AI have yet to be fully imagined. The most successful companies will be those who stay on top of this changing technology and adapt it quickly. Those that identify problems and apply AI will do the best, whereas those who adopt AI just to \u201ckeep up\u201d will only create confusion in their organizations."
    },
    {
        "url": "https://becominghuman.ai/game-of-thrones-ai-and-family-legacy-941540878470",
        "title": "Game of Thrones, AI, and Family Legacy \u2013",
        "text": "My wife and I just binge-watched all available seasons of Game of Thrones. One thing that struck me was the show\u2019s deep theme of Legacy. House Stark honored their banner and sigil \u2014 the Wolf \u2014 with the same fierceness that House Lannister honored theirs \u2014 the Lion. I was fascinated by the universal, gritty focus on preservation of house, tradition, and family.\n\nOnce my nerd brain detected this theme, I pulled it into my mental workshop to size it up, rotate it in brain-space, and experimentally plug it into other ideas I\u2019d been tinkering with. I\u2019ve also been steeped in research about Artificial Intelligence for the past year, so I had to see if there were any interesting combinations. This led me to ponder an intriguing chimera: The Family AI.\n\nAI algorithms and skills are growing increasingly powerful because of enhancements in hardware and software, but even more importantly, because of the availability of data. AI\u2019s are literally built out of the data on which they are trained, so in a way, this data is akin to their DNA, their family line. For example, a natural language chat bot trained on Twitter data will be good at colloquial and slang. But if the same bot architecture and algorithms are trained on a corpus of scientific journals, they will speak like the cast of The Big Bang Theory and have low marks on colloquialism.\n\nWe are a long way off from anything like consciousness or General Artificial Intelligence, but big tech companies like Google, Amazon, and Facebook are stockpiling huge troves of data and using them to train fleets of task-specific AI\u2019s. As these individual algorithms begin to interconnect and overlap, it is conceivable that themes and \u201cpersonalities\u201d driven by the underlying data will begin to emerge.\n\nOn a collision course with these giant battleships of data are growing concerns from the general population. Whispers about consumer protection, data security, and over-concentrations of power are starting to pop up in the newsroom zeitgeist. Later this year for example, the European Union is instituting General Data Protection Regulation (GDPR), the most sweeping regulation of data privacy for the past two decades. If data is the new oil, then we are likely to see some guardrails go up around where one can drill.\n\nMy thought experiment is: What if all these conditions result in people seizing control of their precious data? Protecting it and brokering it for themselves with only the people and institutions that they trust. What if this motivates an uptick in the importance of family connections (which would really just be a reversion to the historical mean)? What if our valuable data and insights compound within family-based AI algorithms as they are passed down from generation to generation?\n\nJust for fun, here are a few hypothetical logs from my Family AI over the next 70 years:\n\nDave Costenaro is Head of Artificial Intelligence Design at AiSoftware.com and Executive Director at Prepare.Ai. Don\u2019t miss the Prepare.Ai Conference on May 8, 2018 in St. Louis."
    },
    {
        "url": "https://becominghuman.ai/only-numpy-deriving-partial-forward-feed-lstm-on-show-attend-and-tell-neural-image-caption-4e44aa2b966d",
        "title": "Only Numpy: Deriving partial Forward Feed (LSTM) on Show, Attend and Tell: Neural Image Caption\u2026",
        "text": "I have numbered each steps to be performed in the 2 Time Stamp LSTM network.\n\nOperation 1 \u2192 Image is given to the CNN and feature vector a are produced\n\nOperation 2 \u2192 Calculate the positive weight when time is 1 [Bahdanau]\n\nOperation 3 \u2192 Calculate the context vector Z(1) [Soft Attention]\n\nOperation 4 \u2192 Calculate the E(0) using the Embedding Matrix\n\nOperation 5 \u2192 Perform the Standard Forward Feed operation on LSTM\n\nOperation 6 \u2192 Calculate the positive weight when time is 2 [Bahdanau]\n\nOperation 7 \u2192 Calculate the context vector Z(2) [Soft Attention]\n\nOperation 8 \u2192 Calculate the E(1) using the Embedding Matrix\n\nOperation 9 \u2192 Perform the Standard Forward Feed operation on LSTM\n\nNow the two question that I have to answer is what is positive weight and context vector. How can we calculate them?"
    },
    {
        "url": "https://becominghuman.ai/tensorflow-object-detection-api-basics-of-detection-2-2-28b348495eec",
        "title": "TensorFlow Object Detection API: basics of detection (2/2)",
        "text": "My first (at all!) post was devoted to 2 basic questions of training detection models using TensorFlow Object Detection API: how are negative examples mined and how the loss for training is chosen. This time I\u2019d like to cover 3 more questions regarding the following:\n\nAs before, I totally recommend to recap the SSD architecture features following the same links as were provided in my previous post.\n\nIn SSD, there is no region-proposal step (in contrast with R-CNN models) and the set of regions to be considered by the model is completely predefined by the configuration. In short, the features from the feature-head of the network are passed to a pipeline of the detection blocks. Every detection block receives a reduced in spatial size tensor (which is still a somewhat representation of input image) and overlays it with a regular grid which nodes are later used as centers for the set of assumed bounding boxes. Each set consists of rectangles with various aspect ratios: 1:1, 2:1, 1:2, 3:1, 1:3, etc. All of these boxes are sent to both the classification and localization blocks, then the corresponding loss components are calculated and so on.\n\nIn our case, the code for generating the boxes (or, how they\u2019re named after the R-CNN papers, anchors) is stored in anchor_generator modules. To customize your choice of generator you can edit the config file at field model/ssd/anchor_generator (lines 31\u201341 in example config):\n\nAs you can figure out, we have 5 default boxes for each grid node: the square one (ratio of 1.0) and the elongated ones (the rest 4 ratios).\n\nOf course, the bounding boxes that you actually get when you run your model may have different aspect ratio as long as the localization block calculates the deltas (for both size and position) for every matched box in order to \u201cfit\u201d the detected object. The more aspect ratios you use, the slower your model is and the more work is done in post-processing steps, but we will talk about it later in this post.\n\nOnce we got a ton of bounding boxes for our image, we have to get rid of the multiple detections of the same object.\n\nThese detections may take place due to the same mechanism as we have just discussed: the default boxes. While training, the loss is calculated only for those default boxes that overlap with any of the ground truth boxes more than specified in configuration file (see below); the rest default boxes are used for negative examples mining (see previous post). But when we use our model for inference, we are not interested in every box (like in motorcycle image above), we will be fine with one box per object.\n\nIn order to filter the duplicate boxes the postprocessing is required and the procedure for that is called . In short, it works the simple way:\n\nThis greedy procedure allows to remove the boxes that overlap too much and keep the not-so-close ones. The only thing you have to take care of is choosing the threshold value, and this can be done using the configuration file (130\u2013136):\n\nallows to filter boxes with low score values (the higher the threshold, the more not-so-sure detections will be removed without a trial). (intersection-over-union) is the overlapping threshold which we mentioned above. Other two params are self-explanatory: the filtering stops when the number of already-found detections is higher than the specified numbers (per class and at all, respectively).\n\nThe final part of the 2-post story will be devoted to the problem that I still work on for my own purpose. In this case feel free to add your ideas of how can speeding up for inference be done in comments. Now I will list my own approaches.\n\nSo, how can we really speed up the model on each step?\n\nOf course, we can use a simpler and/or shorter convolutional network for feature extraction. For SSD in TFODAPI choices are and by default, but you are free to contribute your own architecture based on your favourite network ( / /\u2026). The other option is to resize input images to smaller size (lines 43\u201347 of config example), this can bring you a huge boost in speed but can lower the quality of the model.\n\nThe machinery inside the detection blocks can be simplified by reducing the number of default boxes types. For example, if your model is supposed to detect the ball on the football field, you probably don\u2019t need to use the elongated (1:3, 3:1 and so) boxes, but the close-to-central-symmetric boxes will do. Also, playing around with the / / in of config file affects the grid size for the generation of the boxes that can be tuned as well (see multiple_grid_anchor_generator for details).\n\nIn order to speed up the you can simply change the / thresholds to some higher values (then the model will consider much less boxes in the first place) and two integer thresholds to lower values; in my case, this results in a huge speed up and a small lose of accuracy, but in my case the number of objects on every image is always less then 4\u20135. If you have a lot of objects normally, you should be careful at tuning these options."
    },
    {
        "url": "https://becominghuman.ai/google-the-intelligent-investors-a-i-and-machine-learning-choice-eeb4b4c289ac",
        "title": "Google: The Intelligent Investors A.I. and Machine Learning Choice",
        "text": "The end of 2017 for The Seville Report was spent looking for the company or companies that will take the next step forward in artificial intelligence and machine learning. We started with big companies first, all the names you know, Facebook, Apple, Google, Amazon, Baidu, etcetera. We looked into some smaller companies, many of them privately held companies. After several days we landed on our pick for 2018, Alphabet, also known as Google.\n\nGoogle has been winning the internet since its search engine came online in the 90s. It sounds exaggerated, but we don\u2019t \u201cYahoo it\u201d or \u201cExcite it\u201d or \u201dLycos it,\u201d we \u201cGoogle it\u201d today. The company has been able to leverage their search algorithm into a massive internet advertising business. According to Adweek Google has control of 40.7% of the digital advertising market and 32.4% of the mobile advertising market. In our discussion of Alphabet we\u2019ll look into how the company\u2019s current operations make it a quality investment idea and why a bet on Google\u2019s future could be an investors best bet ever.\n\nAfter dominating the internet search business, Google has expanded and continues to grow its user base across all forms of the internet. When it comes to email, Gmail is one of the most widely used email clients, of all the major email clients available Gmail has a 15% market share, only second to Apple. When it comes to mobile operating systems, Google\u2019s Android platform shares a global duopoly with Apple\u2019s iOS. The chart below gives a visual picture of how far the 3rd most used mobile operating system is behind Android and Apple\u2019s iOs.\n\nGoogle Play, the company\u2019s site and app for digital content, like mobile apps, music, movies, and books reached a billion monthly users in 2015. YouTube (owned by Alphabet/Google) has over 30 million users a day and almost five billion videos viewed daily according to fortunelords.com. Google\u2019s internet browser Chrome is the most popular web browser with 45% of internet users choosing Chrome over other browsers. Apple and it\u2019s Safari browser is second with a 25% market share. Then there is Google Maps, which has over a billion users. Google has been great at developing and acquiring great platforms. Based on what the company has going on for it with the platforms mentioned, and the expectation that spending on digital advertising will continue to grow into 2021, we think Alphabet is a solid investment for anyone looking for a \u201csafe\u201d growth company.\n\nAnyone familiar with Alphabet is aware of the moonshot division or the Other Bets division. If you are not familiar, Alphabet describes its Other Bets division as\n\nAlphabet reported in its 2017 3rd quarter report that the Other Bets division contributed $794 million in revenue through September 30 2017. But it\u2019s not all roses, the division had an operating loss of over $2 billion in that same period. If you were to visit the websites of the companies listed under Other Bets, you would see that a lot of these companies are attempting to do great things that will benefit Alphabet shareholders in the future. However,the two companies that we are placing our bet on to move the needle for Alphabet is Waymo and DeepMind.\n\nWaymo is Alphabet\u2019s autonomous car initiative. It was started back in 2009, a time when the reality of a self-driving car seemed far away, but in 2018 they are here. Waymo\u2019s self-driving cars have driven more miles than any other company\u2019s autonomous vehicles. In late 2017 Waymo began sending out their self-driving cars with out safety drivers. The company and society is getting closer to the day when vehicles drive themselves. Waymo believes they will be able to give people rides in their driverless cars in the not so distant future. If Alphabet were to be the first to commercially release self-driving technology this could have a huge impact on Alphabet\u2019s stock price.\n\nThe second company within Google that we feel will contribute to revenue and profits in the future is DeepMind. In 2014 Alphabet purchased London based Artificial Intelligence company DeepMind Technology. You may have heard of DeepMind\u2019s AlphaGo, which beat a professional Go player in 4 out of 5 games of Go back in 2015. This was seen by many as a triumph for artificial intelligence with human intuition. Go is considered to be the world\u2019s hardest game, it\u2019s described as being a much more intuitive game than chess which is seen more as a game of logic.\n\nAfter beating the best at Go, DeepMind is ready to apply it\u2019s technology to real world problems. DeepMind has partnered with Moorfields Eye Hospital and University College London Hospitals. The plan is to allow the DeepMind algorithms to do the routine work like reading scans, which will allow others more time to attend to patients. For Google, DeepMind\u2019s technology has helped reduce the electricity needed for cooling at the company\u2019s data centers by 40%; and the WaveNet technology has helped to generate the voices needed for Google Assistant.\n\nWhile we have an optimistic view of where Google\u2019s stock price is heading with their development of autonomous vehicles, Google and Waymo aren\u2019t the only companies working on autonomous vehicles. In this list of companies most likely to get self-driving cars on the road first, Waymo ranked seventh, Ford was number one, followed by GM at number two. However, many believe Waymo is close to putting cars on the road in 2018. Aegis Capital\u2019s Victor Anthony stated on CNBC\u2019s Squawk Box that he believes Waymo will come into its own this year by putting out a commercial product at the end of 2018. Anthony believes that Waymo\u2019s accomplishments aren\u2019t priced into the stock price of Google. This is good news for potential investors.\n\nAs for artificial intelligence and machine learning, many companies, public and private are working on some form of AI/ML, and this represents a slight risk to any investment in Google. The advances in AI and ML are moving quickly. In 2011 the world witnessed, read, or heard about IBM\u2019s Watson competing on Jeopardy against two former Jeopardy champions and besting them both. Fast forward several years and AlphaGo \u2014 not related to IBM\u2019s Watson \u2014 is beating a Go champion in 4 out of 5 games. The next AI/ML advancement could come from another company not owned or operated by Google, and that company\u2019s AI/ML tech could possibly leap frog what Google and DeepMind have established.\n\nDeepMind\u2019s impact on Google\u2019s stock price will likely not happen any time soon, but we believe it will happen. For now we believe DeepMind\u2019s advances in AI will show up in Google products like Search, Chrome, Gmail, Google Maps, and Google Play. Google will likely use their AI to better the user experience of its products for its billions of users. We\u2019ve already seen this in Google\u2019s latest phone the Pixel 2. The phone has received great reviews for it\u2019s camera and picture quality, and what made this possible was the use of a small amount of AI and ML tech to manage the camera system. I expect Google to continue adding AI and ML technology here and there to its long list of consumer products.\n\nAs stated earlier Google isn\u2019t the only autonomous vehicle, artificial intelligence, machine learning company out there, but we believe it\u2019s the company that represents the best investment vehicle for anyone looking to invest in autonomous vehicles, artificial intelligence, and machine learning. Google has used its technology to dominate the internet search business and the internet advertising business, and this was all done with better algorithms and software than those that existed before Google. Our bet is that the company that has been great at developing algorithms and software will do it again with AI and ML and become the dominant player in that space as well.\n\nIf your interested in reading more about what Google is doing with AI you can read more here.\n\nIf your interested in our free investment newsletter on Google you can read more here."
    },
    {
        "url": "https://becominghuman.ai/ai%C2%B2-the-journey-from-dumb-to-fun-7ce018d23e30",
        "title": "AI\u00b2 \u2014 The Journey from Dumb to Fun \u2013",
        "text": "I won\u2019t bore my readers with my introduction. Since this is my first article here on medium, kindly hold all scrutinizing for the end. With the pleasantries on there way, just sit back and enjoy!\n\nI have worked on primarily 4 fields for the past 5\u20136 years, including Game development on Unreal Engine 4, a photo editing application (I made one called LimeLight for macOS, heard of it?), an audio engineering framework ( for iOS and macOS systems) and integration Artificial Intelligence iOS and macOS applications.\n\nI am writing this article because of an issue I faced when I was trying to create a new type of neural network called Active Foreground Neural Network (this will be referneced as AFNN from now on). It takes all the goodness of Recurrent and Convolutional Neural Nets, and enhances their undoings. The aim of AFNN is to do live training in the background and do the actual cognitive implementation in foreground. Thus the name Active F-O-R-E-G-R-O-U-N-D Neural Network. Since I needed an application for the functioning of this NN, I had multiple options, including its applications in game development. So long story short, I developed a demo game with its entire AI system running on my AFNN."
    },
    {
        "url": "https://becominghuman.ai/predicting-buying-behavior-using-machine-learning-a-case-study-on-sales-prospecting-part-i-3bf455486e5d",
        "title": "Predicting buying behavior using Machine Learning: A case study on Sales Prospecting (Part I)",
        "text": "Artificial Intelligence (AI) is the new buzz word. We all have heard and read that it will change the world. However, most articles fall short on explaining how exactly AI algorithms can be used to solve real-world problems. This series is my attempt at bridging the gap between technical AI and applications of AI.\n\nFor this series, I will restrict to Machine Learning (ML) algorithms which is a section of AI where we let machines learn from data.\n\nMy focus will be to explore how ML algorithms can be used to model and predict human buying behavior.\n\nDuring the Christmas break, I started reading a book called \u2018Misbehaving: The making of Behavioral economics\u2019. The author, Richard H. Thaler \u2014 winner of Noble prize for economics in 2017, is considered one of the pioneers of behavioral economics. In his book, he gives various examples to show how humans do not behave according to economic models.\n\nAn example taken from his book.\n\nPerhaps that is why no economic model has been successfully built to predict human behavior.\n\nSo what is our buying behavior\n\nMost of our buying decisions are not based on well-defined logic. Emotions, trust, communication skills, culture and intuition plays a big role in our buying decisions.\n\nHow can Machine Learning help in modeling and predicting human buying behavior?\n\nThe most common approach taken by many \u2018AI-based\u2019 sales startups is to identify the next buyer by mining internet data. They look at what people are talking about in social media and then identify those who are searching for a given product or service. However, as pointed out in my article \u2018Want to grow sales? Stop cold emailing. Start prospecting.\u2019, people who are already actively looking online are not the best potential buyers (or prospects) to sell to.\n\nLets try to see how top salespeople identify a prospect?\n\nTop salespeople identify a prospect before he or she goes out and announces publicly that he/she is looking for a product or service. They build relations and identify needs of people, often before the prospects may start looking for a solution.\n\nSo can ML algorithms identify needs of prospects without meeting prospects?\n\nAlthough humans do not follow a well-defined logic, we do have some repeated patterns. We often buy the same things, behave in a similar way and follow similar intuitions. So if we can learn the buyer\u2019s pattern, we may be able to identify the next buyer too!\n\nWhen we look at ML algorithms, Neural networks are one of the most widely used ML algorithms these days. One of the main reason of having widespread use of Neural Networks is because it can create an approximation of any function. The approximation is based on data, which it learns or is trained with. So neural nets are able to learn similar responses for inputs that are similar in nature.\n\nA detailed explanation is beyond scope of this article but if you are interested to know more about Neural networks, you can read here and here.\n\nI have pointed out what constitutes a good prospect and sales process in two of my previous articles, click here and here to read more details.\n\nThe biggest problem that most New Sales Development Representatives face are: a) identifying a good prospect and b) Building a customized process and pipeline suitable for the prospect.\n\nNote: New vs Old buyer\n\nI must note here that the buying behavior (and sales process) for new and old customers are different. In this and next article, I will focus on New customers \u2014 namely called New Sales development for B2B customers. In sales term it is called Sales Prospecting. In Part IV of this series, I will write how Neural Networks can be used to understand buying behavior of existing customers. Such, in particular will be of interest to e-commerce sites. One of the simple Neural Networks that is used for understanding such behavior is word2vec.\n\nSo can machines be taught to behave like a top Sales person? Let\u2019s give a shot.\n\nI ask the following four questions to identify who are ideal prospects (taken from the book \u2018New Sales Simplified\u2019 by Mike Weinberg)\n\n\u2022 Who are your best customer\n\n\u2022 Why they became customers\n\n\u2022 Why they still buy from you\n\n\u2022 Why do prospects choose you over other similar products\n\nThe goal is to identify common features among successful and unsuccessful prospects. Normally this is done manually and intuitively.\n\nIf we had to solve the same problem via Machine Learning we need to use Neural Network Classifier.\n\nClassification can be defined as the grouping of things by shared features, characteristics and qualities or if you will simply dropping things into corresponding buckets, you could for instance classify the following geometric shapes based on their similarity. [Reference]\n\nBased on the four questions mentioned above, we try to extract relevant features from answers of the questions. An example of such features can be as following\n\nWho is your best customer: Customer size, Decision maker, Growth last year\n\nWhy they became customers: Location, First reference (personal contact, content marketing etc), Product features(Feature 1, Feature 2)\n\nWhy they still buy: Customer service, Location, Product features\n\nWhy they choose us over others: First reference, Product features(Feature 1, Feature 2), Location\n\nStep 2 Labeling data: Label the data based on which of the leads took least amount of time to covert, medium time to convert, maximum time to convert and did not convert.\n\nStep 3 Training Neural Network: One labeled, we will use supervised learning algorithm to train a standard Neural Network classifier.\n\nStep 4 Testing Neural Network: In this phase, you test how good the model is with rest to the test data.\n\nStep 4 Executing Neural Network on new data: Once trained any new input with the data will be able to classify into good and bad output. Thus we can input either a person or company data and the Neural network will be able to classify.\n\nOnce you know who can be a good/medium/bad prospect you want to create a customized process for that particular prospect. Top salespeople use intuition and experience to create such a process.\n\nThere are two potential algorithms that can be used for this. Long Short-term Memory (LSTM) and Reinforcement Learning.\n\nA sales process can be seen as set of actions done over time. Current action is dependent on what has been done before and what has been the response.\n\nLSTM networks are perfect for that. These are part of the broader class of neural networks called Recurrent Neural Network (RNN).\n\nOne of the appeals of RNNs is the idea that they might be able to connect previous information to the present task [Understanding LSTM Networks].\n\nAs you can see in Figure 2(below), RNN is a series of connected Neural networks. Picture from here.\n\nHowever RNN suffer from something called Vanishing Gradient problem. Learning is limited within a region of Neural networks and thus RNNs are not able to learn long-term dependencies. LSTM solves that.\n\nWe will go into details of LSTM network in part II of this series.\n\nAnother interesting Machine Learning algorithm is Reinforcement Learning (RL). Reinforcement learning depicts human way of learning. It is a learning based on real-time feedback and not via training data.\n\nThe learning algorithm learns best actions based on rewards and punishments it receives after executing an action in real world. Figure 3(below) shows a basic structure on how reinforcement learning works.\n\nWe will go into details of Reinforcement Learning in Part III of this series.\n\nAn obvious question many of you will ask, do you need to build all of these algorithms ourself? Obviously not. There are libraries like Tensorflow, Keras etc which you can use to train your model. I will go into details of the code in some other post. This article was to give an overview of how ML algorithms can be used.\n\nHowever is everything so great? Not really. One common problem is that your model is as good as your data. That is why what most data scientist do is basically filter out the good data from the bad data. Thats a challenge!\n\nI find this problem deeply fascinating and would love to connect with similar people who have similar interest. Feel free to share, like or comment on this article."
    },
    {
        "url": "https://becominghuman.ai/technological-evolutions-for-2018-and-beyond-b63b5d00a29d",
        "title": "Technological evolutions for 2018 (and beyond!) \u2013",
        "text": "At the end of last year, the Bayard Partnership invited me to an event on digital transformation. The event was interesting and focused on the impact that artificial intelligence (A.I.) will have on the organisation. Different Bayard members highlighted different aspects, balancing techno-optimism with realism regarding the impact on teamwork.\n\nAfter the event, I had an interesting discussion with one of the associates. I mentioned that A.I. will have a clear impact, but that there were several other trends that I saw would impact the way we work. Instead of mailing her a summary, I decided to turn the list into this Medium article.\n\nThe first topic that I'd like to cover is artificial intelligence. A.I. will undoubtably impact the world that we live in. We're already seeing the impact in a very direct way: we can chat to the first A.I. chat bots on the websites of a growing number of companies. Microsoft had their share of problems with an A.I. bot, but overall, the trend is that chat bots are ready for the first level support. Behind the scenes, A.I. assists in interpreting data. The most well-known examples are currently games like go, where Google's Alphago managed to teach itself the rules from scratch. However, A.I. is starting to interpret parts of the world as well, for example correlating the cars it sees in StreetView with U.S. voting behaviour.\n\nWhile I don't believe computers ever will have general intelligence, a viewpoint that I'm joined in by a growing number of analysts, A.I. will definitely have a large impact on how we work. Much of the rote work done by humans in processing centers will be automated. The impact in certain areas will be comparable to what happened to reading the addresses on mail envelopes: 95% or more will be done by computers.\n\nAt the Bayard event, there were a number of voices who saw a positive side for us mortals, though. The part of the work that A.I. will take over, is typically the rote work. That's also the easiest to organise. The part that remains is the more creative side: product design, project management, logistics optimisation, big data interpretation, and so on. The focus for humans will shift from the quantity of the work to the quality of the work.\n\nOne aspect that Bayard, being a consulting company, did not touch upon is neuromorphic computing. That technology tries to mimic how the brain works on a chip. Currently, we use general hardware (or graphic co-processors) to simulate neural networks. With neuromorphic hardware, the hardware would be the neural network. It's comparable to an ASIC that combines precisely what you need onto a chip. But the advantages promise to be much larger because there would be no need to simulate connections between neurons: the hardware would be neurons and connections.\n\nBlockchain technology frequently hits the headlines, particularly when the Bitcoin rallies against the dollar, then drops again at 5\u201310 times the original price. That cycle is so predictable by now that the main surprise is that people are still surprised.\n\nOf course, there's much more to blockchain than the virtual currencies. It's a new form of accounting: triple entry, as compared to basic or double entry. Double-entry accounting was invented in the middle ages in Genoa and the other trading cities in Northern Italy. It solved the problem of companies going under by forcing them to write down not only money going in and out, but also what that money was used for.\n\nBlockchain, being a ledger system, improves accounting in a similar way. It tracks who buys assets from whom. For example, I can see in the transaction log of Bitcoin that I bought or sold Bitcoin and who the other party was. Most importantly, blockchain accomplishes this without the need of a central manager. Unlike a currency with a central bank, everybody can be part of the management of Bitcoin. This protects against the central government losing control or using the asset for its own purposes.\n\nTake for example land property. In Belgium, we have a centralised land registry system and take it for granted that land property stays yours. However, in Venezuela, you have to show the property chain of the land you claim all the way back to 1848. If you cannot do that, or in some cases even if you can, the government or private individuals may just grab your land. As a result, there were about 150.000 applications for land grants in 2007\u20132009. Many of those were simply to contest a piece of land.\n\nA blockchain ledger of land would protect against this sort of practice. You can prove that the land is yours because it\u2019s logged in transparant way. Done well, the management of the blockchain is distributed amongst all participants so it\u2019s impossible to enter a double sale or a fraudulent post-factum change. It\u2019s for that reason that Russia will start a blockchain trial for its land registry early this year. Sweden has already finished its second phase of such a trial. Other valuable and unique assets are getting their own blockchain applications as well: diamonds, digital identity, company audits, and Belgium\u2019s trading place for Internet of Things sensor data. Which brings me to the next topic.\n\nI'm clustering together a few topics here that could easily take up an article each. Robots, drones and autonomous vehicles are applications that combine several of the technologies, with A.I. as an obvious example. But the focus in this section is data collection in-the-field and the consequences.\n\nIn the 1990s, when I started in embedded systems (the name used for IoT at that time), the situation was easy. A customer, usually a big industrial player or a government, asked you to build something. Sensors and processing hardware would be handpicked and assembled to do what was needed. Now, however, the situation is the reverse. Instead of a few specific customers, you have around 2,5 billion potential customers. You know more or less what hardware they have. It\u2019s now up to you to invent something that they will find helpful.\n\nAlmost everything we have collects information. The best example is our smartphone. This overview lists 7 distinct sensors aside from the cameras and the latest innovations in the iPhone X. Such an array of sensors generates huge amounts of data. For example, it is estimated that a connected car will upload 25 gigabytes of data every hour.\n\nThe main question is: what do we want to do with that data? The sensors are there and data analysis finds ever more techniques to extract what you want, either with or without A.I. The challenge is on the application side: what do you want to map, show or teach people?\n\nOf course, the field of smartphones is small compared to the entire IoT universe. Take for example smart cities. You can add motion sensors on the roads to turn on street lighting or to control the traffic lights. With microphones, you can build a noise map of your city. Cities are setting up networks specifically for sensor data, antcipating that this data source will increase steadily. As with the smartphone sensors, hardware and processing are readily available. The challenge is finding the problem you want to solve.\n\nI\u2019m putting augmented reality (AR) first because I\u2019m a firm believer that it will become orders of magnitude larger than virtual reality (VR). But the technical side is similar so they belong together.\n\nIn VR, you present the user with a whole new world. In AR, you show him the world with virtual elements added in. Typically, VR lock you out of the regular senses with a visor that blocks all vision (and possibly sound) and replaces it with its own output. In AR, you typically have a screen showing a camera image with the added elements. In some cases, like the Microsoft HoloLens, you get a transparent visor that also projects the virtual additions where the belong in the scene.\n\nAR rocks because there is hardly any barrier to entry. As a great example, Ikea has an app that lets you preview its furniture in your home using AR. No need for visors, just wave your phone around and you see that new sofa right there in your living room. It\u2019s not just for geeks: it appeals to the masses. And because of that, AR will be big. VR, maybe not so much. That's why Apple has its ARKit standard on every iPhone and Google just kicked out its specialised Tango for the more accessible ARCore that will work on just about any Android phone after the current beta phase. AR can be used now by hundreds of millions of people.\n\nAR will appeal to almost every company in the building industry as well as everything related to home decoration. City designers will get a view of what their planned changes will look like while exploring the site and without the need for painstakingly crafted maquettes. Historians or guided tours can marvel at what a site looked like in the past. And let\u2019s not forget the games: Pokemon Go is still much larger than all the other AR applications combined.\n\nAbout 20 years ago, a colleague of mine painted a vision. His father was working in material sciences, which we in IoT regarded as boring, old-fashioned and without prospects. That\u2019s because we had never heard of polymers. His vision was to have a plastic bag that would be screen, computer, battery and display in one. It would be flexible, just like normal plastic bags, but it would show dynamic images or animated commercials. That vision blew us away. To really turn embedded systems into something hip, we realised, advances in materials were key.\n\nIn those twenty years, there have been some rumours of moldable batteries and roll-up laptops. But the main area of improvement has been in screens. The Chinese brand ZTE has launched the first smartphone with a flexible screen, beating Samsung who was rumoured to get the first one to the market.\n\nRegular screens are also becoming bigger: Samsung is showing a 146\" TV on CES this year. That's 3m70 for non-metrically challenged. More fun: LG has a 65\" TV that you can roll up. For a larger display with a smaller price tag, you can also use a projector: short throw or ultra short projectors shape the light so that you can mount the projector close to the wall. This is what smartboards in schools use: the teacher can point at stuff on the board without casting a shadow. It gets even better if you then turn that surface into a giant touch screen using a can of spray paint.\n\nIt\u2019s clear that there is a future in which screens will be even more omnipresent than they are now. Even ten years ago, it seemed hard to believe that everybody would be staring at small screens. We might all be interacting with wall sized screens ten years from now.\n\nAs one of the applications, I\u2019m thinking of social housing. Governments are getting their budgets cut in large parts of the world. What if we could set up social housing with a wall-sized screen that displays ads? It sounds creepy and I\u2019m not suggesting it should be the way forward. But it would open up the social housing market to other sponsors than the state. Just some food for thought.\n\nOnce upon a time, before the industrial revolutions, many people produced many things. People would produce the clothes and tools they needed, when they were needed. But by automating the manufacturing process, all of that changed. Instead of one person spending one hour to produce one garment, that same person could now produce enough cloth for a thousand garments or more in the same time.\n\nCool! But this came with a disadvantage: to produce, you needed a big, specialised machine. The machine is why we have (centrally located) factories and our traffic jams to get to work. Marx also observed that this puts the power in the hands of the machine owners. But even the less communist inclined readers should feel happy that all of this will be coming to an end soon.\n\n3D printers give you the power to download the designs of any component and to produce it yourself. It's not a specialised tool: the printer will print bicycle parts, a Yoda statue and anything else. It\u2019s also affordable, starting at a few hundred euros for a decent one.\n\nThe impact is already being felt and companies are preparing. For example, DHL has a study of the impact on supply chains. While it sounds like they have a lot to lose, there\u2019s a lot of potential for them. Just like you sometimes get personalised (paper) mail, a 3D printer can generate personalised objects. Even better, because you can just print something when you need it, it\u2019s the ultimate solution in just-in-time logistics.\n\n3D printing also lets you print objects that couldn\u2019t be manufactured before. It\u2019s now even possible to print food.\n\n3D printing has been hyped before but the revolution is quietly continuing in the background. As it always does in Steven Kotler's overview of the life cycle of exponentially growing technology.\n\nI live in Kessel-Lo, a suburb of the lovely university town of Leuven, less than 30 kilometers from Brussels. Kessel-Lo is a mixed affair with some areas more affluent than others. Some areas have apartments but most have houses. And many of those houses have solar panels. Most of those are the result of a subsidy program that the Belgian government launched in the early noughties.\n\nSolar panels are the most obvious example that the energy landscape is changing. While huge power plants still have their role to play (though the trend is to swap coal for huge wind farms), the trend is clearly to replace power plants with more local production. Witness the nuclear stop in for example Germany and Belgium.\n\nThe next step would be to connect all that locally produced energy into a microgrid. That way, the less well-off areas (whose inhabitants spend more time at home than the many 2-worker families in the richer parts) can use the excess energy from the solar panels during the day. Berlin has promoted this idea to the extent that they currently interconnect 120 local microgrids.\n\nThe logical step after that, and according to the EC Energy Directorate currently our \u2018weakest link\u2019, is local energy storage. While traditional plants like coal and nuclear produce continuously, most renewables produce intermittently, i.e. when there is wind or sun. Local storage helps smooth out these cycles by storing the excess during peaks and releasing them during periods of low production. For the home, the most well-known solution is Tesla\u2019s Powerwall battery. As an alternative, some hobbyists are building their own systems using old laptop batteries!\n\nThe changes that I\u2019ve summarised above have already been described in more detail by respectable writers. For example, Jeremy Rifkin, in his book \u201cThe Third Industrial Revolution\u201d, talks about the changes in the energy landscape. One year later, the Economist published a lengthy study of the impact of distributed manufacturing under the name\u2026 the third industrial revolution.\n\nEach one of the trends mentioned here has been declared the Next Big Thing several times before. And for each, experts are waiting for the killer application to arrive and conquer the world. I think that there won\u2019t be a killer app for one technology by itself.\n\nBlockchain by itself is pretty boring: it\u2019s like a BitTorrent version of a database. You store things in it without relying on a central server. It\u2019s when you start combining the fields that the magic starts happening.\n\nOne creative example can be found in this post on automated insurance. If your smartphone has a 3-year battery insurance and it detects an upcoming battery problem after two years, an automated insurance system could be triggered that sends you money for a repair. It's a typical Internet of Things topic, but the insurance is automated using Ethereum digital contracts, being blockchain.\n\nAs with the other examples above, one plus one really is more than two.\n\nDo you know of a combination of the above technologies that is ready for primetime? A fancy combination of pervasive screens and 3D printers, or the ideal way to use AI in VR? Let me know in the comments!"
    },
    {
        "url": "https://becominghuman.ai/why-2018-wont-be-a-year-of-a-i-4ad34f2f61ca",
        "title": "Why 2018 won\u2019t be a year of A.I. \u2013",
        "text": "Why 2018 won\u2019t be a year of A.I.\n\nPrediction is very difficult, especially if it\u2019s about the future. Yet many of us are trying to predict what the future in recruitment is going to look like. Because knowing what the future will bring us also gives us the advantage that we all are looking for.\n\nMany predictions from last year were about A.I. (artificial intelligence) and chatbots. A.I. was quite a topic during the whole of 2017 and I am sure that it will still be a hot topic during 2018. But I don\u2019t think that during 2018 A.I. will dominate as many predicted.\n\nBear with me here, before you write a comment that I am wrong, crazy or I have seen the future coming and don\u2019t have a clue about A.I. All of these things could be true; I am not going to argue with you here, but these are my arguments for why A.I. will be a hotter topic in 2019 than 2018.\n\nA.I. is still a new and unexplored technology for many recruiters. I am sure that almost every recruiter heard about A.I., but many of them still haven\u2019t had a possibility of working with A.I. tools or tried them. For some of them, A.I. technology is still a mystery and they don\u2019t understand it yet.\n\nThe pricing for many tools is outside of the range of many small companies and agencies. The smaller ones can\u2019t ready to afford to buy most of them or their leaders don\u2019t want to invest money in them.\n\nYes, that is my personal opinion and I am aware that this is a strong statement and many of you are going to point out that I am wrong here. But most chatbots that I have seen were not working perfectly. And even though there are many interesting tools and apps on the market that could boost your recruitment process and help you to get more from your ATS, referrals, sourcing etc., sadly most of these tools suck, they could have great A.I. engine, but most of them need lots of improvements and they especially need enough good data that can be used for learning.\n\nCompanies don\u2019t know how to use the full potential of A.I.\n\nMany of my friends who are working at companies where they implement tools with A.I. share with me that they are using only a few functionalities that these tools provide.\n\nThis is mainly because of two reasons. They implement the tool that is not solving their real problem. The second reason is that recruiters are creatures of habit. They stick with things that they already know and not everybody is open or has the time for new tests.\n\nAnd maybe, if they have read how A.I. is going to replace them, they are not eager to start using the tools that could replace them in their work. And if they are not using them they are not getting any results, and if they are not getting results the budget holders are not going to invest the money again for these tools or any other tools anytime soon.\n\nAnd what will be the \u201cmain\u201d topic for 2018?\n\nMy money will go on Growth Hacking. This is my guess and I believe this will be the main topic for 2018, but the \u201cblockchain in recruitment\u201d will try to hijack \u201cmain topic\u201d for this year. Very soon we will see articles such as, \u201cBlockchain recruitment is the only way\u201d; \u201cBlockchain LinkedIn\u201d; \u201cBlockchain this and that\u2026\u201d\n\nAnd I am not saying that growth hacking should be a main topic for this year just because I spent the last 5+ months learning about it :) But below are the main reasons why my forecast is correct (at least I hope it is).\n\nGrowth hacking is a word with strong roots in the world of startups. The origin of the word can be traced to the year 2010 when Sean Ellis coined the phrase \u2018Growth Hacker\u2019 to explain his frustration with getting replacements for himself as he sought to retire from his erstwhile job.\n\nTo put a definition to the phrase, growth hacking is a marketing technique developed by startups that utilize the combination of creativity, analytical thinking, and social metrics to sell products and gain exposure.\n\nIn layman\u2019s term, we can say it is simply the activity of experimenting with different methods to make a company more successful and bigger, i.e. a growth hacker undertakes the initiative of birthing and nurturing the consistent growth of an organization.\n\nMany times growth hackers have been thought of as marketers but in many ways being a growth hacker is much more than just having a marketing degree; a growth hacker can combine marketing creativity and innovation with the ability to code marketing hacks.\n\nFor example, in a startup, the process of charting the part for growth and establishment of the organization might be much more than just establishing a strategic marketing plan or just building a marketing team; growth hacking encapsulates every available strategy to grow and expand the organization\u2019s operation base and capitalization.\n\nRecruiters are already using many growth hacking tricks to get their message among their audience, especially when growth hacking involves thinking outside of the box, which is a necessary thing to do in the current market.\n\nRecruiters are consistently under pressure from line managers to find the best possible fit within the shortest possible time. This recruitment growth hacking involves the use of technology, shortcuts and new ideas to save time by eradicating/automating low-value activities so you can focus on the high-value activities that help you find and hire the right people for your business.\n\nExperimenting with new methods is one of the fundamental principles of the growth hacking process. Although growth hackers share the same aim with marketers, their approach is not as conservative. They use an empirical process in getting to the projected outcome. One aid to the efficiency of growth hackers is the freedom to invent and operate their own self-propagating growth machine that can take the organization to greater heights.\n\nAs I mentioned before, recruiters are already using various types of growth hacks, but this year I am expecting that many of them will start learning more about \u201cgrowth hacking\u201d, how to use data and target more people for less costs.\n\nWhy Growth Hacking will dominate 2018 instead of A.I.\n\nThese are a few points that explain why growth hacking will be more popular in recruitment than A.I.\n\nGrowth hacking is an effective tool for organizational advancement as it curates innovative ways of propagating and promoting both the products of the company as well as the company\u2019s image. It also provides actual tractable data from which recruiters can draw statistical conclusions and base their subsequent actions upon.\n\nSlowly, all the attractive images, videos etc. that recruiters and other people are posting on LinkedIn won\u2019t matter anymore. The clickbait titles will lose their power and the only thing that will matter will be the right content, the right message.\n\nRecruiters that are able to write meaningful content and have the growth hacking skills will be the hidden gem for any organization because they can bring an organization more than just candidates. They also help to spread the news about the company, company culture, and company products.\n\nA.I. will surely bring great things to recruitment in the future. And those who implemented the right tools will gain a big advantage, but the year 2018 won\u2019t belong to A.I., it will belong to growth hackers. But the year 2019 will be the year of A.I.\n\n\u201cForecasting is the art of saying what will happen, and then explaining why it didn\u2019t.\u201d And maybe my first article of 2019 will have a title, \u201cI wasn\u2019t wrong because\u2026\u201d :)\n\nThis article was first published on sourcecon.com\n\nJan is an author of a book \u201cFull Stack Recruiter: The Modern Recruiter\u2019s Guide\u201d and creator of sites like Sourcing.Games, Recruitment.Camp and other projects. As a speaker and blogger, Jan believes that recruitment is a great field and he is constantly trying to make it better."
    },
    {
        "url": "https://becominghuman.ai/sentiment-analysis-in-marketing-time-to-profit-155b5a1cca7a",
        "title": "Sentiment Analysis in Marketing: Time to Profit \u2013",
        "text": "It\u2019s all about digital these days. Feature-teeming apps allow you to finesse on and scale up about any Marketing action you take. Among the many digitized platitudes that all work to some extent, there is one technique a company with enough scale can no longer do without. It is the NLP-enabled technique called Sentiment Analysis.\n\nWhile Artificial Intelligence and Natural Language Processing have fomented disruption in more than one business domain, Sentiment Analysis & Opining Mining are either an edge, or a concern for companies throughout the B2C space. This depends on how early in the trend they embarked on it.\n\nSo what is Sentiment Analysis and how can you use it in your Marketing endeavors?\n\nSentiment Analysis & Opinion Mining allow you to hear and systemize what your clients think about your brand or product, or those of your competitors. You can listen to their opinions practically across the Web, including social networks, forum posts and twits. In essence, you obtain a structured and actionable imprint that reflects the entire panoply of tendencies, relations, opinions and attitudes pertaining to the object of your interest. While the technique is still far from full-crowned maturity, it is mature and sophisticated enough to give your Marketing process a significant boost.\n\nBy now, Sentiment Analysis & Opinion Mining have gained so much footing in the Marketing processes of many companies, that they are regarded by many, along with some other NLP-powered approaches, as the next big thing in Marketing.\n\nHow does the whole thing work?\n\nYou will have probably guessed by now there is no extraterrestrial intellect or anything ultra- mundane behind Sentiment Analysis & Opinion Mining. Based on the object (s) of your interest, NLP experts come up with a set of so called Named Entities that can be the name of your company, brand, those of your competitors, or, even, the name of some event or events associated with them.\n\nThe system locates all occurrences of a Named Entity in vast volumes of textual information, contained within a social network, or residing elsewhere on the Web. They also compose a set of rules derived from your historical data and the practices of your industry. Simply put, they determine the relations that arise between different words and word groups in a certain business or otherwise peculiar context.\n\nIn order to enable machines to identify similar relations and contexts in future, NLP experts use Machine Learning. In accordance with your interests and the purpose of your search, the system identifies occurrences of a Named Entity that appear in a given context and interprets this context.\n\nWhile Sentiment Analysis mostly allows measuring the generic positive and negative sentiment with varying precision (positive, negative, neutral and the degree of polarity within a text source, to be more precise), its more sophisticated sister technique called Opinion Mining is capable of handling much more intricate tasks. For instance, it can locate a context that signals a buying intent, or a situation in which your product is compared with that of your competitor.\n\nAlthough, Sentiment Analysis and Opinion Mining still remain largely rule-based, the relations between words and word groups are now also increasingly determined based on their semantic similarities. This is gradually making the technology all the more powerful.\n\nThe great thing about Sentiment Analysis and Opinion Mining is they are sufficiently mature and totally practicable in a number of ways. In Marketing, there are several purposes and usages in which both the techniques, excel.\n\nLet\u2019s see in more detail how you can benefit from them."
    },
    {
        "url": "https://becominghuman.ai/how-to-influence-buying-intent-by-means-of-data-text-mining-f1e6d4220708",
        "title": "How to Influence Buying Intent By Means of Data & Text Mining",
        "text": "How to Influence Buying Intent By Means of Data & Text Mining\n\nData Mining & Text Mining are gaining a foothold across many verticals, including eCommerce. While many retail merchants are already aware that this Data Science and NLP techniques can benefit them online, they barely have any idea about how much of a boon they could be for their brick-and-mortar business. Nevertheless, making friends with Data & Text Mining can pack a wallop for you, as a retailer, across the board. Moreover, some of the more intricate applications of Data and Text Mining are suitable for both eCommerce and brick-and-mortar stores and you can use them in parallel in order to enhance the effect.\n\nWe\u2019ll dwell on just one such application, so that you can decide whether bringing it into play can bring you closer to your business goals.\n\nOne of the awesome techniques you can apply both online and offline is Basket Analysis. Even the name of this approach suggests something you deal with day in day out in both your physical stores and on the Web: baskets or carts filled with your products by your customers. Did those items wind up in the same basket by pure chance? How often do they find themselves in the same company? How many baskets like that do you process over a meaningful period of time? Can that be considered a trend?\n\nYou will have probably deduced by now it deals with identifying a pattern in accordance with which the products you sell co-occur. Is that possible to do? With Data & Text Mining, it certainly is. You can explore the invisible relationships between the different products and product groups that make up your online and offline inventory and determine which of them \u201cgravitate to one another\u201d.\n\nIn fact, the ability to identify which of your products sell together opens up an entire host of practicable opportunities.\n\nIt is beyond the scope of this article to describe some of the benefits of Basket Analysis that are not associated with influencing buying intent, but they, nevertheless, are present too. In particular, Basket Analysis can be used for product inventory-related predictions. \n\n\n\nSources used for this article: https://en.wikipedia.org/wiki/Affinity_analysis"
    },
    {
        "url": "https://becominghuman.ai/nlp-profitable-clairvoyance-in-ecommerce-45f25af3c026",
        "title": "NLP: Profitable Clairvoyance in eCommerce \u2013",
        "text": "At first blush, text mining may seem like the umpteenth lofty technicality or piece of terminology you as an ecommerce business owner can live without. But is that true? Actually, text mining will satisfy something you, as merchant, have always craved \u2014 the ability to read your customers\u2019 minds. Is this possible? How this gift is channeled and how to use it gainfully in your ecommerce business are among the topics we\u2019ll explore.\n\nText mining is one of the constituent techniques of an area of knowledge known as Natural Language Processing (NLP). This technology is currently making rapid headway enabling systems and appliances to understand human language the way a human would. Creating immense opportunities in a number of verticals and fields, NLP allows ecommerce merchants to monetize its achievements immediately. It lets them derive unparalleled, direct marketing insights from the whole spectrum of their interactions with the consumer. The bottom line is that you can learn your customers\u2019 likes or dislikes about your product or service, which of your product\u2019s features influence their purchases, what they are seeking, and, even, predict what may be of further interest to them. In the increasingly competitive market of ecommerce, Text Mining and Artificial Intelligence are regarded as the features that will propel most of the action in the coming years.\n\nYou can use the whole lot of information that floats around your ecommerce business and its product offering, including blogs, incoming emails, chat bot messages, on-the wall communications in social networks, search engine queries, and forum posts. You canmonitor and analyze all of these media \u2014 by means of a single complex NLP solution that can be added to your ecommerce application.\n\nThe opportunities that answer this question are staggering, so we will examine a few.Optimizing Your Inventory and Adjusting Your Product Offering\n\nText mining makes it possible for you to optimize your inventory by introducing items that are sought after or predicted to be in demand shortly. Further, to better match your audience\u2019s preferences, you can team up with your products\u2019 manufacturers to modify some of the product features or properties.Offering a Complementary Product\n\nBy emulating human understanding, NLP allows your app to capture the meaning behind a search engine query or some other phrase in its entirety. As a result, you can effectively offer your visitors a matching complementary product at the right moment. If your visitor adds to his cart a man\u2019s blue shirt that is stunning when paired with that orange bow tie of the same brand, why not let him know about that? They don\u2019t shop for bow ties every day. Does your shopper have soft spot for printed t-shirts with a specific design? Offer him your newest one, and, odds are, he will be excited to add it to his collection.Clinching It with a (Personalized) Discount\n\nYou can text-mine your data and learn in advance which part of your incoming inventory is likely to be in greater demand. This ability will allow you to start offering it to interested users with an instant discount. You can even offer several similar items, emphasizing the product of interest and tempting them with a discount.Identifying New, Relevant Product Offerings\n\nIf you are thinking about expanding your product offering, Text Mining can aid in this goal, too. Text mining identifies which search engine queries are totally out of sync with your current inventory, groups them together, and determines if shoppers who have used those queries have, actually, given you a valuable suggestion. Apparently, more than one person considered your store to be the right place to shop for that stuff.Ironing Out Bugs in Your Customer Service and Product Delivery\n\nAny beef your customers may have with you can well go public these days and impede your sales. You can text-mine your data to right some grievance and protect your reputation. For example, upon finding a stinging remark on a forum, you could reach out to a patron with an apology, a discount, or both. When done in time, this might even improve your reputation.\n\nAlso worth noting is that competitor\u2019s data can prove to be no less important for your ecommerce business than your own. You can mine the Web to identify product popularity trends, brands that draw negative feedback, or products that are acclaimed by your potential clientele.\n\nIn order to implement an NLP capability that would allow you to perform text mining, you would need a seasoned, professional team. This team must include at least one NLP engineer and one software engineer with a solid grasp of one of the several scripting languages (e.g., Python, Java). Typically, an NLP implementation project team consists of two or three NLP engineers, two or three software developers, one to three computational linguists, one or two QA engineers, a business analyst, and a project manager.\n\nYour NLP team does not have to be based in or around your vicinity. Instead, choosing a remote team can give you several advantages. Looking beyond your borders could save up to 50% of your budget and equip you with professionals with a global perspective and additional languages. Although the technology is generally language-neutral and does not require a native command of a language, a good grasp of another language would be required if you want to be able to perform text-mining in this language.\n\nAs the competition in ecommerce persists, text mining has become the cutting-edge method for a business to stay in the running. Next to mind-reading, text mining with NLP provides the most opportunities to capitalize on buying trends and reach your business potential."
    },
    {
        "url": "https://becominghuman.ai/what-does-deepminds-latest-publication-mean-for-a-i-d496ab06d61f",
        "title": "What does DeepMind\u2019s latest publication mean for A.I.?",
        "text": "What does DeepMind\u2019s latest publication mean for A.I.?\n\nLess than 3 months ago, in October 2017, the research team at DeepMind \u2014 a world leading artificial intelligence research company acquired by Google in 2014 \u2014 published a paper in Nature introducing AlphaGo Zero, the strongest Go player in history. The paper discusses a recent discovery that is set to change the state of play in A.I.\n\nGo is a Chinese board game, over 2500 years old and historically played by Chinese aristocratic scholars in antiquity. The game while resembling chess, is much more complex, with a larger board, longer games and more alternatives to consider per move. For context, it is believed that the lower bound of legal moves in Go is 2 x 10\u00b9\u2077\u2070. \ud83d\ude27\n\nIn Go, the aim is to capture opposing stones. A capture happens when a stone is surrounded by opposing ones.\n\nGames end when both players pass (e.g. as a result of not seeing potential future moves) and are then scored. Games can also be won following the opponent\u2019s resignation.\n\nSince it\u2019s early days, Go has spread to the rest of the world and as of December 2015, the International Go Federation counted 75 member countries, 67 of which are outside East Asia.\n\nAlphaGo is the first program developed by DeepMind to play and exhibit superhuman performance in the game of Go. The AlphaGo algorithm uses deep neural networks and was trained using TensorFlow by supervised learning to accurately predict human expert moves. Its training was started with games of strong amateur players from internet Go servers, after which AlphaGo trained by playing against itself.\n\nAlphaGo Fan is the first published version of AlphaGo that played European champion Fan Hui in October 2015. Like most A.I. applications today, it was distributed over many machines using 176 GPUs\u00b9. This was the first time that a computer Go program had defeated a human professional player, without handicap, in the full game of Go \u2014 a feat that was previously believed to be at least a decade away. AlphaGo won the match 5 games to 0.\n\nPrior to the challenge, AplhaGo was trained over months using a combination of supervised and reinforcement learning.\n\nAlphaGo Lee is in most regards very similar to AlphaGo Fan. It won a Go challenge against 18-time world champion Lee Sedol. The challenge was played in Seoul, South Korea between 9 and 15 March 2016 and over 5 games. AlphaGo won all but the fourth game; all games were won by resignation.\n\nAlphaGo Lee was also distributed over many machines using 48 TPUs\u00b2, rather than GPUs, enabling it to evaluate neural networks faster during search.\n\nAlphaGo Zero\u2019s main difference from its predecessors is that it was only given the perfect knowledge of the game rules. No training data set based on human expert moves or past games was provided to the A.I.\n\nZero played itself starting with random moves and with reinforcement learning until it could anticipate its own moves and how they would affect the game\u2019s outcome.\n\nThe Elo rating exposed in the graph above is a scale that represents the relative skill levels of the AlphaGo learner against its different iterations in time.\n\nAlphaGo Zero was trained in 3 days with 2.9 million self-play games generated. It outperformed AlphaGo Lee after just 36 h defeating it by 100 games to 0.\n\nAlphaGo Zero used a single machine with 4 tensor processing units (TPUs), whereas AlphaGo Lee was distributed over many machines and used 48 TPUs.\n\nFrom DeepMind\u2019s paper on Zero, it is apparent that the novel aspect of this version of AlphaGo is the reinforcement learning algorithm.\n\nReinforcement learning, put (very) simply, through a loss function, lets your algorithm evaluate how close it was to finding the right answer to a question by rewarding the learner.\n\nWhile ML methods like Backpropagation and Monte Carlo Tree Search have been around for a while, they had not necessarily been applied in this particular fashion before.\n\nIn this elegant solution, DeepMind has built a better Go player with an order of magnitude less data, computation, and time, using just reinforcement learning \u2014 it\u2019s a pretty big deal.\n\nI believe the AlphaGo experiments provide us with a taste of where A.I. is at and challenges the view that A.I. is only valuable when in possession of vast amounts of data sets. Zero shows us that within a few days, using a single machine and with only the rules of a game at hand, a learner can rediscover thousands of years worth of knowledge and strategy and provide new insights into the oldest of games. But really where does that leave us?\n\nHigh cognition activities\n\nHistory has shown that every time a machine succeeds at a at a high cognition activity (Deep Blue, Watson), there are expectations that we have leapt towards Artificial General Intelligence.\n\nJust like Profs. Clifford Nass and Byron Reeves exposed in The Media Equation, there is something about machines producing answers that has the human brain ascribe way more authority to it that there necessarily should be.\n\nHowever, it is undeniable that AlphaGo Zero is a major breakthrough in A.I. but can we take the breakthroughs DeepMind has had here and apply them to other domains?\n\nDomain applicability\n\nIn the case of Zero, the learner used is applied to a domain where the rules are perfectly known.\n\nThe same type of learner can be applied to other domains but arguably, it might be more successful where rules are well known e.g. sales forecasting, material design or cybersecurity.\n\nIt is not clear today whether DeepMind\u2019s experiments are key to approaching Artificial General Intelligence. Zero is an elegant solution and we know it worked very well in a particular case. The game of Go, while very complex, is a very well known domain where rules are well known and moves are codified. Outside of these parameters, there\u2019s no guarantee of success.\n\nA.I., new ventures and investors\n\nZero brings about a great opportunity for AI ventures. Until now, companies had to raise money to acquire data sets, annotate them and train models.\n\nThis breakthrough means they can do more with less i.e. run many more A.I. experiments with less data, in less time and for less money. At the current pace of Cloud commoditisation, even scalable Cloud TPUs now available on Google Cloud Platform to speed up Machine Learning workloads \u2014 all you need is a valid credit card.\n\nGoogle has also provided the research community with a cluster of 1000 TPUs that can be used for free in the form of TensorFlow Research Cloud.\n\nAs DeepMind co-founder, Demis Hassabis explains, Zero was not programmed to understand Go specifically, it could be reprogrammed to discover information in other fields: drug discovery, protein folding, quantum chemistry, particle physics, and material design.\n\nDeepMind has already begun using AlphaGo Zero to study protein folding and has promised it will soon publish new findings. Misfolded proteins are responsible for many devastating diseases, including Alzheimer\u2019s, Parkinson\u2019s and cystic fibrosis.\n\nWe should keep an eye on DeepMind\u2019s next publications. It\u2019s future discoveries while always contributing to the larger A.I. field, might also help the company pay its way out of the \u00a396m loss reported last year.\n\nAs Pedro Domingos writes in The Master Algorithm, the quest for a learner that could answer any type of question is something which we should all be keen to see through. A Master Algorithm can do a lot of good in this world and we should be all think about the impact of such technology in our lives and in those of the generations to come\u2026"
    },
    {
        "url": "https://becominghuman.ai/understanding-and-building-generative-adversarial-networks-gans-8de7c1dc0e25",
        "title": "Understanding and building Generative Adversarial Networks(GANs)- Deep Learning with PyTorch.",
        "text": "We\u2019ll be building a Generative Adversarial Network that will be able to generate images of birds that never actually existed in the real world.\n\nBefore we actually start building a GAN, let us first talk about the idea behind GANs. GANs were invented by Ian Goodfellow, heobtained his B.S. and M.S. in computer science from Stanford University and his Ph.D. in machine learning from the Universit\u00e9 de Montr\u00e9al,. This is the new big thing in the field of Deep Learning right now. Yann LeCun, the director of Facebook AI said :\n\nNeural Networks are good at classifying and predicting things, and AI Researchers wanted to make the neural net more human in nature by allowing it to CREATE rather than just letting it see things, and turns out that Ian Goodfellow was successful in inventing a class of Deep Learning Model which could do that.\n\nGANs contain two separate neural networks. Let us call one neural network as \u201cG\u201d, which stands for Generator and the other neural network as \u201cD\u201d, which is a Discriminator. The Generator first generates random images and a Discriminator sees those images and tells the Generator how real the generated images are.\n\nIn the starting phase, a Generator model takes random noise signals as input and generates a random noisy image as the output, gradually with the help of the Discriminator, it starts generating images of a particular class that look real.\n\nThe Discriminator which will be the opponent of Generator is fed with both the generated images as well as a certain class of images at the same time, allowing it to tell the generated how the real image looks like.\n\nAfter reaching a certain point, the Discriminator will be unable to tell if the generate image is a real or a fake image, and that is when we can see images of a certain class(class that the discriminator is trained with.) being generated by out Generator that never actually existed before."
    },
    {
        "url": "https://becominghuman.ai/boost-e-commerce-sales-by-giving-your-customers-more-ways-to-search-3d9a6bc8c31c",
        "title": "Boost E-Commerce Sales by Giving Your Customers More Ways to Search",
        "text": "Boost E-Commerce Sales by Giving Your Customers More Ways to Search\n\n\u201cA lot of the future of search is going to be about pictures instead of keywords\u201d \u2014 @Pinterest CEO Ben Silbermann\n\nIt\u2019s true. It isn\u2019t that we\u2019re all tired of typing, it\u2019s just that there is so much data out there to search that sometimes keywords are insufficient for finding what you\u2019re looking for.\n\nFortunately for us, a picture is worth 1000 words. Turning a picture into a vector and searching by it really means translating it into lots of very specific keywords. So if you\u2019ve got a lot of image-based assets that you want your customers to find, the first step is to make sure they have relevant keywords associated with them. But to really accelerate your site to a relevant, targeted, and useful customer experience, include a visual search capability.\n\nThe use cases are myriad. Imagine someone wants to upload a photo of a dress or handbag he or she saw in a movie or television show and search your store for similar items. Or perhaps a car with a certain look, food recipes with a certain presentation, or even stock images to match the composition of other work. Visual search has powerful ramification for relevance and search.\n\nGoogle images has had this capability for years. However, it tends only to show results of exact matches to that image. But what if you want to build something more robust, and of course have it embedded into your site or UX? Machine learning is expensive and requires a lot of expertize and time to go from a proof-of-concept to full production, right?\n\nIn fact, I\u2019d wager you could build a working version of visual search into your site in less than a week. My startup Machine Box is designed to help solve this kind of problem really quickly.\n\nPlay with this \u2014 then come back and see how we did it. Or just check out this video below:\n\nUsing our image recognition system, you can quickly teach a machine learning model all of your products. You don\u2019t have to label anything, just run a script that posts every photo to Tagbox\u2019s teach endpoint. Then, just add a place on your site that lets customers upload a photo. Post that photo to Tagbox\u2019s similarity endpoint. You\u2019ll get back a list of your own images that are visually very similar. Don\u2019t take my word for it, check out this guide on how we built an implementation of this in a couple of hours, and then head on over to Machine Box and give it a try."
    },
    {
        "url": "https://becominghuman.ai/utilizing-beacons-and-ai-to-enhance-the-brick-mortar-experience-b0914eab4ccd",
        "title": "Utilizing Beacons and AI to Enhance the Brick & Mortar Experience",
        "text": "Stores are always seeking ways to connect with customers \u2014 and cut through the noise. What if you could upsell a customer while he or she is on the premises, increasing the chance that he is interested, and might actually buy the item?\n\nYou can, with the help of beacons.\n\nBluetooth beacons are transmitters. They are hardware devices that broadcast a signal, which can be read and interpreted by nearby smartphones, tablets, smartwatches, and other devices. The devices come in a variety of styles, such as USB sticks, dongles, plastic rock-shaped objects, or even keychains. Beacons are usually powered by batteries, but some are USB or solar-powered.\n\nBeacons use Bluetooth Low Energy (BLE), which is a little different than the Bluetooth you\u2019re probably already using for some devices. Like Classic Bluetooth, BLE transmits data over short distances, but uses less energy \u2014 lasting up to three years on a coin-sized battery. BLE is cheaper than Classic Bluetooth. However, BLE can\u2019t handle complex applications that require constant communication and data exchange. Instead, it\u2019s ideal for simple tasks. Also, beacons aren\u2019t usually connected to the Internet.\n\nBluetooth beacons are a one-way transmitter, which is also different from some other location technology. That means beacons send a signal, but the devices do not send information back to the beacon. For someone to receive the signal, they must install an app to interact with the beacons. The beacons themselves will not track users but will send a signal to the installed app. This is also helpful for store managers, who must obtain permission to send push notifications.\n\nBoth Google and Apple have Beacon platforms \u2014 Eddystone and iBeacon, respectively. Although Google\u2019s is open-source and Apple\u2019s is not, both work across the platforms. However, Google and Apple don\u2019t actually make the beacons, which you can find from a variety of manufacturers for between $10 and $30 each.\n\nBeacons have many uses in businesses and offices. For example, managers at a manufacturing facility, could install beacons as a way for employees to clock in as they walk through the door, instead of punching in somewhere. Beacons are also appearing in arenas, conferences, museums, and even schools and hospitals.\n\nRetail stores have been early adopters of beacons for a good reason: they enable stores to connect with customers, stay top of mind, and send messages. The Rite Aid drugstore chain announced it will install beacons in 4,500 U.S. locations. Facebook has its own beacons, free for businesses so they can trigger Facebook Place tips.\n\n\u25cf Track customers \u2014 Find out how long someone spends in your store and which areas of the store they visit.\n\n\u25cf Trigger a location-based action. A beacon might encourage someone to check into that location on social media.\n\n\u25cf Send messages or notifications at a specific point, such as near a particular area of the store, or send a notification about a sale taking place as someone passes nearby. For example, if your clothing store is having a sale on coats, a beacon could send a message only as someone walks near the coat racks. These messages are opened more often because they are relevant. Early studies on this have shown that beacons can increase sales.\n\n\u25cf Mobile payments \u2014 Connect beacons to your point of sale system.\n\nBeacons can help you gather data about your customers\u2019 behavior, which a valuable asset for a company. Once you have a large data set, you can send more targeted messages. Without AI, your beacon might \u201csee\u201d Customer Joe walk into your store and send him a message that says, \u201cYou have three days left to use your loyalty points before they expire.\u201d The program recognizes Joe and can pull his data. AI can help connect Joe to other events, such as his friend\u2019s birthday coming up or the fact that he just got a new job and therefore may need new clothes.\n\nOne company is trying an application that gives users the power to click on a beacon ad and then get more information through a chatbot, which can then lead them through the purchase \u2014 as they are standing on the sidewalk or sitting in a coffee shop. Amazon is combining beacons and other sensors in its Amazon Go store with AI to learn what items customers pick up and for how long and whether they ultimately put the item in their carts.\n\nThese are just a few of the possibilities. Because beacons and AI are newer to many businesses, there is a lot of potential to think big. Talk to us about connecting beacons and data for your company.\n\nThis article was originally published on the Imaginovation blog."
    },
    {
        "url": "https://becominghuman.ai/preparing-for-artificial-intelligence-d0b644087537",
        "title": "Preparing for Artificial Intelligence \u2013",
        "text": "We recently announced our first annual conference at Prepare.Ai and are busy assembling the agenda, speakers, and presentations. A few friends and colleagues have asked me to provide a bit more detail on the goals and objectives of our organization.\n\nArtificial Intelligence is expanding in capability and influence at an astonishing rate across virtually every industry. Like any technology that can do things faster, better, and cheaper, this is an exciting development. This means new and improved products and services for all of us. But the speed and scale of AI is such that some long-standing norms are facing disruption. At the Prepare.Ai Conference on May 8, 2018 in St. Louis, we are gathering thought leaders from industry and academia to unpack these opportunities and challenges, and then explore what each of us can do to prepare for this bold new future.\n\nMachines have historically improved our standard of living through the automation of human muscle. Artificial Intelligence is now enabling the automation of human cognitive tasks.\n\nTasks that involve prediction, pattern recognition, visual perception, language, and combinations of all of the above are increasingly being added to the resumes of AI systems.\n\nPerhaps the most prominent example of applied AI in the news today is the sensor-laden, self-driving vehicle. Once it emerges from the numerous pilot and testing programs ongoing already, it will allow commuters to more safely and efficiently traverse city streets while unlocking new time for both productivity and leisure.\n\nIn the healthcare arena, some AI algorithms are diagnosing diseases with greater accuracy than a panel of doctors, while others are serving to personalize medicine for a more relevant, individual fit. See the article The Promise of AI in Healthcare written by our Board President, David Karandish.\n\nThe AI that I am most familiar with is natural language processing (NLP). David and I also work together at the company he founded, Ai Software, with a team of data scientists, designers, and developers building \u201cJane,\u201d a virtual team member that will provide access to all of a company\u2019s information in the simplest way possible \u2014 through natural language chat. Once Jane has access to a company\u2019s data and applications, the appropriately authorized users can access and inquire 24/7/365 from any device, all without interrupting one of their team members.\n\nJeff Bezos, CEO of Amazon, says this about the potential of AI: \u201cWe are now solving problems with machine learning and artificial intelligence that were\u2026in the realm of science fiction for the last several decades\u2026it really is an amazing renaissance.\u201d (See amazon-jeff-bezos-artificial-intelligence-ai-golden-age.html)\n\nBy its very nature, technology has disrupted the status quo over and over throughout history. So this is certainly nothing new, but disruption always poses challenges. Let\u2019s zoom in on a related example in textiles and clothing manufacturing. At first, this was a labor-intensive process, where workers would manually sew cloth into a small number of high-cost garments. When the power loom was invented, however, entrepreneurs could earn more money with less labor, and a large number of those manual jobs were eliminated. \u2026But the story doesn\u2019t stop there. In response to higher productivity, better quality, and lower prices, consumer markets began to demand more clothing. Soon, production would far outstrip pre-automation levels, and new high-wage jobs would be created, such as: clothing designers, marketers, machinists, business managers, maintenance professionals, and so on.\n\nThis same pattern has taken place faster and faster since the first industrial revolution. Each time, the eliminated jobs were replaced by new jobs, often filled by younger workers who had learned the newly desired skills in school.\n\nThe speed of disruption from artificial intelligence today is automating many tasks at a faster rate than previously observed, threatening to strand some mid-career workers in obsolescence. The reach of disruption is also noteworthy, transforming the nature of select tasks across all sectors and industries.\n\nI believe that history has already shown us the appropriate response to changes and challenges such as these: Adaptation. The prevailing paradigm that we are faced with is continuous innovation, and we must respond with continuous, data-driven learning.\n\nIf AI is the way to competitively improve products and services in a global marketplace, we must embrace it. If AI encroaches on our values and institutions, we must harness and control it. If AI causes technology cycles to quicken beyond once-per-generation, we must reimagine our once-and-done educational system. If AI\u2019s are working among us, we must learn how to collaborate best with them.\n\nTo realize this vision, we need grassroots movements. Communities of people exchanging ideas at the water cooler, at coffee shops, and at startup incubators. Courageous experimentation with new AI technologies and business models. Open events, meetups, informal lunches, and social media conversations. The audacity to follow your personal curiosity.\n\nWe want Prepare.Ai to be a driving force in this movement, helping individuals and businesses to adapt and prepare.\n\nOur annual conference will be the centerpiece of this effort. Attendees will be a diverse group of professionals, developers, scientists, executives, professors, and students from all walks. Through the event, we will foster a regional and national hub to broker the import and export of AI ideas, relationships, intellectual property, resources, data sets, case studies, academic findings, and industry leadership.\n\nOne unique aspect of our conference is that each session will be tagged on a sliding scale between technical and business tracks \u2014 so you can get access to the right content at whatever level you care about. Because of the cross-cutting and horizontal nature of AI, both tracks are probably relevant to you and your organization.\n\nIf any of the issues mentioned in this article strike a chord with you, I think you should be there on May 8 to take part in the discussion and make a contribution. I hope to see you there!"
    },
    {
        "url": "https://becominghuman.ai/the-apple-of-my-ai-gdpr-for-good-3fc2ab66642a",
        "title": "The Apple of my AI \u2014 GDPR for good \u2013",
        "text": "A new type of black box learning means we may not need human data at all. Falling into the category of \u2018deep reinforcement learning\u2019, we are now able to create systems which achieve super human performance in a fairly broad spread of domains. AIs are able to generate all training data themselves from simulated worlds. The poster-boy of this type of machine learning is AlphaZero and its derivatives from Google\u2019s Deep Mind. In 2015 we saw the release AlphaGo which demonstrated the ability for a machine to become better than a human in a 5\u20130 victory against Go (former) champion Mr Fan Hui. AlphaGo reached this level by using human generated data of recorded professional and amateur games of Go. The evolution of this however was to remove the human data with AlphaGo Zero, beating its predecessor AlphaGo Lee 100:0 using 1/12th the processing power over a fraction of the time, and without any human training data. Instead AlphaGo Zero generated its own data by playing games against itself. While GDPR could force a drought of machine learning data in the EU, simulated data from this kind of deep reinforcement learning could re-open the flood gates.\n\nPlaying Go is a pretty limited area (though AlphaZero can play other board games!) and is defined by very clear rules. We want machine learning which can cover a broad spread of tasks, often in far more dynamic environments. Enter Google\u2026 again\u2026 Or rather Alphabet, the parent company of Google and their self-driving car spinoff Waymo. Level 4 and 5 autonomous driving presents a much more challenging goal for AI. In real time the AI needs to categorise huge numbers of objects, predict their paths in the future and translate that into the right control inputs. All to get the car and it\u2019s passengers where they need to be on time and in one piece. This level of autonomy is being pursued by both Waymo and Tesla, but seemingly Tesla gets the majority of the press. This has a lot to do with Tesla\u2019s physical presence.\n\nTesla has around 150,000 cars on the road equipped and boasted over 100 million miles driven by AutoPilot by 2016. This doesn\u2019t even include data gathered while the feature is not active or more recent data (which I am struggling to find \u2014 if you know please comment below!). Meanwhile Waymo has covered a comparatively tiny 3.5 million real world miles, perhaps explaining the smaller public exposure. Google thinks it has the answer to this, again using deep reinforcement learning, meaning that their vehicles have driven billions of miles in their own simulated worlds, not using any human generated data. Only time will tell whether we can build a self-driving car, which is safe and confident on our roads alongside human drivers without human data and guidance in the training process. The early signs for deep reinforcement learning look promising. If we can do this for driving, what\u2019s to say it can\u2019t work in many other areas?\n\nBeyond being a tick in the GDPR box there are other benefits to this type of learning. DeepMind describes human data as being \u2018too expensive, unreliable or simply unavailable\u2019, the second of these points (with a little artistic license) is critical. Human data will always have some level of bias, making it unreliable. On a very obvious level, Oakland Police Department\u2019s \u2018PredPol\u2019, a system designed to predict areas of crime to dispatch police, trained on historical and biased crime data. It resulted in a system which dispatched police to those same historical hotspots. It\u2019s entirely possible that just as much crime was going on in other areas, but by focusing its attention on the same old area and turning a blind eye to others the machine struggled to break human bias. Even when we think we\u2019re not working on an unhealthy bias our lives are surrounded by unconscious bias and assumptions. I make an assumption each time I sit down on this chair that it will support my weight. I no doubt have a bias towards people similar to me, believing that we could work towards a common goal. Think you hold no bias? Try this implicit association test from Harvard. AlphaGo learned according to this bias, whereas AlphaGo Zero had no bias and performed better. Looking at the moves the machine made we tend to see creativity, a seemingly human attribute in its actions, when in reality their thought processes may have been entirely unlike human experience. By removing human data and therefore our bias machine learning could find solutions in possibly any domain which we might never have thought of, but in hindsight appear a stroke of creative brilliance.\n\nPersonally I still don\u2019t think this type of deep reinforcement learning is perfect, or at least the environment it is implemented in. Though the learning itself may be free from bias, the rules and play board, be that a physical game board or rather road layout, factory, energy grid or anything else we are asking the AI to work on, is still designed by a human meaning it will include some human bias. With Waymo, the highway code and road layouts are still built by humans. We could possibly add another layer of abstraction, allowing the AI to develop new road rules or games for us, but then perhaps they will lose their relevance to us lowly humans who intend to make some use from the AI.\n\nFor AI, perhaps we\u2019re beginning to see GDPR as an Apple in the market, throwing out the old CD drive, USB-A ports or even (and it still stings a little) headphone jacks, initially with consumer uproar. GDPR pushing us towards black box learning might feel like we\u2019re losing the headphone jack a few generations before the market is ready, but perhaps it\u2019s just this kind of thing that creates a market leader."
    },
    {
        "url": "https://becominghuman.ai/fintech-trends-to-look-out-for-in-2018-b1135674fad3",
        "title": "Fintech Trends to look out for in 2018 \u2013",
        "text": "Although the mysteriousness that surrounds bitcoin might cause some companies to be wary of using it, the fact remains that the digital currency is officially out in the market. Even being listed on the stock exchange.\n\nWith all the highs that bitcoin has seen in 2017, our prediction is that it will give way for other cryptocurrencies like Ethereum and Litecoin to gain power. There are speculations that the market cap of Ethereum will surpass even that of Bitcoin by next year.\n\nCompanies and governments have been focused on understanding how to integrate this into their systems to not be left out. Even more investments have been made to develop solutions of their own. In this new year, we will start seeing more experimentation and real usage of cryptocurrencies.\n\nWe have seen how this technology has pushed the limits of science and technology by machine learning, robotics, and chatbots.\n\nAs experimentations continue, we will start seeing processes being automated leading to diminished errors and costs. In 2018 it will add value in the form of analytics and big data to enhance decision making and improve the speed of services.\n\nAI will not be just a \u201cnice-to-have\u201d in 2018 but a \u201cmust-have\u201d.\n\nAlthough the focus of digital transformation has been on the front-end, our prediction is that 2018 will use technologies to power the back-end processes and systems to offer a fully seamless experience for the customer.\n\nTo deliver a transformed business model, institutions must integrate across touch-points and channels to deliver real-time customer engagement."
    },
    {
        "url": "https://becominghuman.ai/face-recognition-realtime-masks-development-9d3a399b4c3",
        "title": "Face recognition: realtime masks development \u2013",
        "text": "In the beginning of December 2017 one of the offices of Akvelon company in Russia, Kazan city held a hackathon event named HACKVELON. Ideas for the event were collected during 2-weeks and reviewed by office leads. Each developer had an opportunity to vote for the project he/she liked and participate in it. There were only two limitations - using Cutting-edge technologies and team size (max 5 people on one project).\n\nI\u2019ve started my research of interesting popular technologies, frameworks, mobile applications and found a lot of articles with reviews related to realtime masks on Snapchat and Facebook. For me it was absolutely new area and a big challenge to create something interesting using such technologies on one week.\n\nPeople care about their look. A bad haircut is something that can disappoint everyone.\n\nDevelopment of the mobile application for clients of barbershops for haircut, mustache, beards selection using real-time masks which gives an ability to SEE your haircut on your face before haircutting started.\n\nThe user selects a hairstyle that he/she likes from our set and the application substitutes the mask in real time video stream using the smartphone\u2019s front camera. We called it PocketBarber. Let me show you how it\u2019s became a reality\u2026\n\nThe facial recognition process normally has four interrelated phases or steps. The first step is face detection, the second is normalization, the third is feature extraction, and the final cumulative step is face recognition.\n\nDeep Learning (using multi-layered Neural Networks), especially for face recognition, and HOGs (Histogram of Oriented Gradients) are the current state of the art for a complete facial recognition process.\n\nNote: Our team decided to use open source libraries to commercialize the project in future."
    },
    {
        "url": "https://becominghuman.ai/impact-of-modern-automation-on-employment-by-utpal-chakraborty-e384a5cfebc3",
        "title": "Impact of Modern Automation on Employment, By-Utpal Chakraborty",
        "text": "Automation is gradually proliferating almost every aspect of our lives and almost every industry. Whether it is banking and financial, automobile, aviation, manufacturing, customer service, healthcare & medicine, it\u2019s everywhere and almost every day automation is conquering more and more new fields. Some experts and economists already forecasted that there\u2019s going to be some degree of unemployment in many sectors due to Automation and Artificial Intelligence which they term as \u201cTechnological Unemployment\u201d.\n\nThe most important question that arises today while an organization planning to introduce any modern automation techniques is how the employees affected will react to it? Will they form a compatibility with those tiny automation programs or they will start seeing them as a competition? Believe me, every management has gone through this sensitive aspect while deliberating an automation initiative in their organizations.\n\n But good news is, if we look back in the history, many of the great inventions happened in last 200 years has been designed to replace human labor. And in numerous occasions, the scholars, experts & economists had raised an alarm telling \u201cWe will be running out of jobs and we are making our skills obsolete\u201d. \n\n But fortunately, every time those alarms turn out to be false in the past.\n\nFor an example, when the Automated Teller Machine (ATM) was introduced to automate the basic and routine banking functions, it did not destroyed the role of a Bank Teller; rather banks operated more efficiently and Human teller role shifted to do more complex tasks. As a result allowing banks to open more and more branches than it had been before leading to an overall increase in the amount of banking jobs.\n\nIs it going to be the same this time?\n\nAs per a report published by World Bank, estimates around 65% of Indian service sector jobs are at risk due to automation. And India isn\u2019t alone, in China it would be around 77% and similarly other countries. In Indian IT Service Sector alone, approximately 6.5 lakhs low-skilled positions are in danger.\n\nThese experts also forecasted that the new wave of modern automation is \u201cBlind to the Color of your Collar\u201d. That means it\u2019s going to affect both Blue as well as White Collar jobs. More and more routine office jobs like customer support and clerical work will be affected the most, but even other semi-skilled and skilled jobs are also not fully safe.\n\nLet\u2019s analyze why this time it\u2019s different than the past false alarms.\n\nIndustrial revolution has given the \u201cMechanical Power\u201d to the machines, the power to do repetitive things in much larger scale, faster and with more efficiently than a normal human can do. IT revolution in 80\u2019s and 90\u2019s has given \u201cComputational Ability\u201d to the machines, ability to process repetitive computational tasks in much faster and efficient way. But this time, Artificial Intelligence has given the \u201cCognitive Ability\u201d to the machines. Cognitive skills include, ability to learn, analyze, reason and apply those learnings. So, machines and tools we are using are becoming more and more intelligent. Is it going to be a real threat for our employment?\n\nAnother group of experts has some different opinion, they argue that in the long run automation actually creates more jobs than what it eliminates. Machines are best at completing repetitive tasks and increase overall productivity which frees up lot of time for the employees to focus on \u201cIntellectually Challenging\u201d and \u201cCreative Works\u201d. And automation and Artificial Intelligence will essentially complement human skills and capabilities rather than completely replacing them. They also believe that it\u2019s going to be an era of \u201cMan & Machine Partnership\u201d. Although, Artificial Intelligence will eliminate or reduce jobs which are repetitive in nature and less intellectually challenging but new forms of employment will take their place.\n\nSo, Automation in many cases helps redefining jobs rather than eliminating those entirely. Considering this shift in paradigm, only a Four Years college degree may no longer be sufficient for a lifelong employment in the future. Rather it\u2019s going to be Lifelong Learning and acquiring skills throughout life to remain relevant in the industry. Automation is not about altogether replacing the human element, but about elevating the role people play and the value they bring to their job roles. While there are some strategic challenges that automation may bring with itself, it has become a necessity for almost all organizations today and we need to also consider the merits it will have on the organization."
    },
    {
        "url": "https://becominghuman.ai/a-deeper-understanding-of-nnets-part-2-rnns-b32240998fa9",
        "title": "A deeper understanding of NNets (Part 2) \u2014 RNNs \u2013",
        "text": "Last week we talked about a very particular type of NNet called Convolutional Neural Network. We can definitely dive deeper into Conv Nets but the essence of the topology was broadly covered in the previous post. We will revisit the Conv Nets after we have covered all the topologies, as discussed in the previous post.\n\nThe architecture for this week is Recurrent Neural Network or RNN. The key difference between a RNN and any Feed Forward Normal/Deep Network is the recurrence or cyclic nature of this architecture. It sounds vague in the first go but lets unroll this architecture to understand it better. We will also be discussing two special cases of RNN namely LSTM and GRU in the next post.\n\nLets take a use-case of RNN, Natural Language Processing (NLP), traditional NLP techniques used statistical methods and rule-based approach to define a language model. Language models computes a probability for a sequence of words: P(w1, w2, \u2026.. wn) which is useful in machine translation and word prediction.\n\n3. To estimate any probability they had to compute n-grams.\n\nComputation of so many n-grams has HUGE RAM requirements, which gets practically impossible after a point. Also, above models relied on hand engineered linguistic features to deliver state-of-the-art performance.\n\nRNNs solve the above problems by using a simple solution called \u201cstatefulness\u201d and \u201crecurrence\u201d. Deep Learning allows RNN to remember or forget things based on few logical values, as we will see later, and perform cyclic operations within the network to achieve better results. Before we start exploring how all this happens, lets first understand a crucial input that goes into RNN, Word Embeddings."
    },
    {
        "url": "https://becominghuman.ai/how-machine-learning-and-artificial-intelligence-will-revolutionize-our-world-9a8212977a63",
        "title": "How Machine Learning and Artificial Intelligence will revolutionize our world.",
        "text": "Design:\n\nLet\u2019s say you run a company that makes handbags. You have a design in mind and it\u2019s all done apart from the colour scheme.\n\nObviously your colour needs to fit it\u2019s audience, but what do your audience want? You could spend time and money creating a survey asking a random selection of people to choose what colour they would like on a handbag, but because you have asked a random selection of people the chances are many of them don\u2019t know anything about fashion or handbags. To filter out those who know nothing about fashion would lessen the number of results significantly so what could you do instead?\n\nYou (your company) could write a program that gathers images of handbags from popular e-commerce and fashion sites. You gather X amount of images of one, two, three, four and five star handbags.\n\nYou use these images to train a neural network (I won\u2019t go into details about them but you can read more here: Neural Network). You now have a program that can differentiate between what makes one star and five star handbag. You could make several samples of the handbag and see which one is better or you could super impose the the colour onto a picture of a white version of your handbag.\n\nSoon your program returns the best possible colour. You know have a handbag that should hopefully prove to be very popular, but why stop there? Instead of just one colour why not trial colour schemes? Or entire handbags?"
    },
    {
        "url": "https://becominghuman.ai/5-tech-trends-that-will-dominate-digital-media-in-2018-545cad03204",
        "title": "5 Tech Trends That Will Dominate Digital Media In 2018",
        "text": "5 Tech Trends That Will Dominate Digital Media In 2018\n\nAs artificial intelligence (AI) becomes more mainstream, its use in digital media will grow at a faster clip. In 2018, we will see more and more companies beginning to experiment with AI-powered tools and invest in AI-related technologies. With chatbots, voice search, virtual assistants, automated content curators, AI-generated content, augmented reality, and more, technology is once again poised to change the lives of marketing and communication professionals.\n\nAI is already playing a noteworthy role in all areas of content development and management \u2014 from discovery to curation and sharing. AI is powering content marketing services take content discovery to the next level using social search by letting content managers find the content that resonates with their audience and helping them curate it swiftly and effortlessly to improve engagement and content performance. Tools such as BuzzSumo now use machine learning to help improve and simplify the work of a typical digital agency that searches, tracks, filters, and analyzes the impact of the content they create and share.\n\nContent robots are finally here, and AI is helping them write intelligent narratives using automated storytelling technology and natural language generation (NLG).\n\nFor instance, Automated Insights is a company that is helping companies automate over 1.5 billion narratives annually through Wordsmith, which it calls \u201cthe world\u2019s most powerful natural language generation platform.\u201d Its API for NLG today can generate millions of narratives in a matter of milliseconds \u2014 from hotel descriptions and city guides to daily weather forecasts and sports recaps.\n\nAnother big player in content AI is Narrative Science, the developer of Quill, a program it says humanizes data with technology that \u201cinterprets your data and transforms it into \u2018intelligent narratives\u2019 at speed and scale.\u201d Quill is currently writing over a million words a day for companies such as Forbes, Credit Suisse, and Groupon.\n\nEven many traditional media houses and news agencies have deployed AI-powered writing bots: There\u2019s The Washington Post\u2019s in-house automated storytelling technology, Heliograf, which writes news stories and creates social media posts. The Associated Press now produces 3,700 quarterly earnings stories with the help of AI writers. Then, there\u2019s the Los Angeles Times\u2019s QuakeBot, which churns out quick and short news posts and tweets related to earthquakes with real-time data from the U.S. Geological Survey.\n\nIf you\u2019re surprised that content written by artificial intelligence is this advanced, try it for yourself: Go to ai-writer.com and feed their AI Author a headline, and it will do all the research and writing work for you. I just got a 200-word article written on Hinduism, which the bot emailed me in an hour. It\u2019s not bad, but still quite far from an original article on the topic written by an expert.\n\nLong ago, Gartner predicted that \u201cby 2020, customers will manage 85 percent of their relationship with the enterprise without interacting with a human.\u201d While chatbots could soon be one of the most basic consumer applications of AI, they are not intended to completely take over customer conversations \u2014 and customer service jobs \u2014 but to simplify those interactions, offer basic information and perform other simple jobs.\n\nAccording to a Salesforce Research study, 40 percent of customers say they sometimes have trouble finding information online on their own about a product or service, while 25 percent say they often or always have trouble. Facebook is already offering to deliver businesses automated customer support, e-commerce guidance, content and interactive experiences through chatbots within Facebook Messenger. With advances in AI, bots will become smarter and chatbots will find more widespread use in social media and across all digital platforms in 2018.\n\nA more advanced use of AI has come to life in the voice search space. Companies may need to revisit their SEO strategy in 2018 and beyond to cater to voice searches via AI-powered assistants such as Siri or Cortana, and devices like Amazon Echo and Google Home. Last year, Google said that voice searches made up 20 percent of all searches. Communicators and digital marketers should prepare for this new trend in search user interfaces by prioritizing a voice-first approach to search marketing.\n\nOne technology that can bring to life a truly digital experience on screen like no other is augmented reality (AR), and brands are showing great interest in incorporating it in their product showcase. Apple\u2019s new iOS 11 operating system on its latest iPhones and iPads are bringing to life the AR experience to millions of users, while its ARKit and the Android ARCore are empowering developers to create AR experiences for their customers and brands.\n\nAR is creating new opportunities in experiential marketing by enabling marketers to explain the unexplained and influence impulse buyers like never before. On the other hand, AR is helping potential customers with a more definitive try-before-you-buy experience to make more informed decisions. AR can be a real game changer for any brand that wants to take digital marketing to the next level.\n\nThis article by me first appeared in the January 2018 edition of the \u201cCommunication World\u201d Magazine of the International Association of Business Communicators (IABC). Members can access my full article here\u2026"
    },
    {
        "url": "https://becominghuman.ai/10-ways-to-activate-instagram-influencers-without-sponsoring-a-post-7fdb3b7084b8",
        "title": "10 Ways to Activate Instagram Influencers Without Sponsoring a Post",
        "text": "For those companies who have tried Influencer marketing successfully or to the point of failure. Here are a few ways to pay influencers without directly paying for a single post.\n\nSometimes a post is a dud. It\u2019s not your fault for picking the wrong creator or the creator\u2019s fault for not creating good content. It\u2019s everything. It\u2019s timing. It\u2019s hashtags. Let\u2019s get more creative and enjoy a partnership with creators that goes beyond a single post.\n\nIf you found an influencer consistently posts content in your category, and is growing, then their profile is going to be viewed a lot. A single post will reach a certain amount of people their bio as well.\n\nThis can be hard to negotiate and may be pricy. A creator will many times use this space to promote their own writing or another platform. You\u2019ll need to make it worth their while to put your link in there instead of any others.\n\nWithout taking over their link, a mention to your page in their profile may incur a low cost alternative to a direct link. I have 44k followers and had 1,010 profile views in the past 7 days.\n\nYou can sponsor not the post, but the creator as whole. For a week they create their own content and in each caption you have a special spot. This could even go further for the creator and they could selling recurring spots in their captions at a low cost for the brand.\n\nThey take over your account for a week. I\u2019ve seen this successfully done in the travel industry and it works great for both entities.\n\nInfluencers gain a lot from this because they can create content in your voice for 1 week and get to drive your followers to their accounts.\n\nIf you had a different influencer each week for 50 weeks, you\u2019d see massive growth yourself!\n\nHave the creator come to your office and talk about their journey with your employees. This can be recorded as an interview with your CEO or staff. Maybe call it a \u201cTown Hall\u201d. Broadcast it live or offer it on demand.\n\nSend them to a conference where they get better, you pay for it, and they shout you out for a week while they are there.\n\nInstead of paying for an entire post or an entire trip, join with other brands to sponsor their trip. This could have huge potential as collaboration with other brands will bring you exponential attention.\n\nInstead of paying for the content and the distribution as you do when you sponsor a post, only buy the content. This can be great to target smaller growing creators on Instagram that may be amazing photographers or other type of creative artist. Now you can use their works in your commercial work because you\u2019re specifically buying those rights.\n\nGet a few fun logos made by creative Instagrammers. Maybe even get your entire staff all animated drawings of themselves to use online or on business cards. Get great illustrators to draw you an end of the year illustration.\n\nCommission Instagrammers who have arts skills to create totally unique objects. Put them in your office or in your place of business."
    },
    {
        "url": "https://becominghuman.ai/to-meet-global-ambitions-look-to-local-marketplaces-5de05af0c3df",
        "title": "To Meet Global Ambitions, Look To Local Marketplaces",
        "text": "In the age of globalization, no retailer anywhere, whether large or small, can afford to limit its ambitions to just one country. Naturally, some sellers are further than others down the road of developing an international strategy \u2014 but what none can afford to ignore is the importance of online marketplaces in developing their ability to sell across borders.\n\nAnyone in any doubt only has to look at the figures \u2014 in markets as diverse (and fast-growing) as Poland and Australia, nearly half of e-commerce trade is now taking place via marketplaces. Shift your sights to China, and you can make that figure 90%.\n\nBecause they are platforms for huge numbers of businesses, marketplaces have a strong proposition for building brand recognition, and the largest of them are among the most-visited sites on the Internet, ranking up there with Google and Facebook.\n\nWhen retailers sell their products via marketplaces, they get the power of all that traffic and brand recognition at their heels \u2014 without having to splash out for extravagant marketing.\n\nBecause they\u2019re all set up and ready to go, and have already gained the trust of local consumers in whichever new market you may be looking to move into, marketplaces allow you to enter that market smoothly and speedily.\n\nMeanwhile retailers, with an eye to their ease of use and low cost, increasingly find marketplaces an ideal arena in which to soft-launch in new territories where they still need to fine-tune their understanding of local market conditions. After all, the products that\u2026Continue reading here: https://roobykon.com/blog/posts/77-to-meet-global-ambitions-look-to-local-marketplaces"
    },
    {
        "url": "https://becominghuman.ai/deep-learning-for-sensor-based-human-activity-recognition-970ff47c6b6b",
        "title": "Deep learning for sensor-based human activity recognition",
        "text": "The choice of dataset heavily depends on our application. Generally, activities we are interested to recognize are Sitting, Standing, Walking, Running, Climbing Stairs Up, Climbing Stairs Down, etc. but I have applied HAR once to predict different yoga positions (Bosch Hackathon 2017, Finalist). So, as I said before, it only depends on our application.\n\nI have applied my approach to a dataset collected by Allan et al. (1,6 GB). This dataset contains readings from two sensors (accelerometer and gyroscope). Readings were recorded when users executed activities in no specific order, while carrying smartwatches and smartphones. The readings are from 9 users performing 6 activities (Sitting, Standing, Walking, Biking, ClimbStair-Up and ClimbStair-Down) using 6 types of mobile devices.\n\nSome common issues we face when applying HAR to a custom task are the imbalance of the dataset and the lack of enough training data. In the first case, I usually apply SMOTE oversampling technique and naturally adapt it as a data augmentation solution for the second case.\n\nWe propose a model combining a CNN (Convolutional Neural Network) and a RNN (Recurrent Neural Network). Input sensor measurements are split into series of data intervals along time. The representation of each data interval is then fed into a CNN to learn intra-interval local interactions within each sensing modality and intra-interval global interactions among different sensor inputs, hierarchically. The intra-interval representations along time are then finally fed into a RNN to learn the inter-interval relationships.\n\nThe CNN automatically extracts local features within each sensor modality and merges the local features of different sensor modalities into global features hierarchically. This beats the classical hand-crafted feature generation used in existing approaches I mentioned previously.\n\nThe RNN extracts temporal dependencies. To understand how this might be useful, I tested existing implementations by performing an activity in various positions (changing the configuration of my body). My goal was to confuse the model and get accelerometer and gyroscope values it was not used to, for that specific activity. The prediction accuracy dropped as expected and the explanation is quite simple: the model has learned to predict on the basis of raw values instead of considering how these values vary with time. But this is exactly how I think it should work because different performances of the same activity, independently of who performed it or the position used, produce approximately the same variation of Ax, Ay, Az, Gx, Gy, Gz over time. So why not take this into consideration in the training phase? This is exactly why our model features a RNN.\n\nBecause of the consistency of our architecture, our final model generalizes pretty well. The rest is up to fine-tuning.\n\nBecause building a deep learning model from scratch requires high performing computers and GPUs, it is preferable to build the model on a cloud platform. I used Google Cloud Platform for this purpose. Also, I designed the algorithm using Google TensorFlow with Python 3.4.\n\nIt is crucial today to be able to deploy machine learning models on mobile platforms. And in our case, HAR practical solutions are increasingly integrated into mobile apps. Fortunately, Google offers libraries to use TensorFlow on Android. This allows us to use models built with TensorFlow in android apps. All we need to do is \u201cFreeze\u201d the model meta-graph and export it as a file (\u201c.pb\u201d format). Using Google inference library for Android, we can now feed the exported model with real-time sensors data.\n\nWe see that we do not need to compute extra features before serving our model for prediction. We can therefore assume that the resource consumption of our technology is limited only to collecting sensors data periodically and predicting by feeding our model graph."
    },
    {
        "url": "https://becominghuman.ai/2018-austin-tech-predictions-resolutions-5e5b97a35900",
        "title": "2018 Austin Tech Predictions & Resolutions \u2013",
        "text": "What\u2018s ahead for the Austin tech / startup ecosystem for 2018? We asked 40+ city leaders for their prediction or resolution for the next 12 months (and beyond). Answers covered a variety of topics \u2014 but more AI, more engagement, and more inclusion emerged as recurrent themes. Plus most of the people queried anticipate another big year for the Central Texas scene.\n\nAmber Allen (Double A): My resolution is to help make technology more experiential; to blend the digital with the physical!\n\nDavid Altounian (St Edward\u2019s University): My prediction is that this is the year of consolidation and breakout successes with more mergers, acquisitions, and some big company growth. We have reached a critical mass of innovators and innovations here and acquirers and investors are taking notice.\n\nAngelos G. Angelou (Angelou Economics): My prediction is that in 2018 we will further expand our national brand for innovation, venture capital investing and angel and corporate funding. It is an exciting time for entrepreneurs in Austin.\n\nRichard Bagdonas (Twinedge): My prediction is that the Austin tech / startup ecosystem is going to pop with acquisitions in SaaS and CPG driving reinvestment in BI, healthcare, and AI. And Kendra Scott Jewelry will get purchased by a PE firm.\n\nBarbary Brunner (Austin Technology Council): My prediction is that we\u2019ll see at least a couple of companies prepping to IPO, our fascination with crypto will continue to grow, and we\u2019ll launch a number of new accelerator programs.\n\nShawna Butler (Singularity University): My resolution is to ampllify the entrepreneurs addressing sensitive, complex social and human care challenges like end-of-life care, living with a disability, and the social isolation epidemic.\n\nRuben Cantu (LevelUp Institute): My resolution is to share a new model on how social impact ecosystems are built and how Austin can help lead in that effort globally.\n\nLeigh F. Christie (Austin Chamber): My prediction is that our local female leaders will continue to soar and their companies and our community will be better for it.\n\nEd Curtis (YTEXAS): My resolution is to get more engaged and educated on the opportunities and challenges facing the Austin startup community.\n\nGordon Daugherty (Capital Factory): My prediction is that the Austin tech / startup ecosystem will evolve into the hub for a globally recognized and coordinated Texas startup ecosystem.\n\nCandice Digby (Austin Design Week & General Assembly): My resolution is to spur thoughtful, strategic improvements to startup-supporting infrastructures, from education & community to operations & design.\n\nJosh Jones-Dilworth (Jones-Dilworth, Inc): My resolution is to get involved politically, and especially locally. National races will dominate headlines leading up to November midterms, but our local elections matter even more.\n\nBen Dyer (TechSquare Labs): My resolution is to strengthen the bridges between the Austin and Atlanta startup ecosystems. The two cities are quite complementary and not really competitive.\n\nDavid Edmonson (Austin Tech Alliance): My prediction is that the tech sector becomes even more involved in working to address Austin\u2019s civic challenges.\n\nRod Favaron (Spredfast): My resolution is to engage more with Austin\u2019s tech leaders and groups like Culturati, Capital Factory and Techstars. I\u2019m looking forward to being more involved and giving back to the tech community.\n\nScott Francis (BP3 Global): My prediction is that the Austin tech / startup ecosystem is will have another breakout startup success in 2018, and that Indeed will continue to grow leaps and bounds, yet under the radar.\n\nAshish Gadnis (Banqu): My prediction is that the Austin tech / startup ecosystem will produce 2\u20133 amazing examples of how tech can transform lives for the homeless and create gender equality.\n\nSteve Golab (Creative Alignments): My prediction is that the Austin tech / startup ecosystem will become more segmented with divided focus on more social innovation versus only capital growth.\n\nBijoy Goswami (ATX Equation): My resolution is that we steward entrepreneurs to create completely unique ventures with Austin\u2019s \u201cbe yourself\u201d ethos to guide them!\n\nBrett Hurt (data.world): My prediction is that Austin is going to continue its epic rise with tech startups (the $250m deal with WP Engine is just the beginning). I\u2019m doing my part to help by rekindling my passion for writing and helping entrepreneurs (check out my latest post on learning).\n\nAmir Husain (Spark Cognition): My prediction is that Austin\u2019s AI companies will land at least another $100M in investments, further enriching the already booming ATX AI landscape.\n\nGerardo Interiano (Google): My resolution is is that we continue to identify ways to support our local community through initiatives that narrow the opportunity gaps and represent Austin\u2019s values.\n\nMitch Jacobson (Austin Technology Incubator): My prediction related to the Austin tech / startup ecosystem is that there will be more, more, more! More startups, more resources, more money, more activity, more excitement, more publicity for Austin!\n\nPreston James (DivInc): My prediction is that Austin will become the national leader in creating authentically inclusive tech communities.\n\nManinder Kahlon (Dell Medical School): My resolution is to connect the possibilities of AI more concretely with improving health to inspire meaningful innovation.\n\nJon Lebkowsky (Polycot Associates): My prediction is that we\u2019ll see increasing focus on emerging technologies \u2014 big data, AI, autonomous cars, mixed reality, etc. \u2014 and that we\u2019ll see the local launch of 2\u20133 potential unicorns.\n\nCindy Lo (Red Velvet Events): My resolution is to help as many early startups figure out how to best use their marketing dollars to produce memorable and meaningful events.\n\nOksana Malysheva (Sputnik ATX): My prediction is that the new tax plan will push us to become a nation of entrepreneurs and it will manifest itself in Austin in a particularly meaningful way. The rate of startup formation will go up, and we will see real benefit of that in 2018 and beyond.\n\nElijah May (The Experience Firm): My resolution is to lead by example. I intend to answer the challenge of one of my mentors and spend less time consulting and go build a tech company of my own, a company obsessed with constructive culture and complete customer experience.\n\nRobyn Metcalfe (Food+City): My prediction is that tech will finally meet food in 2018, giving Austin a place at the table for creating a new food system for the future. Also, Austin will wake up to its potential as a really smart city, including a networked, aero-dynamic food system.\n\nMike Millard (Mass Challenge): My prediction is that the Austin tech / startup ecosystem will increasingly be viewed as the Texas tech / startup ecosystem.\n\nManoj Saxena (Cognitive Scale): My resolution is to shift the perception of \u201cAI = Artificially Inflated\u201d to \u201cAI = Amazing Innovations\u201d through proven client outcomes.\n\nFred Schmidt (Bullseye Business Development): My prediction is that the Austin tech / startup ecosystem will benefit even more from being a fresh and inclusive alternative to the Brotopia cultures of The Valley, DC, NYC and LA.\n\nEugene Sepulveda (Entrepreneurs Foundation): My prediction is that Austin tech attracts even more top talent and investors as a result of our companies\u2019 commitment to fostering inclusion, environmental sustainability and social equity and the resulting boosts to performance and values.\n\nChris Shonk (ATX Seed Ventures): My prediction is that there will be a resurgence across our city\u2019s vast talent in the video game industry kindled by virtual reality and augmented reality demand.\n\nThom Singer (Conference Catalyst): My resolution related to the Austin tech / startup ecosystem is to interview more of the amazing local entrepreneurs for my \u201cCool Things Entrepreneurs Do\u201d podcast (anyone who wants to be on the show who is a CEO should reach out to me!).\n\nSuzi Sosa (Verb Inc): My resolution is to connect the women CEOs of fast-growth start-ups, like Bumble, Outdoor Voices, You Earned It, Rallyhood, and KEY Concierge, so we can build a better support network for women-led tech companies in Austin. No more bro culture!\n\nJoah Spearman (Localeur): My prediction is there will be several more meaningful acquisitions / exits in the ecosystem leading to more people moving here from Silicon Valley as valuation and metric sensitivity continues to cool down Seed and A funding.\n\nChris Valentine (SXSW Accelerator Pitch Event): My resolution is to continue to develop ways to bring more international startup companies to Austin.\n\nIngrid Vanderveldt (Empower a Billion Women by 2020): My resolution is to personally mentor at least 20 emerging women through the EBW Chapter and to bring at least 100 global tech leaders (from outside Austin) who fund and support women to Austin for the EBW Global Summit in November 2018.\n\nGordon Walton (ArtCraft Entertainment, Inc): My prediction is that we are going to see a lot more company \u201cwins\u201d this coming year. There is a giant array of local entrepreneurial companies poised for success, whether it is through hyper growth or exit.\n\nWhurley (Strangeworks): My prediction is that the Austin tech / startup ecosystem is going to grow dramatically with the addition of major new employers and new startups in non-traditional areas.\n\nPete Winstead (Winstead PC): My resolution is to get the Austin Chamber Opportunity Austin 4.0 undertaking focused on securing a least one billion dollar new enterprise located in Austin (not centered somewhere else) over each of the next ten years.\n\nHugh Forrest serves as Chief Programming Officer at SXSW, the world\u2019s most unique gathering of creative professionals. He also tries to write at least four paragraphs per day on Medium. These posts often cover tech-related trends; other times they focus on books, pop culture, sports and other current events."
    },
    {
        "url": "https://becominghuman.ai/how-to-build-a-deep-learning-server-based-on-docker-bea70b8bd2c7",
        "title": "How to build a deep learning server based on Docker",
        "text": "My setup is based on docker, so all that is necessary to get the server up and running is to install nvidia drivers, docker and nvidia-docker2 on a clean Ubuntu Server 16.04. You can clone this repository and execute build-1-nvidia-driver.sh and build-2-nvidia-docker-v2.sh. More details below.\n\nThe primary objective in setting up the software was straightforward configuration and easy maintenance. Tensorflow was a requirement. Nvidia 1080 Ti proved to be especially difficult to set up at the time of writing, as it is not compatible with drivers provided in a CUDA8.0 package (contrary to the older cards, like 1080). It works fine with CUDA9.0, but tensorflow does not support it yet (this will change with version 1.5). There seemed to be three options:\n\nNote that last two solution allow to completely skip installation of CUDA / cuDNN manually from Nvidia website! They also allow to use multiple versions of these packages on the same machine.\n\nI decided to rely on the docker solution, as it seems to be the cleanest way. It is also future-proof, currently I\u2019m using tensorflow 1.3 with CUDA8.0 and cuDNN6.0, but as soon as version 1.5 with CUDA9.0 and cuDNN7.0 support is published, all it would take to switch is update of Docker image.\n\nContainers are isolated, lightweight environments where applications can be launched. Contrary to virtual machines, they do not require their own operating system, resulting in fewer resources needed and faster boot times. Images are files which define the container, generated using Dockerfiles, and can be uploaded to the online registry like Docker Hub.\n\nDocker containers are designed to be software and hardware independent, which allows the same application to run across different hardware setups and operating systems. This would not work for applications depending on Nvidia GPU acceleration, as it clearly needs specialized hardware. Nvidia-docker is an extension of Docker which allows GPU-accelerated applications to run across machines equipped with Nvidia GPU (e.g. home desktop with GeForce GPU and AWS server with Tesla GPU).\n\nDocker tutorial is a good starting point for learning about containerization. Overview of nvidia-docker can be found here.\n\nI chose Ubuntu 16.04. Another interesting option is CentOS7. Ubuntu has a larger user base, on the other hand development of CentOS is more stable, which makes it generally more compatible. If you plan to turn your box into Docker Cloud node, the latter system may be a better option as Docker Cloud does not support Ubuntu 16.04 yet.\n\nSever version of Ubuntu would be generally a better choice, as it comes with minimal system. Following instructions assume clean server version to be installed, although there is good chance that everything will work on a desktop version. It is easy to install some desktop environment on a server version if needed (or even full unity desktop, although it is not recommended), more details below.\n\nIf using Windows on the same machine is required, it should be installed first, and then free space can be created on a hard drive by shrinking Windows partition in a Disk Management tool. For Windows and Ubuntu to coexist correctly, Ubuntu has to be installed in UEFI mode (should be possible to set Ubuntu USB stick to boot in UEFI mode in BIOS settings, installation screen should look like the one below).\n\nThere is an option to install drivers from ppa repository, although it requires specifying driver version. Installing directly from nvidia repository seems to be a cleaner solution, as a generic \u201ccuda-drivers\u201d package is installed, which I assume will be kept up to date with current driver version.\n\nCurrent version of Docker community edition has to be installed. For nvidia-docker there are two options: version 1 which is currently supported and version 2, in alpha state. I\u2019m using version 2, although I managed to deploy to AWS only with version 1.\n\nTo install docker, nvidia-docker, and optionally docker-machine for deploying to the cloud follow instructions on linked websites or use the scripts available on GitHub.\n\nAfter completing previous steps and running \u201cnvidia-smi\u201d command, state of the GPU should appear, together with the current \u201cperformance level\u201d, ranging from P8 (lowest) to P0 (highest). Lowest performance level consumes 10\u201315W when idle, while the highest around 50\u201360W. For my setup it was stuck at the highest level, and I found out that launching X server on a GPU solves it (this issue does not seem to affect server-grade cards like Tesla K80, obviously). The easiest way was just to install some minimal desktop environment, shut down, connect monitor to HDMI port on a GPU and launch again. It will result in \u201c/usr/lib/xorg/Xorg\u201d process running on a GPU, and driver functioning properly, reducing performance level to P8 when idle. Also nvidia-settings becomes accessible. It is likely possible to \u201cfake\u201d a monitor by adjusting \u201c/etc/X11/xorg.conf\u201d, but a pragmatic solution is just to keep the monitor connected, even with disconnected power supply.\n\nThis answer on askubuntu gives good overview of possible desktop environments for a server. This one worked for me:\n\nIt is a matter of personal preference, but for using a server to do basic daily tasks I could not recommend i3 window manager more. It takes some time to get used to it, but it is a great productivity booster. This is a good starting point. Installing xubuntu-core prior to i3 was still necessary to get Nvidia drivers working properly.\n\nIt is a good idea to create a script which configures a desktop environment, in case there is a need to reinstall it somewhere else (I keep mine here). My configuration is based on zsh shell with oh-my-zsh and pure prompt for integration with git and Google Chrome to get basic apps (calendar, keep, inbox, messengers). It also installs Slack and Spotify. As a final touch it displays current Nvidia GPU temperature / memory available on a status bar (together with the values for CPU / RAM)."
    },
    {
        "url": "https://becominghuman.ai/2018-forecast-the-future-is-now-39ce30d1f09b",
        "title": "2018 Forecast: The Future Is Now \u2013",
        "text": "Each new year provides the opportunity for reflection upon how far we have come and how far we still have to go, on both a personal and societal level.\n\n2017 was the year that Bitcoin \u2014 until recently seen as a plaything for paranoid libertarians and spotty nerds \u2014 showed up on your banker\u2019s Christmas wish-list. It was also the year that it took you at least a few minutes to realise the customer agent answering your queries in that little chat-box wasn\u2019t human, when you picked up a VR headset from your local toyshop for the price of a pizza, when you found yourself in far too many political arguments around the water-cooler, and when you began seriously questioning whether a computer might someday take your job \u2014 maybe for the second time that year.\n\n2018 promises a consolidation and acceleration of these leading themes in artificial intelligence, blockchain, automation, and the friction between globalizing and insular forces. We will see continuing tensions within and between countries, as 20th century nationalist sentiments push resentfully against 21st century supranational integration. There will be moments when it feels like only technology can save us, followed by events which remind us of how perilous our inventions can be when we still barely understand them.\n\nThe following is not investment or professional advice of any kind, and is intended only to promote discussion and reflection on some of the rising trends and ideas of our time. This article reflects only the opinions of the author, and the author may have his own stakes and positions in any of the assets indicated below. One should never invest more than one is willing to lose, and one shouldn\u2019t take advice on any topic in life \u2014 least of all financial \u2014 from a single internet post.\n\nLet\u2019s take a look at some intriguing and inspiring predictions for what 2018 has in store.\n\n\u2022 An entire original novel will be written by Artificial Intelligence, a feat it achieves after feeding upon hundreds of bestsellers, extracting patterns, and generating its own sentences, characters, themes and plots based on the most statistically frequent tropes and turns of phrase within these works. The resulting book will be published and widely available for sale in bookstores. This will be dismissed by technophobic critics as a publicity gimmick, the prose being derivative in the most literal possible sense, but glowing reviews will be posted online\u2026 by other AI programs.\n\n\u2022 At least one country in 2018 \u2014 likely either Venezuela, Estonia, Israel, Japan or South Korea \u2014 will launch a digital national currency, with more countries to follow over coming years.\n\n\u2022 2018 will mark the first time that a YouTube video amasses a total number of views exceeding the world population (>7.5 billion, surpassing the current single-video viewership record of 4.6 billion). The video that achieves this will be a music video for a new song released in 2018. When this happens, it will be remarked upon widely in the media and pointed to as evidence of how \u201cthe whole world is now online.\u201d\n\n\u2022 Smartphone-based augmented reality (aka \u201cmixed reality\u201d) will become a major mainstream phenomenon, with many popular apps launching in this area. Think Pokemon Go as applied to shopping, dating, advertising, gaming, networking, medicine, office work, etc. This will spawn the invention of a completely new category of advertising based on AR/mixed-reality, which will be called \u201cMixvertising.\u201d\n\n\u2022 Customizable virtual reality pornography using insertable faces of real people will become a controversial new development.\n\n\u2022 Amazon.com and other leading e-tailers will announce their acceptance of one or more cryptocurrencies as a payment method, resulting in major pumps for both their share prices and the cryptos in question.\n\n\u2022 A major disease will be cured through the technique of CRISPR gene editing.\n\n\u2022 China will announce their plan to put a Chinese astronaut on Mars within 15 years. Americans will panic at the geopolitical power statement and will respond by greenlighting unprecedented congressional funding for NASA and/or SpaceX to ensure that the first flag on Mars is American. People will begin seriously envisioning what a Mars landing will look like. Specifically, the idea will emerge that the first step taken by a human on another planet should be made by a female astronaut, serving as a poetic response to Neil Armstrong\u2019s \u201cone small step for man\u201d and bringing the story of human progress full circle. This idea will gain widespread currency, becoming an online meme and international campaign.\n\n\u2022 You will be able to buy a device that rapidly provides an accurate and detailed three-dimensional scan of any person or object from a distance. This technology will soon after become available in smartphones. Debates will ensue around privacy and intellectual property concerns as people use this to scan other people without consent, to digitize things into virtual models, and, in combination with 3D-printing, to essentially copy-paste and cheaply reproduce physical objects. This marks the rise of \u2018digibles,\u2019 the blurring of digital media and physical matter into something both digital and physical at the same time.\n\n \n\n \u2022 South Korean \u2014 North Korean relations will cool in early 2018 for the sake of diplomatic PR during February\u2019s Winter Olympics in Pyeongchang, but US \u2014 North Korean tensions rise sharply through the year.\n\n\u2022 Breakthroughs in the photovoltaic efficiency of solar panels per square inch, including the ability to convert and beam solar power via microwaves across significant distances, present new possibilities for sustainable post-oil energy systems including the prospect of launching space-based solar arrays in the 2020s.\n\n\u2022 The Catalonia independence question will be settled in the form of a 2018 referendum which the \u2018unification\u2019 (anti-secession) side wins, officially keeping Catalonia within Spain. Nonetheless, political instability remains, demonstrations continue and many businesses relocate from Barcelona to Madrid.\n\n\u2022 The most profitable investments of 2018 will be cryptocurrencies, precious metals and AI initiatives. People will not notice this at first because NASDAQ/FTSE100 will continue their record stock rallies through early 2018 and still attract most mainstream investment attention. The above three assets, at their comparatively low buy-in prices and over a 3+ year hold, will produce the greatest returns over this period.\n\n\u2022 A terrorist attack will be carried out using drones which have been modified to carry explosives and/or using untraceable 3D-printed handguns. This eventuality has been a timebomb for some years now and when this finally occurs it will prompt severe regulatory crackdowns on these technologies.\n\n\u2022 At least one national currency will completely collapse and its citizens will cope by moving cash into cryptocurrencies. Several other countries will also experience major currency devaluations, resulting in further diversion of their citizens\u2019 monies into crypto.\n\n\u2022 The Chinese government will begin offering hugely lucrative multi-billion contracts to companies that can pitch plausible solutions to the country\u2019s worsening pollution crisis.\n\n\u2022 Someone will bring a prominent case arguing for the legal right to marry their robot/AI companion. It will be dismissed by the court but will provoke wide public discussion. Legal scholars will intensify debates over whether synthetic intelligences (robots and AI) should someday have legal rights of their own and be treated like people, or at least be afforded ethical considerations at the level of animal rights. Most people laugh at the notion today, but by 2025 this subject will be treated very seriously, and by 2040 there will be individuals in numerous jurisdictions around the world who are fully legally married to synthetic intelligences.\n\n\u2022 Brexit \u2014 currently scheduled for official initiation in 2019 \u2014 will not ultimately happen, and the tide will have turned conspicuously against it by late 2018. Economic impact reports will be leaked and reveal the extent of damage which has been knowingly suppressed by its proponents. The protocol required to fully activate Brexit \u2014 Parliament subjecting the government\u2019s final withdrawal plan to a vote before any implementation can occur \u2014 will result in a majority opposing as MPs respond to mounting public pressure against Brexit, and/or a second referendum will take place with a record turnout and the Remain side winning. Currently the scheduled start date for Brexit is May 2019, but by December 2018 the various initiatives against it will have coalesced into a strong movement with signs pointing firmly to Remain.\n\n\u2022 Despite the best efforts of Robert Mueller\u2019s investigation and ever-growing criticism, Donald Trump will not be impeached and he will survive in office to complete his first term through 2020. By the end of 2018, rumours build that George Clooney will announce a serious presidential bid as a Democrat contender for the 2020 election.\n\n\u2022 The encrypted communications app \u201cTelegram\u201d becomes a global phenomenon for its widespread use among dissidents under oppressive regimes, proving its effectiveness in Iran before gaining popularity across the world.\n\n\u2022 A prototype device will be unveiled which enables pet dogs and cats to \u201ctalk\u201d with their owners, based on AI analysis of pitch/frequency/duration sound patterns corresponding to behaviours in both the individual animal and its breed, and enabling you to \u201ctranslate\u201d simple words and phrases of your own into accurate yelps, growls, meows and purrs which the animal actually understands and responds to. This will be rushed for commercial availability within the next year or two, the early models being primitive and gimmicky but the growing AI data pool making them surprisingly advanced within coming years.\n\n\u2022 Taxi and trucking companies begin putting the first generation of self-driving, self-charging electric vehicles on the road, heralding a new age of transportation while igniting anxious conversation about the looming economic impact for millions of driving-related workers around the world.\n\n\u2022 A small implantable Bio-ID chip makes its prototype debut, with the first wave of adventurous folks signing up. The device monitors various health markers, flagging them for your doctor while running machine-learning pattern-spotting algorithms through the growing data pool of ID-carriers in order to refine the correlation between symptoms and causes, vastly improving prognostic medicine. The Bio-ID also carries all your personal digital information and can be used as a passport, wallet, and the key to your car, office and house.\n\n\u2022 There will be increasingly turbulent weather and the devastation of several cities across the world as a result of worsening climate change. The Antarctic breakup hastens with the Larsen and Pine Island Ice Shelves splitting off and the rising sea levels flooding coastal cities.\n\n\u2022 The Gig Economy, the new normal for millennials and an increasingly multi-generational share of the workforce, becomes big business in 2018 through rapidly growing services that support part-time/freelance workers and entrepreneurs, especially co-working spaces such as the ever-expanding WeWork, whose valuation balloons dramatically on the back of this macro-trend. To the bemusement of boomer parents and Gen X professionals, more and more people under 35 don\u2019t seem to have \u201creal jobs\u201d so much as \u201cprojects\u201d \u2014 in most cases by necessity \u2014 which they pursue for weeks or months at a time for income and fulfilment on their own terms.\n\n\u2022 Crossrail opens in London, dramatically improving the efficiency of travelling across the city, to airports and the country. Combined with the disintegration of Brexit by early 2019, this will positively restore the quality of life in London and its image worldwide as a desirable city, with property values rising significantly in the next decade.\n\n\u2022 A drug becomes available which prevents obesity.\n\n\u2022 Anxious public discussion grows around the notion of a universal basic income system to address the imminent employment crisis posed by the rising automation of many industries, due to intensify through the 2020s. Serious trial initiatives will be formalized in this direction in one or more socialism-friendly countries. (No American politician will touch this subject towards the 2020 elections, but by 2024 it is the defining issue of that election year.)\n\n\u2022 At least one major country will begin allowing its citizens to vote in political elections using their smartphones. Despite a robust infrastructure using blockchain, eye-and-finger scans, and two-factor passcode authentication, debate grows around hacking and privacy issues.\n\n\u2022 Synthetic meat alternatives \u2014 foods that taste and look exactly like real meat, but with no animals ever involved \u2014 hit the mainstream and become big business.\n\n\u2022 Whereas Bitcoin dominated headlines in 2017, this year sees mainstream attention turn strongly to the \u2018altcoins,\u2019 Bitcoin\u2019s younger but growing family of crypto-cousins.\n\nThese other coins become household words right alongside Bitcoin in a general yearlong rush of cryptomania, with up and downs, spikes and spills, but still arriving in many cases by December at multiples beyond their January start. Investors are enticed by the notion of grabbing hold of \u201cthe next Bitcoin,\u201d in terms of a more accessible buy-in and the promise of high-multiple price gains through the year.\n\nGoldman Sachs begins officially including \u201cCrypto Assets\u201d as a new category in its clients\u2019 portfolios, and other banks start trialling the same.\n\nLay investors do the math and realise that even if 8 out of 10 selected cryptocurrencies in their portfolio go all the way to zero, a 10x growth in any two of them \u2014 which seems to be the norm for many coins in a given year \u2014 still yields an impressive ROI, making the risk-reward balance more than attractive for those willing to put a survivable 1% to 5% of their net worth into a mix of 10 altcoins.\n\nBasic Attention Token (BAT) gains traction as big companies see the benefit of being able to precisely track online ad spending for the first time. In China, NEO gains wide support from both government and major corporate partnerships, with corresponding price leaps.\n\nInvestment and institutional support floods into Ethereum, OmiseGo, Stellar, Cardano, Verge, NEM, EOS, Vertcoin, Dash, Qtum, Lisk, Power Ledger, TenX, Golem, and PIVX, along with pretty much anything in the Top-20-Market-Cap, as a rising tide lifts all boats and crypto-fever grips everyone from hedge fund billionaires in high-rises to teenagers on the schoolbus with instant trading apps.\n\n\u2022 Bionic prosthetic implants for hands, legs, ears and eyes become highly advanced and widely publicized for their prototype recipients, in some ways conferring advantages beyond \u2018normal\u2019 physiology and prompting conversations about the transhumanist evolution happening in our midst. Machines become more human and humans become more machine-like, with a growing trend of young people \u2014 particularly in Japan \u2014 gleefully blurring the line with cyborg-like implants, circuit-tattoos, and other high-tech adornments.\n\n\u2022 The 90th Academy Awards ceremony at the Dolby Theatre in March shines a spotlight on sexual harassment issues in the entertainment industry, as revealed through 2017\u2019s Weinsteingate. The Oscars make tech history by recognizing director Alejandro Inarritu as the first ever award-winner in its brand new category of Virtual Reality Filmmaking, for his 2017 work \u201cCarne y Arena.\u201d In the same way that a special new statuette for \u201cToy Story\u201d in 1995 led to the creation of the best animated feature category, this will mark VR\u2019s growing legitimacy as a new artistic medium, with many indie titles over coming years before VR movies achieve mainstream popularity with big-studio productions in the early 2020s.\n\n\u2022 The first line of \u201csmart fridges\u201d will be launched \u2014 refrigerators which are aware of all their contents and automatically order replacement groceries as needed, while monitoring your consumption levels and coordinating this with your FitBit, dieting app and the resistance level on your treadmill. The first generation will be finicky but within a few years this \u2014 as with many other \u201csmart home\u201d products like door locks linked to your smartphone or based on face-scans \u2014 will become sophisticated and widespread.\n\n\u2022 In 2018, 500 million people in the developing world will begin using the internet for their very first time, and they will do so entirely through smartphones. Africa will enter a boom of digital entrepreneurship. Fortunes will be made by people setting up tech companies in Kenya, Ghana, Uganda and other African countries which largely consist of business models copied from the developed world, in the style of Rocket Internet incubator/accelerator models. These new internet-users will begin widely using cryptocurrencies on their smartphones instead of their countries\u2019 unstable currencies.\n\n\u2022 One of the major social media networks adds a crypto-token to their \u201clike buttons\u201d in order to generate micro-payments with each click for the benefit of content-posters or their selected charities.\n\n\u2022 Therapeutic use of certain psychedelic substances gains wider credibility after leading medical institutions certify their clinical efficacy for mental health concerns, prompting new legislative initiatives and growing support services around so-called \u201cpsychonautics.\u201d This promises to grow over coming years into a mainstream movement similar to the normalisation of cannabis over recent years in various US states, with \u201cpsychonaut\u201d startups attracting support from prominent Silicon Valley figures, some of whom have been privately microdosing for years.\n\n\u2022 One of the \u201cBig 5 Tech Companies\u201d unveils a world-changing innovation, something at the level of providing free WiFi internet to the entire planet or launching a revolutionary new life-management platform based on AI.\n\n\u2022 The 2018 FIFA World Cup takes place in June-July in Russia, but is marred by political controversy, although talk of boycotts will be directed moreso at the 2022 host Qatar. At the final match on July 15, Vladimir Putin behaves oddly while carrying the ball to centre-field for the final match, prompting speculation that he affected the outcome by slipping a KGB bug inside the ball.\n\n\u2022 A medical breakthrough presents the possibility of large numbers of people living in good health to 120+ years.\n\n\u2022 \u201cMeditation\u201d joins yoga and Netflix as the most commonly reported recreational activity for thirtysomethings. \u201cMindfulness/meditation\u201d becomes big business, especially in tech circles where people starting wearing EEG-monitoring headbands which promise to \u2018train\u2019 you for better concentration, creativity, relaxation, and sleep.\n\n\u2022 An advanced and highly personalized AI assistant will become available on your smartphone. The sophistication level will be very impressive but not great \u2014 somewhere halfway between Apple\u2019s current \u201cSiri\u201d and the AI assistant voiced by Scarlett Johansson in the 2013 movie \u201cHer.\u201d It will be like having your own virtual 24/7 secretary, customized to your personal tastes and behaviour patterns, who will talk to you in your earpiece, understand and act intelligently upon your spoken directions, make your restaurant reservations, organize your holidays, send flowers to your mother, and compose your emails.\n\nThis marks the beginning of the transition to the \u201cpost-smartphone\u201d platform, which begins emerging in 2019 and will consist of an AI earpiece \u2014 a personalized virtual assistant in your ear catering to your whims and managing your life \u2014 combined with augmented-reality projection on a lens in front of your eye. The lens will display everything that you currently expect to see on the screen of your smartphone/laptop, but with the added dimension of projecting interactive content that changes in a real-time context-specific way according to your surrounding physical environment, and with a keyboard projected onto any surface or in mid-air for typing.\n\nThis itself is merely the stepping stone toward direct brain-computer interfaces which arrive in the 2030s.\n\n\u2022 Construction of Hyperloop transport links begin between several European cities, while construction also breaks ground on links between NYC-DC and SFO-LAX.\n\n\u2022 2018 will show signs towards a full society transition to digital cash and the abandonment of physical money, a process which will be pervasive within 5 years. By 2022, paper/coin money will for all intents and purposes cease to exist in most countries.\n\n\u2022 Geologists will successfully drill for the first time beyond the crust into the mantle of the Earth, 10 kilometers deep. Samples will reveal the presence of extremophiles, organisms capable of living at unimaginable extremes of temperature and pressure, raising the probability that life could exist on other planets and moons where conditions are not even this extreme. This drilling project also forms the basis for a new global network of early-warning seismic sensors installed throughout the deep crust, revolutionizing earthquake detection and saving countless lives over coming decades.\n\n\u2022 A new superstrong and superlight material will be synthesized, strong as diamond but light as cotton, offering unprecedented industrial applications.\n\n\u2022 There will be revolution and regime change in North Korea by year\u2019s end with the exile/death of Kim Jong-un. This will appear to be internally organic but will be orchestrated by US intelligence agencies.\n\n\u2022 A NASA space probe will send back astonishingly detailed photos of Enceledaus, a moon of Saturn, showing a warm ocean beneath the icy crust possibly supporting life. Spacefaring initiatives converge with pop-culture trends (Hollywood\u2019s current sci-fi renaissance) to raise public discussion of extraterrestrial life and mankind\u2019s future in space.\n\n\u2022 A major international corporation will make headlines when it appoints its own in-house Artificial Intelligence as one of the C-level executive heads of the company. This will raise further public debate about the role of AI in automating away even the supposedly \u2018safe\u2019 white collar jobs.\n\n\u2022 By December 2018 there will be a severe extended crash in the financial markets, abruptly ending the bull rally which is already in its historic ninth year and statistically overdue for a crash. The US dollar and US stock market will collapse. Panicking investors will pull out of stocks and divert all their money in record numbers into \u2018safe haven\u2019 investments like gold and crypto which will accordingly skyrocket to record levels.\n\nThe people who will make the most from this over the next five years are those who spend the next few months buying gold, silver and crypto, with an aim to firmly sit on these assets for several years before selling; and then wait for the crash \u2014 Jan 2019 onward \u2014 to buy as many blue-chip stocks as they can at deeply discounted recessionary prices, and sit on them for some years until the next bull cycle.\n\n\u2022 A growing number of eminent scientists will seriously explore the question of whether our entire so-called reality might just be a computer simulation, probably being run as a historical exercise by our 30th-century descendants inside a hyper-dimensional computer to determine the true nature of causality, free will, love, and the mind of you \u2014 the very person reading this sentence.\n\nThe future isn\u2019t something that merely happens. It\u2019s a constant process of renewal and hope in the face of cynicism and fear. Any exercise of making a prediction, as with making a new year\u2019s resolution, is an acknowledgment of real risks and opportunities within our immediate present. Only by reflecting on those upsides and downsides can we chart a safe path through the middle.\n\nIn an age of seemingly unstoppable technological momentum, it serves to remember that technology doesn\u2019t make itself nor is it inherently virtuous. The same can be said of government and culture. They\u2019re only as good as the people who make them and perpetuate them, and they rely on each successive wave of fresh minds to choose which legacies from a previous time to preserve, which to discard, and which to upgrade.\n\nProgress is never guaranteed. As our tools become invisible but project blinding power, and as the velocity of change spins even the most level head, we need to think more clearly, speak more frankly, and walk more sustainably than ever before through the possibilities unfolding so quickly before us.\n\nAs with each new year, the future is an invitation to grow into our better selves. Here\u2019s to 2018 and to all the promise it holds. For a wider portion of humanity than ever before, and hopefully for the lighter side in most."
    },
    {
        "url": "https://becominghuman.ai/the-3-pillars-of-voice-c41089014612",
        "title": "The 3 Pillars Of Voice \u2013",
        "text": "The Voice space is heating up quickly, in case you didn\u2019t notice. Over the last two years, the technology space has adopted voice as a core component of functionality. With voice, it\u2019s not only about what you see, but it\u2019s what you say that makes things happen.\n\nThe landscape of voice is also expanding rapidly as companies are investing in more technology to empower machines to respond to voice. It started with Apple with Siri and was quickly followed by Amazon with Alexa, along with a host of others. The reason voice is so important is simply that we\u2019ve witnessed a tipping point in the technology world where previously humans had to learn how to understand machines, but now machines are learning how to understand humans. The computing systems in the background have become so advanced and complex that you can\u2019t expect the general consumer to understand them, but you can absolutely expect the computers to understand human language and direction. That tipping point is where AI is coming in, but that\u2019s a different story for a different column.\n\nAs technology is adopting voice there are clearly 3 pillars upon which companies are building value. These are:\n\nThis pillar is focused on the interaction between human and machine to get insight and/or gather information. This is where Google focuses the majority of its efforts to continue to aggregate the world\u2019s information. 50% of searches are now done via voice and many of the capabilities of Google Home are focused on providing access to information. It\u2019s used for things like reading the news, getting the weather, and more. It does connect to a series of Home devices, but for the most part, the use cases are focused on data and information.\n\nBeing information-centric means these providers are device-centric as well. Google Home works with their physical devices as well as with apps embedded in cellphones and computers. Google\u2019s voice service has to be activated through some kind of Google device, which is not seen as limiting in any way but it does require that foundation from which to perform.\n\nThis second pillar is focused on the interaction between a human and a machine to drive actions and is typically dependent on a very specific device. This is exemplified by Amazon and their Alexa assistant. Alexa offers a host of physical actions that can be driven which include things like turning on the lights, playing a song or driving purchases for the home. Alexa also delivers things like games and trivia, both of which create actions in the home and require you to interact with a device.\n\nAmazon recently announced Alexa for Business and this is a logical extension of their home strategy where they establish a marketplace for apps that create action in the office. Booking a conference room or scheduling a call are the intended uses, but over time they will expand to include additional kinds of office actions.\n\nThis third pillar is one where we focus on Voicera and is around extracting value from person-to-person conversations. Rather than having a conversation between a human and a device, we focus on when a value is created through the content of a conversation. Other players are in this space as well. Some focus on recording for historical value while others are intended for coaching. Our point of view is to focus on collaboration and the activation of value that comes from meetings, which are conversations between a minimum of two people. Meetings are the most adopted form of collaboration but they are commonly disconnected from the rest of the enterprise workflow.\n\nOur goal is to both activate the content of the meeting and connect that output to your workflow (i.e. CRM, communication, etc..). We care about activating the content of the meeting and are 100% device agnostic (we have no device). Being conversation-centric means being where the conversation takes place and having permission to be involved. This is a huge differentiator from the companies that are tethered to a device where the microphone may be on, but they do not have consent to be involved in the meeting.\n\nMachines are certainly getting smarter, and their role is to make our lives better. Interaction with machines is getting easier as a result of the advances in voice to drive engagement. You will use the tools created by these approaches to augment your own abilities and to go about your day in a more effective manner. As these systems become more impactful, the way you interact with them will become simpler. You will talk to them rather than type. You will speak rather than be forced to learn new UI\u2019s. Your voice will become the key that unlocks all that potential and unifies all these different technology tools.\n\nThe next few years will be very exciting as more companies take advantage of these 3 different approaches as their own pillars of developing a voice-based strategy."
    },
    {
        "url": "https://becominghuman.ai/3-ai-and-voice-trends-to-watch-for-in-2018-69927b551a4a",
        "title": "3 AI and Voice Trends to Watch for in 2018 \u2013",
        "text": "Nearly four years ago, one of the most buzzed about smart home acquisitions seemed to suggest that smart home adoption was going to be driven by the thermostat. Google had high hopes for Nest when the company acquired the thermostat maker in early 2014.\n\nWhether due to mismanagement, growing pains, or a combination of both, innovation from Nest stalled, creating opportunity for other companies to step in and grab consumers\u2019 attention with exciting new products. That new product has turned out to be the smart speaker.\n\nStarting with Amazon\u2019s release of the Alexa enabled Echo in late 2014, smart speakers with built-in voice capabilities have been flooding the market. Google has released a line of voice enabled speakers. Apple announced the HomePod, that will use Siri as the voice assistant to the speaker. At CES this year, we already expect to see lots of entrants to this fast growing market.\n\nOne of the big factors driving this growth is the ease of use provided by a good voice interface. It is easy for consumers to say \u201cPlay \u2018Like a Rolling Stone\u2019 by Bob Dylan,\u201d instead of opening an app and searching through a play list. Because of this ease of use, consumers are buying this product. Typically, once consumers have at least one connected product in the home and have a good experience with it, they are more open to exploring others."
    },
    {
        "url": "https://becominghuman.ai/what-exactly-is-a-virtual-assistant-db3a5df79e33",
        "title": "What Exactly Is A Virtual Assistant? \u2013",
        "text": "There\u2019s a whole new category of business that blew up in the last year \u2014 the category of \u201cVirtual Assistants\u201d. This is a direct result of the advances in AI and organizational expertise and they are having an immediate impact on the worker of today.\n\nVirtual Assistants are a strong element of the AI Exoskeleton that we\u2019ve been speaking about for some time now. The term Virtual Assistant is a difficult one though because it has different meanings to different people and it depends strongly on whether you are evaluating term in a home-based or work-based environment.\n\nIf you go to Google and search for \u201cvirtual assistants\u201d you return 3.49MM query results. This is a lot of clutter for a topic that didn\u2019t even exist a few years back. The services range from having outsourced people who act as an assistant for things like scheduling meetings and managing communications all the way to the more popular digital, AI-based virtual assistants. If you dive deeper into this subdivision of the category you quickly uncover that the term is broadly used to refer to any AI-based system that offers help with any of a multitude of tasks. You have the broad-reaching Siri, Alexa, Cortana and other virtual assistants along with the narrower use case systems like X.ai and Clara for calendaring as well as our very own Eva, an in-meeting virtual assistant that takes notes and help organize your follow up from meetings.\n\nVirtual assistants are tools \u2014 they are not meant to replace people. They are intended to replace some of the day-to-day functions and responsibilities of people. They decrease the time you have to spend on mundane tasks in order to allow you to free up your time to do more interesting, strategic and hopefully impactful things.\n\nAs time progresses over the coming months I fully expect to see virtual assistants engage with one another using basic API technology or through the utilization of voice. Voice is the technology of choice for engaging with these tools because we are now at the stage of technological development where the machines can better understand how humans speak rather than vice versa. We don\u2019t need to learn the complicated UI of interacting with a machine when we can speak to it and ask it what to do. Alexa and Siri are doing a great job of laying the groundwork here and getting the average consumer comfortable with speaking to machines and tools like Voicera will continue to benefit immensely from that groundwork being set.\n\nHere are just a few of the ways you can expect to use a virtual assistant on a day to day basis:\n\nVirtual assistants are an amazing way to create value by the better utilization of time. They save you time, which you can then use for other, better things.\n\nIt\u2019s an exciting time to be in technology!"
    },
    {
        "url": "https://becominghuman.ai/transfer-learning-retraining-inception-v3-for-custom-image-classification-2820f653c557",
        "title": "Transfer Learning: retraining Inception V3 for custom image classification",
        "text": "Our problem statement looks very realistic and it turns out it is a real one in fact.\n\nThe applications of Deep Neural Networks are literally on a roll. Whether it is in healthcare, transportation or retail, companies across all industries are excited about investing in building intelligent solutions. Meanwhile, let\u2019s hope human intelligence remains uncontested :).\n\nIn the actual case, a solution such as a powerful image classifier can help the company track shelf inventory, categorize products, record product volume, etc. from raw products images captured in real time by dedicated devices (drones ? robots ?). And for sure, being able to identify a product and predict its category from a given picture is part of the deal and this is what this experiment is all about. We shall train a bot to predict the category of each product from images.\n\n\u201cWhat an easy task!\u201d, We think as human beings but it is not necessarily the case for robots. First of all, yes it is obvious that every adult should be able to correctly identify a product from its image. The first observation is that this assumption holds true as far as we consider only adults. A 5-years-old child, for example, can\u2019t beat an experienced person in this task. And an \u201cexperienced person\u201d in this case, when we zoom in, is simply someone who has seen enough product images in his life that he can easily recognize any product he has seen before, from a given image.\n\nThis concept of experience is what we try to transfer to robots when we train them with existing labelled data so that they can learn on their own how to accurately distinguish each image from a training dataset. In this sense, we use Artificial Neural Networks which are nothing but an imitation of how human brains actually work. The knowledge that robots build using these algorithms are later tested on unlabeled observations. They label given images based on what they have learned and that is why this kind of problem is usually referred to as supervised learning: robots try to map a new image instance to one of the class labels they encountered during the training phase. And talking about performance, it has been noticed that well trained robots tend to give better accuracy than humans in most cases of supervised learning. And here, you will be surprised to see how our algorithm can exceptionally outperform humans, under difficult conditions (blurred images, poor quality images, etc.).\n\nWe have a dataset of products images from hackerearth to be downloaded here. How should we proceed and why are we using transfer learning ?"
    },
    {
        "url": "https://becominghuman.ai/careers-guide-2050-a-fool-proof-guide-to-a-long-and-healthy-career-in-a-world-ruled-by-robots-e5b503c06390",
        "title": "Careers guide 2050; A fool-proof guide to a long and healthy career in a world ruled by robots",
        "text": "With the turn of 2018 are you considering a new job? Or even perhaps an entirely new career? With the progress in the field of machine learning and automation in 2017, you could be forgiven for some concern over what new work you might do and whether those jobs will be automated by the time you are qualified.\n\nFear of automation is not new. The industrial revolution took away people\u2019s jobs, with the luddites famously destroying machines to protect human work. Automation happened, people lost jobs, new jobs were created (often higher paid jobs) and aside from a difficult transitionary period, things got better.\n\nThe same thing is happening today. Machine learning and robotics are finding better or cheaper ways of doing the same things humans are employed to do in various areas. As this happens, we can move human workers to different roles. Again, as with the industrial revolution, the transitionary period is unlikely to be pain-free, with people needing re-training and time taken for new roles to be invented for the newly available workforce. If anything, it\u2019s happening faster today so the transition may be even more painful than before.\n\nWhat would happen though if we were to create an artificial general intelligence? A machine which is at least as good as a human in everything we can do? Provided it can do the same things as a human at a lower cost, free market economics will essentially guarantee that all jobs go to AI. The issue only gets worse if we explore the possibility of Super Intelligence, an AI which is orders of magnitude better than a human at everything we can do. Provided the cost doesn\u2019t go up proportionally with ability, it would seem we mere humans would be in a pretty bad place. But this is a guide about how to guarantee your job well into the future! So, what should you be doing?"
    },
    {
        "url": "https://becominghuman.ai/demystifying-the-most-talked-terms-of-ai-and-machine-learning-79bb9323bb53",
        "title": "Demystifying the most talked terms of AI and Machine Learning",
        "text": "This is my first post and i will be dedicating this post to most common and basic terms you might have heard or seen while googling your way around trying to understand what machine learning really is.\n\nArtificial Intelligence(AI) has taken over the computer software industry with a huge storm lately so it is quite usual you might have heard of words such as Machine Learning(ML), Deep Learning, Neural Networks etc. Let\u2019s face it!!! Drilling all of the information related to machine learning and Artificial Intelligence from around the internet through the skull is an exhausting task, given all of the humongously techsavy terms used.Understanding the terminologies and concepts behind them can be more than difficult for someone who is new to the field (like me). This post aims at boiling down some of the common terminologies used in ML world to a state where a layman can grasp it. Now without wasting any more time lets dive in and learn something new."
    },
    {
        "url": "https://becominghuman.ai/my-rant-about-unfounded-claims-that-the-universal-basic-income-discussion-is-pointless-6fbc71183d41",
        "title": "The Universal Basic Income discussion is not pointless. Period.",
        "text": "The Universal Basic Income discussion is not pointless. Period.\n\nOne of the topics that is inevitably tied to the big digital transformation themes like AI, machine learning, robotics and digital transformation in general is the universal basic income (let\u2019s call it UBI from here). Like it or not, it\u2019s worthwhile considering which tools, mechanisms and concepts need to be adapted or created when this massive transformation will roll over us. I believe it would be naive not to assume that our political economy, and governance will desperately need new design patterns as we enter this new phase of the digital revolution.\n\nThere\u2019s different ways to position or regard the concept of UBI. Ranging from:\n\nI think except for the latter, they are all somehow related, or respectively are spurred by the same drivers. And these are the drivers that in my opinion determine the times we live in and that \u2014 at accelerating speed \u2014 will determine future times:\n\nEven without number 1, numbers 2 to 4 form a repetitive and self-supporting cycle leading to more labor (blue and white-collar) taken over by non-human actors and thus less labor/jobs left for humans. Unless of course totally new jobs and tasks are created for a growing number of people.\n\nThere\u2019s a couple of articles, opinions and judgments I have stumbled upon recently on UBI that kind of started to make my blood boil a little bit. What strikes me, is the apparent degree of \u201cnot being informed\u201d or at the least \u201cnarrow vision\u201d that can be observed. I get the impression that the stronger the opinion against UBI, the less informed the knowledge and consideration basis seems to be.\n\n\u201cMankind is hard-wired to work. We gain satisfaction from it. It gives us a sense of identity, purpose and belonging \u2026 we should not be trying to create a world in which most people do not feel the need to work.\u201d\n\n\u201cEarning more through work, not a basic income, is the way out of poverty.\u201d\n\n\u201cYou give up the principle of ensuring dignity through work.\u201d\n\n\u201cThe fear of artificial intelligence is nonsense. Today\u2019s AI can only do one thing very well \u2014 play chess, play go, etc.\u201d (in German)\n\n\u201cNew jobs are much better than universal basic income.\u201d (in German)\n\nAnd I am not even talking about opinions like \u201cif you give people UBI then they will be lazy and never work again\u201d. Remember, we\u2019re not talking about the world in 2017, but most likely the world in 2030 and onwards. And then it will not be about people not pursuing a job because of UBI, but UBI should be there because they cannot pursue a job in the first place.\n\nBefore I move on to elaborate on the argument, that things will just work out like they did in the past (\u201cold jobs go, new jobs will come\u201d) I would like to take a look at the \u201cAI is not smart enough to justify being scared\u201d. It is not a strong argument that certain AI algorithms are only good at one thing but not able to generally compete with human intelligence or capabilities.\n\nCritics say that all current AI progress is respectively only centered on one field or domain. AlphaGo can play Go, but not peel carrots. So what? Then there is one system for mastering Go and one system for peeling carrots or flipping burgers. Not one single system needs to master everything. As long as there is an AI solution mastering tax law and accounting, another one mastering medical diagnosis, and another one manufacturing a car, and another one driving that car, we\u2019re still in deep shit. We don\u2019t need strong AI, not articial super intelligence, no intelligence explosion, no singularity to start sweating when thinking about automation and job replacement.\n\nThe severity of the discussion should get even clearer when considering concepts like reinforcement learning in the machine learning discipline. They show us where we are heading. This is about algorithms learning what to do from scratch, without human intervention, surpassing human capabilities in a certain field within hours.\n\nYou often hear the argument that technology is both creating and destroying jobs and that this has always happened. \u201cJust look what happened in the past.\u201d Agricultural and handicraft trade jobs have been destroyed and replaced by industrial jobs. Industrial jobs have been destroyed and replaced by service and knowledge worker jobs. The carriage driver was replaced by the car. But then there were new jobs like mechatronics engineers or automotive engineers.\n\nBut what comes next? Yes, it sounds like a logical development that when we replaced physical labor by machines, people would find new labor in more intellectually focused professions. But what when you replace intellectual labor conducted by humans with software logic and algorithms? What\u2019s next then?\n\nTo me it is simply an uninformed contribution to the discussion to argue that exactly this chain of events will continue and that there will be something next. Talking to people about this topic, you also find ingredients to the discussion like \u201cwe humans cannot allow that\u201d, \u201cit is inhumane to allow for machines to replace human labor\u201d or just the simple claim for \u201cwe must create new jobs\u201d. Like we have IT-administrators, and web developers, and app developers today. All of them jobs that didn\u2019t exist 25 years ago. But how many people work as web designers? Isn\u2019t it the truth that the new jobs in technology that have been created are by far lower than those numbers of jobs that used to exist in manufacturing and classic industry focused professions?\n\nTech companies employ significantly fewer people than traditional industry players while playing in a totally different ballpark when it comes to market capitalization. And it\u2019s only a matter of time when those three tech players will surpass the industry players in terms of revenue. In terms of profit they are already in another dimension today.\n\nSo, looking at the technologies that will further drive progress in tech and increase pressure on automation and replacing human labor, AI and robotics have huge potential. I believe artificial intelligence and robotics that is AI-powered are general-purpose technologies, like e.g. electricity. We\u2019re seeing a definite ramp up and acceleration of progress and utilization of such technologies in our economy right now. And even \u2014 as can in fact be noticed \u2014 when there is a lower than expected productivity ramp up, this in my view is likely an up front condition as companies invest in these technologies without necessarily seeing a direct return in the short-term. But this will come later. And it will come with force. It is hard to imagine what kind of jobs there will be for humans in the very long run once machines and robots have reached levels where they easily surpass human labor in the same field both with regards to time and costs.\n\nIn the short to medium term I foresee the following development.\n\nFirst, intelligent machines and algorithms will augment human labor, so that its productivity will be significantly increased. In the combination of human and machine labor, the human value in the overall contribution level will be higher than that of the machine.\n\nSecondly, intelligent machines and algorithms will make significant progress in their capabilities so that human contribution and interaction will be needed to close the last remaining gaps. Think \u201cintuition\u201d and \u201ccommon sense\u201d. It\u2019s that ingredient to decision-making and supervision that is hard to describe and to grasp, but that makes all the difference. In this sense, humans will have the role of supervising, but altogether the share in the overall value contribution decreases significantly.\n\nAnd lastly, even this last resort will be covered by even more capable machines and software algorithms. Leaving the human out of the equation. In this stage, human involvement would be counterproductive, probably even massively inferior. This will go to a degree where it is difficult to grasp for humans how those algorithms actually work, both from an intellectual point of view and from a speed of processing point of view.\n\nAnd then it boils down to the question how we envision human participation in income and wealth distribution in such a scenario? Concepts like a universal basic income seem like a pretty reasonable approach to me.\n\nComing back to claims like \u201cbut humans need work for dignity\u201d, this to me feels like thinking trapped in past conditions. Contributors should consider that we are not talking about today but about a future that has ever more accelerating progress of technological development. It is crucial to anticipate an upcoming development, whose mechanics are based on exponential development and growth and that is in itself further fueled by the law of accelerating returns.\n\nYes, it is a valid and comprehensible claim to provide enough work for all people. To have a fair (or somewhat fair) share in wealth distribution, in finding meaning in life and gain satisfaction from a professional career.\n\nBut the likelihood that this will be possible is getting smaller and smaller. Therefore it is a necessity to explore how a universal basic income could work. I don\u2019t know how and if it will work. But elaborating on it to me seems much smarter than to claim that we should not give up against automation and the machines and simply invent new jobs. Yes, there will be jobs that we have no imagination of so far. But I doubt those will keep billions of people busy and employed.\n\nDon\u2019t get me wrong. It is extremely challenging to figure out a working UBI scheme. It is extremely challenging to figure out the math and financials. However, I believe it is more worthwhile spending our time and brain power on tackling this, than to argue that we \u201csimply need more and new and better paying jobs\u201d. I don\u2019t see those coming in the quantities needed to sustain our current way of working and living. I think technological progress is going destroy the labor market as we know it, and it\u2019s going to create a desperate need to find solutions in order to provide social stability."
    },
    {
        "url": "https://becominghuman.ai/ces-2018-will-be-the-shape-of-things-to-come-80e9d7d293d0",
        "title": "CES 2018 will be the shape of things to come \u2013",
        "text": "I don\u2019t go to CES to find THE product. The days of major tech companies using the Las Vegas-based consumer electronics show to help launch a singular, ground-breaking gadget are long gone.\n\nInstead, CES defines the shape of the body consumer electronics. Companies like Apple, Microsoft and Samsung may produce the major organs, but most other tech firms. \u2014 big, small, and largely unknown \u2014 typically spend the rest of the year making products based on the form CES outlined in January.\n\nAnd there will be thousands of companies spread across two convention halls and multiple other Vegas outposts at the event, which starts on January 9 (800 alone at the Eureka Park startup village). Trying to see them all is a fool\u2019s errand, like trying to memorize Grey\u2019s Anatomy.\n\nMy goal is to find the gems, quietly shining in narrow aisles and off-the-beaten-path hotel rooms. As my guide, I\u2019ll look to the major trends that have defined the last few years of technology and now help define CES 2018.\n\nOur fixation with artificial intelligence will not just permeate products throughout the conference (expect everyone to say they have \u201cAI,\u201d like companies in the mid 1990\u2019s that all told me they had \u201cInternet support\u201d), it will have its own dedicated area at CES 2018. Since AI is all programming, algorithms and machine learning, I have a little trouble envisioning what a trade show floor-area filled with AI companies will look like.\n\nPerhaps it\u2019ll be a lot of robots, though that has its own area, too (twice as big as last year\u2019s, in fact). Or maybe it\u2019ll be dozens of Echo-like speakers with third-party digital voice assistants. It\u2019s more likely, though, that if there are speakers or any kind of intelligence in most of the AI hardware, it\u2019ll probably come from Amazon\u2019s Alexa voice assistant (Alexa is almost synonymous with consumer AI).\n\nSpeaking of robotics and AI, autonomous driving (or, if you prefer, \u201cDriverless Cars\u201d) will be on display at the show. Well, outside the show, since the driverless car demonstration area is in parking lot right in front of the Las Vegas Convention Center. The first time I ever drove in a driverless car was at CES and I hope to get another hair-raising autonomous experience this year, as well. The big difference between then and now will be that the autonomous cars probably won\u2019t look any different than cars that still need us.\n\nIt\u2019s not just cars that are getting smarter. CES 2018 will be a showcase for a ton of Smart City innovation, including, I think, the answer to when we will finally experience 5G.\n\nTechnology\u2019s kissing cousins, AR and VR will be everywhere, but I hope it\u2019s not just a bunch of knock-off Oculus Rifts and HoloLens\u2019s. What I\u2019d like to see are some truly excellent VR gloves.\n\nDrones are back, too, but they won\u2019t stir up much excitement unless they out-innovate industry-leader DJI.\n\nCES is of course, also the place for big-ticket home appliances and electronics. This year we\u2019ll see ever-larger 4K and 8K OLED HDTVs from Samsung and LG, as well as ultra-thin, transparent and flexible displays. They\u2019ll all support HDR and, probably, voice commands. There\u2019ll also be smart refrigerators, vacuum cleaners, washing machines and dryers. Most of them will probably feature cameras so you can watch your cloths dry, your fridge empty and your food cook from wherever you are.\n\nI\u2019d be shocked to find a single home gizmo that can\u2019t talk to an App, the Internet \u2014 and to each other.\n\nThere\u2019ll also be hundreds and hundreds of oddball gadgets, things that make you stop and wonder how people dream up this stuff. Those are always my favorite products.\n\nCES 2018 is also home to a full-slate of panels and presentations, many of which will deal with the changing landscape in digital content, Hollywood, commerce, and AI.\n\nEven though the show officially kicks off January 9 (and runs through the 12), most exhibitors will be previewing their wares starting on January 7. There\u2019ll be large, press-only events like Unveiled and a bunch of keynotes from companies like Intel, Sony, Samsung, Huawei, TCL and many more. At each one, companies will announce one or more products and innovations.\n\nWhich means, if you follow my feeds, you\u2019ll get the inside scoop on everything CES 2018 starting Sunday."
    },
    {
        "url": "https://becominghuman.ai/natural-language-ivr-what-why-how-e5a2c9a4e1c3",
        "title": "Natural language IVR \u2014 what, why, how? \u2013",
        "text": "We talk to a lot of people about their IVR (Interactive Voice Response) systems, and we make a point of calling up all sorts of different companies to see how their IVR systems sound. Something we\u2019ve noticed is that more companies are asking about, and deploying, \u2018Natural Language\u2019 IVR solutions. Another thing we noticed is that their reasons, expectations and outcomes are quite varied. For some it\u2019s driven by a desire to reduce customer effort, others want to keep up with their competitors, and some have been sold on a technology that actually makes little sense for their business. So, what\u2019s a Natural Language IVR? Why should you deploy one? And how do you do it?\n\nA Natural Language (NL) IVR is one that uses a particular type of automated speech recognition (ASR) technology that allows callers to say what they\u2019re calling about in a wide variety of ways, so instead of prompting them to say specific phrases, the system will typically just say something like: \u2018Welcome to VoxGen, how can I help you today?\u2019.\n\nSounds great, and pretty natural, but callers can be confused by such an open-ended prompt because they need to work out what they think an appropriate response will be. You often get better accuracy by providing a few examples, like: \u2018you can say things like: what\u2019s my balance or I want to pay my bill\u2019. Which starts to sound a little less natural.\n\nThere\u2019s also a problem with callers giving very long explanations\u2026 which is somethingthat I do a lot when speaking with a human agent, but it\u2019s a bit too much for an automated system to understand, which is why some systems try to avoid this by saying: \u2018in a few words, tell me how I can help you today?\u2019\n\nSo what\u2019s going on under the cover? The difference is in the way the speech recognition works. Bear with me here, because it will help to understand the limitations, and also the process for building and tuning NL systems. An NL system consists of two key elements: a statistical language model (SLM) and a statistical semantic model (SSM). The language model helps the system recognise the sequence of words that the caller said, while the semantic model helps the system understand what it meant \u2014 the caller\u2019s intent. The key thing to understand, is that they are just \u2018pattern matchers\u2019 that are trained on a large number of sentences that callers typically say when asked \u2018how can I help you today?\u2019. The SLM simply learns what sequences of words are spoken, and the SSM learns how to match a bunch of words with a request type. After training, the NL system learns how to take an input from the caller, like: \u2018I want to pay my bill\u2019, and convert it into a meaning, like: Call_type=pay_bill. In some systems, it will also try to capture extra information, so if the caller says: \u2018I want to pay my gas bill\u2019, the system might be able to capture a more complex meaning, like: Call_type=pay_bill, Product_type=gas.\n\nSo why would you want to do this? The idea is that it can create a better caller experience, and help get callers to the right place, with less effort, which could be a win-win, because the experience becomes effortless for the caller, and they get to the right place: a well-designed automated system that lets them complete their task quickly and easily, saving callers time and your business money, or the right agent who can help them, avoiding annoying and costly internal transfers. Get it right, and there\u2019s a strong business case, including cost-savings and customer experience benefits that can lower costs, and drive loyalty and retention, ultimately increasing profits. An NL solution is not right for every situation, but if you\u2019ve got a large number of different destinations, or agent skill groups that calls need to be directed to, and there\u2019s a reasonably high volume of calls that can justify the initial investment and ongoing tuning costs, it\u2019s definitely worth considering.\n\nRemember, the NL solution won\u2019t understand every caller, and some callers will find it difficult to respond to the open question. You could just route those calls to a general agent pool, but that might mean an extra transfer, which costs money and impacts customer experience, so you will probably still need a good \u2018directed dialogue\u2019 call routing menu as a backup.\n\nThere are 8 key steps to implementing an NL solution:\n\nInitial design: Design the prompting strategy and record the opening prompts\n\nData collection: Build a simple IVR application that uses the prompts from the initial design. A percentage of calls are then sent to this application and caller responses, known as \u2018utterances\u2019, are recorded and stored. You\u2019ll typically need at least 30,000 utterances, but the more the better, and some systems will need 100,000 or more to build a good model.\n\nTranscription: The utterances need to be transcribed. This starts with manual transcription, but you can speed things up by using the speech recogniser to come up with the initial transcription, and then let human transcribers verify and correct the automated transcriptions.\n\nTagging: To train the Statistical Semantic Model (SSM), which interprets the words a caller says and determines their intent, the transcriptions need to be \u2018tagged\u2019 with an interpretation, or intent. Again, this can be done with a combination of manual and semi-automatic tagging, and you\u2019ll need to develop a \u2018tagging guide\u2019 that makes sure the transcribed sentences are tagged consistently, and in a way that suits the breakdown of call types and skill groups or routing destinations in your business\n\nSentence generation: Often there will be certain utterances and tags that are not well represented in the tagged utterances, so the data can be augmented with additional tagged utterances that are created by hand or generated from a grammar\n\nModel build and test: The transcriptions are used to train the statistical language model (SLM) and the statistical semantic model (SSM). Typically, around 10% of the data is held back as a \u2018test set\u2019 and used to test the models that are built \u2014 this helps to give an indication of how well the system will perform in live\n\nDeploy: Once built, tested and tuned \u2018offline\u2019, it\u2019s time to integrate the NL model into your existing IVR application, and deploy it for live callers. It\u2019s a good idea to do this gradually, so you just send a small percentage of calls to the system to start with, to make sure everything\u2019s working well before ramping up the volumes of calls\n\nTune: After go-live, you\u2019ll have a lot more data to work with, so watch the results carefully, check the impact on call routing accuracy, and collect recordings from the live system that you can use for tuning. Tuning is basically an additional cycle through these steps, including updates to the design of the prompting, which is sometimes necessary to improve performance \u2014 e.g. by providing some examples of what callers can say in the initial prompt.\n\nQuite often, the caller will not give enough information in their initial response, or the recogniser won\u2019t pick up all the details, so in addition to the open-ended part of the NL solution, you\u2019ll need some extra dialogue steps to \u2018disambiguate\u2019 a caller\u2019s intent. In a study we conducted that compared a NL call routing design and a more directed dialogue solution, we found that callers prefer the experience of a well-designed and accurate NL solution, compared with a the directed dialogue approach, but they liked it even more when the system used data to predict why they were calling and ask them a simple question, like: \u2018I looked up your number and found you have a prescription due for renewal, is that what you\u2019re calling about?\u2019\n\nNL solutions can improve customer experience by reducing customer effort and ensuring customers get to the right agent, or self-service automation to complete their tasks quickly and easily. And it can reduce costs with fewer internal transfers and greater automation, but it\u2019s not right in every situation, and this type of solution is more expensive to build, run and maintain, so it\u2019s important to run the numbers and take a careful look at the costs as well as the benefits and check there\u2019s a compelling ROI. An NL solution is best when combined with directed dialogue for disambiguation and as a fall-back mechanism when callers don\u2019t respond, or aren\u2019t understood, and if you\u2019re able to capture the number of the caller, and find a matching record in your CRM system that lets you predict their reason for call, that can be the ultimate experience. It\u2019s like going into a coffee shop and ordering your favourite beveridge. It\u2019s great when you get a welcoming smile and an enthusiastic: \u2018How can I help you?\u2019, but it\u2019s even better when you go to your local coffee shop and they say: \u2018Regular decaf Americano as usual?\u2019. Yes please!"
    },
    {
        "url": "https://becominghuman.ai/web-3-0-the-third-generation-of-web-1202ec63ebb7",
        "title": "Web 3.0- The third generation of web \u2013",
        "text": "The term Web 2.0 was never clearly defined and even today if one asks ten people what it means one will likely get ten different definitions. However, most people in the Web industry would agree that Web 2.0 focuses on several major themes, including AJAX, social networking, lightweight collaboration, social bookmarking, and media sharing. While the innovations and practices of Web 2.0 will continue to develop, they are not the final step in the evolution of the Web.\n\nIn fact, there is a lot more in store for the Web. We are starting to witness the convergence of several growing technology trends that are outside the scope of what Web 2.0 has come to mean. These trends have been gestating for a decade and will soon reach a tipping point. At this juncture the third-generation of the Web will start.\n\nIt\u2019s easier to identify the major differences between Web 1.0 ( users passively consult web pages and for the most part don\u2019t participate in generating content) and Web 2.0 ( users create content and interact with sites and with each other through social media, forums, etc).\n\nBelow are 5 main features that can help us define Web 3.0:\n\n1) Semantic Web\n\n The next evolution of the Web involves the Semantic Web. The semantic web improves web technologies in order to generate, share and connect content through search and analysis based on the ability to understand the meaning of words, rather than on keywords or numbers.\n\n2) Artificial Intelligence\n\n Combining this capability with natural language processing, in Web 3.0, computers can understand information like humans in order to provide faster and more relevant results. They become more intelligent to satisfy the needs of users.\n\n3) 3D Graphics\n\n The three dimensional design is being used extensively in websites and services in Web 3.0. Museum guides, computer games, e-commerce, geo-spatial contexts, etc. are all examples that use 3D graphics.\n\n4) Connectivity\n\n With Web 3.0, information is more connected thanks to semantic metadata. As a result, the user experience evolves to another level of connectivity that leverages all the available information.\n\n5) Ubiquity\n\n Content is accessible by multiple applications, every device is connected to the web, the services can be used everywhere.\n\nThese features bring us closer to a Web 3.0 definition. Now, let\u2019s look at an example that brings these 5 features together.\n\n In Web 3.0, while you are driving, you can simply ask your automotive assistant a question (\u201cI would like to watch a romantic movie and eat Japanese food\u201d). The search engine embedded in the car assistant provides you with a personalized response that takes into account your location, suggesting the closest cinema that matches your request and a good Japanese restaurant by automatically consulting the reviews on social media. Then it might even present a 3D menu from the restaurant in the display.\n\nThis scenario of Web 3.0 is not a dream\u2026 For the most part, it\u2019s already a reality today (for example the semantic web and artificial intelligence) thanks to cognitive technology.\n\nDear readers,I want to present you with a insight of what a web 3.0 will look like in demo below,have a look.\n\nCheck out our demo to see how it works."
    },
    {
        "url": "https://becominghuman.ai/will-we-banish-ai-from-our-modern-garden-of-eden-1e8318ab47dc",
        "title": "Will we banish AI from our modern Garden of Eden?! \u2013",
        "text": "Will we banish AI from our modern Garden of Eden?!\n\nFor the first time we can see that in the last two years there is a technology that people from all ages and from all around the world are really scared of. This technology is artificial intelligence (AI), a technology and concept that was born more than 60 years ago with Alan Turing\u2019s test, which we\u2019ll come back to later.\n\nThe basic idea behind AI is the goal to create a computer program (the \u201cartificial\u201d part) that will be able to solve \u201cproblems\u201d, which is achieved through a continuous learning process (the \u201cintelligence\u201d part). These problems may be any sort of tasks or operations a human can do, and this is the part that scares people. The potential of AI to automate jobs and make people redundant has been published during the last four years by numerous universities and research organizations.\n\nThe most recent Ford Trends 2018 Survey shows that 52% of adults globally think that AI will do more harm than good. Another recent survey by the Pew Research Center shows that 72% of Americans are worried \u201cthat machines might do many of the jobs currently done by humans.\u201d Research done by SelectHub showed that 41% of Americans are afraid to be replaced by AI, and 50% of people of Gen X understand that they will have to switch careers to a different industry after that happens. The last poll that I\u2019ll mention was done by TechPro Research and it shows that 34% of people are afraid of the concept of AI, 17% think that it will be bad for business while 24% think it will be bad for society.\n\nThese researches do not stand on their own, as there are major public figures who speak on AI and its potential. As most people really do not understand what AI is and what can, or can\u2019t it do, they rely on what these celebrity experts say. Elon Musk stated in 2014 that \u201cAI is our biggest existential threat\u201d. And recently he said that AI must be regulated before it will be too late for us. Prof. Stephen Hawking claimed lately that AI could be the \u201cworst event in the history of our civilization\u201d and essentially end mankind.\n\nBut, although the fear from the automation of jobs is understandable, I still think these results do not answer the real reason why people are afraid of AI. Maybe it is related to the fact that since its inception, AI\u2019s goal is to mimic how children learn, think, and even introspect. The neural network of AI is based on our brain\u2019s neural network structure and function. Alison Gopnik, a psychology and philosophy professor from Berkeley, published an article in Scientific American called \u201cMaking AI more human\u201d, where she wrote that people develop \u201csuch strong feelings because of our deep-seated fear of the almost human.\u201d She continues and writes that \u201cthe idea that creatures might bridge the gap between human and the artificial has always been deeply disturbing\u2026\u201d\n\nIf we compare AI to the mind and spirit of humans, we can extend the comparison to include robots as the replacement for our bodies. Today, there are already numerous robots that are AI enabled to better perform, learn and improve their operation. In a recent survey done by The Royal Society in the UK, the notion of a robot animated by AI was coined \u201cembodiment.\u201d Already decades ago science fiction movies showed robots in a form very similar to humans, but today there are robots that look more and more like real people. So, what will happen if these \u201chardly-discernable-from-human\u201d robots will gain also the capacity to learn and evolve?!\n\nCan you think of a similar story that has a lot of resemblance to this one? And, no. I\u2019m not referring to previous industrial revolutions, or the fear for the end of jobs that was in the 50\u2019s and 60\u2019s of the 20th century.\n\nI\u2019m referring to a much older story, that you probably all know. The story of Adam and Eve and their banishment from the Garden of Eden. It is not about religion, but about the concept as there are numerous parallels between what happened in the Garden of Eden to what is happening today in the world.\n\nLet\u2019s do a mental exercise and try to imagine a population that reached such a high level of scientific discovery that they really do know everything, and through this knowledge they can also live forever as they master genetics, medicine and biotechnology. This can be a very boring life, because there is nothing new to discover and you live forever. So, maybe one of the scientists in this society wants to play a bit with science and create new and exciting things. As they have solved also the mystery of life, this scientist could create any kind of creature he likes. These creatures could roam around the lab, or the scientist\u2019s \u201cgarden\u201d, and interact. But again, this is basic science for him, so he decides to raise the stakes and create a much more interesting being. He decides to create a creature that is a clone of him.\n\nThen, as the story goes, this scientist inserted his new clone(s) to the lab (garden), so that it could also interact with the other creatures. But, he gave the clone much higher permissions than the other creatures. As you know, in labs there are servers that hold all the knowledge and data, but they are useless to the creatures of the lab- \u201cIn the middle of the garden were the tree of life and the tree of the knowledge\u2026\u201d. The scientist probably created the clone to do the work that he doesn\u2019t want to do, or just because he can- very similar to AI today.\n\nWe can even see that this clone was the first time in history to utilize AI techniques such as unsupervised and reinforced learning.\n\nThere is a very simple explanation for it, and that is the scientist\u2019s own fright the his \u201cexperiment will exceed its limits\u201d, and his management and ethics board will hear of it and punish him. But, this is as I mentioned a very simplistic explanation. If we look at it from a different angle, we can see that the scientist\u2019s fright is rooted in a much deeper reason. He understands that once his clones access the knowledge, they will become like him, and basically become part of the society. This is a major danger and precedence that must not be crossed. Unfortunately for the scientist, he was not the only one who knew this basic truth.\n\nSocieties that reach such high level of scientific and technological discovery will be also very libertarian, with a high moral code, without the death penalty, and with a lot of personal liberties and independence. So, it means that the scientist already knew that his threat was baseless, and he will not be able to really execute his clones. But he did know that he must at least mitigate the potential damage, and limit the clones access to the second knowledge base.\n\nThe scientist\u2019s only action so as not to accept these enhanced clones to his society, was by banishing them from his planet and sending them away, and put defense mechanisms from preventing them from coming back, or from accessing the second knowledge base (tree of life).\n\nThe fear of creating something that looks like us and behaves like us is something that humanity has tackled from the dawn of history, and even before history itself.\n\nThere are, as mentioned, many similarities to the state of AI today and why people fear AI, and AI-enabled humanoid robots. There is also the famous story about Sophia, the first robot in the world to gain citizenship. But, if we examine it closely we will notice that the country that granted the citizenship is Saudi Arabia, a very restricted and traditionalist society, where women have less rights than men (and even are considered the property of them). So, how come Sophia still has an owner if she gained her citizenship and independence?!\n\nWe are still not there yet, and we will not see any western country giving citizenship to a robot, as they understand the full implications of such a move.\n\nMaybe the current Garden of Eden of scientific and technological progress and advancements will choose to banish these beings once we create them. And they, on their own right will establish the next colony somewhere else.\n\nWill we reach a point where we regret creating AI and try stopping it?"
    },
    {
        "url": "https://becominghuman.ai/phonvert-repurposing-retired-smartphones-with-ai-d9f22776df85",
        "title": "phonvert : Repurposing retired smartphones with AI \u2013",
        "text": "So you bought the shiny iPhone X. Your iPhone 6 now sleeps in your closet. But\u2026what if you can upcycle it by combining it with AI?\n\nAt the workshop held in Waag Society on December 7th 2017, we did an ideation session to \u201cphonvert\u201d retired smartphones with AI.\n\nThe word \u201cphonvert\u201d is a made-up word that combines phone and convert. As big corporations create new models of smartphones each year, many capable yet less desirable retired smartphones pile up.\n\nIDC estimates that of the 1.4 billion smartphones sold in 2015, more than 280 million working smartphones were replaced without being recycled.\n\nWhile many of these still work and have usable sensors, most of them remain unused or worse yet, end up in landfills.\n\nThrough spreading the word, project phonvert is building a movement and a community that is dedicated to finding alternative usages for old phones.\n\nFrom a microscope for smart diagnostics in Uganda to an earthquake monitor in Japan, new ways of phonverting are being developed around the world.\n\nThis project was acclaimed internationally and was exhibited at Design of the Year 2016 at the London Design Museum.\n\nIn the workshop, we explored the potential of combing retired smartphones with image recognition system that can see and identify objects, powered by Google Cloud Vision.\n\nEach group worked with an old smartphone with an application which data from multiple sensors were visible. To demonstrate the potential of image recognition, we used the Thing translator by Dan Motzenbecker.\n\nWe also had badges for the sensor data and the things that can be detected. At each session the participants drew random contexts cards to decide on which social context they would phonvert in. The context card ranged from developing countries to government and politics.\n\nThe ideas that came from the workshop went beyond the idea of repurposing retired smartphones. It made us realize we are entering an age where intelligent cameras recognize the world like we do.\n\nThese intelligent cameras on small devices brings in a new power balance into our society. One idea that came from the participants illustrated this problem clearly.\n\nAccording to the participant, this is a lost pet finder. By feeding several pictures of your pet animal to the system, it detects where your pet is through the eyes of the smartphones attached across the city.\n\nAt a glance this is a funny innocent idea. However when you consider that this can also be used to track people, it opens up a lot of other questions.\n\nWhen a system that can do complex image recognition is in the hands of everyone, what could happen?\n\nSince most of the image recognition system work cloud-based, we found out that it works perfectly on a 5-year-old android smartphone as long as it has the latest version of Google Chrome.\n\nGoogle has just released their AIY toolkit for makers to tinker with visual recognition system. I expect in a year or two the world will be filled with apps and devices that use it. This could mean \u201cbig brothers\u201d everywhere that perform actions based on what it recognizes.\n\nSome dystopian ideas by the participant \u2014 a protest sensor. This read the emotions and texts in the protest to measure their violence level to be used to ensure a \u201cpeaceful protest\u201d.\n\nOverall, the workshop was a huge success where we collectively tried to figure out what is possible with AI on retired smartphones. We also had an interesting debate over ethical concerns surrounding it.\n\nProject phonvert will continue to develop this workshop further, so anybody who is interested in joining/hosting this workshop please feel free to contact me through here."
    },
    {
        "url": "https://becominghuman.ai/three-key-market-research-predictions-for-2018-5b7b6b5d2d19",
        "title": "Three key market research predictions for 2018 \u2013",
        "text": "For most, 2017 was a taxing year. Traditional companies experienced a great deal of strain on revenues, and many traditional clients in the CPG/FMCG space moved to zero-based budgeting with added pressure on how their money is spent. Here are my market research predictions for 2018.\n\nThe expansion in marketing budgets overall is the largest for just under a year, but market research budgets have fallen by 6.2%. \u2014 Research Live\n\nI\u2019m hoping 2018 is more positive. Generally, as we\u2019re faced with this underlying trend of lower budgets (even in larger companies \u2014 Unilever and Diageo have both talked up the success of zero-based budgeting programs), there\u2019s a renewed requirement for clients to start looking at new technologies and new techniques to obtain valid insights faster and cheaper than ever before.\n\nAs an avid research technologist, having worked as both a quantitative and qualitative researcher, and now as founder and CEO of Zappi, I\u2019ve seen how the industry\u2019s status quo can take time to shift.\n\nMarketing projects are happening quicker, there\u2019s a disparity of media types, and we all know about the \u2018agile\u2019 trend (at least we\u2019ve heard the buzzword). We\u2019re seeing more and more companies take to working in agile, outside of development, where it originated.\n\nGiven this increase in requirements for speed, additional levels of data and data complexity at the same time as the natural human urge for simplicity, I predict a move towards clients wanting a single research platform.\n\nIn the same way that Salesforce is the key provider of sales materials, sales recording, and sales insights, we\u2019ll see a dominant force emerge in the field. Of course, that\u2019s where Zappi would like to be and where we\u2019re working towards with our Pro platform, but multiple other companies are working on this too. We\u2019re only at the very start of this movement right now \u2014 but it\u2019s easy to imagine how transformative a single platform will be for the industry, where all of your analyzed data and reports are aggregated by machines.\n\nWe\u2019re seeing clients beginning to think about this now, so it seems 2018 will be the year that more companies realize the benefits of having a single space for research and cross-analysis, with the ability to run different types of projects, cross-analyze findings, and have all key data in one location.\n\nWe\u2019ll see the early stages of the trend very soon, without any major market shifts in my view until at least 2020.\n\nIn 2018, we\u2019ll come to see larger traditional research companies deciding to push a very specific consulting offer.\n\nMore clients will be in dire need of help as they manage fundamental changes when becoming more \u2018agile\u2019 and see ever increasing amounts of data crossing their desks. Seeing as research project execution itself has been largely automated, those larger research companies with great talent, thinkers, and storytellers will start to look at how they can provide deeper insights and skillful storytelling \u2014 whatever the project.\n\nIn the face of so much evolving technology, there\u2019s still space for this kind of consultation-type approach to clients.\n\nIn 2017, there\u2019s been a lot of talk around Artificial Intelligence, being unkind you could suggest that much of it was \u201chot air\u201d. But, if this year has been spent talking about AI \u2014 I\u2019d like to think that in 2018, we will see more of it understood and implemented.\n\nIn the technology world, the focus is on machine learning which is a branch of AI. The research industry is doing a lot of thinking around this, there are many companies, including Zappi, who are experimenting with, and implementing, these solutions to solve real-world business needs. The industry needs this to make research better, faster, and most importantly \u2014 cleverer.\n\nThe trend will continue for years and we are all unsure of where or how it will end. We are focussed on improving the researcher, making them the hero through technology (the Iron Man or Woman of research!) but we will see multiple different approaches to the use of these technologies.\n\n2017 was tumultuous politically and for businesses everywhere. I\u2019m pleased to say that despite this, it was a year worthy of celebration for Zappi:\n\nNext year will be even bigger. We\u2019ll be spending a lot of time at industry events, learning more about the industry\u2019s breakthrough innovations, and we can\u2019t wait to see which of the above predictions will come first."
    },
    {
        "url": "https://becominghuman.ai/towards-forms-text-recognition-using-deep-learning-f8b39840b984",
        "title": "Towards Forms Text Recognition using Deep Learning \u2013",
        "text": "Banks, universities and shops are using forms in order to keep track of some information. Having an application that automatically will transform forms into digital data would have a lot of popularity.\n\nLine/word/character text recognition handwritten or typed have good results in the research and industry community. In order to recognize a full page most people detect words using some machine learning techniques: use a threshold in order to get just the written part out of background, then in order to get the line/word techniques like energy based, histogram, filters are used. But for a lot of these methods some files can be found that fail the method.\n\nIn the following I will present a simple method to get the lines from a photo.\n\nI will be using Python and OpenCV. First read the image, convert from 3 channels to 1 channel and use a threshold to get just the text.\n\nThe image used was from the IAM dataset.\n\nNext we will use a filter in order to make the text bigger and delete some noise that are not text.\n\nThe find contours, the function will follow the margins of the forms in the images and make for each a box. We are interested in a bigger boxes.\n\nHere is the result:\n\nHere is a example of a failure:\n\nThis happened because the lines are close to each other and some characters from different lines are touching.\n\nMy idea is to use deep learning in order to detect these lines. As long as we have a dataset with labels for lines it will not be difficult. I have use the notion of Attention, convolutional neural networks(CNN) and 2D Recurent Neural Networks(2D-RNN).\n\nWith CNN and 3D-RNN the model will create a feature map, then the loss function to be minimize, will be the sum of the pixel out the words minus the sum of the pixel inside the words boxes. This loss function will make the network to learn how to identify the lines!\n\nMore information about the technical parts will follow in the next articles!"
    },
    {
        "url": "https://becominghuman.ai/machine-learning-apis-are-expensive-for-normal-developers-thats-why-we-built-machine-box-1e7eacaa98d",
        "title": "Machine learning APIs are too expensive: that\u2019s why we built Machine Box",
        "text": "The hottest shit around right now is blockchain\u2026 I mean Machine Learning. It\u2019s what powers self-driving bitcoin, right?\n\nThe truth is, developers only really care about something when it\u2019s useful. And we can only figure out what\u2019s useful if we can take it for granted as a capability. Otherwise, we\u2019ll never have the true freedom to innovate.\n\nThe early internet (when it was expensive to take part) was pretty rubbish by today\u2019s standards; dancing Jesuses, the element and repeating backgrounds where any old hex code could be thrown up on the webpage until you\u2019re left dry retching into your mouse-that-had-a-ball-in-it-for-some-reason. It wasn\u2019t until basically everybody had access to it, that the floor was raised and the online revolution could properly begin.\n\nIf you try to innovate on top of Machine Learning APIs, and try to test it, using cloud hosted Machine Learning APIs or heavy enterprise hosted solutions, you\u2019ll quickly run up against either extreme costs (at a previous company, we had a bill of $40k in one month \u2014 for testing) or impossible setup with complicated licensing, all of which will stop you dead in your tracks.\n\nAt Machine Box, we want to do something different. Machine Box aims to give developers what they need, right now, for free so they can start their innovation, and get this bloody Machine Learning revolution off the ground once and for all.\n\nFor example, Facebox unlocks the ability to teach and recognize faces in images and videos, with two or three simple APIs. You don\u2019t need to read pages of documentation or learn complicated workflows to incorporate start-of-the-art face detection and recognition into your app today. Tagbox does the same thing for image recognition. And there are more boxes too, and more coming.\n\nAll Machine Box technology runs inside a neat Docker container, that can easily be spun up and down in one command. Once the box is running, each developer gets their own private console (hosted from inside the container, Morty!) containing the API docs, a web based Try it now panel, and even interactive apps that let them explore the possibilities of this new capability. Once the box has fulfilled its request, you can spin it down and it stops existing.\n\nSince everything runs inside a Docker container, you are free to run them in your own environment; one that you control whether on-prem or not. So you don\u2019t need to send sensitive information over the web to the cloud. Google and Microsoft never even see your data, and neither does Machine Box. If you believe in privacy, this approach becomes invaluable.\n\nOne of the defining differences between Machine Box and its competitors is the pricing. To use the boxes for commercial use, you just need to buy a simple flat-rate monthly subscription. And you can use the boxes as much as you like.\n\nWe think this is the first step towards making Machine Learning capabilities truly accessible to developers.\n\nMachine Box is currently a very young company and small team of dedicated people, which we think makes us ideal for a partnership or integration.\n\nWith \u201cimpressive support\u201d and \u201cthe best developer experience of a product ever\u201d we are ready to be entrusted to always deliver cutting edge tech and service.\n\nIf you own the product and have a little technical understanding, then you should check out the next section; because it\u2019s actually really quite simple once you know how to run commands in a terminal and use a web browser.\n\nIf typing commands into a black rectangle isn\u2019t your bag but are still interested in ways Machine Box can help improve your app, feel free to contact us and we\u2019ll give you a quick demo, and chat about what you\u2019re up to.\n\nThe hardest thing developers have to do to get started is go to https://machinebox.io/account and enter their email address in exchange for an API key.\n\nYou don\u2019t even really need to create an account right now.\n\nYou\u2019ll be emailed a magic-link which you can use to access your key.\n\nOnce you\u2019ve set your key (in an environment variable as recommended), you just need to use the command to run the box.\n\nThe box will download and run locally. Head over to and check out under the hood:"
    },
    {
        "url": "https://becominghuman.ai/we-need-to-spend-more-time-talking-about-data-science-failures-4f53040b529e",
        "title": "We need to spend more time talking about data science failures",
        "text": "In Star Wars: The Last Jedi, Yoda reminds us, \u201cthe greatest teacher, failure is\u201d. As a student of data science, sporting a \u2018data science\u2019 google alert, my news feed brims with the boasts of data science research applications. There\u2019s very few disclaimers and even fewer failures (a phenomena that has not escaped this guy, either, although this gal is a delightful exception). Having spent almost two years applying data science in the domain of the administration of public services, I\u2019m going to put out there three lessons no-one should learn the hard way. Because I did. And it would be a waste.\n\nFirst, a little primer on how machines learn. They learn much like children do. Take the example of a picture book of animals, labelled with their names. We show the child the picture and say the animal\u2019s name. We repeat the process until a child sees, for example, a cat on the street without the label, is able to recognise it as a cat, and exclaims \u201ccat!\u201d. In the class of machine learning referred to as supervised learning, the picture book with the labels, is the training set of data. And the cat on the street is the test set of data. Machine learning accuracy is measured by the rate at which it sees an animal without the label and recognises it correctly.\n\nLesson 1: Context (there is none)\n\nIn machine learning there is no context beyond what is shown to the machine. In the example of our child and the picture book, imagine the book only contains pictures of cats. It is entirely possible that a child, out in the world, having spied a four-legged furry thing with two ears and tail, exclaims \u201ccat\u201d when they are actually looking at a dog. The same applies to machine learning algorithms. It knows no external truths. And here is a really very *funny* experiment that illustrates this.\n\nIn the domain of the administration of public services, the context that is missing usually is policy. These administrative datasets typically contain that \u2018person a\u2019 received \u2018service b\u2019 on \u2018date c\u2019. There is no field denoting that this took place during \u2018policy setting d\u2019. It\u2019s important because a change in policy could be material on the criteria for persons receiving \u2018service b\u2019. It\u2019s also important because that\u2019s the first thing a policy person is going to say. And, if your\u2019e not ready for it, it can sink your work.\n\nWhy? How it plays out in the data is a whole lot of \u2018service b\u2019s administered up until the date of commencement of \u2018policy setting d\u2019. Followed by a substantial decline in \u2018service b\u2019 after that date (for example). In the absence of policy information, the machine learning algorithm is looking for explanations in the data it can see to account for that decline. Anything it finds will not be valid. It will call a dog a cat. And you\u2019ve set it up for failure.\n\nIt\u2019s not hard to engineer features representing policy. Is_policy_setting_D becomes a binary (or boolean) field added to the dates, as \u20181\u2019 (or \u2018true\u2019) for the dates it was in force. Do that. The policy person will ask. And you will be ready.\n\nThis one seems obvious. There are plenty of articles warning of the dangers of how easy it is to find correlations in big data. What could I possibly add? It has to do with the context within which you are performing your work, who your audience is and what you\u2019re ultimately producing. To illustrate, let\u2019s say you\u2019ve performed a proof-of-concept prediction model on a government administrative dataset. You\u2019ve reported on what you did, including your results \u2014 the model\u2019s accuracy and predictive features.\n\nEnter: the policy people. They read your report and\u2026\n\nPolicy people are used to reading reports that can inform new policies. They want to formulate policies that will maximise positive outcomes for citizens whilst maintaining fiscal responsibility. They want to know which service or combination of services caused better outcomes (and services, the absence of which, caused worse outcomes). Their questions have causation at their very heart. Your machine learning algorithm (and your written report about it) shows correlations, not causation. The only policy decision a written report on a machine learning algorithm can inform, is whether or not to deploy the machine learning algorithm in situ.\n\nWhat you can do about it? There\u2019s a few things. Firstly, let the predictive features discovered by your machine learning algorithm (you know the correlated ones), let them be the \u2018shortlist\u2019 for further analysis or testing. Second, use those predictive features you\u2019ve discovered to revisit the evidence-base and research articles, the existing body of evidence. You may have only discovered correlated features but if those already exist in the evidence base within a causal relationship then the further analysis has been done for you. In short, make causation analysis, the \u2018next steps\u2019.\n\nThis lesson can go either way. At its core is the question \u2018do you have sufficient data to predict what you are trying to predict\u2019. One school of thought is, for a machine learning algorithm that is to replace a human decision, then, everything the human takes into account when making the decision should be available in the form of data for the machine to also parse. The other school of thought is, \u2018hey, we know we don\u2019t have all the data that, strictly speaking, would sensibly account for the thing we are trying to predict. But we DO HAVE A F@*K TONNE OF DATA so let\u2019s just see if we find a proxy for it\u2026!!#*@!!\u2019. I\u2019m not saying this isn\u2019t a valid approach. You could find an incredible breakthrough proxy predictor\u2026 But not without two clear risks. The first risk is that you won\u2019t find that proxy. Second, even if you do discover the proxy, you are signing up to technical debt in the form of the fact that YOU ONLY EVER HAD A PROXY and you will constantly need to tune and check your algorithm for concept drift in lieu of the fact that it will only ever be a proxy and that its relationship as a proxy may change or fail over time.\n\nThis one is hard to stay on top of. Everything in a data scientists\u2019s mind will be telling you, with pupil\u2019s dilated to the size of your budget, \u2018let\u2019s just pass this pile of data in and see what we find!\u2019. After all, this is the new gold rush. But if there is no logical reason for these data points to predict what you are trying to predict, then be prepared. Be prepared to state the risks aloud. Quantify them. Let\u2019s be adults.\n\nYou may not be working in the domain of machine learning on government administrative datasets (luck you). You may have learnt nothing. Nothing more than to, in the least, share your failures. And perhaps, you will have taken away different lessons. I mean, is the second lesson the knowledge that you are working with correlation and not causation, or is it really about knowing from the outset what the threshold of success for your algorithm is, whom is the stakeholder that would ultimately implement and own it, and how repeatable (implementable) is your data processing chain (tell me all the data you used is actually available in real time, ahead of time, or in enough time to predict [and take action on] what the hell you are trying to predict).\n\nI live in a world where there is a great temptation to start using existing government administrative datasets to power data-driven decision making. I warn that we do so with care. Get the project scope right, from the outset. Learn from the University of Chicago\u2019s Data Science for Social Good\u2019s vast experience. Comment here with your learnings. Let\u2019s share our failures in order to make this crazy little thing we call data science a better place to be."
    },
    {
        "url": "https://becominghuman.ai/priming-neural-networks-with-an-appropriate-initializer-7b163990ead",
        "title": "Priming neural networks with an appropriate initializer.",
        "text": "Testing setup Lets start with the testing phase now that we understand how these different initializers work. In this quick experiment, we will be testing the effect of different weight/bias initializations on a very simple variational autoencoder (VAE). As a side note and if you are not super familiar with this architecture, VAE\u2019s are used in unsupervised learning where no labels exist. They \u201cencode\u201d a given input image and squash it into a lower dimension space, then decodes it back to its original size. The goal is to recreate the input as accurately as possible from this lower dimension space. The compression happening here is lossy as some information is lost. Although not super useful except maybe for denoising simple images like hand written characters, the architecture is simple enough for our use case.\n\nWhy? Adjusting optimizer-specific parameters such as the learning rate, batch size and number of epochs are up there on the list of things to tweak while training deep neural networks. Other architecture-specific hyper parameters are sometimes completely overlooked. This is often the case with high level api\u2019s such as keras where many arguments are set to defaults and are rarely modified. Initialization is one of these overlooked hyper parameters. By kicking off your training with weights/biases at an optimal location, you are basically helping the optimizer reach a minimum (hopefully a global minimum) much faster.\n\nWe will be using the MNIST dataset to prototype. The VAE will essentially have 3 layers:\n\nEnsuring deterministic behavior We need to do this as we want to study the effect of varying one variable and one variable only i.e. the weights/biases. This can be very tricky in larger networks. We need to examine any source of randomness in our network. Luckily in our smaller isolated problem, we are able to make the process deterministic. Firstly, we are running everything on the CPU. This essentially means we can stay away from 3rd party libraries such as cuDNN that exhibit a deterministic behavior as a means to trade a bit of accuracy for speed. Additionally, we don\u2019t have any operations that are non-deterministic by nature (even on CPU). We also set the global graph-level seed so that all operations use that fixed seed. Finally, our training data itself is locked without any shuffling. Now with a deterministic process, we can change a single variable (initialization strategy in this case) while ensuring that varying results are caused by that change and that change only.\n\nTest 1 \u2014 vary the weight tensor initializer For the first test, we will be training 8 networks, each with a different weight tensor initializer and with the bias initializer set to zeros. Lets examine the training loss curve after 10 epochs.\n\nHere, we can clearly see we have two losers straight off the bat. The ones and random_uniform initializers preform relatively poor and start at a very high loss. They also converge further down the step count. By looking at their distributions, we can tell why this is happening: They both have all positive values. Now lets zoom in to get a better look at how the others did. Now that we can embed plotly\u2019s interactive plots in medium, this one is fully interactive so that you can zoom in and explore different areas.\n\nWe can also see that random_normal and zeros do not preform as well either. Random_normal has a lot of relatively large values close to 2 and -2. Zeros, however, enforces all weights to be the same. This means that they will have the same error and thus there will be no asymmetry between the neurons in the network.\n\nNext we have the identity initalizer with a relatively better performance. Admittedly, this initializer only works properly with square tensors. In our earlier exploration, we made sure this was the case. Here, however, our weight tensor is not square. Nevertheless, there are still some ones in the identity weight tensor (grey). We can compare this with the zeros weight tensor (blue) and immediately see how adding a few ones in the matrix is able to break the symmetry and improve performance.\n\nFinally, we have both the glorot and the orthogonal intializers giving us the best performance at any given step. Again, by looking at their distribution, one can spot what they have in common. All three are centered around zero with all samples drawn from within ~0.2 on either side. Nicely zero-centered and within a -1,1 range or small random numbers\u201d as CS231 suggests.\n\nTest 2 \u2014 vary the bias tensor initializer For the second test, we will train the networks again but with a different bias tensor initializer each \u2014 and with the weight initializer set to glorot_unifom. We will be working with 6 in liue of 8 initializers here as we drop the orthogonal and identity intializers \u2014 they do not work with 1 dimension tensors (vectors). Let\u2019s look at the training loss curve after 10 epochs.\n\nNot surprisingly, we find that the performance variance when using different bias initializers is not very significant. We also notice, the difference between different bias performance diminishes towards the end of the training. As the weight tensor controls the gradients of the linear operations within each neuron, the bias tensor controls the shift. The relatively larger values in the ones and random_normal initializers might be throwing off the training slightly as we see on the plot. This is specific to our use case since we are using relu activation on our input layer and sigmoid activation on the hidden layer. Many large values are set to 1 affecting our training loss. However, all bias initializers provide more or less similar performance as the heavy lifting i.e. symmetry breaking is done by the weight tensor.\n\nA jupyter notebook with the entire experiment can be found here. And as always, keep flossing!"
    },
    {
        "url": "https://becominghuman.ai/day-7-data-cleaning-all-that-you-need-to-know-about-it-23b05738abe7",
        "title": "Day 7: Data cleaning \u2014 All you need to know about it",
        "text": "Today we are on the 7th day of our journey to acquire skill set in data science using R programming. To know more about this series, refer this article. To refer other articles, read this.\n\nData cleaning could seem simple when seen for the first time. But is a difficult process involving lot of steps carefully chosen and sometimes tailor made for the data set. It is not always performing a stipulated set of tasks and getting results. It may involve repetitive , redundant and cyclic methods applied right from the stage of data collection till finalising the model.\n\nData ,when collected from various resources as stated in the previous article, can be really untidy.\n\nLet\u2019s see what kind of problems data can pose while you are being a data explorer and what can be done to handle them all. Also, in this article, we are going to see the various stages which data undergoes while it is being cleaned.\n\nData technically undergoes three stages in order to become fit for statistical analysis.\n\nThe one shown below is a raw data. It can be even more ambiguous than this too.\n\nTechnically correct data is something which is placed in an understandable format like a table or data frame with acceptable data types and scaling.\n\nConsistent data refers to that data which is without missing values, outliers and perfect to apply a statistical model on it.\n\nNow let us see what happens while raw data is being converted to technically correct data\n\nData set can be very untidy as shown above. It may not be stored in files of same formats. It may have no header information, no acceptable values and many such problems. Let\u2019s study them and try to come up with some solutions.\n\nInconsistency with data types occurs when all the values of a feature are not of same data type. This leads to difficulty in applying operations on the columns.\n\nFor example, while we are dealing with age of a person, we expect it to be of type \u2018numeric\u2019, but a sudden character value may pop in making our numeric operations difficult. Hence, it is always better to find the data types. This can be done with following functions.\n\nis.numeric() , is.character() ,is.XXX() (XXX refers to data types) functions help us to find the data type of values. Then they can be converted to desired data type using as.numeric() , as.character(). This process is called Explicit Coercion.\n\nConsider a case of predictive modelling [we will know more about it in coming articles] of \u2018Cancer data\u2019. Let the output feature be \u2018diagnosis\u2019 which can have two values- Malignant , Benign.\n\nSome records of the dataset may have this feature value as \u2018M\u2019 , some may have \u2019 mal\u2019 and some may have \u2018B\u2019 or \u2018benign\u2019. You can see how this data is pretty much disturbing. Hence, we do character and string manipulation as follows\n\n4. White spaces in between strings are removed. \u2018stringr\u2019 package provides functions like trim() which help in removing white spaces in strings.\n\nNow that we have a visually appealing technically correct data set, start understanding more about it. We can do that using our head() , tail() , summary() , names() and dim() functions. Please refer this article to know more about them.\n\nAll of these functions are applied in the below example.\n\nWe can see that technically correct data set has data neatly placed in columns, with distinct data types, formats and can disguise itself as a consistent data. But it has hidden problems like\n\nMissing value is that placeholder in R whose data type is known but value is not known and represented by \u2018NA\u2019.\n\nis.na() function helps us to find if there are \u2018NA\u2019 s in a column. It returns a boolean value.\n\ncomplete.cases() is also a function to find NAs. It returns bool value based on all the data cell values in a record.\n\nna.omit() function removes all those records who have NAs with in them.\n\nWhile above methods are simple ways of handling missing values, they can\u2019t always be enough. Hence, we go for more advanced methods called \u2018Imputation methods\u2019.\n\nImputation is the method of filling missing values with appropriate value using different methods\n\nIn this method, we fill the missing value with mean of the column.\n\nIn this method, missing value is filled with the value from a similar record. Meaningful hot deck imputations wouldn\u2019t cause any problem to our data analysis.\n\nWhile implementing hot deck imputations, we may choose random values. Using random values may result in different outputs for different executions. In order to get random but repeatable values, use following function.\n\nIn this method, we try to get a predicted value of the missing value with the help of finding nearest neighbour donor records.\n\nOutliers are the values much distant from the range of the values allowed for that feature. Their inclusion can lead to a bad fit later while building a model. For example, have a glance at following two linear regression models.\n\nConsider a case where the age of a person is negative in a record. These kind of inconsistencies are quite normal with the data. Hence, it is always good to find the range of the values of a column and finding the unaccepted values. Also, there might arise a need to preprocess only certain part of data while leaving other part as such like changing units from meters to centimeters in a length feature.\n\nLike this, there can be many inconsistencies. We need to carefully handle them. While doing so, it is always better to log the details of our process carefully for future reference.\n\nThus data cleaning is a process involving more of intuition of the data scientist or statistician rather than using a set of functions. The best data cleaning method will always be the one which is found only after thoroughly scrutinising data.\n\nThis has been inspired by this awesome work \u2014 An introduction to data cleaning with R. Do read it too. Did you like reading this article? If so, don\u2019t move away without clapping and sharing among other enthusiasts. Happy Learning R!!"
    },
    {
        "url": "https://becominghuman.ai/magics-of-quantum-computing-1bf15dbc540d",
        "title": "Magics of Quantum Computing \u2013",
        "text": "Technology, Boy! Technology! Well, not meant to be racist. An expression. Click that button and you get your food delivered. Swipe right and you meet your date on Tinder. Visit a website and you get your Hit-man. Make some clicks and you buy your own personal jet. And it all happens through a value of some binary numbers in dumb devices which we call computers.\n\nBefore, computers were huge and bulky in size occupying enormous space. They used to produce lots of heat. They have had the very low processing power, no high-end graphics like we have today. The storage capacity of the computer used to be measured in Megabytes. Can you imagine it? Compare those days with the modern trends. For an instance, today\u2019s single high definition movie needs storage at the least of a couple of Gigabytes.\n\nSince half a century, the size of computers has been decreasing while exponentially increasing the processing power. Take an example of nanobots that are being built for medicine and industry.\n\nThe supercomputers today have reached the processing capacity like our brain while consuming huge powers, generating more heat and taking enormous space. Previously, cats and dogs were more intelligent on recognizing objects, people and had a better sense of decision making. At least machines have gotten smarter than the pets in the past couple of years through artificial intelligence.\n\nThere is a popular paradox on finding the shortest path. Suppose, a salesman has to urgently make a pizza delivery through shortest route available. The salesman is new in town. Going through every available route to know the shortest distance is a huge waste of time. But he has no choice than to try this only option. The regular activity computers like laptops, smartphones and even supercomputers tend to go through this very strategy.\n\nIt\u2019s the job of quantum computing to make it happen.\n\nQuantum Computing is a computing phenomenon that involves the calculation and signal processing using superposition and entanglement of an electron or similar quantum particle. Like binary bits (0 and 1), a qubit is a unit of quantum information. Qubit is determined by the above-mentioned superposition and entanglement. The design, architecture, and mechanism to process and read qubits are still evolving and getting more advanced. Likewise, a qubit can be zero and one at the same time. Since it can show many values at the same time, it can process many operations at the same time (unlike multi-threading in a classical computer). Hence, it can travel all the routes at the same time and find the shortest path at the very first step.\n\nHear more, ever heard of jet software in the air planes? Well, It is too complex for the classical computers to process and test those jet software. From the classical computers, I meant today\u2019s sophisticated supercomputers. Or what about exploring planets and stars across the universe. Given enough time and development, the quantum computer has the potential to get the job done more accurately, faster and efficiently. For an instance, suppose you need to find a user\u2019s record from the data servers containing not Terabytes but Zettabytes of data. Let alone leave the idea of searching the same file from the whole goddamn internet. And for the moment, the capacity of the internet is measured in Petabytes.\n\nQuantum computers can be leveraged to research on the diseases while ultimately developing more effective drugs. Is it the issue of GDP optimization? Knock knock, the quantum computer is here. Or is about making the advanced next-generation supercomputer? Quantum computer is still here. Oh, wait, forgot! It is about researching on artificial intelligence to develop the super-conscious machine. Well, you got the point. See folks, quantum is the new age of computing and it has the potential to do amazing things. There are many countries in around the world who are currently researching on quantum computing. Till date, there have been many companies manufacturing the quantum computers. Some popular companies are D-waves, IBM, NEC Corporation and so on.\n\n\u201cA recent research suggests that a single human brain has the capacity and could store one petabyte of data (or a quadrillion bytes)\u201d.\n\nFun fact, classical computers store data in the form of 0 and 1 (in two forms). And while brain cells use 26 different forms while storing and processing data. And then quantum computers store, process and analyse the data on the basis of the position, the momentum of the electron.\n\nNo matter the potential, the means to operate a quantum computer is very complex. And it also has different architecture. Modern classical computers are indeed best for carrying out daily life activity. Quantum computers are huge, bulky and are very expensive. Rest assured, the commercial version of the product is still a long way journey.\n\nFollow me on Medium to hit my upcoming articles. Comment to send your precious insights. Do Clap and Share."
    },
    {
        "url": "https://becominghuman.ai/weve-been-pre-programmed-for-self-driving-cars-since-the-50s-but-why-is-that-s-exciting-ba532a88b6af",
        "title": "We\u2019ve been pre-programmed for self-driving cars since the 50's, but why is that exciting?",
        "text": "Whether we\u2019re prepared for it or not, the future of mobility might mean removing ourselves from the labor of operating vehicles. That\u2019s not profound, but what I recently realized during a heated recent debate with family, is that we\u2019ve been groomed for robots to drive us for over half a century.\n\nToday, by law, all new passenger cars sold in the EU and in the US are required to implement an anti-lock braking system. We all take ABS for granted. It\u2019s the system that allows the wheels on your car to remain on the road surface when you slam on your brakes. You don\u2019t have to be a professional drifter anymore with cadence braking (what the old-timers refer to as \u201cpump ya brakes\u201d). To oversimplify what actually happens when you brake with ABS, you\u2019re sending a signal to a robot to make the decision on how to apply the brakes so you don\u2019t die. Yes, stepping on the brakes means a robot assesses speed, road conditions, and other factors then decides whether you live or you die. Robots have been a thing in cars since the 50\u2019s from your brakes to cruise control and other technologies we take for granted today.\n\nThe point is, each passenger vehicle utilizes robots, so you can do human things like text and drive. I\u2019m talking to you, we all know you do it, I see you on the road each morning.\n\nWe have all been been primed for this future, but then what happens when the inevitable happens and we don\u2019t have to drive anymore? Will we even need to own a car in this future? This post, a brief look at autonomous vehicles in the ride hailing \u201cUber-like\u201d world we live in today and how robotics soon fits into that world.\n\nGetting a ride today means we don\u2019t have to stand on the side of the road, arm extended with a thumb out. Companies like Alphabet are evolving how we get around. Alphabet for example has bet big since 2009, to solve not needing to drive with self-driving technology. Alphabet\u2019s mission, like many in this industry, is to get everyone around easily and safely. Naturally, with tens of thousands of logged self-driving miles weekly, Google\u2019s goal is to let the robots do the hard work so you can have fun or sleep during your commute.\n\nWe\u2019ve been groomed to accept the benefits of avoiding repetitive tasks, it\u2019s what makes us human. Amongst the many reasons why autonomous commute and services will benefit society I\u2019d like to highlight three:\n\nImagine all the things that disappear when you don\u2019t have to drive. The space dedicated to driveways, parking lots get reclaimed. It becomes exciting what we can do with this space. Centralized fleet services will emerge making auto repair shops irrelevant. That space can be used to make our environment more practical for humans and not cars. Above is a photo of the Brooklyn Bridge, proof that people will walk more if space is reclaimed. There are many examples of reclaimed space, another example is SF pier. These spaces are thereby rapidly adopted and heavily trafficked without vehicles.\n\nWith the confluence of connected infrastructure, vehicle to vehicle communication, we are quickly approaching a world where an accident while driving nears a zero-probability event. A Virginia Tech study concluded that the national crash rate of 4.2 accidents per million miles is higher than the crash rate for self-driving cars (L3 autonomy), which is 3.2 crashes per million miles .\n\nThe time spent not waiting on the next car bumper watching it move an inch can be used to make us happier. We can entertain ourselves, increase our productivity, connect with each other, and live better lives.\n\nSelf-driven hailing or taxi services will soon be a crowded market. This nascent industry will be led by incumbent ride hailing services like Lyft and Uber. Car companies like Toyota, Tesla, Diamler, and others will want a piece of the pie but it is the approach by startups I\u2019m more excited about. Drafting behind Google\u2019s 3.5 million-mile traction in the future of self-driven transport is a little known company called Voyage.\n\nVoyage is an autonomous vehicle service from a team working to bring about the end-goal of self-driving cars: a world where anyone, anywhere can summon a car directly to their doorstep, travel safely to their destination, all for an \u201cextremely low price\u201d. They are claiming:\n\nA bit about Voyage the company was Co-Founded in 2016 by former Udacity, Apple, Uber, and Google employees.\n\nThey have seen a bit of traction in 2017 with three deployed Voyage self-driving taxis in the San-Francisco, California region.\n\nLike with Lyft, you summon a Voyage self-driving car right to your doorstep with an app.\n\nDuring your ride, you control your \u201cVoyage\u201d (AC, music etc.) right from the app, disconnect if you want because the car will take you directly right to your destination.\n\nWell, yeah, I can\u2019t imagine not but maybe that\u2019s not all bad. My guess is that over time, it\u2019s that fine tuned personalization during the time I spend not driving that provides me a more personalized and comfortable ride. This is the business in my view where Voyage sells user data from user personalization. Again, this is just my guess. This space will be very crowded very soon:\n\nIf I am right, the Voyage business model will be to address current user behavior trends in the market. Younger Americans for example find the cost of owning and operating their own vehicles less desirable. Legacy car manufacturers from Ford to GM\u2019s Maven ride share service are exploring business models to solve problems within this trend.\n\nThis insight is evident in approaches like Voyage; in addition to having an intuitive first-user experience through an app, the core service is focused on user comfort, and entertainment while you ride, not whether the vehicle is a 6-cylinder or 8 as our parents use to obsess about.\n\nAs a seasoned product lead, I\u2019d like use a quick example at how I\u2019d think through some of the issues with the known business models around self driving cars.\n\nFirst there is the issue with user adoption. Autonomous transport is a nascent industry and as such consumers may not immediately not see the value, safety, freedom of affordable autonomous transport. Approaches include using free rides as a hook and innovating on how to deliver personalized content during the first ride.\n\nThough a car may look different on the outside, a car as a machine hasn\u2019t changed much in a century. Think about how we changed our world around cars. We built highways, cities, we have evolved the entire way modern civilizations live around owning and using car.\n\nAmericans spend 30 billion hours collectively a year commuting and it\u2019s killing us not just through car crashes, but for those with longer commutes, obesity, high cholesterol, high blood pressure, back and neck pain, divorce, even depression.\n\nIt\u2019s easier to fear what we may lose with the advent of self driving cars, after-all, many American\u2019s depend on driving for their livelihood. It is harder to imagine what new opportunities grow from in it\u2019s wake, that\u2019s why I\u2019m excited. Perhaps being pre-programmed for a robot-driven future is not a bad thing after all."
    },
    {
        "url": "https://becominghuman.ai/how-can-i-become-a-data-scientist-9ee42106153d",
        "title": "How can I become a data scientist? \u2013",
        "text": "I could only tell you what I did till now and what I intend to work on additionally to become a better data Scientist.\n\nWhat follows is my own Data science Curriculum. This is aimed at Computer Science with a Specialization in Machine Learning.\n\nMy main aim here is to learn about Mathematics, Statistics, Computer Science and Machine Learning, though not necessarily in the same order.\n\nI have categorized the courses here as of two types:\n\nA Great Class by a great Teacher. I Would definitely recommend this class to anyone who wants to learn LA.\n\nThis is an Introduction to Computer Science class taken by David Malan. Helped me with many misunderstandings and helped build intuition around the whole CS playground. Starts with a basic introduction to C and some programming exercises. Ends up teaching basics of PHP, Javascript and HTML/CSS as well. The projects in this class are really awesome. The github code repository for this class is at HERE\n\nThe course is an introduction to many of the important concepts in computer science.\n\nTalks about simple algorithms, Asymptotic times, Classes, OOP, Trees, Exceptions, Assertions, Hashing and a whole lot of other stuff.\n\n(F3) Algorithms and Data Structures \u2014 MIT OCW: CURRENTLY Working on\n\nThis is a series of 6 short but good courses. I worked on these courses as Data science will require you to do a lot of programming. And the best way to learn programming is by doing programming. The lectures are good but the problems and assignments are awesome. It consists of three main courses:\n\n1> Interactive Programming in Python: The Course starts with teaching Python but suddenly moves into creating graphical user interfaces and games using python in codeskulptor. I created some very basic games in this course as part of the coursework. Some of them are:\n\n2> Principles of Computing : This course adds on to the previous course but here the focus is more on thinking programmatically rather than GUIs. The projects are really great as the course progresses with creating games.\n\n3> Algorithmic Thinking: This course starts with a focus on graph algorithms and data structures. The codes are sourced at Github\n\nI took this course to enhance my understanding of probability distributions and statistics, but this course taught me a lot more than that. Apart from Learning to think conditionally, this also taught me how to explain difficult concepts with a story.\n\nThis was a Hard Class but definitely fun. The focus was not only on getting Mathematical proofs but also on understanding the intuition behind them and how intuition can help in deriving them more easily.Sometimes the same proof was done in different ways to facilitate learning of a concept.\n\nOne of the things I liked most about this course is the focus on concrete examples while explaining abstract concepts. The inclusion of Gambler\u2019s Ruin Problem, Matching Problem, Birthday Problem, Monty Hall, Simpsons Paradox, St. Petersberg Paradox etc. made this course much much more exciting than a normal Statistics Course.\n\nI will definitely be on a lookout for more courses by Joe after this and I have already done one more course by him \u2014 CS109. More on that later.\n\nThe Top 10 Ideas covered in this class are:\n\nSolving the problem sets and the midterm reviews helped me a lot in grasping the abstact concepts.\n\nUses Degroot and Schervish for instruction. No lecture videos available so I plan to read the book and Complete Problem Sets Online from the Stat111 website. I so wish the lectures were there.\n\nA lecture Series on Bayesian statistics by Jarad Niemi at ISU.\n\nGot highly interested in Probability after STAT 110 so added this here. It is an alternative to one of the next courses to take after STAT 110 that Professor Joe Blitzstein talks about in the course apart from STAT 111.\n\nThis is a fantastic course for learning about R as well as the implementations of various machine learning algorithm in R. Very Basic. Very Crisp and very informative. The scenarios and examples range from Moneyball to Watson. The only problem with this course is that it\u2019s problem sets feel a little repetitive.\n\nHere is the location of my R code repository for this course\n\nMy first ML Class. It took a little bit long to grasp the concepts but in hindsght it might be because of my lack of exposure to the material. It was my first grapple with tools like R and Python. Covers a whole lot of base from R to Python to Mapreduce. Would put it here as it gives a thorough perspective of the whole data science space.\n\n(F3) Data Science CS109: \u2014 Again by Professor Blitzstein. Again an awesome course. Watch it after Stat110 as you will be able to understand everything much better with a thorough grinding in Stat110 concepts. You will learn about Python Libraries for data science, along with a thorough intuitive grinding for various Machine learning Algorithms. Course description from Website:\n\nContains the maths behind many of the Machine Learning algorithms. The Game Changer machine learning course. I will put this course as numero uno as this course motivated me into getting in this field and Andrew Ng is a great instructor.\n\nVery Easy Course. Taught the Fundamentals of Hadoop streaming with Python taken by Cloudera on Udacity. I am doing much more advanced stuff with python and Mapreduce now but this is one of the courses that laid the foundation there.\n\nThis is a series of courses in Spark taught by Anthony D. Joseph,a Professor in Electrical Engineering and Computer Science at UC Berkeley and Ameet Talwalkar, a well known name in Spark community.\n\nThis course delivers on what it says. It teaches Spark. Total beginners will have difficulty following the course as the course progresses very fast. That said anyone with a decent understanding of how big data works will be OK.\n\nThe top ideas covered in this course are:\n\nI certainly liked the Mini Projects in the class:\n\nSome of the courses here may seem repetitive but they all have provided some sort of additional skills therefore I have put them here.\n\nI will update this answer for more details as I complete the TODO courses on the list. I also did the Data Science Track from Johns Hopkins on Coursera but didn\u2019t find it good enough.\n\n1. Maintain an active Blog: I have my own blog MlWhiz where I try to put down whatever I have learned. This helps me whenever I need a code fragment or I need to revise something that I have forgotten.\n\n2. Learn to use Git: I try to put all my codes up on Github. This helps me to keep track of the things I am doing and also makes me more organized when it comes to coding.\n\nOriginally posted as an answer for:Data Science: What classes should I take if I want to become a data scientist?"
    },
    {
        "url": "https://becominghuman.ai/self-driving-miniature-car-race-cb870a27b8da",
        "title": "Self-Driving Miniature Car Race \u2013",
        "text": "The autonomous mobility meetup group based in Berlin is regularly meeting to provide training in AI for self-driving vehicles, have get-togethers and race autonomous RC-size cars. In this blog post I want to introduce the two teams that competed at our latest race in the rooms of OpenCISCO on the 8th December 2017.\n\nThe first team I would like to introduce is Coding Force which consists of Arndt and Bernhard Wei\u00dfhuhn and their car DDDonkey. What distinguishes them is their use of differential drive in their Donkey race car (therefore the name DDDonkey).\n\nFor their software stack, they use the Donkey Car system plus a self-written differential drive part for steering. The hardware consists of a Raspberry Pi 3 as the central computational unit, a PCA 9685 PWM board, which in combination with an l298n H-Bridge controls the motor. With the usual Ackermann approach an electric speed controller would suffice, but since this is a differential drive vehicle, these two components are used to control left and right side speed. Coding Force also included the most critical features of self-driving cars, which are knight rider lights.\n\nThe vehicle employs the standard raspberry 165\u00b0 Fisheye camera; the rollover bar is 3D printed. Separate battery packs power the motor and the computational units, which is a standard procedure.\n\nSo how does it work? Coding Force uses the standard Donkey Car TensorFlow model to predict throttle and steering values for the vehicles current camera frame. The Model is trained with up to 20000 images each in combination with a small json containing steering and throttle values. The car uses its camera as the only sensor.\n\nAs they have told me, their main difficulties were to get the required software running on their machines, as it is not yet straightforward enough. Also gathering enough training data was problematic since an ample space is required to drive the vehicle. This is not available in the standard Berlin flat.\n\nDifferential drive works very well with Coding Force\u2019s implementation. Usually, the Donkey Car would expect an Ackermann steering model, but their extension to Donkey Car extends it with differential drive.\n\nThe vehicle is very slow in its current form. It required 1:57,6 minutes on its fastest lap on the big track, but this is due to the platform of the vehicle not being set up to be quick.\n\nYou can see DDDonkey in action here:\n\nWhen asked for tips for beginners they told me to read the Donkey Car slack channel as most questions that might come up when building the vehicle have already popped up for someone else and have already been answered on the slack. Also, they recommended sticking as close to the standard implementation to be able to benefit from the available knowledge."
    },
    {
        "url": "https://becominghuman.ai/lets-talk-about-advanced-analytics-a-brief-look-at-artificial-intelligence-bf1c7a7d3f96",
        "title": "Let\u2019s talk about Advanced Analytics: A brief look at Artificial Intelligence",
        "text": "In recent years, the use of certain terms to refer to new technology trends has become normal. We can find plenty of news on artificial intelligence (AI), machine learning, Big Data and advanced analytics, for example. But what does each of these terms mean? And, what do they mean in a business sense?\n\nOver time, I\u2019ve frequently found that there is some confusion about what these technologies cover and what they do. With this post, I will try to explain these concepts, all of them linked.\n\nArtificial Intelligence is an emerging term that has created a growing dialogue among businesses leaders and prosperous niche, appearing startups and solutions based on AI. Some well-known examples of products based on AI include recommendation systems, chatbots and self-driving cars. We can find multiple instances of solutions based on AI present in our day-to-day transforming the ways businesses operate.\n\nBut what do we understand by AI? Artificial Intelligence is a broad science concept that describes the ability of machines to make smart things based on logic processes. How is it created? To develop AI, experts use software that allows AI to make conclusions at the same it processes smart data.\n\nThe concept of AI dates back the imagination of human beings from ancient Greece, and the first computers were logic machines with software intelligent enough to solve basic calculations. As we have increased our understanding about the human mind, the definition of AI has evolved. Nowadays, we can say the concept of AI is the ability of machines to imitate the decision-making process of human beings and execute smart tasks as human being would.\n\nHuman intelligence is made up of several capacities: learning, reasoning, problem solving, perception and the ability to understand language. All of these involve analytic skills. AI solutions also use analytical skills. In the graph below, we can see the different levels and types of analytics, from the most basic to the most sophisticated.\n\nThe basis of all analytic activity focuses on understanding what is happening using historic information. This is covered by descriptive analytics, which helps to understand what happened, and diagnostic analytics, which try to understand why it happened. From business perspective, these methods have traditionally been covered by Datamining and Business Intelligence.\n\nThe next level is covered by three disciplines known as advanced analytics. They try to use and add knowledge to make better decisions in the future and they also provide additional insight. This is what we understand nowadays as AI.\n\nEach of these disciplines use statistics and math techniques to achieve the proposed goals. They help us understand what and why something happens, what could happen, and what someone could do to make something specific happen. Additionally, these techniques also focus on how to make decisions like humans would. The AI technologies can automatize decision-making by guaranteeing the quality of these decisions and their contributions to business.\n\nAll of them need large amounts of data for make this possible. Here is when other terms come into play. Perhaps you\u2019ve heard of one of them: Big Data.\n\nTherefore, AI makes sense if we have data that can be mined through analytics technologies to achieve our goals. These goals can help us: getting better knowledge about our customer, improving the process efficiency and increasing the profitability of services. This reason place value on this concept acquired in business organizations. This is the reason why business organization place value on AI.\n\nThis concept focuses the activities related to systems for treating and handling large amounts of data. Big Data involve using enormous amounts of data that we need to fulfill our goals, as noted before. Big Data supplies the infrastructure and capability to capture, store, treat and transfer this large amount of data.\n\nBig Data allows experts to handle unstructured data from any source (traditional data base, social networks, IoT devices, etc.), any format (image, audio, video or text) in real time.\n\nThese are the five defining concepts of Big Data:\n\nWell, we are going to focus on these disciplines around advanced analytics, and zoom in on each of their techniques.\n\nMachine learning has been used extensively by data scientists, business managers and executives for some time. There is some confusion surrounding the concepts of machine learning and predictive analytics. Which do these terms mean? And, what is the difference?\n\nBoth disciplines have the same objectives: forecasting. The difference lies in the amount of data involved and the human participation during predictive models building.\n\nPredictive analytics uses statistical techniques for evaluating behaviors and determining whether a result is viable. This means, trying to address what can happen in predicted future situations. Its nature is probabilistic; because it tells us what is the probability that something will happen. We can do this by trying to find relationships and patterns between variables by using present and historic information. This way, we can extract conclusions and prediction through data. The most well-known applications of Big Data are the credit ratings used by financial companies to evaluate probability of future defaults. There are many cases, such as a customer\u2019s calculated propensity of purchase to acquire a product, churn scoring, etc. The greater the amount of data, the more accurate the information is.\n\nThere are different modeling statistic techniques (as shown in the following graph). Choosing one depends on the necessity of having a descriptive analysis of result for understanding why take a result, the types of data and their structure.\n\nMachine learning is a discipline that tries to build complex models and algorithms, in order to create prediction. This make it different with other cases because it does so without programming explicit instructions, and data learning, allowing models to evolve and adapt while they add new data. That is, we only work with data available and the desired result.\n\nThe result of machine learning is a prediction that can guide decisions in real time without depending on human intervention.\n\nThe main approach includes the used of neural networks, generic algorithms, rules induction and analytics learning. But, in contrast to predictive analytics (which the different models use independently), machine learning is hybrid, mixing different types of models.\n\nThe combination of analytics models can guarantee efficient results, repeatable and reliable. This is particularly appreciated in business solutions.\n\nMany times we can see this discipline associated with cognitive computation as we will discuss later. This is because one of the applications of machine learning is the automatization of knowledge acquisition in systems that intend to emulate the decision-making process by human beings.\n\nThere are cases with great impact. For instance, Amazon uses it for customized recommendations of products based on behavior surfing and the customer shopping experience. Google uses it to improve search results and Facebook uses it in image recognition, labeling and connecting known people in pictures. In addition, we can check other business applications as shown in the following graph:\n\nI can find a simile that I have seen well suited in SHARP SIGHT LABS. He tells us that, ML and predictive statistics modeling are identical twins. They are nearly identical. If you compare them, they have the same DNA like twins. However, they are different, relating with different people and behaving distinctly."
    },
    {
        "url": "https://becominghuman.ai/creating-intricate-art-with-neural-style-transfer-e5fee5f89481",
        "title": "Creating Intricate Art with Neural Style Transfer \u2013",
        "text": "Ever since neural artistic transfer algorithm was published by Gatys, we have been seeing plenty of pictures being turned into artwork.\n\nThe algorithm uses a feed-forward network to apply the \u2018style\u2019 of a painting to a given picture. We also saw an impressive approach for non-artistic neural style transfer, where \u201cnon-paintings\u201d or everyday objects can be tiled as style image to create art. Later on, improvements were made in this area to develop a fast neural style transfer approach by Johnson et al. This paved way to many mobile applications, the notable one being Prisma, which allows users to create an artwork within seconds out of a picture they took with their phone.\n\nHowever, most of the artwork generated by these applications have mainly used pictures as the content image.\n\nI am a huge fan of artists who can hand-draw \u201czentangles.\u201d This is a type of art where intricate patterns or doodles are fit into a rectangle or any other shape. They look sophisticated and beautiful.\n\nThese have been lately used in coloring books for adults. As one can imagine, it requires a lot of patience to produce this kind of artwork.\n\nFor those who lack patience (including me), but still want to create these sophisticated art pieces, deep learning comes to the rescue.\n\nThe architecture is based on Gatys\u2019 style transfer algorithm with a few minor modifications. In this case, the content image is a silhouette and style image can be any pattern (ranging from simple black and white doodle to more complex color mosaics). The code also contains a module to invert and create a mask based on the content image, which will eventually be applied to the generated pattern.\n\nWeights from pre-trained networks (VGGNET) are used for this application. Initial feature layers are used for style image and later feature layers are used for the content image. Gram matrix is defined to measure style loss and content loss, and the combined loss is minimized at every iteration.\n\nOnce the final combined image is generated, mask transfer is applied and saved as output. The algorithm is implemented in keras with tensorflow backend. Gihub link to the code with more details on implementation can be found here.\n\nTo follow the tradition of machine learning use cases, the algorithm is first tested with a cat silhouette.\n\nNot too bad. This was styled from a simple set of doodles tiled together.\n\nLet\u2019s try more complex color patterns for style input. These require anywhere between 100 to 250 iterations. If run for fewer iterations, the output may contain some \u2018blackness\u2019 leftover from the silhouette.\n\nWhen geometric patterns are given as style input, we get this interesting \u2018stained glass\u2019 effect in the generated artwork.\n\nThe code also allows to specify background image or background color optionally.\n\nRunning out of pattern ideas? Take a picture of your grandmother\u2019s quilt!\n\nNow, Darth Vader and your grandmother have something in common!\n\nWhy do we have to go through all this trouble to produce this? Couldn\u2019t we just clip from an existing pattern? Or we could just use white noise to generate a new kind of pattern and then clip to any shape. Why specifically silhouettes?\n\nTurns out, using silhouettes as content has its advantages. Take the below three generated art pieces for example. (X) is the original pattern merely clipped to the shape of the dancer, (Y) has patterns generated from white noise content, and clipped to the shape after the final image was generated, and (Z) used the dancer silhouette as content and later mask transfer was applied to clip the image.\n\nFor the first two images (X) and (Y), we see that the patterns (generated or not) do not complement the shape of the dancer. With (Y), we get new patterns, but they do not align with the shape. Whereas in (Z), the patterns almost fit like a skeleton within the shape, giving a neat finishing.\n\nThe artwork generated from quilt pattern (see Darth Vader above), gives an illusion that the \u2018stitches\u2019 appear along the edges as if the fabric was cut and custom-made! This does not happen when you train on a picture or white noise or clip directly from the pattern.\n\nMoreover, the silhouettes need not be in black specifically. In fact, different colors act as \u201cseeds\u201d to generate different variations of the artwork. One can choose the color depending what dominant color they want to retain (note: black color acts as a sink \u2014 it is neutral).\n\nThis work was originally presented as a poster at Self-Organizing Conference on Machine Learning, 2017.\n\nThis work would not have been made possible without the GPU access provided by Palo Alto Research Center, and valuable feedback from PARC researchers."
    },
    {
        "url": "https://becominghuman.ai/from-perceptron-to-deep-neural-nets-504b8ff616e",
        "title": "From Perceptron to Deep Neural Nets \u2013",
        "text": "As a machine learning engineer, I have been learning and playing with deep learning for quite some time. Now, after finishing all Andrew NG newest deep learning courses in Coursera, I decided to put some of my understanding of this field into a blog post. I found writing things down is an efficient way in subduing a topic. In addition, I hope that this post might be useful to those who want to get started into Deep Learning. Alright, so let us talk about deep learning. Oh, wait, before I jump directly talking about what a Deep Learning or a Deep Neural Network (DNN) is, I would like to start this post by introducing a simple problem where I hope it will give us a better intuition on why we need a (deep) neural network. By the way, together with this post I am also releasing code on Github that allows you to train a deep neural net model to solve the XOR problem below. The XOR, or \u201cexclusive or\u201d, problem is a problem where given two binary inputs, we have to predict the outputs of a XOR logic gates. As a reminder, a XOR function should return 1 if the two inputs are not equal and 0 otherwise. Table 1 below shows all the possible inputs and outputs for the XOR function: Now, let us plot our dataset and see how is the nature of our data. def plot_data(data, labels):\n\n \"\"\"\n\n argument:\n\n data: np.array containing the input value\n\n labels: 1d numpy array containing the expected label\n\n \"\"\" Maybe after seeing the figure above, we might want to rethink whether this xor problem is indeed a simple problem or not. As you can see, our data is not linearly separable, hence, some well-known linear model out there, such as logistic regression might not be able to classify our data. To give you a clearer understanding, below I plot some decision boundaries that I built using a very simple linear model:\n\nWithout going into too much detail on a biological neuron, I will give a high-level intuition on how the biological neuron process an information. Our neuron receives signals through Dendrite. This information or signals are then passed through to the Soma or the Cell Body. Inside the cell body, all of the information will be summed up to generate an output. When the sumed up result reaches a threshold value, the neuron fires and the information will be carried down through the axon, then to other connected neurons through its synapses. The amount of signal transmitted between neurons depends upon the strength of the connections. The whole aforementioned flow is something that is adopted by the artificial neural network. You can think of the dendrite as the weighted inputs based on their synaptic interconnection in an artificial neural network. The weighted input is then summed up together inside a \u2018cell-body\u2019 of the artificial neural network. If the generated output is greater than the threshold unit, the neuron will \u201cfire\u201d and the output of this neuron will be transferred to the other neurons. So, you can see that the ANN is modeled using the working of basic biological neurons. So, How do this Neural Network works? In order to know how this neural network works, let us first see a very simple form of an artificial neural network called Perceptron. For me, Perceptron is one of the most elegant algorithms that ever exist in machine learning. Created back in the 1950s, this simple algorithm can be said as the foundation for the starting point to so many important developments in machine learning algorithms, such as logistic regression, support vector machine and even deep neural networks. So how does a perceptron works? We\u2019ll use a picture represented in Figure 2 as the starting point of our discussion. Figure 2 shows a perceptron algorithm with three inputs, x1, x2 and x3 and a neuron unit which can generate an output value. For generating the output, Rosenblatt introduced a simple rule by introducing the concept of weights. Weights are basically real numbers expressing the importance of the respective inputs to the output [1]. The neuron depicted above will generate two possible values, 0 or 1, and it is determined by whether the weighted sum of each input, \u2211 wjxj, is less than or greater than some threshold value. Therefore, the main idea of a perceptron algorithm is to learn the values of the weights w that are then multiplied with the input features in order to make a decision whether a neuron fires or not. We can write this in a mathematical expression as depicted below: Now, we can modify the formula above by doing two things: First, we can transformed the weighted sum formulation into a dot product of two vectors, w (weights) and x (inputs), where w\u22c5x \u2261 \u2211wjxj. Then, we can move the threshold to the other side of the inequality and to replace it by a new variable, called bias b, where b \u2261 \u2212threshold. Now, with those modification, our perceptron rule can be rewritten as: Now, when we put all together back to our perceptron architecture, we\u2019ll have a complete architecture for a single perceptron as depicted below: A typical single layer perceptron uses the Heaviside step function as the activation function to convert the resulting value to either 0 or 1, thus classifying the input values as 0 or 1. As depicted in Figure 4, the Heaviside step function will output zero for negative argument and one for positive argument. Figure 4. The shape of the Heaviside step function \u2014 Source The Heaviside step function is particularly useful in classification task in cases where the input data is linearly separable. However, recall that our goal is to find a classifier that works well in a non linearly separable data. Therefore, both a single layer perceptron and the Heaviside step function is obviously pointless here. Later, as you\u2019ll see in the next section, we\u2019ll need to have a multiple layers that consists of several perceptrons along with a non linear activation function. Specifically, there are two main reasons why we cannot use the Heaviside step function \u2014( see also my answer on stats.stackexchange.com): At the moment, one of the most efficient ways to train a multi-layer neural network is by using gradient descent with backpropagation (we\u2019ll talk about these two methods shortly). A requirement for backpropagation algorithm is a differentiable activation function. However, the Heaviside step function is non-differentiable at x = 0 and it has 0 derivative elsewhere. This means that gradient descent won\u2019t be able to make a progress in updating the weights. Recall that the main objective of the neural network is to learn the\n\n values of the weights and biases so that the model could produce a\n\n prediction as close as possible to the real value. In order to do\n\n this, as in many optimisation problems, we\u2019d like a small change in\n\n the weight or bias to cause only a small corresponding change in the\n\n output from the network. Having a function that can only generate either 0 or 1 (or yes and no), won\u2019t help us to achieve this objective. The activation function is one of the most important components in the neural network. In particular, a nonlinear activation function is essential at least for three reasons: It helps the neuron to learn and make sense of something really complicated. They introduce nonlinear properties to our Network. we\u2019d like a small change in weight to cause only a small corresponding change in the output from the network We\u2019ve seen that the Heaviside step function as one example of an activation function, nevertheless, in this particular section, we\u2019ll explore several non-linear activation functions that are generally used in the deep learning community. By the way, a more in-depth explanation of the activation function, including the pros and cons of each non-linear activation function, can be studied in these two great post written by Avinash Sharma and Karpathy. The sigmoid function, also known as the logistic function, is a function that given an input, it will generate an output in range (0,1). The sigmoid function is written as: Figure 5. The shape of the Sigmoid Function Figure 5 draws the shape of the sigmoid function. As you can see, it is like the smoothed out version of the Heaviside step function. However, the sigmoid function is preferable due to many factors, such as: It is nonlinear in nature. Instead of outputing 0 and 1, now we have a function that can give a value output 0.67. Yup, as you might guess, this can be used to represent a probability value. Still related to point (2), now we have our activations bound in a range, which means it won\u2019t blow up the activations. However, the sigmoid activation function has some drawbacks: The vanishing gradient. As you can see from the preceding figure, when z, the input value to the function, is really small (moving towards -inf), the output of the sigmoid function will be closer to zero. Conversely, when z, is really big (moving towards +inf), the output of the sigmoid function will be closer to 1. So what does this imply? In that region, the gradient is going to be very small and even vanished. The vanishing gradient is a big problem especially in deep learning, where we stack multiple layers of such non-linearities on top of each other, since even a large change in the parameters of the first layer doesn\u2019t change the output much. In other words, the network refuses to learn and oftentimes the time taken to train the model tend to become slower and slower, especially if we use the gradient descent algorithm. Another disadvantage of the sigmoid activation function is that computing the exponential can be expensive in practice. Although, arguably, the activation function is a very small part of the computation in a deep net compared to the matrix multiplication and/or convolution, so, this might not become a huge problem. However, I think it is worth to mention. Tanh or hyperbolic tangent is another activation function that is commonly used in deep neural nets. The nature of the function is very similar to the sigmoid function where it squash the input into a nice bounded range value. Specifically, given a value, tanh will generate an output value between -1 and 1.\n\nFigure 6. The shape of the tanh function As mentioned earlier, the tanh activation function has characteristics similar to the sigmoid function. It is nonlinear and it is bound to a certain range, in this case (-1, 1). Also, not surprisingly, tanh shares the same drawbacks as what the sigmoid has. It suffers from the vanishing gradient problem and as you can see from the mathematical formula, we need to compute the exponential, which oftentimes is computationally inefficient. Here it comes the ReLu, an activation function that was not expected to perform better than sigmoid and tanh, yet, in practice it does! In fact, this lecture says by default, use the ReLU non-linearity. The ReLu has a very nice mathematical property in which it is very computationally efficient. Given an input value, the ReLu will generate 0, if the input is less than 0, otherwise the output will be the same as the input. Mathematically speaking, this is the form of the ReLu function Figure 7. The shape of the ReLu function Now, you may ask, \u201cisn\u2019t that a linear function? why do we call ReLu as a nonlinear function?\u201d First, let us first understand what a linear function is. Wikipedia says that: In linear algebra, a linear function is a map f between two vector spaces that preserves vector addition and scalar multiplication: Given the definition above, we can see that max(0, x) is a piece-wise linear function. It is piece-wise because it is linear only if we restrict its domain to (\u2212inf, 0] or [0,+inf). However, it is not linear over its entire domain. For instance So, now we know that ReLu is a nonlinear activation function and it has a nice mathematical property and also computationally more efficient compared to sigmoid or Tanh. In addition, ReLu is known to be \u201cfree\u201d from the vanishing gradient problem. However, there is one big drawback in ReLu, which is called the \u201cdying ReLu\u201d. The dying ReLu is a phenomenon where a neuron in the network is permanently dead due to inability to fire in the forward pass. To be more precise, this problem occurs when the activation value generated by a neuron is zero while in forward pass, which resulting that its weights will get zero gradient. As a result, when we do backpropagation, the weights of that neuron will never be updated and that particular neuron will never be activated. I highly suggest you to watch this lecture video which has more in-depth explanation on this particular problem and how to avoid the dying ReLu problem. Please go check it! Oh, one more thing about ReLu that I think worth to mention. As you may notice, unlike the sigmoid and tanh, ReLu doesn\u2019t bound the output value. As this might not become a huge problem in general, it can be, however, become troublesome in another variant of deep learning model such as the Recurrent Neural Network (RNN). Concretely, the unbounded value generated by the ReLu could make the computation within the RNN likely to blow up to infinity without reasonable weights. As a result, the learning can be remarkably unstable because a slight shift in the weights in the wrong direction during backpropagation can blow up the activations during the forward pass. I\u2019ll try to cover more about this in my next blog post, hopefully :) How does a Neural Network Predicts and Learns? The architecture depicted in Figure 8 above is called the multi-layer perceptrons (MLP). As its name implies, in MLP, we simply stacked multiple perceptrons into several layers. The one depicted above is a network with 3 layers: an input layer, a hidden layer, and an output layer. However, in the deep learning or neural net community, people don\u2019t call this network as three layers neural network. Usually, we just count the number of hidden layers or number of hidden layer along with the output layer, hence two-layers neural network. The hidden layers simply mean neither an input nor an output layer. Now, as you might guess, the term deep learning solely implies, we have \u201cmore\u201d hidden layers :). So how do a neural net generates a predictions? A neural network generates a prediction after passing all the inputs through all the layers, up to the output layer. This process is called feedforward. As you can see from Figure 8, we \u201cfeed\u201d the network with the input, x, compute the activation function and pass it through layer by layer until it reaches the output layer. In supervised setting task, such as classification task, we usually use a sigmoid activation function in the output layer since we can translate its output as a probability. In Figure 8, we can see that the value generated by the output layer is 0.24, and since this value is less than 0.5, we can then said the prediction, y_hat, is zero. Then, as in typical classification task, we\u2019ll have a cost function which measures how good our model approximate the real label is. In fact, training in neural network simply means, minimize the cost as much as possible. We can define our cost function as follows: So the objective is to find some combination of w\u2019s and b that could make our cost J, as small as possible. To do this, we\u2019ll rely on two important algorithms which are Gradient Descent and Backpropagation. For those of you who have been doing machine learning might already know about the gradient descent algorithm. Training a neural network is not much different from training any other machine learning model with gradient descent. The only notable difference would be the effect of nonlinearities in our network that makes our cost function become non-convex. To give a better intuition, let us assume that our cost function is a convex function (a big one bowl) as depicted in Figure 9 below: Figure 9. The schematic of the Gradient Descent \u2014 Source In the diagram above, the horizontal axes represent our space of parameters, weights and biases, while the cost function, J(w, b) is then some surface above the horizontal axes. The red circle depicted in the diagram above is the original value of our cost w.r.t the weights and bias. To minimize the cost, we now know that we have to go to the steepest path down the bowl. But the question is, how do we know which direction to step? Should we increase or decrease the value of our parameters? We can do a random search, but it will take quite a long time and obviously computationally expensive. There is a better way to find which direction we should go, in tweaking the learnable parameters, weights and biases. Calculus teaches us that the direction of the gradient vector, at a given point, will naturally point in the steepest direction. Therefore, we\u2019ll use the gradient of our cost function w.r.t our weights and our biases. Now, Let us simplify things by just looking the cost w.r.t the weights as depicted in Figure 10 below. Figure 10 pictures the value of our cost function w.r.t to the value of the weights. You can think of the black circle above as our original cost. Recall that the gradient of a function or a variable can be positive, zero or negative. A negative gradient means that the line slopes downwards and vice versa if it is positive. Now, since our objective is to minimize the cost, we then need to move our weights in the opposite direction of the gradient of the cost function. This update procedure can be written as follow: where \u03b1 is a step size or learning rate and we\u2019ll multiply this with the partial derivative of our cost w.r.t the learnable parameters. So, what does \u03b1 used for? Well, the gradient tells us the direction in which the function has the steepest rate, however, it does not tell us how far along this direction we should step. Here is where we need the \u03b1, which is a hyperparameter that basically control the size of our step, like, how much should we move towards a certain direction. Choosing the right value for our learning rate is very important since it will hugely affect two things: the speed of the learning algorithm and whether we can find the local optimum or not (converge). In practice, you might want to use an adaptive learning rate algorithms such as momentum, RMSProp, Adam and so forth. A guy from AYLIEN wrote a very nice post regarding various optimization and adaptive learning rate algorithm. In the previous section, we\u2019ve talked about the gradient descent algorithm, which is an optimization algorithm that we use as the learning algorithm in a deep neural network. Recall that by using gradient descent means that we need to find the gradient of our cost function w.r.t our learnable parameters, w, and b. In other words, we need to compute the partial derivative of our cost function w.r.t w and b. However, if we observe our cost function J, as depicted in Figure 12 below, there is no a direct relationship between J and both w and b. Only if we trace back from the output layer, the layer that generates y_hat, way to the input layer, we\u2019ll see that J has an indirect relation with both w and b, as shown in Figure 13 below: Figure 13. The schematic of the backpropagation Now, you can see that in order to find the gradient of our cost w.r.t both w and b, we need to find the partial derivative of the cost with all the variables, such as a (the activation function) and z (the linear computation: wx + b) in the preceding layers. This is where we need a backpropagation. Backpropagation is basically a repeated application of chain rule of calculus for partial derivatives, which I would say, probably the most efficient way to find the gradient of our cost J w.r.t all the learnable parameters in a neural network. In this post, I\u2019ll walk you to compute a gradient of the cost function J w.r.t W2, which is the weights in the second layer of a neural network. For simplicity, we\u2019ll use the architecture shown in Figure 8, where we have one hidden layer with three hidden neurons. To find the rate of change of y_hat w.r.t z2, we need to differentiate our sigmoid activation function with respect to z. Now, once we have the value of our partial derivative J w.r.t W2, we can update the value of our W2 using the formula shown in Figure 11 in the previous section. Basically, we\u2019ll perform the same computation with all the weights and biases repeatedly until we have a cost value as minimum as possible. Great! I guess we\u2019ve covered pretty much everything that we need to know in order to build a neural network model, and even a deep learning model, that would help us to solve the XOR problem. While writing this post, I\u2019ve built a simple neural network model with only one hidden layers with various number of hidden neurons. The example of the network that I used is shown in Figure 14 below. Also, I presented some decision boundaries generated by my model with different number of neurons. As you can see later, we can say that having more neurons will make our model become more complex, hence creating a more complex decision boundary.\n\nBut, what would be the best choice? having more neurons or going deeper, meaning having more layers? Well, Theoritically, the main benefit of a very deep network is that it can represent very complex functions. Specifically, by using a deeper architecture, we can learn features at many different levels of abstraction, for instance identifying edges (at the lower layers) to very complex features (at the deeper layers). However, using a deeper network doesn\u2019t always help in practice. The biggest problem that we will encounter when training a deeper network is the vanishing gradients problem: a condition where a very deep networks often have a gradient signal that goes to zero quickly, thus making gradient descent unbearably slow. More specifically, during gradient descent, as we backprop from the final layer back to the first layer, we are multiplying by the weight matrix on each step, and thus the gradient can decrease exponentially quickly to zero or, in rare cases, grow exponentially quickly and \u201cexplode\u201d to take very large values. So, to wrap up this lengthy post, these are a few bullet points that would briefly summarise it: The intuition, the Neural Net introduces non-linearities to the model and can be used to solve a complex non-linearly separable data. Perceptron is an elegant algorithm that powered many of the most advancement algorithms in machine learning, including deep learning. Intuitively, deep learning means, use a neural net with more hidden layers. Of course, there are many variants of it, such as Convolution Neural Net, Recurrent Neural net and so on. Activation function is extremely an important component in neural net and YES, you should understand it. Currently, Gradient Descent with Backpropagation is the best combination that we use to train a (deep) Neural Net. Having more hidden layers not necessarily improves the performance of our model. In fact, it suffers from one well-known problem called the vanishing gradient problem."
    },
    {
        "url": "https://becominghuman.ai/forget-silicon-valley-innovation-is-happening-in-china-now-c6cfdbd74bc4",
        "title": "Forget Silicon Valley. Innovation is happening in China now",
        "text": "Forget Silicon Valley. Innovation is happening in China now\n\nThis story was first published at The Aleph Report. If you want to read the latest reports, please subscribe to our newsletter and our Twitter.\n\nThose that follow technology closely have noticed a significant trend in the field, China. The more you read, the more you encounter increasing coverage on China\u2019s tech dealings.\n\nIn the last few weeks, we\u2019ve seen Tencent\u2019s market capital surpass that of Facebook; Venture Capital activity reach US VC levels; LinkedIn\u2019s Chinese rival MaiMai, outperform LinkedIn in the country and the amount of Chinese Women in Tech pass that of the US.\n\nAccording to the Forbes list, out of the top 10 largest companies in the world, four are Chinese, five are American, and one is Japanese.\n\nIf you narrow the scope to tech-only companies, China has seven companies in the top 20. Two of them, Tencent and Alibaba, among the top six.\n\nThe China that most people imagine has nothing to do with the current technology superpower that China is now. Andrew Ng, former Chief Scientist at Baidu, hits the nail when he states,\n\nThis asymmetry is helping China fly under the radar. Most organizations are so focused on the Silicon Valley dream that they\u2019re missing the elephant in the room.\n\nEducation is a critical aspect of any country. This is especially true when we\u2019re speaking of innovation. Historically, China\u2019s educational levels have been subpar with the rest of the world. This hasn\u2019t been the case for a while now. The truth is, China\u2019s university are already outperforming many of their international peers.\n\nWhile institutions like Stanford still hold on to their perch of the global ranking, universities like Pekin\u2019s University, are closing in. Stanford outranks them in specific scores but lags in others like technology transfer.\n\nIn comparison, it\u2019s worth noting that there are precisely zero European universities among the top 30 (excluding the United Kingdom due to Brexit).\n\nChina\u2019s educational institutions still have a pending subject; attracting foreign talent. The country is trying to fix the lack of an international crowd applying a mixture of strategies with various degrees of success.\n\nBut better universities aren\u2019t the only reason for China\u2019s ascent to the innovation Olympus. In 2006, the Chinese General Secretary of China Communist Party Hu Jintao, and Wen Jiabao, President of People\u2019s Republic of China declared their intention to transform China into an \u2018Innovation-oriented\u2019 nation.\n\nThese declarations brought forward the term \u2018Indigenous innovation.\u2019 It refers to the capacity to produce innovative products and services from within a national context.\n\nTo achieve such a lofty goal, they knew they needed better local knowledge that the one they had. Improving their university system was strategic to making this, but it wasn\u2019t enough.\n\nAt the time, Chinese researchers and professors lacked knowledge in critical fields. To reduce the gap, they decided to bring foreign experts to the mainland through what\u2019s called the 1000 Talents Program.\n\nThe results of the program, though, were mixed. While the program is still active, the government decided to try a different approach.\n\nAs with Chinese education, for years, Chinese startups have been looked down upon due to their lack of competitiveness. Local startups grew mostly as American copycats. Despite the negative connotations, these companies brought a wealth of knowledge to the entrepreneurs. It taught them how to build products, and how to do it fast.\n\nChina might have started as the land of the copycats, but it quickly evolved and started developing their innovations. New Chinese startups emerged that, not only served the local market\u2019s need but did this at a scale never seen in the US.\n\nSuch has been the evolution of the Chinese startup ecosystem that their products and services are starting to outperform their American peers.\n\nAll this was happening, while Internet and Mobile penetration were increasing. In a way, China skipped an innovation step and went directly to mobile.\n\nThis leap has created some unique mobile behaviors that are giving a massive edge to Chinese companies.\n\nAt the heart of the rise of the Chinese startups lies the field of Artificial Intelligence. As I\u2019ve written before, Artificial Intelligence (AI) is becoming the de-facto disruptive technology. Any company that wants to compete needs to be deploying AI systems.\n\nChina\u2019s innovation efforts have squarely targeted the development of AI and Deep Learning technologies. Nonetheless, attaining AI expertise isn\u2019t easy. Investing in AI demands spending on the three building blocks that make it possible; hardware, data, and talent.\n\nPeople know China for their hardware production. Even so, their expertise on the design aspect of high-tech semiconductors has remained elusive. If China wanted to up their game, they needed to increase their knowledge in the space.\n\nThat\u2019s what they started doing via foreign investments. Such was the pace that the Committee on Foreign Investment (CFIUS) issued a warning to Congress about it and started blocking some of these operations.\n\nAccess to massive amounts of data is paramount for AI. Yet, data is one thing China has in excess. With an Internet population of 731 million users (2,5x more than the US) and very lax privacy regulations, they\u2019re well equipped to train their systems with large swaths of information.\n\nData and hardware aren\u2019t enough. You need people to man the algorithms. The government started doubling down on AI research money to increase the number of skilled AI and Deep Learning researchers. In contrast, the Trump administration began slashing the 2017 National Science Foundation budget by 11.2%. The effect has been dramatic.\n\nIn October of 2016, the US National Science and Technology Council released a paper titled \u201cThe National Artificial Intelligence Research and Development Strategic Plan\u201d (PDF). The document indicated that China had surpassed, for the first time, the US number of peer-reviewed publications mentioning Deep Learning. It also sets the first US Artificial Intelligence R&D strategic plan ever.\n\nThe rise of Chinese AI researchers has been felt worldwide.\n\nThe trend not only hasn\u2019t reverted, but it\u2019s widening. According to a recent analysis by The Financial Times,\n\nOne of the unspoken advantages of many Chinese researchers is that they have access to the best of both worlds,\n\nThis increased research is starting to yield incredible results. Some of the current Chinese startups are becoming the AI reference in their fields. Such is the case of Face++, whom recently won the first place in the International Conference of Computing Vision 2017 (ICCV), ahead of teams from Facebook, Google or Microsoft.\n\nArtificial Intelligence and Deep Learning have become so critical to China that in July of 2017, they released a State Council Notice called \u201cThe Next Generation Artificial Intelligence Development Plan\u201d (PDF). The plan is worth reading due to its prescient nature on several aspects.\n\nThe government recognizes, though, that local talent is still hard to come by. To offset this, they\u2019re deploying a dual strategy. On the one hand, they\u2019re investing heavily on AI-based startups, both locally (SensaTime Group\u2019s 410 million dollars in July 2017, Megvii\u2019s Face++ 460 million dollars in October) and outside the mainland. While the US is investing in the field, the hunger for more money is patent. China is investing in companies that the US money is neglecting.\n\nOn the other hand, China is trying to make it easier for foreign talent to come and work with them. To accomplish this, the big three of China, Baidu, Alibaba and Tencent (BAT) have been opening AI, and Deep Learning focused research centers on the West Coast.\n\nBaidu already has two research centers in Sunnyvale, CA. Tencent has been operating a data center out of Silicon Valley and a new AI Research center in Seattle, WA. Alibaba recently announced they\u2019re opening seven new research labs worldwide, one of them in the Seattle area and another in San Mateo, CA. Uber Rival Didi Chuxing, is also another of the big Chinese startups that opened shop in Mountain View in March 2017.\n\nUnderestimating China is easy. For many years it\u2019s been the land of the cheap mediocre copycats. Chinese culture is foreign to most Westerners. It\u2019s plagued with idiosyncrasies that cultured western institutions have defined as inferior or wrong. The fact that few outside of China speaks or read Chinese doesn\u2019t help. We disregard and downplay that that\u2019s different or unknown to us.\n\nBut the truth is, it\u2019s becoming increasingly hard to ignore the fact that China is on the verge of becoming the world\u2019s technological leader.\n\nWhile Chinese universities still have a low rate of international participation, that will change fast. It\u2019s a matter of time before foreign students start flocking China, looking for the next Stanford.\n\nMeanwhile, more and more companies are turning to China for funding and customers. The US and Europe are lagging behind in technological adoption. Robotics, AI-based systems, automated education, Quantum computing or smart mobility are all happening in China, not in the US. The market is in China, the funding is in China, and the regulation is in China.\n\nThe US is becoming progressively more hostile to startups. More and more entrepreneurs are fleeing America. Some are finding in China, the perfect market for their cutting-edge technology.\n\nIt\u2019s hard to see how the Trump administration can correct the innovation decline. Much worse is the situation in Europe where there seem to be no strategic plans around key technologies like AI and Deep Learning. The difference in research, investment and execution capacity between China and Europe is staggering.\n\nOrganizations worldwide should keep a close eye on Asia, both China\u2019s big four (BATJ, Baidu, Alibaba, Tencent, and JD) and conglomerates like Japan\u2019s Softbank.\n\nChina will, most probably, dictate the rules of the next technological revolution. Organizations need to invest in understanding the new landscape before they get obliterated in the crossfire."
    },
    {
        "url": "https://becominghuman.ai/evaluating-the-quality-of-translation-5b6708d4838a",
        "title": "Evaluating the Quality of Translation \u2013",
        "text": "I found a very good article on slator, where it was mentioned that\n\nBLEU score gives you the sense of the particular machine translation that you have build. BLEU was one of the first metrics to claim a high correlation with human judgments of quality. It is fast/cheap, Language agnostic, high correlation with human evaluation because of which it is widely adopted strategy for evaluating translation.\n\nBLEU Score is typically measured on the scale of 0\u20131. So, it can be thought of as the probability that is assigned to the output. Here, a score of 0 would mean that machine translated text is completely different from the human reference text whereas, a score of 1 would mean that both the outputs are a perfect match. Better score means less post-editing will be required to achieve publishable translation quality. The basic notion of automatic scoring is to have a reference that you actually compare against and then we generate a score.\n\nBLEU is highly sensitive to words that are being used in the output of the translated text. BLEU is a generic methodology that works with any language, but highly depends on the accuracy of the word tokenizer used for that specific language i.e. languages such as Chinese and Japanese where word segmentation is not as trivial as in English where the notion of word is pretty clear, a poor performing word breaking rules/model would effect the BLEU score a lot.\n\nI could have written it from scratch, but what\u2019s the need in re-inventing the wheel. We will be using NLTK for this purpose.\n\nAbove examples, clearly show best and worst cases in translation.\n\nBy default, NLTK calculates BELU-4 (cumulative 4-gram BLEU score) on sentence_bleu or corpus_bleu.\n\nI would encourage the reader to go through official docs and code for detailed understanding. Above code can be found here.\n\nFeel free to comment down your thoughts and don\u2019t forget to spread the word if you \u2764\ufe0f\u200d it."
    },
    {
        "url": "https://becominghuman.ai/deep-learning-made-easy-with-deep-cognition-403fbe445351",
        "title": "Deep Learning made easy with Deep Cognition \u2013",
        "text": "This past month I had the luck to meet the founders of DeepCognition.ai. Deep Cognition breaks the significant barrier for organizations to be ready to adopt Deep Learning and AI through Deep Learning Studio.\n\nBefore continuing and describe how Deep Cognition simplifies Deep Learning and AI, lets first define the main concepts for Deep Learning.\n\nDeep learning is a specific subfield of machine learning, a new take on learning representations from data which puts an emphasis on learning successive \u201clayers\u201d of increasingly meaningful representations.\n\nDeep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction.\n\nThese layered representations are learned via models called \u201cneural networks\u201d, structured in literal layers stacked one after the other.\n\nActually what we use in Deep Learning is something called artificial neural network (ANN), that\u2019s a network inspired by biological neural networks which are used to estimate or approximate functions that can depend on a large number of inputs that are generally unknown.\n\nAlthough deep learning is a fairly old subfield of machine learning, it only rose to prominence in the early 2010s. In the few years since, it has achieved great things, Fran\u00e7ois Chollet list following breakthroughs of Deep Learning:\n\nAs Fran\u00e7ois Chollet states in his book until the late 2000s, we were still missing a reliable way to train very deep neural networks. As a result, neural networks were still fairly shallow, leveraging only one or two layers of representations, and so they were not able to shine against more refined shallow methods such as SVMs or Random Forests.\n\nBut in this decade, with the development of several simple but important algorithmic improvements, the advances in hardware (mostly GPUs), and the exponential generation and accumulation of data, with the help of Deep Learning nowadays it\u2019s possible to run small deep learning models on your laptop (or in the cloud).\n\nLet\u2019s see how we normally do Deep Learning.\n\nEven though this is not a new field, what is new are the ways we can interact with the computer to do Deep Learning. And one of the most important moments for this field was the creation of TensorFlow.\n\nTensorFlow is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them.\n\nTensors, defined mathematically, are simply arrays of numbers, or functions, that transform according to certain rules under a change of coordinates.\n\nBut in this scope a tensor is a generalization of vectors and matrices to potentially higher dimensions. Internally, TensorFlow represents tensors as n-dimensional arrays of base datatypes.\n\nWe need tensors because what NumPy (the fundamental package for scientific computing with Python) lacks is creating Tensors. We can convert tensors to NumPy and vice\u00adversa. That is possible since the constructs are defined definitely as arrays/matrices.\n\nTensorFlow combines the computational algebra of compilation optimization techniques, making easy the calculation of many mathematical expressions that would be difficult to calculate, instead.\n\nThis is not a blog about TensorFlow, there are great ones. But it was neccesary to introduce Keras.\n\nKeras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.\n\nThis was created by Fran\u00e7ois Chollet and was the first serious step for making Deep Learning easy for the masses.\n\nTensorFlow has a Python API which is not that hard, but Keras made really easy to get into Deep Learning for lots of people. It should be noted that Keras is now officially a part of Tensorflow: https://www.tensorflow.org/api_docs/python/tf/contrib/keras\n\nKeras is the winner for now, it is interesting to see that people prefers an easy interface and usability.\n\nIf you want more information about keras visit this post I made on LinkedIn: https://www.linkedin.com/feed/update/urn:li:activity:6344255087057211393\n\nSo normally we do Deep Learning programming, and learning new APIs, some harder than others, some are really easy an expressive like Keras, but how about a visual API to create and deploy Deep Learning solutions with the click of a button?\n\nThis is the promise of Deep Cognition.\n\nAs they say The Deep Cognition platform was founded to \u201cdemocratize AI\u201d.\n\nArtificial intelligence is already creating significant value for the world economy. There is a (big) shortage of AI expertise though that creates a significant barrier for organizations ready to adopt AI. And this is what they are solving.\n\nTheir platform, Deep Learning Studio is available as cloud solution, Desktop Solution ( http://deepcognition.ai/desktop/ ) where software will run on your machine or Enterprise Solution ( Private Cloud or On Premise solution).\n\nThe Desktop version allows people to use their own computers with GPU without hourly fee.\n\nFor this we will be using the Cloud version of the Deep Learning Studio. This is a single-user solution for creating and deploying AI. The simple drag & drop interface helps you design deep learning models with ease.\n\nPre-trained models as well as use built-in assistive features simplify and accelerate the model development process. You can import model code and edit the model with the visual interface. The platform automatically saves each model version as you iterate and tune hyperparameters to improve performance. You can compare performance across versions to find your optimal design.\n\nDeep Learning Studio can automagically design a deep learning model for your custom dataset thanks to our advance AutoML feature. You will have good performing model up and running in minutes.\n\nAnd yes AutoML is what you think, automatic Machine Learning, here applied specifically to Deep Learning, and it will create for you a whole pipeline to go from raw data into predictions.\n\nAs a small tutorial / try, of the Deep Learning Studio let\u2019s study the classical MNIST.\n\nMNIST is a simple computer vision dataset. It consists of images of handwritten digits like these:\n\nIt also includes labels for each image, telling us which digit it is.\n\nLet\u2019s train a model to look at images and predict what digits they are using Deep Cognition Cloud Studio and AutoML.\n\nWhen you have an account you just need to enter in the http://deepcognition.ai webpage and click on Launch Cloud App.\n\nNow this will take you to the UI, you\u2019ll see that you can choose from some sample projects:\n\nOr create a new project that is what we are going to do now:\n\nThis will take you to a page where you can choose the training-validation-test ratio, load a dataset or used an already uploaded one, specify the types of your data and more.\n\nThe Model tab will allow you to create your own models using advance Deep Learning features and different types of layers and neural networks, but we will use the AutoML feature so Deep Cognition take care of all of the modeling:\n\nWe choose Image because this is the type of data that we are trying to predict.\n\nAfter you click Design you will have your first DL model available to customize and analyze:\n\nThe model looks like this:\n\nSo you can see that all the complexity of modeling for Deep Learning and coding has been simplified a LOT with this great platform.\n\nIf you want you can also code in a Jupyter Notevook inside the platform, with all the necessary installations already done for you:\n\nThe reason is that neural networks are notoriously difficult to configure and there are a lot of parameters that need to be set. Hyperparameter tuning is the hardest in neural network in comparison to any other machine learning algorithm.\n\nBut with Deep Cognition this can be done really easy and in a very flexible way, in the HyperParameters tab you can choose from several Loss functions and Optimizers to tune your parameters.\n\nNow the funny part. Training your model. In the Training tab you can choose from different types of instances (with CPU and GPU) support to to this. It will also help you monitor your traning process and create a Loss and Accuracy graph for you:\n\nAbove there is an small gif of the training process.\n\nThe results your traninig can be found in the Results tab. You will have there all your runs.\n\nAnd finally you can use this model you have trained for the testing and validation set (or other you can upload) and see how well it performs when predicting the digit from an image.\n\nSomething that will come yo your mind is: ok I\u2019m doing deep learning but I have no idea how.\n\nBecause of that you can actually download the code that produced the predictions, and as you will see it is written in Keras. You can then upload the code and test it with the notebook that the system provides.\n\nThe AutoML features have the best of Keras and other DL frameworks in a simple click, and the good thing about it is that it chooses the best practices for DL for you, and if you are not completely happy with the choices you can change them really easy in the UI or interact with the notebook.\n\nThis system is built with the premise of making AI easy for everyone, you don\u2019t have to be an expert when creating this complex models, but my recommendation is that is good that you have an idea of what you are doing, read some of the TensorFlow or Keras documentation, watch some videos and be informed. If you are an expert in the subject great! This will make your life much easier and you can still apply your expertise when building the models.\n\nRemember checking the references for more information about Deep Learning and AI.\n\nPhysicist and computer engineer. He holds a Master\u2019s Degree in Physical Sciences from UNAM. He works in Big Data, Data Science, Machine Learning and Computational Cosmology. Since 2015, he has been part of the collaboration of Apache Spark in the Core and MLlib library.\n\nHe\u2019s Chief Data Scientist at Iron performing distributed processing, data analysis, machine learning and directing data projects for the company. In addition, he works at BBVA Data & Analytics as a data scientist performing machine learning, doing data analysis, maintaining the life cycles of the projects and models with Apache Spark."
    }
]