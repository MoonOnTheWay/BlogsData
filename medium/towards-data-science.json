[
    {
        "url": "https://towardsdatascience.com/when-even-a-human-is-not-good-enough-as-artificial-intelligence-c39c9fda4644?source=---------0",
        "title": "When Even a Human is Not Good Enough as Artificial Intelligence",
        "text": "I love measuring and explaining things in numbers. The most common and quick measurement I do is with scores and sub-scores. If I need to compare two things, I list the variables and give them weights. With that list, I give scores to variables, multiply them with weights and sum up. This gives me a single number. For many things I find it easy to use. Picking which TV to buy, which task to work on next, or testing if a method is better than other.\n\nWhile some things are easy to measure, intelligence is not one of them. Intelligence is a very abstract and complex thing to measure. Even the definition of intelligence is complex. What is intelligence and how to measure it are not the kind of questions I am after to answer. The curiosity that I have for this post is how do we, people, perceive artificial intelligence and what are our expectations from it. It seems to me that when people judge AI\u2019s ability, we are harsh. We want AI to be perfect. We do not show the flexibility we provide to human mistakes proportionally to AIs. We want AI to be \u201clike a human\u201d. Maybe it is because of the recent \u201cworks like brain\u201d advertisements (google \u201cacts like brain\u201d).\n\nWhatever the reason is, it is a common pattern. We expect artificial intelligence to be comparable to human intelligence.\n\nI conducted a series of data collection experiments that got me writing this post in 2012. I was conducting a research about sketch recognition (i.e. classifying hand written symbols and drawings) using machine learning. If you are curious about sketch recognizers or would like to have a more concrete idea, here is a great one you can try: https://quickdraw.withgoogle.com/ . One of the things I wanted to achieve was to convert a real-time strategy video game\u2019s interaction model from mouse/keyboard to sketching. With a sketching interface, people would play the game via sketching instead of mouse clicks and keystrokes. Imagine when you want to construct a building, instead of picking it from a menu and placing it onto map with point-and-click, you would sketch a simple house onto the part of the map where you want to construct it.\n\nThe plan was to develop a sketch recognizer with machine learning. In order to achieve it, the first step was to collect many hand drawn sketches from multiple people so I could use them to train a model. Instead of asking people to sketch what they see on the screen, I wanted to put them into a real environment. The environment that they would play the game with sketch like the AI was already in place. I thought that with this setting people would sketch more \u201crealistically\u201d than they would do in a relaxed, no-time constrained, and no-stress \u201ccopy the picture\u201d environment. In order to collect data in that environment, I created a \u201cwizard of oz\u201d environment.\n\nThe idea is simple. You prepare a setting where there are two rooms/spaces. In one space you put the computer where the attendee will use. In the second room you put another computer that you will use. The idea is to connect those two computers so that you can see what the attendee is up to, and take actions accordingly from your own computer. In my case it was for sketch recognition. So if attendee draws a house, I was able to see it from my computer and give the command for the house drawing. The goal was to replace an AI system that was not in place, so it would look like there is an actual AI, which is actually a human in a different room.\n\nWhen my \u201cOz\u201d was ready, I invited people to join my experiment (which is not an easy task to ask people give up their time for free). As explained, there was a room with a computer and a drawing screen where attendee sits, and another room where attendee cannot see where I mimic the AI. I instructed attendees that they would play a game using sketching as the only input medium and there is an AI that will recognize their sketches and perform actions accordingly.\n\nAfter the experiment, I asked them to fill a questionnaire followed by informal small talks as some people were interested in this way of playing a game. This is the part where I was confused. Many people told me and wrote down in the questionnaire the same thing:\n\nYou can see why I was confused. I was the \u201cAI\u201d that was recognizing the sketches and they tell me that AI was underperforming! How come is this possible? Have I done mistakes recognizing the symbols? It might be the case but the symbols were simple enough so I did not think that I did a lot of mistakes.\n\nThese anecdotal evidences never went into the research results but got me puzzled for a long time. If I, as a human, failed the expectations of people as a recognizer how can we come up with an AI that satisfies the users? Would it be different if I would disclose I was the AI? Were they pre-judged that an AI would always do mistakes? What do we expect from an AI?\n\n90% accuracy is a milestone for most of the recognizers. It feels like a great number. \u201cYour model got 90% accuracy\u201d. I would expect myself that in most cases I would declare the problem solved and present it to stakeholders. I would also expect them to be happy with the result. Imagine you scored 90% in an exam.\n\nHowever there is one thing. It actually may not be that good in real-life use. Imagine that you are sketching a circle and 9 times out of 10, the AI recognizes it correctly and takes action accordingly. But that 1 time out of 10, it recognizes 1 as another thing (e.g. triangle) and still takes an action. And there you are trying to stop the action that AI took and redo it. If you are using a voice assistant you may probably already had that: \u201cNO!, STOP!, DO NOT PLAY THAT SONG!, THAT IS NOT EVEN CLOSE OF WHAT I ASKED! ABORT! ABORT!\u201d. Do you think that speech recognition performance is not good enough? Microsoft announced 5.1% error rate (95.9% accurate) in its blog in 2017 (https://www.microsoft.com/en-us/research/blog/microsoft-researchers-achieve-new-conversational-speech-recognition-milestone/). They also measured the human\u2019s error rate with the same metric as 5.1%. So even if you think that the performance of your speech assistant is bad, it is actually very close to a human\u2019s.\n\nWhen you start an AI project you need to determine the expectations correctly from the stakeholders. If you manage expectations, both parties will be happier. For instance if your stakeholder had a perfect image recognizer in her mind, you have already failed at start. Even if you develop a great recognizer it will not be as perfect as she have thought. So manage expectations, and think about the accuracy in terms of real life setting. If it is not acceptable to shout \u201cABORT\u201d to your voice assistant every once a while, maybe you should think twice before you buy or start developing one."
    },
    {
        "url": "https://towardsdatascience.com/airflow-by-google-a-first-impression-of-cloud-composer-f4d9f86358cf?source=---------1",
        "title": "Airflow by Google: A first impression of Cloud Composer",
        "text": "A few days ago, Google Cloud announced the beta version of Cloud Composer. In brief, Cloud Composer is a hosted solution for Airflow, which is an open-source platform to programatically author, schedule and monitor workflows. For data folks who are not familiar with Airflow: you use it primarily to orchestrate your data pipelines.\n\nAs I had been looking at hosted solutions for Airflow, I decided to take Cloud Composer for a spin this week. The nice thing about hosted solutions is that you as a Data Engineer or Data Scientist don\u2019t have to spend that much time on DevOps \u2014 something you might not be very good at (at least I\u2019m not!).\n\nSo, below is a very brief write-up of the experience testing out Cloud Composer. This is by no means an exhaustive evaluation \u2014 it\u2019s simply my first impression of Cloud Composer.\n\nIt\u2019s extremely easy to set up. If you have a Google Cloud account, it\u2019s really just a few clicks away (plus ~20 minutes of waiting for your environment to boot). You can also easily list your required Python libraries from the Python Package Index (PyPI), set environment variables, and so on.\n\nDeployment is simple. Your DAGs folder sits in a dedicated bucket in Google Cloud Storage. This means you could literally drag-and-drop the contents of your DAG folder to deploy new DAGs. Within seconds the DAG appears in the Airflow UI. Of course, drag-and-drop is not the only option. You could also do deployment programmatically by using Google Cloud\u2019s gcloud.\n\nWhen your environment is up and running, the Google Cloud UI is clean and hassle-free: it just links to the DAG folder and to your Airflow webserver, which is where you\u2019ll be spending most of your time.\n\nOverall, the theme on the positive side is simplicity and ease-of-use, which is probably what you\u2019re looking for in a hosted solution.\n\nCloud Composer runs Python 2.7. There should at least be an option to run Python 3.6 \u2014 I would honestly have expected it to be the default.\n\nThere are still a few bugs to iron out \u2014 after all, this is a beta. For instance, when trying to clear a task I get this screen:\n\nThe pricing could be more transparent. After 3 days of running Cloud Composer I have a bill of \u20ac25, so assuming that\u2019s somewhat linear, I\u2019d be paying approximately \u20ac250/month. This is pretty much in line with the pricing example provided by Google Cloud (note they assume 25% utilisation in their example).\n\nHonestly, this is great value if you\u2019re startup in need of Airflow and you don\u2019t have a lot of DevOps folks in-house. But it\u2019s likely too expensive if you\u2019re primarily looking at Airflow for your hobby projects.\n\nIf you\u2019re looking for another hosted solution for Airflow where you can get more hands-on support and training, Astronomer might be a good option.\n\nWhat does your Airflow setup look like? Let me know in the comment field below."
    },
    {
        "url": "https://towardsdatascience.com/who-is-going-to-make-money-in-ai-part-i-77a2f30b8cef?source=---------2",
        "title": "Who is going to make money in AI? Part I \u2013",
        "text": "Weare currently experiencing another gold rush in AI. Billions are being invested in AI startups across every imaginable industry and business function. Google, Amazon, Microsoft and IBM are in a heavyweight fight investing over $20 billion in AI in 2016. Corporates are scrambling to ensure they realise the productivity benefits of AI ahead of their competitors while looking over their shoulders at the startups. China is putting its considerable weight behind AI and the European Union is talking about a $22 billion AI investment as it fears losing ground to China and the US.\n\nAI is everywhere. From the 3.5 billion daily searches on Google to the new Apple iPhone X that uses facial recognition to Amazon Alexa that cutely answers our questions. Media headlines tout the stories of how AI is helping doctors diagnose diseases, banks better assess customer loan risks, farmers predict crop yields, marketers target and retain customers, and manufacturers improve quality control. And there are think tanks dedicated to studying the physical, cyber and political risks of AI.\n\nAI and machine learning will become ubiquitous and woven into the fabric of society. But as with any gold rush the question is who will find gold? Will it just be the brave, the few and the large? Or can the snappy upstarts grab their nuggets? Will those providing the picks and shovel make most of the money? And who will hit pay dirt?"
    },
    {
        "url": "https://towardsdatascience.com/my-take-on-data-scientist-interview-questions-part-1-6df22252b2e8?source=---------3",
        "title": "My take on Data Scientist Interview Questions [ Part 1 ]",
        "text": "One of my friend have compiled a list of questions for people who are getting into Data Science. And I wanted to answer them, please note I don\u2019t know the origin of these questions if anyone knows where this questions are from please comment down below.\n\nAlso, I\u2019ll try to provide a correct answer for every questions. However, I am always open to learning and growing, so if you know a more optimal solution please comment down below."
    },
    {
        "url": "https://towardsdatascience.com/can-a-computer-name-lipstick-colors-1897a8208b17?source=---------4",
        "title": "Can a computer name lipstick colors? \u2013",
        "text": "I\u2019ve often wondered who names makeup colors. If you\u2019ve never browsed the cosmetic aisle at your local drugstore, you might not have noticed the flirtatious labels that distinguish an endless array of lipstick shades. For example, Revlon sells Audacious Mauve, Dare to Be Nude, Rich Girl Red, Kissable Pink and Relentless Raisin.\n\nBased on my hunch that these lipstick names follow a certain unspoken formula, I wanted to know if a computer could learn their patterns and produce new ones. I took hundreds of lip color shades (including balms, gloss, liners and lipsticks) from the Sephora website, as well as additional shades from drugstore standbys like Revlon. I fed this dataset to a neural network, a deep learning model that learns the structure of text and can produce its own rendition of whatever material it was trained on. You can get the model I use for yourself here.\n\nHere\u2019s what it came up with, featuring homemade illustrations. Starting with my favorites\u2026\n\nAlthough I was able to scrape hundreds of lipstick shade names, this is actually a very small dataset in terms of raw text available to the neural network, sine the names are rarely more than 2\u20133 words long. Neural networks tend to perform best when they have a great deal of training data. One cheap and dirty trick I used to get better training was simply to inflate the training set by repeating the list of unique names several times. I was concerned that this would lead to overfitting and simply regurgitating the training set names, but in this case it didn\u2019t. I trained a 3 layer, 256 node network, and found a \u201csweet spot\u201d on the temperature dial. Too low, and the network mostly produced common words like \u201cPink\u201d again and again. Too high, and it spat gibberish. But around 0.45\u20130.6, I got a really nice smattering of new lipstick names that were juuuuuuust on the edge of believable."
    },
    {
        "url": "https://towardsdatascience.com/deep-learning-for-machine-empathy-robots-and-humans-interaction-part-i-8142fccd6050?source=---------5",
        "title": "Deep Learning for Machine Empathy: Robots and Humans Interaction \u2014 Part I",
        "text": "When we think about the imminent development of the next digital revolution, humanity will face an unprecedented wave of automation. More and more smart and connected devices will coexist with us. This revolution is already taking place, from cell phones, to autonomous vehicles and even our refrigerator. Something is for sure, robots are already here, and they are here to stay.\n\nThe question is not whether we agree, but how we will interact with these new tenants. Beyond the classic principles of design, as utility and style, a new criterion will gain relevance: machine empathy. This tendency will become stronger as more companies understand that human-machine interaction (HMI) is key to secure technology adoption.\n\nBut, what can we do to improve human-machine interactions? Can we at least soften our coexistence?\n\nThe key to social integration is to master the ability to understand what other people feel and think, and react accordingly. Until now, this capacity has been reserved only for (some) humans. This virtue called empathy improves socialisation, and humans are sociable by nature.\n\nSo the answer could be to give machines the ability to understand how we feel, what we need and what our goals are. So they can react accordingly maximising our comfort. This also includes giving them the correct form. Will this new generation of robots be humanoids? Gentle automata like a harmless Roomba? Or perhaps terrifying as the Black Mirror\u2019s Metalhead robot \u2018dogs\u2019 and their real life distant relatives from Boston Dynamics. This is part of a whole when discussing about HMI.\n\nMany researchers have worked on this field, particularly the Humanoid Robotics Group at MIT. They developed Kismet, a social robot. Kismet kindly reacts to the emotions shown by its viewers, engaging people in natural and expressive face-to-face interactions. Preliminary results show a great improvement in the interaction between humans and these machines.\n\nIt is clear the success of this new wave of incoming automation will depend to a large extent on the empathy and personality of the robots. Imagine a car that detects that you are felling sad, and automatically plays a song that you love to make you feel better, or a robot medical assistant that recognises your needs and reacts to give you maximum attention and comfort. By adding powerful automatic speech recognition and natural language processing (extensively developed by Amazon Alexa and others) the possibilities are endless.\n\nSuch a system could be fed by external sources of information, making it evolve based on experience. Your device will continuously learn from you. This hyper personalisation will have a direct consequence: uniqueness. Uniqueness is the fuel of attachment, and attachment is intrinsically human.\n\nIn the science fiction movie Real Steel (2011), Atom, the boxer robot suffers serious damage several times during combat. Suddenly, emotions begin to appear, as an obvious sign, we don\u2019t want to lose Atom; it is unique. We know what made Atom so special compared to other robots, it showed feelings; it was empathetic.\n\nBut don\u2019t worry, at that time cloud storage and telecommunications technology will be so developed, that there is little chance of losing your robot\u2019s personality.\n\nIt is not clear how this could change the technology industry and affect consumer habits. Would you change your car as frequently as you did before? Would you have the impression that your device is unique? Will you get to bond with it?\n\nThe reality is that we still do not have answers to these questions. This revolution is beginning, and its potential consequences are not yet fully understood. Then this topic will be part of an open discussion in the upcoming years.\n\nToday, I think no one doubts about the extraordinary capabilities of Deep Learning (DL) for image recognition. As an example of that, we have its remarkable performance on large-scale datasets such as Imagenet and COCO.\n\nWhen it comes to emotion recognition in images, Convolutional Neural Networks (CNN) is clearly the way to go. However, just as for any other DL application, it is necessary to have an adequate set of sample images. For this post, we used the widely known Emotion Recognition FER2013 dataset, publicly available in Kaggle. This dataset is composed of 32298 greyscale images, labelled manually.\n\nDuring the supervised training process, the CNN automatically learns low and high level characteristics by projecting the input image into several feature spaces. You can see it as a chained set of classical convolutional filters, whose weights are set during training by back-propagating errors and optimizing using the gradient descend algorithm. This iterative process allows the network to learn which features are relevant to identify the emotions that are shown in the image. A very intuitive explanation of how Convolutional Neural Networks work can be found here.\n\nThe first layers of the network learn to identify the edges and small details of the face, intermediate layers find a combination of edges to construct contours. As we go deeper, an increasingly explicit representation of the input image is obtained \u2014 as the network learns to combine the previous low-level features to finally care about the actual content of the image, instead of individual pixel information. A very detailed explanation of this can be found in the notorious paper \u2018A Neural Algorithm to Transfer Style\u2019.\n\nThe following video is the result of a one-week immersion in real-time emotion recognition using Deep Convolutional Neural Networks. To test the solution, we chose the famous \u2018Sad Ben Affleck\u2019 interview. The preliminary results are shown here (more improvement is coming):\n\nIn the next post we will go directly to the implementation of this basic (and functional) empathy module for robots based on Deep Learning. We will delve into computer vision techniques, ranging from classical fast face detection algorithms to Deep Neural Networks for emotion recognition and transfer learning.\n\nI hope you enjoyed this publication as much as I did while doing it. I will be happy to read your opinion on this topic. Do not hesitate to leave your comments, claps are also welcome. Follow me to keep you informed about the next part of this article. If you want to know more about Axionable, projects and careers please visit our website and follow us on Twitter."
    },
    {
        "url": "https://towardsdatascience.com/gated-multimodal-units-for-information-fusion-966a9a2e1c54?source=---------6",
        "title": "Gated Multimodal Units for Information Fusion \u2013",
        "text": "In the paper they describe a nice synthetic data set which demonstrates how the GMU works.\n\nHere we\u2019ll implement the same data set, and find out for ourselves whether or not the GMU actually works (spoiler alert: it does).\n\nFirst, let\u2019s do the imports:\n\nDon\u2019t let the graph scare you \u2014 later on you\u2019ll find a visualization of the data generated by this graph.\n\nBasically what the graph says is that the target class C depicts the values of the modalities y\u1d5b and y\u1d57 \u2014 with some randomness of course.\n\nIn the next step the random variable M decides which of the inputs y\u1d5b, y\u1d57 to ignore, and instead to use a source of noise y\u0302\u1d5b, y\u0302\u1d57.\n\nIn the end, x\u1d5b and x\u1d57 contain either the real source of information which can describe the target class C, or random noise.\n\nThe goal of the GMU block is to successfully find out which one of the sources is the informative one given a specific example, and to give all the attention to that source."
    },
    {
        "url": "https://towardsdatascience.com/data-science-for-startups-tracking-data-4087b66952a1?source=---------7",
        "title": "Data Science for Startups: Tracking Data \u2013",
        "text": "Part two of my ongoing series about building a data science discipline at a startup. You can find links to all of the posts in the introduction.\n\nIn order to make data-driven decisions at a startup, you need to collect data about how your products are being used. You also need to be able to measure the impact of making changes to your product and the efficacy of running campaigns, such as deploying a custom audience for marketing on Facebook. Again, collecting data is necessary for accomplishing these goals.\n\nUsually data is generated directly by the product. For example, a mobile game can generate data points about launching the game, starting additional sessions, and leveling up. But data can also come from other sources, such as an email vendor that provides response data about which users read and click on links within an email. This post focuses on the first type of data, where tracking events are being generated by the product.\n\nIt\u2019s been said that data is the new oil, and there\u2019s a wide variety of reasons to collect data from products. When I first started in the gaming industry, data tracked from products was referred to as telemetry. Now, data collected from products is frequently called tracking.\n\nThis posts discusses what type of data to collect about product usage, how to send data to a server for analysis, issues when building a tracking API, and some concerns to consider when tracking user behavior.\n\nOne of the first questions to answer when deploying a new product is:\n\nThe answer is that it depends on your product and intended use cases, but there are some general guidelines about what types of data to collect across most web, mobile, and native applications.\n\nFor these three types of events, the data may actually be generated from three different systems. Installation data might come from a third party, such as Google Play or the App Store, a session start event will be generated from the client application, and spending money in an application, or viewing ads, may be tracked by a different server. As long as you own the service that is generating the data points, you can use the same infrastructure to collect data about different types of events.\n\nCollecting data about how many users launch and log into a application will enable you to answer basic questions about the size of your base, and enable you to track business metrics such as DAU, MAU, ARPDAU, and D-7 retention. However, it doesn\u2019t provide much insight into what users are doing within an application, and it doesn\u2019t provide many data points that are useful for building data products. In order to better understand user engagement, it\u2019s necessary to track data points that are domain or product specific. For example, you might want to track the following types of events in a multiplayer shooter game for consoles:\n\nMost of these events translate well to other shooter games and other genres such as action/adventure. For a specific game, such as FIFA, you may want to record game specific events, such as:\n\nLike the prior events, many of these game-specific events can actually be generalized to sports games. If you\u2019re a company like EA with a portfolio of different sports titles, it\u2019s useful to track all of these events across all of your sports titles (the red card event can be generalized to a penalty event).\n\nIf we\u2019re able to collect these types of events about players, we can start to answer useful questions about the player base, such as:\n\nA majority of tracking events are focused on collecting data points about released titles, but it\u2019s also possible to collect data during development. At Microsoft Studios, I worked with the user research team to get tracking in place for playtesting. As a result, we could generate visualizations that were useful for conveying to game teams where players were getting stuck. Incorporating these visualizations into the playtesting results resulted in a much better reception from game teams.\n\nWhen you first add tracking to a product, you won\u2019t know of every event and attribute that will be useful to record, but you can make a good guess by asking team members what types of questions they intend to ask about user behavior and by implementing events that are able to answer these questions. Even with good tracking data, you won\u2019t be able to answer every question, but if you have good coverage you can start to improve your products.\n\nSome teams write tracking specifications to in order to define which tracking events need to be implemented in a product. Other teams don\u2019t have any documentation and simply take a best guess approach at determining what to record. I highly recommend writing tracking specifications as a best practice. For each event, the spec should identify the conditions for firing an event, the attributes to send, and definitions for any event-specific attributes. For example, a session start event for a web app might have the following form:\n\nTracking specs are a highly useful piece of documentation. Small teams might be able to get away without having an official process for writing tracking specs, but a number of scenarios can make the documentation critical, such as implementing events on a new platform, re-implementing events for a new backend service, or having engineers leave the team. In order for specs to be useful, it\u2019s necessary to answer the following questions:\n\nIn small organizations, a data scientist might be responsible for all of the aspects of tracking. For a larger organization, it\u2019s common for the owners to be a product manager, engineering team, and testing group.\n\nAnother consideration when setting up tracking for a product is determining whether to send events from a client application or a backend service. For example, a video-streaming web site can send data about which video a user is watching directly from the web browser, or from the backend service that is serving the video. While there are pros and cons to both approaches, I prefer setting up tracking for backend services rather than client applications if possible. Some of the benefits of server-side tracking are:\n\nGenerating tracking from servers rather than client applications helps avoid issues around fraud, security, and versioning. However, there are some drawbacks to server-side tracking:\n\nA general guideline is to not trust anything sent by a client application, because often endpoints are not secured and there is no way to verify that the data was generated by your application. But client data is very useful, so it\u2019s best to combine both client and server side tracking and to secure endpoints used for collecting tracking from clients.\n\nThe goal of sending data to a server is to make the data available for analysis and data products. There\u2019s a number of different approaches that can be used based on your use case. This section introduces three different ways of sending events to an endpoint on the web and saving the events to local storage. The samples below are not intended to be production code, but instead simple proofs of concept. The next post in this series will cover building a pipeline for processing events. All code for the samples below is available on Github.\n\nWeb Call\n\nThe easiest way to set up a tracking service is by making web calls with the event data to a web site. This can be implemented with a lightweight PHP script, which is shown in the code block below.\n\nThis php script reads the message parameter from the URL and appends the message to a local file. The script can be invoked by making a web call:\n\nThe call can be made from a Java client or server using the following code:\n\nThis is one of the easiest ways to start collecting tracking data, but it doesn\u2019t scale and it\u2019s not secure. It\u2019s useful for testing, but should be avoided for anything customer facing. I did use this approach in the past to collect data about players for a Mario level generator experiment.\n\nWeb Server \n\nAnother approach you can use is setting up a web service to collect tracking events. The code below shows how to use Jetty to set up a lightweight service for collecting data. In order to compile and run the example, you\u2019ll need to include the following pom file. The first step is to start a web service that will handle tracking requests:\n\nIn order to process events, the application reads the message parameter from the web request, appends the message to a local file, and then responds to the web request. The full code for this example is available here.\n\nIn order to call the endpoint with Java, we\u2019ll need to modify the URL:\n\nThis approach can scale a bit more than the PHP approach, but is still insecure and not the best approach for building a production system. My advice for building a production ready tracking service is to use a stream processing system such as Kafka, Amazon Kinesis, or Google\u2019s PubSub.\n\nSubscription Service\n\nUsing messaging services such as PubSub enables systems to collect massive amounts of tracking data, and forward the data to a number of different consumers. Some systems such as Kafka require setting up and maintaining servers, while other approaches like PubSub are managed services that are serverless. Managed services are great for startups, because they reduce the amount of DevOps support needed. But the tradeoff is cost, and it\u2019s pricer to use managed services for massive data collection.\n\nThe code below shows how to use Java to post a message to a topic on PubSub. The full code listing is available here and the pom file for building the project is available here. In order to run this example, you\u2019ll need to set up a free google cloud project, and enable PubSub. More details on setting up GCP and PubSub are available in this post.\n\nThis code example shows how to send a single message to PubSub for recording a tracking event. For a production system, you\u2019ll want to implement the onFailure method in order to deal with failed deliveries. The code above shows how to send a message with Java, while other languages are supported including Go, Python, C#, and PHP. It\u2019s also possible to interface with other stream processing systems such as Kafka.\n\nThe next code segment shows how to read a message from PubSub and append the message to a local file. The full code listing is available here. In the next post I\u2019ll show how to consume messages using DataFlow.\n\nWe now have a way of getting data from client applications and backend services to a central location for analysis. The last approach shown is a scalable and secure method for collecting tracking data, and is a managed service making it a good fit for startups with small data teams.\n\nOne of the decisions to make when sending data to an endpoint for collection is how to encode the messages being sent, since all events that are sent from an application to an endpoint need to be serialized. When sending data over the internet, it\u2019s good to avoid language specific encodings, such as Java serialization, because the application and backend services are likely implemented in different languages. There\u2019s also versioning issues that can arise when using a language-specific serialization approach.\n\nSome common ways of encoding tracking events are using the JSON format and Google\u2019s protocol buffers. JSON has the benefit of being human readable and supported by a wide variety of languages, while buffers provide better comprension and may better suited for certain data structures. One of the benefits of using these approaches is that a schema does not need to be defined before you can send events, since metadata about the event is included in the message. You can add new attributes as needed, and even change data types, but this may impact downstream event processing.\n\nWhen getting started with building a data pipeline, I\u2019d recommended using JSON to get started, since it\u2019s human readable and supported by a wide variety of languages. It\u2019s also good to avoid encodings such as pipe-delimited formats, because you many need to support more complex data structures, such as lists or maps, when you update your tracking events. Here\u2019s an example of what a message might look like:\n\nWhat about XML? No!\n\nTo build a production system, you\u2019ll need to add a bit more sophistication to your tracking code. A production system should handle the following issues:\n\nIt\u2019s also useful to have a process in place for disabling tracking events. I\u2019ve seen data pipelines explode from client applications sending way too much data, and there was no way of disabling the clients from sending the problematic event without turning off all tracking.\n\nIdeally, a production level system should have some sort of auditing in place, in order to validate that the endpoints are receiving all of the data being sent. One approach is to send data to a different endpoint built on a different infrastructure and tracking library, but that much redundancy is usually overkill. A more lightweight approach is to add a sequential counting attribute to all events, so if a client sends 100 messages, the backend can use this attribute to know how many events the client attempted to send and validate the result.\n\nThere\u2019s privacy concerns to consider when storing user data. When data is being made available to analytics and data science teams, all personally identifiable information (PII) should be stripped from events, which can include names, addresses, and phone numbers. In some instances, user names, such as a player\u2019s gamertag on Steam, may be considered PII as well. It\u2019s also good to strip IP addresses from any data being collected, to limit privacy concerns. The general recommendation is to collect as much behavioral data as needed to answer questions about product usage, while avoiding the need to collect sensitive information, such as gender and age. If you\u2019re building a product based on sensitive information, you should have strong user access controls in place to limit access to sensitive data. Policies such GDPR are setting new regulations for collecting and processing data, and GDPR should be reviewed before shipping a product with tracking.\n\nTracking data enables teams to answer a variety of questions about product usage, enables teams to track the performance and health of products, and can be used to build data products. This post discussed some of the issues involved in collecting data about user behavior, and provided examples for how to send data from a client application to an endpoint for later analysis. Here are the key takeaways to from this post:\n\nAs you ship more products and scale up your user base, you may need to change to a different data collection platform, but this advice is a good starting point for shipping products with tracking.\n\nThe next post will introduce different approaches for building data pipelines."
    },
    {
        "url": "https://towardsdatascience.com/building-ethical-ai-in-healthcare-why-we-must-demand-it-ca60f4d28412?source=---------8",
        "title": "Building ethical AI in healthcare: why we must demand it",
        "text": "Now, in this story, it is important to note that the error, entirely real and affecting today\u2019s juniors doctors in the UK, was a script programming error and not \u2018AI\u2019. Apparently, some spreadsheets were formatted differently to each other, and an automated script to compile results didn't account for this. Nevertheless, it starkly highlights the ethical need for accountability, robustness and accuracy in administrative systems, especially since AI is touted as being the replacement tool for very similar tasks.\n\nThe Royal College of Physicians is currently working on this problem (which has spawned the typically British hashtag of #ST3cockup on Twitter), and at the time of writing there are still many junior doctors like Rachel and David still waiting to find out what this error means for them and their families.\n\nJunior doctors in the UK are increasingly feeling like they are an expendable resource rather than humans in a caring system. This error comes at a time when the dust was only just settling after the recent Dr Bawa Garba case (where a junior doctor was struck off the medical register under extraordinarily unfair circumstances), and only a couple of years after the hugely damaging junior doctors strikes over their imposed new working contract. For an administrative system to produce such a shocking new error, the effects of which are still unfolding, at a fragile time like this, it is not surprising to hear the uproar and disappointment from the medical ranks of the NHS.\n\nThere is a wider picture here, however, that I would like to focus on: the need for deep consideration of the potential impacts of automation.\n\nIt is heart-wrenching to think that without building ethical AI, we, as a society, run the risk of introducing blindly autonomous systems into the mix, capable of even more brutal and opaque decision making than the example above. AI does not have the capacity to understand a larger social context in which to assess its errors, and if such an administrative error were ever to be made by a machine, I\u2019m sure the uproar would be even louder. Regarding this current situation, heads are likely to roll. But will machine-heads roll if and when they make similar errors?\n\nRecently, there has been published an exquisite report funded by the Wellcome Trust, written by Future Advocacy, on the ethical, social and political challenges of AI in health. A few quotes stand out to me from this report:\n\nI couldn\u2019t agree more with this statement. To put it within the context of the ST3 application error, it is clear that certain groups of junior doctors have been burdened more than others, and the problem is only visible once those affected start reporting it. I am under no doubt that whoever wrote the offending spreadsheet script had checked it worked (i.e. overall performance was good), but was totally unaware of what their code was capable of doing, and likely was only notified once the proverbial hit the fan.\n\nThis second statement is also interesting in this context. Had the recruitment and selection system been more open, transparent, and even co-designed with those to be affected by it, would this error have occurred at all? For instance, if junior doctors knew the ranking system, the format of the spreadsheets, and the code used to compile results \u2014 would someone have noticed beforehand? Now, I\u2019m not suggesting that in this context that we should aim for that level of ideal transparency \u2014 I\u2019m simply posing the question \u201cwhere do we draw the line between a closed system and a transparent one?\u201d\n\nWhen it comes to translating AI and autonomous systems into even more potentially dangerous situations, such as life and death decision making (e.g. autonomous cars, cancer diagnoses) the ethical problems loom even larger.\n\nI was fortunate enough to have spent some time with Prof Alan Winfield of the Bristol Robotics Laboratory. He is an eminent thinker on ethical AI and robotics, and widely regarded as an authority in this field. He is working with IEEE to create new standards for the introduction of ethics to future technologies, a clearly topical subject! What I have learnt from him is that a system is not ethical until it has fully considered and included everyone it could possibly impact. This is a profoundly interesting thought, and one that I think should be mandated across the board.\n\nAI is essentially a form of computational statistics. Mathematically, AI systems in healthcare that provide a classification output (e.g. \u2018cancer\u2019 or \u2018no cancer\u2019 on a scan) can be assessed on Receiver Operator Curves (ROC curves). (Don\u2019t worry, I\u2019m not going to start a lecture about statistics). All you need to know as a lay reader is that these curves are a simple way to easily compare systems against each other. The better a system performs, the further up and towards the left the curve goes. A perfect 100% accurate system would not even be a curve, it would be a right angle in the top left hand corner of the graph.\n\nIn this example, AUCc performs the best with an Area Under the ROC curve (AUROC) of around 99%. This is near perfect, but not quite. AUCb is second best with an AUROC of around 85%, and AUCa is the worst performing at around an AUROC of 80%. In healthcare, you can draw up ROC curves of AI systems and compare them to human performance. Often in the media when you hear of an AI \u2018beating humans\u2019 it is because the AUROC is better, or the curve lies above and to the left of the point at which humans operate on the graph (I\u2019m not going to go into the semantics any further \u2014 but trust me, there is lot of heated debate on this topic).\n\nHowever, AUROC is NOT ethical. In fact it is entirely unethical. It reports only the positives success rate, and completely ignores the negative. There is an ethical grey area that hardly anyone considers \u2014 what I call the Area Outside the ROC curve (AOROC). This area can be thought of representing all the possible times an AI system gets a decision wrong. This area may well be smaller for an AI system than it is for a range of human doctors, but humans can at least understand and recognise their errors, change their mind, and explain their reasoning.\n\nNow, in the example above, this AOROC area for the best performing system is very small, maybe about 1%. But what does it mean for people within that 1% when their automated decision is incorrect? How can we train systems to recognise when they are wrong, and how to change their mind? How do we build in risk mitigation to account for these errors and the potentially life changing affects they could have? The answers to these questions are tough, and accordingly often ignored.\n\nIt irks me when I read mainstream media reporting \u2018computer beats doctor at X\u2019 because I know somewhere along the line the risks of being within the Area Outside the ROC curve have been neglected, even by medical device regulators who are supposed to champion clinical safety. I challenge anyone reading this article to find me a regulatory-approved AI system which has published statistics regarding their failure rate, and an ethical statement on how they have mitigated against those that could be adversely affected. I\u2019ve tried to find these, but have failed.\n\nEven worse, these statistics completely fail to take into account bias in the underlying training data. For instance, Google had to publicly apologise when their highly performant classifier system tagged black people as gorillas, simply because the training data did not include as many black faces as white. A ROC curve will never be able to demonstrate this type of data bias, so we must mandate other methods of transparency to ensure AI developers are stating their data quality alongside their performance.\n\nThankfully there is a growing consensus that ethics for AI is absolutely essential. The recent House of Lords Select Committee report that I consulted on had a strong recommendation that the UK should forge a distinctive role for itself as a pioneer in ethical AI.\n\nThe impact of ethics in AI is only now hitting mainstream consciousness. Indeed, following the recent Facebook data sharing scandal, the company has set up an internal ethics unit to look at these kind of problems, and Microsoft have even pulled sales over ethical concerns.\n\nI would urge anyone developing AI tools for healthcare to take a long hard look at the potential risks of their systems, and try to think outside of the box on how to ensure an ethical approach. Too often have we seen developers rush to be \u2018first to market\u2019 and make headline claims of performance, without care or mention of ethics, and it is my belief that it is only a matter of time before we begin to see the brutal dystopian impact created when these systems ultimately fail, which they are guaranteed to do at some point \u2014 take the recent NHS breast cancer screening error. Here, an automated system didn\u2019t invite women for screening at the right time, leading some observers to claim that up to 270 women may have died as a result. The ethical ramifications here are staggering, and have even been debated in the House of Commons. No-one yet knows who is responsible; even the contractor who runs the system is deferring blame elsewhere\u2026\n\nIf we are to avoid the vision of a cold inhumane future where real lives are cast aside by automated decisions and their inevitable errors, we must start talking about, and building in, ethics right now. Otherwise, it will be too late to save ourselves from the machines."
    },
    {
        "url": "https://towardsdatascience.com/about-towards-data-science-d691af11cc2f?source=---------9",
        "title": "About \u2013",
        "text": "We strive to present well-written, informative articles that our audience is excited to read.\n\n\u201cIdeas and perspectives you won\u2019t find anywhere else.\u201d\n\nThe emphasis on people rather than on advertisements renders Medium unique. It not only helps writers to present their original ideas, but it also offers a clean reading experience to a broad audience of engaged readers.\n\nThe Medium ecosystem is kept clear to remain transparent with a user-friendly Terms Of Service and simple rules accessible to all. Writers are better protected because they are in control of the licenses for their work.\n\nFurthermore, in 2017 Medium launched the internet\u2019s first open Paywall. This paywall enables creators to \u201cearn money from a community of readers that believes ideas and creativity are worth paying for\u201d. We are thrilled to be a part of it!\n\nWe joined Medium\u2019s vibrant community in October 2016. In the beginning, our goal was simply to gather good posts and distribute them to a broader audience. Just a few months later, we were pleased to see that we had a very fast growing audience and many new contributors.\n\nToday we are working with more than 10 Editorial Associates to prepare the most exciting content for our audience. We provide customized feedback to our contributors using Medium\u2019s private notes. This allows us to promote our latest articles across social media without the added complexity that we might encounter using another platform.\n\nThe success of Medium\u2019s Ecosystem attracts more and more unique voices eager to share something that matters. In February 2017 Ev Williams, CEO of Medium, reported that more than 50,000 writers publish on Medium every week. This striking figure is still growing in 2018. Better still, Medium is a platform for everyone, meaning you get authors from everywhere - who are professional and also new to the journey.\n\nThis impressive community of authors is what lifted our following, drove the early days of Towards Data Science, and continues to be the reason we\u2019re still here. In April 2018, we counted more than 1500 authors that were located around the globe, and we are grateful to receive so many of your new contributions every day.\n\nPeople join for all sorts of reasons, whether it\u2019s to reach a broader audience, obtain critical feedback or to build professional networks within our community. Others like publishing with us as they remain the sole owner of their original work, and can edit (or delete) their posts at any moment, even after it is published. \n\n \n\nWhether our authors choose to publish with us once week, a few times a month or several times during the year, they all offer a unique contribution that makes us who we are today.\n\nFor instance, Jonny Brooks-Bartlett has published several articles on probability. Ethan Arsht used math to study if Napoleon was the Best General Ever. Susan Li shared her latest codes and tutorials with the data science community. George Seif explained different types of clustering algorithms amongst a host of other concepts.\n\nSince we started, we have been amazed by the quality of the posts that we receive. We can feel the hard work and the willingness to share something important. We feel privileged to help and support our contributors to get their message across.\n\nWe are a international team of 15 people who work on the content that gets released on Towards Data Science. Every day we review the submissions and offer feedback to a multitude of authors. Our goal is to present well-written, informative articles that our audience is excited to read.\n\nOn top of that, we are always working on new projects (like developing a podcast channel) to further help our contributors to reach our audience. For everything we do, we try to involve our community as much as possible.\n\nUltimately, we hope to make the world a better place and we believe that data-related knowledge is a crucial part of achieving this; so we work hard to bring more value to our data science community.\n\n1. Stay Positive: Sometimes the best way to protest is to propose something new. We help the people who are trying to present and share new ideas. The more we offer new ideas and share knowledge, the quicker we can solve the pressing problems.\n\n2. Focus on Improvement: As we care about the long-term, we always seek to achieve manageable and gradual improvement. We continually want to earn and build on our readers\u2019 trust!\n\n3. Be Grateful: Without Medium, or without our community, we never could have existed. We are thankful to all the people that have made this possible. We are especially grateful to our writers, Patreon supporters, Editorial Associates and anyone else who has helped us along the way.\n\nBy remaining conscious of our Patreon community, it not only reminds us to stay close to our values and to choose transparency when possible, but it also challenges us to find new ways to develop our organisation. Every day we strive to be the reason our supporters remain generous.\n\nOur work would not be possible without our Editorial Associates. Since we started recruiting in October 2017, they have raised the quality of the content published on Towards Data Science. We now have a team of kind and responsible people, eager to learn, to help and to understand our authors\u2019 ideas.\n\nLike in many bottom-up movements, Towards Data Science was also built with friends, family and people who decided to help without wanting anything in return. We thank them, and you, for everything. \u2728"
    },
    {
        "url": "https://towardsdatascience.com/the-logistic-regression-algorithm-75fe48e21cfa",
        "title": "The Logistic Regression Algorithm \u2013",
        "text": "Logistic Regression is one of the most used Machine Learning algorithms for binary classification. It is a simple Algorithm that you can use as a performance baseline, it is easy to implement and it will do well enough in many tasks. Therefore every Machine Learning engineer should be familiar with its concepts. The building block concepts of Logistic Regression can also be helpful in deep learning while building neural networks. In this post, you will learn what Logistic Regression is, how it works, what are advantages and disadvantages and much more.\n\nLike many other machine learning techniques, it is borrowed from the field of statistics and despite its name, it is not an algorithm for regression problems, where you want to predict a continuous outcome. Instead, Logistic Regression is the go-to method for binary classification. It gives you a discrete binary outcome between 0 and 1. To say it in simpler words, it\u2019s outcome is either one thing or another.\n\nA simple example of a Logistic Regression problem would be an algorithm used for cancer detection that takes screening picture as an input and should tell if a patient has cancer (1) or not (0).\n\nLogistic Regression measures the relationship between the dependent variable (our label, what we want to predict) and the one or more independent variables (our features), by estimating probabilities using it\u2019s underlying logistic function.\n\nThese probabilities must then be transformed into binary values in order to actually make a prediction. This is the task of the logistic function, also called the sigmoid function. The Sigmoid-Function is an S-shaped curve that can take any real-valued number and map it into a value between the range of 0 and 1, but never exactly at those limits. This values between 0 and 1 will then be transformed into either 0 or 1 using a threshold classifier.\n\nThe picture below illustrates the steps that logistic regression goes through to give you your desired output.\n\nBelow you can see how the logistic function (sigmoid function) looks like:\n\nWe want to maximize the likelihood that a random data point gets classified correctly, which is called Maximum Likelihood Estimation. Maximum Likelihood Estimation is a general approach to estimating parameters in statistical models. You can maximize the likelihood using different methods like an optimization algorithm. Newton\u2019s Method is such an algorithm and can be used to find maximum (or minimum) of many different functions, including the likelihood function. Instead of Newton\u2019s Method, you could also use Gradient Descent.\n\nYou may be asking yourself what the difference between logistic and linear regression is. Logistic regression gives you a discrete outcome but linear regression gives a continuous outcome. A good example of a continuous outcome would be a model that predicts the value of a house. That value will always be different based on parameters like it\u2019s size or location. A discrete outcome will always be one thing (you have cancer) or another (you have no cancer).\n\nIt is a widely used technique because it is very efficient, does not require too many computational resources, it\u2019s highly interpretable, it doesn\u2019t require input features to be scaled, it doesn\u2019t require any tuning, it\u2019s easy to regularize, and it outputs well-calibrated predicted probabilities.\n\nLike linear regression, logistic regression does work better when you remove attributes that are unrelated to the output variable as well as attributes that are very similar (correlated) to each other. Therefore Feature Engineering plays an important role in regards to the performance of Logistic and also Linear Regression. Another advantage of Logistic Regression is that it is incredibly easy to implement and very efficient to train. I typically start with a Logistic Regression model as a benchmark and try using more complex algorithms from there on.\n\nBecause of its simplicity and the fact that it can be implemented relatively easy and quick, Logistic Regression is also a good baseline that you can use to measure the performance of other more complex Algorithms.\n\nA disadvantage of it is that we can\u2019t solve non-linear problems with logistic regression since it\u2019s decision surface is linear. Just take a look at the example below that has 2 binary features from 2 examples.\n\nIt is clearly visible that we can\u2019t draw a line that separates these 2 classes without a huge error. To use a simple decision tree would be a much better choice.\n\nLogistic Regression is also not one of the most powerful algorithms out there and can be easily outperformed by more complex ones. Another disadvantage is its high reliance on a proper presentation of your data. This means that logistic regression is not a useful tool unless you have already identified all the important independent variables. Since its outcome is discrete, Logistic Regression can only predict a categorical outcome. It is also an Algorithm that is known for its vulnerability to overfitting.\n\nLike I already mentioned, Logistic Regression separates your input into two \u201eregions\u201d by a linear boundary, one for each class. Therefore it is required that your data is linearly separable, like the data points in the image below:\n\nIn other words: You should think about using logistic regression when your Y variable takes on only two values (e.g when you are facing a classification problem). Note that you could also use Logistic Regression for multiclass classification, which will be discussed in the next section.\n\nOut there are algorithms that can deal by themselves with predicting multiple classes, like Random Forest classifiers or the Naive Bayes Classifier. There are also algorithms that can\u2019t do that, like Logistic Regression, but with some tricks, you can predict multiple classes with it too.\n\nLet\u2019s discuss the most common of these \u201ctricks\u201d at the example of the MNIST Dataset, which contains handwritten images of digits, ranging from 0 to 9. This is a classification task where our Algorithm should tell us which number is on an image.\n\nWith this strategy, you train 10 binary classifiers, one for each number. This simply means training one classifier to detect 0s, one to detect 1s, one to detect 2s and so on. When you then want to classify an image, you just look at which classifier has the best decision score\n\nHere you train a binary classifier for every pair of digits. This means training a classifier that can distinguish between 0s and 1s, one that can distinguish between 0s and 2s, one that can distinguish between 1s and 2s etc. If there are N classes, you would need to train NxN(N-1)/2 classifiers, which are 45 in the case of the MNIST dataset.\n\nWhen you then want to classify images, you need to run each of these 45 classifiers and choose the best performing one. This strategy has one big advantage over the others and this is, that you only need to train it on a part of the training set for the 2 classes it distinguishes between. Algorithms like Support Vector Machine Classifiers don\u2019t scale well at large datasets, which is why in this case using a binary classification algorithm like Logistic Regression with the OvO strategy would do better, because it is faster to train a lot of classifiers on a small dataset than training just one at a large dataset.\n\nAt most algorithms, sklearn recognizes when you use a binary classifier for a multiclass classification task and automatically uses the OvA strategy. There is an exception: When you try to use a Support Vector Machine classifier, it automatically runs the OvO strategy.\n\nOther common classification algorithms are Naive Bayes, Decision Trees, Random Forests, Support Vector Machines, k-nearest neighbor and many others. We will also discuss them in future blog posts but don\u2019t feel overwhelmed by the amount of Machine Learning algorithms that are out there. Note that it is better to know 4 or 5 algorithms really well and to concentrate your energy at feature-engineering, but this is also a topic for future posts.\n\nIn this post, you have learned what Logistic Regression is and how it works. You now have a solid understanding of its advantages and disadvantages and know when you can use it. Also, you have discovered ways to use Logistic Regression to do multiclass classification with sklearn and why it is a good baseline to compare other Machine Learning algorithms with.\n\nThis post was initially published at my blog (https://machinelearning-blog.com)."
    },
    {
        "url": "https://towardsdatascience.com/spam-detection-with-logistic-regression-23e3709e522",
        "title": "Spam Detection with Logistic Regression \u2013",
        "text": "I still remember my first day in machine learning class. The first example which was provided to explain, how machine learning works, was \u201cSpam Detection\u201d. I think in most of the machine learning courses tutors provide the same example, but, in how many courses you actually get to implement the model? We talk how machine learning involved in Spam Detection and then just move on to other things.\n\nThe idea of this post is to understand step by step working of the spam filter and how it helps in making everyone life easier. Also, next time when you see a \u201cYou have won a lottery\u201d email rather than ignoring it, you might prefer to report it as a spam.\n\nThe above image gives an overview of spam filtering , plenty of emails arrive everyday, some goes to spam and rest stays in our primary inbox(unless you have further categories defined). The blue box in the middle \u2014 Machine Learning Model, how does it decide which mail is spam and which one is not.\n\nBefore we start talking about the algorithm and the code, take a step back and try relating that simple explanation of spam detection with monthly active Gmail account(which is approximately 1 billion). The picture seems pretty complicated, isn\u2019t it? Let\u2019s get an overview on how does gmail use the filtering for a huge number of accounts.\n\nWe all know the data Google has, is not obviously in paper files. They have data centers which maintain the customers data. Before Google/Gmail decides to segregate the emails into spam or not spam category, before it arrives to your mailbox, hundreds of rules apply to those email in the data centers. These rules describe the properties of a spam email. There are common types of spam filters which are used by Gmail/Google \u2014\n\nBlatant Blocking- Deletes the emails even before it reaches to the inbox.\n\nBulk Email Filter- This filter helps in filtering the emails that are passed through other categories but are spam.\n\nCategory Filters- User can define their own rules which will enable the filtering of the messages according to the specific content or the email addresses etc.\n\nNull Sender Disposition- Dispose of all messages without an SMTP envelope sender address. Remember when you get an email saying, \u201cNot delivered to xyz address\u201d.\n\nThere are ways to avoid spam filtering and send your emails straight to the inbox. To learn more about Gmail spam filter please watch this informational video from Google.\n\nMoving on to our aim of creating our very own spam detector. Let\u2019s talk about about that blue box in the middle of above image. The model is like a small kid unless you tell the kid, the difference between salt and sugar, he/she won\u2019t be able to recognize it. The similar idea we apply on machine learning model, we tell the model beforehand what kind of email can be spam or not spam. In order to do that we need to collect the data from users and ask them to filter the few of the email as spam or not spam.\n\nThe above image is a snapshot of tagged email that have been collected for Spam research. It contains one set of messages in English of 5,574 emails, tagged according being legitimate(ham) or spam.\n\nNow that we have data with tagged emails \u2014 Spam or Not Spam, what should we do next? We would need to train the machine to make it smart enough to categorize the emails on its own. But, the machine can\u2019t read the full statement and start categorizing the emails. Here we will need to use our NLP basics (check out my last blog).\n\nWe will first do some pre-processing on message text, like removing - punctuation and stop words.\n\nOnce the pre-processing is done, we would need to vectorize the data \u2014 i.e collecting each word and its frequency in each email. The vectorization will produce a matrix.\n\nThis vector matrix can be used create train/test split. This will help the us to train the model/machine to be smart and test the accuracy of its results.\n\nNow that we have train test split, we would need to choose a model. There is a huge collection of models but for this particular exercise we will be using logistic regression.Why?\n\nGenerally when someone asks, what is logistic regression? what do you tell them \u2014 Oh! it is an algorithm which is used for categorizing things into two classes (most of the time) i.e. the result is measured using a dichotomous variable. But, how does logistic regression classify thing into classes like -binomial(2 possible values), multinomial(3 or more possible values) and ordinal(deals with ordered categories). For this particular post we will only be focusing on binomial logistic regression i.e. the outcome of the model will be categorized into two classes.\n\nFrom the definition it seems, the logistic function plays an important role in classification here but we need to understand what is logistic function and how it helps in estimating the probability of being in a class.\n\nThe formula mentioned in the above image is known as Logistic function or Sigmoid function and the curve called Sigmoid curve. The Sigmoid function gives an S shaped curve. The output of Sigmoid function tends towards 1 as z \u2192 \u221e and tends towards 0 as z \u2192 \u2212\u221e. Hence Sigmoid/logistic function produces the value of dependent variable which will always lie between [0,1] i.e the probability of being in a class.\n\nFor the Spam detection problem, we have tagged messages but we are not certain about new incoming messages. We will need a model which can tell us the probability of a messages being Spam or Not Spam. Assuming in this example , 0 indicates \u2014 negative class (absence of spam) and 1 indicates \u2014 positive class (presence of spam), we will use logistic regression model.\n\nSo, first we define the model then fit the train data \u2014 this phase is called training your model. Once the training phase is finished we can use the test split and predict the results. In order to check the accuracy of our model we can use accuracy score metric. This metric compares the predicted results with the obtained true results. After running above code we got 93% accuracy.\n\nIn some cases 93% might seems a good score. There is a lot other things we can do with the collected data in order to achieve more accurate results, like stemming the words and normalizing the length.\n\nAs we saw, we used previously collected data in order to train the model and predicted the category for new incoming emails. This indicate the importance of tagging the data in right way. One mistake can make your machine dumb, e.g In your gmail or any other email account when you get the emails and you think it is a spam but you choose to ignore, may be next time when you see that email, you should report that as a spam. This process can help a lot of other people who are receiving the same kind of email but not aware of what spam is. Sometimes wrong spam tag can move a genuine email to spam folder too. So, you have to be careful before you tag an email as a spam or not spam."
    },
    {
        "url": "https://towardsdatascience.com/5-strategies-to-write-unblock-able-web-scrapers-in-python-5e40c147bdaf",
        "title": "5 strategies to write unblock-able web scrapers in Python",
        "text": "People who read my posts in scraping series often contacted me to know how could they write scrapers that don\u2019t get blocked. It is very difficult to write a scraper that NEVER gets blocked but yes, you can increase the life of your web scraper by implementing a few strategies. Today I am going to discuss them.\n\nThe very first thing you need to take care of is setting the user-agent. User Agent is a tool that works on behalf of the user and tells the server about which web browser the user is using for visiting the website. Many websites do not let you view the content if the user-agent is not set. If you are using the library you can then do something like below:\n\nYou can get a User-Agent string by writing what is my user agent in Google search bar and it will return you your own User-Agent.\n\nAlright, you already have set a User-Agent but how about making it more real? Well, the best way to do is to pick a random User-Agent from a text file, a Python or from the database. Udger shares a massive list of UAs w.r.t browsers. For instance, for Chrome it looks like this and for Firefox like this one. Now let\u2019s make a function that will return a random UA that you could use in\n\nThe contains one UA per line from the website I shared above. The function will always return a unique UA from the text file. You can now call the function like:\n\nThe next thing you would like to set is the referer. The general rule of thumb is that if it\u2019s a listing page or home page then you can set Google\u2019s main page URL of that country. For instance, if I am scraping then I\u2019d set instead of\n\nIf you are going to scrape individual product pages then you can either set relevant category URL in referrer or can find the backlinks of the domain you are going to crawl. I usually use SEMRush for the purpose. For the link SEMRush returns something like below:\n\nIf you click to view the larger version of the image you could see some links that are pointing to my required category. Once you collect all such genuine backlinks you can then use them as referrer by copying the logic inside return random referrer. The will now become something like given below:\n\nI can\u2019t emphasize enough for it. Look, if you are into serious business then you have to use multiple proxy IPs to avoid blocking. Majority of websites block crawlers based on the static IP of your server or hosting provider. The sites use intelligent tools to figure out the pattern of a certain IP or a pool of IP and simply block them. This is why it\u2019s recommended to buy several IPs, 5o-100 at least to avoid blocking. There are various services available but I am happy with Shaders, now called OxyLabs. They are expensive but the quality of service is also good. Make sure when you order multiple IPs, ask for random IPs or at least they don\u2019t follow a certain pattern like to The site admin will simply put a rule to block all IPs belongs to. It\u2019s as simple as that.\n\nIf you are using you can use it like below:\n\nAnd if you are using Selenium and wants to use proxy IPs with Selenium then it\u2019s a bit tricky.\n\nNeedless to say that is the method that returns a unique random proxy like you are getting unique and random UAs and Referer above.\n\nYou also can come up with a system where you can set the frequency of an IP to visit the website per day or per hour and if it exceeds it then it put into a cage till the next day. The company I work I devised a system where not only I have set the frequency of the IP but also record which IP is blocked. At the end, I just request the proxy service provider to replace those proxies only. Since this is beyond the scope of this post so I\u2019d not get into details of it.\n\nThings you have implemented so far are good to go but there are still some cunning website that asks you to work more, they look for certain request header entries when you access the page and if the certain header is not found they would either block the content or will spoof the content. It\u2019s very easy to mimic the entire request you are going to make on a website. For instance, you are going to access a certain Craigslist URL and wants to know which headers are being requested. Go to Chrome/Firefox and inspect the page being accessed, you should be able to see something like below:\n\nIf you click image and view, you will find various entries besides the referee and user-agent. You can either implement all or implement and test one by one. I always set these entries regardless of whichever website I am going to access. Make sure you don\u2019t just copy/paste across sites as these entries often vary one from site to other:\n\nIt\u2019s always good to put some delay between requests. I use for that purpose where I pass a list of random numbers I would like to delay the service:\n\nYou can also use for the same purpose if you are not already using the library.\n\nIf you are really in hurry then you can execute URLs in parallel which I have explained here.\n\nThe uncertainty of web scrapers getting block will never go zero but you can always take some steps to avoid it. I discussed a few strategies which you should implement in your web crawler one way or other.\n\nIf you know other strategies or tricks then do let me know by sharing in comments. As always, your feedback is most welcome."
    },
    {
        "url": "https://towardsdatascience.com/fast-near-duplicate-image-search-using-locality-sensitive-hashing-d4c16058efcb",
        "title": "Fast Near-Duplicate Image Search using Locality Sensitive Hashing",
        "text": "If you have some education in Machine Learning, the name Nearest Neighbor probably reminds you of the k-nearest neighbors algorithm. It is a very simple algorithm with seemingly no \u201clearning\u201d actually involved: The kNN rule simply classifies each unlabeled example by the majority label among its k-nearest neighbors in the training set.\n\nThis seems like a very naive, even \u201csilly\u201d, classification rule. Is it? Well, depends on what you take as your distance metric, i.e: how do you choose to measure the similarity between examples. Yes, the naive choice \u2014 using simple Euclidean distance in the \u201craw\u201d features \u2014 often usually leads to very poor results in practical applications. For example, here are two examples (images) whose pixel-values are close in Euclidean distance; but arguably, one would be crazy to classify the left image as a flower, solely based on it being a neighbor of the right image.\n\nBut, as it turns out, coupling the kNN rule with the proper choice of a distance metric can actually be extremely powerful. The field of \u201cmetric learning\u201d demonstrated that when machine learning is applied to learning the metric prior to using the kNN rule, results can improve significantly.\n\nThe great thing about our current \u201cDeep Learning era\u201d is the abundance of available pre-trained networks. These networks solve certain classification tasks (predicting an image category, or the text surrounding a word), but the interesting thing is not so much their success on those tasks, but actually the extremely useful by-product they provide us with: dense vector representations, for which simple Euclidean distance actually corresponds to high-level, \u201csemantic\u201d similarity.\n\nThe point is that for many tasks (but namely, general-purpose images and text), we already have a good distance metric, so now we actually can just use the simple kNN rule. I\u2019ve talked about this point a lot in the past \u2014 e.g, in a previous post I attempted to use such searches to verify the claim that generative models are really learning the underlying distribution and not just memorizing examples from the training set.\n\nThis leaves us with the task of actually finding the nearest neighbors (I will refer to this as a NN query). This problem \u2014 now a building block in literally any ML pipeline \u2014 has received a lot of traction, both in the CS-theory literature and from companies that need highly optimized solutions for production environments. Here again, the community benefits, because a couple of the big players in this field have actually open-sourced their solutions. These tools use carefully crafted data structures and optimized computation (e.g on GPUs) to efficiently implement NN queries. Two of my favorites are Facbook\u2019s FAISS and Spotify\u2019s Annoy. This post should hopefully bring you up to speed on what happens \u201cbehind the hood\u201d of these libraries.\n\nA first distinction when we talk about nearest neighbors queries is between exact and approximate solutions.\n\nExact algorithms need to return the k nearest neighbors of a given query point in a dataset. Here, the naive solution is to simply compare the query element to each element in the dataset, and choose the k that had the shortest distances. This algorithm takes O(dN), where N is the size of the dataset and d is the dimensionality of the instances. At first glance this might actually seem satisfactory, but think about it: 1) this is only for a single query! 2) while true that d is fixed, it can often be very large, and most importantly 3) in the \u201cbig data\u201d paradigm, when datasets can be huge, being linear in the dataset size is no longer satisfactory (even if you\u2019re Google or Facebook!). Other approaches for exact queries use tree structures and can achieve better average complexity but their worst-case complexity still approaches something that\u2019s linear in N.\n\nApproximate algorithms are given some leeway. There are a couple of different formulations, but the main idea is that they only need to return instances whose distance to the query point is almost that of the real nearest neighbors (where \u2018almost\u2019 is the algorithm\u2019s approximation factor). Allowing for approximate solutions opens the door to randomized algorithms, that can perform an ANN (approximate NN) query in sublinear time.\n\nGenerally-speaking, a common and basic building block for implementing sublinear time algorithms are hash functions. A hash function is any function that maps input into data of fixed size (usually of lower dimension). The most famous example, which you might have encountered by simply downloading files off the internet, is that of checksum hashes. The idea behind them is to generate a \u201cfinger-print\u201d \u2014 i.e, some number that is hopefully unique for a particular chunk of data \u2014 that can be used to verify that the data was not corrupted or tampered with when it was transferred from one place to another.\n\nThese hash functions were designed with this sole purpose in mind. This means that they are actually very sensitive to small changes in the input data; even a single bit that\u2019s changed will completely change the hash value. While this is really what we need for exact duplicate detection (e.g, flagging when two files are really the same), it\u2019s actually the opposite of what we need for near duplicate detection.\n\nThis is precisely what Locality Sensitive Hashing (LSH) attempts to address. As it\u2019s name suggest, LSH depends on the spatiality of the data; in particular, data items that are similar in high-dimension will have a larger chance of receiving the same hash value. This is the goal; there are numerous algorithms that construct hash functions with this property. I will describe one approach, that is amazingly simple and demonstrates the incredibly surprising power of random projections (for another example, see the beautiful Johnson-Lindenstrauss lemma).\n\nThe basic idea is that we generate a hash (or signature) of size k using the following procedure: we generate k random hyperplanes; the i-th coordinate of the hash value for an item x is binary: it is equal to 1 if and only if x is above the i-th hyperplane.\n\nThe entire algorithms is just repeating this procedure L times:\n\nLet\u2019s understand how LSH can be used to perform ANN queries. The intuition is as follows: If similar items have (with high probability) similar hashes, then given a query item, we can replace the \u201cnaive\u201d comparison against all the items in the dataset, with a comparison only against items with similar hashes (in the common jargon, we refer to this as items that landed \u201cin the same bucket\u201d). Here we see that the fact that we were willing to settle for accuracy is precisely what allows for sublinear time.\n\nSince inside the bucket we compute exact comparisons, the FP probability (i.e, saying that an item is a NN when it truly isn\u2019t) is zero, so the algorithm always has perfect precision; however, we will only return items from that bucket, so if the true NN was not originally hashed to the bucket, we have no way of returning it. This means that in the context of LSH, when we talk about accuracy we really mean recall.\n\nFormally, an ANN query using LSH is performed as follows: 1) Find the \u201cbucket\u201d (hash value) of the query item 2) Compare against every other item in the bucket.\n\nLet\u2019s analyze the computational complexity of this algorithm. It will be quick and easy, I promise!\n\nStage 1) costs dk; Stage 2) costs N/2^k in expectation (because there are N points in the dataset and 2^K regions in our partitioned space). Since the entire procedure is repeated L times, the total cost is, on average, LDK+LDN/2^k. When k and L are taken to be about logN, we get the desired O(logN).\n\nWe\u2019ve seen the basic algorithm for LSH. It has two parameters, k (size of each hash) and L (the number of hash-tables) \u2014 different setting of the values for k,L correspond to different LSH configurations, each with its own time complexity and accuracy.\n\nAnalyzing these formally is a little tricky and requires much more math, but the general take-away is this:\n\nGenerally a good approach for settling such trade-offs empirically is to quantify them on a well-defined task, which you can hopefully design using minimal manual labor. In this case, I used the Caltech101 dataset (yes, it\u2019s old; yes, there were image datasets that predated ImageNet!), with images of 101 simple objects. As input to my LSH scheme, I used the 4096-dimensional feature-vectors obtained by passing each image through a pre-trained VGG network. To keep things simple, I assumed that the other images from the same category are true NN in the feature space.\n\nWith a \u201cground truth\u201d at hand, we can try out different hyperparameter combinations and measure their accuracy (recall) and runnning time. Plotting the results gives a nice feel for the accuracy-time trade-off:"
    },
    {
        "url": "https://towardsdatascience.com/automated-survey-processing-using-contextual-semantic-search-2dfe68338ab5",
        "title": "Automated Survey Processing using Contextual Semantic Search",
        "text": "With the recent advances in deep learning, the ability of algorithms to analyze text has improved considerably. Now analyzing digital and social media is not restricted to just basic sentiment analysis and count based metrics. Creative use of advanced artificial intelligence techniques can be an effective tool for doing in-depth research. We believe it is important to classify incoming customer conversation about a brand based on following lines:\n\nThese basic concepts when used in combination, become a very important tool for analyzing millions of brand conversations with human-level accuracy. In the post, we take the example of Uber and demonstrate how this works. Read On!\n\nSentiment analysis is the most common text classification tool that analyses an incoming message and tells whether the underlying sentiment is positive, negative or neutral. You can input a sentence of your choice and gauge the underlying sentiment by playing with the demo here.\n\nEmotion Analysis can accurately detect the emotion from any textual data. People voice their opinion, feedback, and reviews on social media, blogs, and forums. Marketers and customer support can leverage the power of Emotion Detection to read and analyze emotions attached with the textual data. Our Emotion Analysis classifier is trained on our proprietary dataset and tells whether the underlying emotion behind a message is: Happy, Sad, Angry, Fearful, Excited, Funny or Sarcastic.\n\nIntent analysis steps up the game by analyzing the user\u2019s intention behind a message and identifying whether it relates an opinion, news, marketing, complaint, suggestion, appreciation or query.\n\nNow, this is where things get really interesting. To derive actionable insights, it is important to understand what aspect of the brand is a user discussing about. For example, Amazon would want to segregate messages that related to late deliveries, billing issues, promotion related queries, product reviews etc. But how can one do that?\n\nWe introduce an intelligent smart search algorithm called Contextual Semantic Search (a.k.a. CSS). The way CSS works is that it takes thousands of messages and a concept (like Price) as input and filters all the messages that closely match with the given concept. The graphic shown below demonstrates how CSS represents a major improvement over existing methods used by the industry.\n\nA conventional approach for filtering all Price related messages is to do a keyword search on Price and other closely related words like (pricing, charge, $, paid). This method, however, is not very effective as it is almost impossible to think of all the relevant keywords and their variants that represent a particular concept. CSS on the other hand just takes the name of the concept (Price) as input and filters all the contextually similar even where the obvious variants of the concept keyword are not mentioned.\n\nFor the curious people, we would like to give a glimpse of how this works. An AI technique is used to convert every word into a specific point in the hyperspace and the distance between these points is used to identify messages where the context is similar to the concept we are exploring. A visualization of how this looks under the hood can be seen below:\n\nTime to see CSS in action and how it works on textual data below:\n\nThe algorithm classifies the messages as being contextually related to the concept called Price even though the word Price is not mentioned in the messages.\n\nWe analyzed the online conversations happening on digital media about a few product themes: Cancel, Payment, Price, Safety, and Service.\n\nFor a wide coverage of data sources, we took data from latest comments on Uber\u2019s official Facebook page, Tweets mentioning Uber and latest news articles around Uber. Here\u2019s a distribution of data points across all the channels:\n\nAnalyzing sentiments of user conversations can give you an idea about overall brand perceptions. But, to dig deeper, it is important to further classify the data with the help of Contextual Semantic Search.\n\nWe ran the Contextual Semantic Search algorithm on the same dataset, taking the aforementioned categories in an account (Cancel, Payment, Price, Safety, and Service).\n\nNoticeably, comments related to all the categories have a negative sentiment majorly, bar one. The number of positive comments related to Price has outnumbered the negative ones. To dig deeper, we analyzed the intent of these comments. Facebook being a social platform, the comments are crowded random content, news shares, marketing and promotional content and spam/junk/unrelated content. Have a look at the intent analysis on the Facebook comments:\n\nThus, we removed all such irrelevant intent categories and reproduced the result:\n\nThere is a noticeable change in the sentiment attached to each category. Especially in Pricerelated comments, where the number of positive comments has dropped from 46% to 29%.\n\nA similar analysis was done for crawled Tweets. In the initial analysis Payment and Safetyrelated Tweets had a mixed sentiment.\n\nTo understand real user opinions, complaints and suggestions, we have to again filter the unrelated Tweets(Spam, junk, marketing, news and random):\n\nThere is a remarkable reduction in the number of positive Payment related Tweets. Also, there is a significant drop in the number of positive Tweets for the category Safety(and related keywords.)\n\nBrands like Uber can rely on such insights and act upon the most critical topics. For example, Service related Tweets carried the lowest percentage of positive Tweets and highest percentage of Negative ones. Uber can thus analyze such Tweets and act upon them to improve the service quality.\n\nUnderstandably so, Safety has been the most talked about topic in the news. Interestingly, news sentiment is positive overall and individually in each category as well.\n\nWe classified news based on their popularity score as well. The popularity score is attributed to the share count of the article on different social media channels. Here\u2019s a list of top news articles:\n\nThe age of getting meaningful insights from social media data has now arrived with the advance in technology. The Uber case study gives you a glimpse of the power of Contextual Semantic Search. It\u2019s time for your organization to move beyond overall sentiment and count based metrics. Companies have been leveraging the power of data lately, but to get the deepest of the information, you have to leverage the power of AI, Deep learning and intelligent classifiers like Contextual Semantic Search.\n\nYou can also use our Excel Add-in to analyze surveys without writing a single line of code. You can download the add-in from here.\n\nWe hope you liked the article. Please Sign Up for a free ParallelDots account to start your AI journey. You can also check demo\u2019s of PrallelDots AI APIs here."
    },
    {
        "url": "https://towardsdatascience.com/b-tree-balanced-and-multi-branched-52ef308d67a",
        "title": "B-tree: balanced and multi-branched. \u2013",
        "text": "The previous article implemented the impressive red black tree, which maintained balance as the tree grew \ud83c\udf32. This article covers another balanced tree, the b-tree. To build an intuition, we will define and illustrate b-tree properties, link a web app, and code.\n\nB-trees are useful for a variety of applications, such as indexing in databases. For example, the innodb in MySQL uses the b-tree for database rows and indexes, here.\n\nThe b-tree is an ideal candidate for the following reasons: (1) keys are kept in sequential ordering, i.e.[1,2,3]. (2) There is a hierarchical indexing, look up is logarithmic. (3) B-trees can quickly add entries, (4) and the b-tree ensures balance.\n\nThe b-tree is a composite of structures, such that the generalized structure is tree shaped with blocks of nodes. In the b-tree each node has an index, contained within the block, which has a pointer to lead down a unique branch. Unlike the basic binary tree, this multi-branching is one of the many impressive features that makes the b-tree unique.\n\nFurther, a b-tree can be optimized for data by being deployed at a variable of block size."
    },
    {
        "url": "https://towardsdatascience.com/deep-learning-book-notes-chapter-3-part-1-introduction-to-probability-49d13c997f2a",
        "title": "Deep Learning Book Notes, Chapter 3 (part 1): Introduction to Probability",
        "text": "These are the first part of my notes for chapter 3 of the Deep Learning book. They can also serve as a quick intro to probability. These notes cover about half of the chapter (the part on introductory probability), a followup post will cover the rest (some more advanced probability and information theory).\n\nMy notes for chapter 2 can be found here:\n\nAs usual, this post is based on a Jupyter notebook that can be found here.\n\nPerhaps one way to describe probability is as similar to logic, but when uncertainty comes in.\n\nFor instance, logic could take the statements \u201cif something flies, then it is a bird\u201d and \u201cif something is a bird, then it is delicious\u201d and allow us to deduce that \u201cif something flies, then it is delicious\u201d, but it can\u2019t help us in cases where not all that flies is a bird or not all that is a bird is delicious. Probability can: with probability we can be given the facts \u201cIf something flies, then there\u2019s a 90% chance it\u2019s a bird\u201d and \u201cIf it\u2019s a bird, there\u2019s an 80% chance it\u2019s delicious, otherwise it definitely isn\u2019t delicious\u201d. From this we can infer that there\u2019s a 72% chance that something that flies is delicious, which we couldn\u2019t have done using logic.\n\nThere is a difference between the \u201cfrequentist\u201d and \u201cbayesian\u201d interpretations of probability\u2026 I won\u2019t go too much into that. Frankly most people typically use both approaches in different situations, which essentially boils down to saying that they accept that just about everything we talk about in the language of probabilities in our daily lives are indeed probabilities.\n\nHere\u2019s an interesting video on the topic:\n\nSo why do we need probability for deep learning? According to the book, there are two reasons:\n\nIn my opinion, there is a crucial reason that is missing, however, which is simply that logic is discrete and probability is continuous. Take this picture for example, is it a dog or a cat?\n\nI think we all agree that there is no probability involved here. This is definitely, 100% a cat, so the \u201cvery few things are certain\u201d argument above shouldn\u2019t apply.\n\nIndeed, I think it doesn\u2019t, but it is still important for our model to return a probability that something is a cat rather than a certainty, simply because probabilities are continuous and certainties are not: if our algorithm was outputing certainties and thought the picture above was a dog, it would be really hard to know how to change its parameter so that it outputs the right answer without also ruining its performance on the examples it does perform well on. By contrast, with probabilities there is a straightforward way to increase the probability that it gives to the picture being a cat, which we see in chapter 4.\n\nThe book gives 3 (or arguably 4) sources of uncertainty. I think it is possible to combine them into two:\n\nA random variable is a \u201cvariable\u201d whose value changes randomly.\n\nFrom a code perspective, this doesn\u2019t make any sense, so you can think of a random variable as a function that takes no argument and returns a random value. Here\u2019s an example of a random variable X that is randomly 1 or 2.\n\nSo when I talk about the random variable X, am I talking about the function defined using or am I talking about the return value that I get by calling , you may ask?\n\nThe answer is: it depends. The truth is that mathematical notation is sloppy like that, but in practice it will generally be straightforward to figure out whether people are talking about or .\n\nAs a final note, if the random variable is used as a number in a mathematical expression, it normally isn\u2019t re-evaluated each time, so for instance: X < 1.5 and X > 0.5 means:\n\nRandom variables can be continuous or discrete. Do not read the definition of continuous and discrete random variables, because it is wrong, and I don\u2019t mean \u201cwrong\u201d as in \u201ca bit simplified\u201d, but more \u201cwrong\u201d as in \u201cso simplified that people will mistakenly think that certain r.v.s are discrete when they are actually continuous, and that it completely misses the point of why there is a distinction in the first place\u201d.\n\nHere is a simplified definition of both that I think preserves accuracy:\n\nThe distinction is important for two reasons:\n\nA distribution is a way of describing the probabilities of the different possible outputs of a random variable. A distribution can tell us what the probability is that a random variable will output a given value or a value in a given interval.\n\nIn Python, many distributions are supported in the module. We will explain a bit about how to explore that module here.\n\nA discrete distribution has a probability mass function (PMF). It takes in a value and outputs a probability, so for instance if P is the PMF of our variable X above, you have P(1) = 0.5. Sometimes we write P(X = 1) so that we are sure that it is the PMF of X.\n\nIf you graph this function, the x axis will show the different values it can take on, while the y axis will show the probability of that value. Of course, the sum of all the y values must be equal to 1 since the sum of probabilities for a random variable must be 1.\n\nThe discrete distributions supported by scipy are found here. This allows us to create random variables using known distributions, which is more convenient than writing our own function. One thing to note is that to sample from a r.v. we now need to call the function on the distribution object. For example, let's sample from a \"binomial\" distribution:\n\nBecause the values of the discrete r.v.s are disconnected, a PMF looks \u201cspiky\u201d like this:\n\nThere can be a PMF for multiple variables, in which case we call it a \u201cjoint PMF\u201d. So the PMF P(X = x, Y = y) gives the probability that X is equal to x and Y is equal to y. Often this is the same as the probability that X is equal to x times Y is equal to y, but not always (see after).\n\nHere\u2019s what a joint PMF might look like:\n\nContinuous random variables, on the other hand, have probability density function. It is also a function of the values that the the r.v. can take, but it does not return the probability that a given value: as we\u2019ve said, the probability is 0 for every possible value.\n\nHowever, we might want to express the notion that values near 0 should be more likely than values near 2, for instance, even though both 0 and 2 have 0 probability. This is where the probability densityfunction comes in: the value of the PDF p at x (p(x)) is proportional to the probability of getting a number that is within an interval very close to x.\n\nSpecifically it is defined so that the probability of the value falling within an interval is the area under the curve of the PDF in that interval. In other words, to get the probability of the value being in an interval you need to integrate the PDF over that interval, just like if you wanted to do the same with a discrete r.v. you would need to sum the values of the PMF in that interval: integrating is basically the continuous equivalent of summing.\n\nSadly, unlike summing, integrating by hand is pretty hard. Fortunately, there are two ways to easily get integrals thanks to modern software:\n\nThe continuous distributions in scipy can be found here. Let\u2019s redefine the r.v. Y above, which as you will recall was uniformly distributed from 0 to 1:\n\nAs we recall, Y is a continuous random variable for which all values from 0 to 1 have equal probability of being chosen and all values outside have 0 probability. Let\u2019s show what this looks like in terms of its PDF:\n\nWe would expect that the probability that Y returns a number between 0.3 and 0.4 would be 0.1 since that interval covers 10% of the allowable range. Likewise, clearly the probability that the value falls between 0 and 1 must be 1, and indeed the probability that it falls between -1 and 2 must also be 1.\n\nJust like with PMFs, a PDF can relate to two different random variables. We then call it a \u201cjoint PDF\u201d. Here\u2019s an example below:\n\nWhat if you are given a joint PMF and want to know the PMF or PDF corresponding to a single variable? In other words, you know P(X=x, Y=y) and want to know just P(X=x).\n\nLogically, it seems like the probability that X = x ignoring Y should be the probability that X=x and Y=0 (or whatever the first possible value of Y is) plus the probability that X=x and Y=1 etc. for all the possible values of Y. In other words, we take the sum of P(X=x, Y=y) for all possible y. In math:\n\nThis process is indeed correct and is called marginalization. The resulting distribution of X is called the marginal distribution. Let\u2019s see what it looks like for a PMF, as a reminder, we have a set of joint probabilities in our variable:\n\nIn this case we recovered the original PMF we showed for X, because contained the probabilities for independent variables. This will not necessarily always be the case.\n\nMarginalization also works for PDFs, and again the only difference is that we need to use integration over all possible ys instead of sum, ie:\n\nLet\u2019s see what it looks like when we marginalize for X in our previous continous join PDF:\n\nPerhaps a way to view marginalization is as a form of \u201csquashing\u201d: imagine compressing the 3D pictures you saw above along the y axis, pushing the probability mass on top of itself until the 3D graph is completely flattened, leaving just the original x axis.\n\nOf course, since you can have joint distributions of more than two variables, this visualization only goes so far, but remember our quote from last time:\n\nMarginalization allows us to get the distribution of variable X ignoring variable Y from the joint distribution of X and Y, but what if we want to know the distribution of X given a specific value of Y?\n\nThis probability is called the conditional probability of X given Y, and is denoted like so:\n\nThe vertical bar is usually pronounced \u201cgiven\u201d in this case.\n\nTo give a specific example, suppose you want to know whether someone comes from Germany, and you know that they speak fluent German. You are given the following joint PMF showing the proportion of the world that speaks German and lives in Germany:\n\nHow do you know the probability that someone who speaks German is from Germany? Well it seems like you need to know how many people in the world speak German as well as how many people there are in Germany in total. Indeed, you don\u2019t need the actual count of people in either case, the proportion of the world population will suffice. In other words:\n\nThe way we get the denominator is of course using marginalization as described above, ie by summing the languages of everybody who lives in Germany.\n\nA 84.2% chance, which is not surprising of course, since Germany is such a large German speaking country.\n\nAs you might guess, the general rule is as follows:\n\nApplying this rule to all possible values of X gives us a new distribution called the conditional distribution of X given Y.\n\nSimilar to how we said marginalization can be thought of as \u201csquishing\u201d the graph of the joint distribution, taking the conditional distribution is similar to slicing the graph, and then renormalizing it so that the final distribution still sums to 1. Let\u2019s \u201cslice\u201d our 3D joint PDF from before along Y = 1.0:\n\nIt is important to not confuse a conditional probability with something that is actually actionable. In our Germany example, it is clear that teaching someone German will not make them any more likely to befrom Germany, but often this pitfall is not so obvious.\n\nWe now know how to go from a joint distribution to an marginal distribution and from a joint and marginal distribution to a conditional distribution. Let\u2019s now complete the cycle by going from a marginal and conditional distribution to a joint distribution.\n\nThe process is very simple, again. Suppose you know the probability that someone is German given that they speak German and also the probability that someone anywhere in the world speaks German and you want to know the probability that someone is both German and speaks German. It seems like you need to simply multiply the conditional and marginal probability:\n\nIndeed, this follow directly from the definitior of conditional probability above.\n\nFinally suppose you don\u2019t want to know the probability that your friend is German given than he speaks German but rather the probability that someone speaks German given that they are from Germany.\n\nOf course, we could do it with the full joint PMF table from before, but unfortunately we don\u2019t always have access to those. Suppose we know the probability from above (that someone who speaks German is from Germany) as well as the probability that someone anywhere in the world is German and the probability that someone in the world speaks German. Can we figure it out?\n\nOf course, we just need to recover the joint probability, which we did above, and then divide by the proportion of people who are from Germany.\n\nIn general, we have:\n\n(from the chain rule of conditional probability).\n\nThis last equation is famously known as Bayes\u2019 rule. It is very useful in any number of situations, though as you can see, in spite of its fancy name, it is just a straightforward application of our definitions.\n\nThe above covers only about half of chapter 3. This took a lot longer to write than I expected and I think we have reached a good natural break in the material (the rest is more of a grab-bag of various fun techniques), so I will finish my notes for chapter 3 in a subsequent post. See you soon!"
    },
    {
        "url": "https://towardsdatascience.com/learn-to-create-your-own-datasets-web-scraping-in-r-f934a31748a5",
        "title": "Learn To Create Your Own Datasets \u2014 Web Scraping in R",
        "text": "In this example I followed a two part process to get the lyrics of the most popular songs from the top 10 artists:\n\nFirst of course you will need to install and load the following packages.\n\nPART 1 : Extracting the Top 10 Pop Artists of All Time \u2014 Source: www.billboard.com\n\nPART 2 : Extracting Popular Songs and Lyrics of the top 10 Artists \u2014 Source: www.genius.com\n\nNow that you have the Top 10 Pop Artists, you can use the genius.com website to identify the most popular songs and extract their lyrics.\n\nAnd here\u2019s a snapshot of what your dataset will look like.\n\nThat\u2019s all you need to know to create your own dataset. Thus giving you endless possibilities to experiment with data you want.\n\nHope you enjoyed this tutorial and are now inspired to create your very own dataset."
    },
    {
        "url": "https://towardsdatascience.com/weekly-selection-may-4-2018-c6293c8f2e5a",
        "title": "Weekly Selection \u2014 May 4, 2018 \u2013",
        "text": "Have you ever had to answer this question at least once when you came home from work? As for me \u2014 yes, and more than once. From Netflix to Hulu, the need to build robust movie recommendation systems is extremely important given the huge demand for personalized content of modern consumers."
    },
    {
        "url": "https://towardsdatascience.com/20-apis-that-prove-what-ml-and-prediction-is-capable-of-257bb7d71ed2",
        "title": "20 APIs that prove what ML and prediction is capable of",
        "text": "Do you want to develop advanced apps and software? If so, why not to use prediction and machine learning APIs as the part of your development?\n\nIn this post, I collected 20 best APIs built on machine learning and prediction. They are divided in 4 groups by the area of implementation:\n\nThough the API has been released less than a year ago, it has firmly established itself as a leader amongst similar offerings.\n\nOne of the perks of the API is that it can tell a difference between a real face and a picture of it thanks to spoof detection. Also, TrueFace offers reusable snippets of code.\n\nYou can use Kairos both to recognise a human face and to analyze the one on the picture. The API has a pretty big set of features, such as:\n\nThe API is a good fit for developers who create apps and software that\u2019s integrated into Amazon Web Service. With Amazon Rekognition, here\u2019s what you can do:\n\nThe API is developed by Lambda Labs. When a user puts in the URL of an image, the API is able to tell whether or not there\u2019s a celebrity on the photo and who exactly is pictured, like this:\n\nExept for this feature, there are a few others:\n\nThe API is among the cheapest on the market, so if you are new to facial recongition and unsure of whether you really need the feature in the app, it\u2019s a good starting point.\n\nThis API can come in handy for someone who wants to collect insights about consumers behaviors, emotions, etc. The API not only analyzes faces but also pays attention to context \u2014 where is the person on the picture located, what objects are nearby \u2014 all this data provides a deeper understanding of a potential client, prospect, etc.\n\nGood news is, FaceRect is a completely free API for facial recognition. Still, its features are quite powerful:\n\nIBM is quite famous for implementing innovation and technological advancements in its solutions. So, no wonder that when a company of a scale this global produces a facial recognition API, it\u2019ll turn out to be extremely well-done.\n\nHere are the main features of the API:\n\nThe face detection by Skybiometry is also a solid solution that allows the detection of people with or without glasses, captures multiple faces on the photo and analyses their emotion.\n\nAlso, the API gets updated quite frequently, so new features may appear pretty soon.\n\nIn order to analyze and recognise faces more efficiently, the API works by transforming a 2D picture into a 3D model. The recognition is performed by detecting a face and comparing it to an existing set of faces until the match is found.\n\nThe API can be used for web. It also has an offline SDK for iOS and Android. It can provide facial detection and comparing even when a user\u2019s phone has no reception. That gives a broader range of possibilities to developers.\n\nSpeaking of predictibility APIs, it would be, at the very least, weird to not mention the one developed by Google. Google Cloud Prediction API can predict new trends based on the old ones. Here\u2019s what you can use it for:\n\nBig ML is an API that allows you to operate datasets, clusters, predictions, models, batches comfortably. It\u2019s an easy-to-use machine learning service that can be integrated to your app or software in a few clicks.\n\nWith this API, you can create a recommendation engine for a shopping website or software. In a few clicks, a user can increase conversion by offering the products that will be complementary to those a user has already bought.\n\nThe API provides the environment for development, prototyping, and deploying of data patterns. A user can deploy the models both localy and in the cloud. MLJAR offers a unified interface for different project and the built-in hyper-parameter search.\n\nBitext is the API that allows a user to get more insights and useful data from social media mentions, website ratings, etc. Here are the technical highlights of the API:\n\nAlso, good news is, the API is free.\n\nSwift API uses machine learning and data processing to deliver insights to retailers based on in-store transactions and online behavior of customers. Thanks to data processing, the app can predict:\n\nThe API offers a machine learning environment with 14 classifiers and NLP-empowerment. Its features include:\n\nInsideView is an ML-powered API that helps SMEs fasten sales and marketing. It aggregates user data from thousands of different sources and turn it into a consumable form.\n\nIt helps sales-managers identify the decision-maker within the company and find the right lead.\n\nThe API allows a user to turn the audio of different length to text. The dataset has over 120 languages which broadens the range of possibilities for developers.\n\nThe API has proper formatting and recognises proper nouns. Also, there are pre-built models for specific case of use.\n\nThe API\u2019s main positioning is \u201cbringinig artificial intelligence everywhere\u201d. The solution is intelligent and powerful indeed. Here are only some of its features:\n\nThose are the APIs I (and a bunch of my friends as well) am fond of. However, I\u2019m looking for new ones as well. So if you know any worthy data-driven APIs or produce any), mention them in comments \u2014 I\u2019ll add them to the list as well.\n\nAre there any other things you\u2019d like me to cover? If so, go on with suggestions.\n\nThat is a wrap. Thanks for stopping by."
    },
    {
        "url": "https://towardsdatascience.com/is-deep-learning-without-programming-possible-be1312df9b4a",
        "title": "Is Deep Learning without Programming Possible ? \u2013",
        "text": "Alright, now after introducing you all to the Deep Learning Studio, it\u2019s time to demonstrate how this Software platform actually work. As being said \u201cYou should practice what you preach\u201d so with the same intent Mandeep Kumar, the CEO and Co-Founder at Deep Cognition, Inc. gave a practical exhibition of using Deep Learning Studio to apply 3D convolutional Neural Network over a CT scan dataset through a Kaggle post, the step by step implementation of which is presented below.\n\nSign up and get access to Deep Learning Studio at\n\n[Super Important Point: once you\u2019ll register for a free account on Deep Cognition Website you\u2019ll receive 2 hours of free NVIDIA GPU training time too, now this is what I call extremely fascinating.]\n\nThen he Enable cached dataset in his account by uploading two small files that he downloaded from his Kaggle account, uploading of these files insured that he had access to Kaggle dataset (Follow markers 1 to 4)\n\nThen, he built a new project by going to project menu on left and clicking on + button.\n\nhe gave a name and description to his project, and then opened the project by clicking on box+arrow icon on project bar.\n\nHe did training with 1200 samples and he used 197 samples for validation for this example.\n\nAfter selecting dataset he clicked on \u201cModel\u201d Tab and started building model as shown below by dragging layers from left menu bar to the canvas and connecting these layer blocks.\n\nThe code block shown below, reflects the actual generated source code for the entire model which was built by Mandeep Kumar using Deep Learning Studio and can be seen in the Figure 5.\n\nThen he clicked on \u201cHyperparameters\u201d tab and made sure batch size is set to 1. This is important because anything bigger will not fit GPUs memory and training will fail.\n\nFinally he moved to \u201cTraining\u201d tab. Selected GPU-K80 as instance and click on \u201cStart Instance\u201d. Once Instance started, he clicked on \u201cStart Training\u201d. Note that training is going to be very slow because of sheer size of dataset and computations needed.\n\nAfter trying out 2 epochs he was able to get loss of about 0.58 on validation set.\n\nBefore making a model, he also did some Pre-processing on the CT scan dataset, so the intuition about the model he built and full information about the Pre-processing steps he took can be found here.\n\nSo, this mark the end of my journey in making you aware of this new Software platform in the market which simplifies and accelerates the process of Deep Learning through a Drag and Drop GUI and allow you all to design, train and deploy Deep Learning model with no coding involved. But don\u2019t forget to have a look over the Reference section which is filled with other exciting resources for you to cherish.\n\nThank you for your attention\n\nYou using your time to read my work means the world to me. I fully mean that.\n\nIf you liked this story, go crazy with the applaud( \ud83d\udc4f) button! It will help other people to find my work.\n\nAlso, follow me on Medium , Linkedin if you want to! I would love that."
    },
    {
        "url": "https://towardsdatascience.com/ai-applications-evolving-challenges-and-opportunities-for-procurement-982f8acb0674",
        "title": "AI Applications: Evolving Challenges and Opportunities for Procurement",
        "text": "There is no debate that application of Artificial Intelligence within Procurement is the need of the hour. It\u2019s also well known that Procurement functions in most organizations are still coming up the curve of data science and AI adoption. Unfortunately, time is running out as the backdrop for Procurement is evolving rapidly. Procurement leaders are at a cusp where it is imperative that they quickly make the leap towards embracing cutting-edge ideas, tools and technologies (many of them grounded in data science and AI) in order to stay relevant.\n\nIn today\u2019s world, Procurement is faced with unprecedented changes, driven by rapid transformations in the business landscape.\n\n(1) Increasing Complexity: The pace at which businesses have been transforming over the last decade is mind-boggling. It isn\u2019t short of the kind of changes that happened during the Industrial Revolution. Procurement has not been left untouched by these changes.\n\n(2) Changing View of Vendor Risks: There was a time when Procurement professionals could neatly quantify and measure risks associated with any vendor and build that into their sourcing strategies. With the advent of technology and globalization, the very definition of \u2018risk\u2019 is going through a metamorphosis. Geo-political risks no longer confine themselves to one country \u2014 a problem in India could well affect your vendor in Philippines. Similarly, currency risks are correlated. Technology is rendering businesses obsolete at a pace that necessitates tracking technology-led risks. The list goes on!\n\n(3) Pace of Internal Changes: Organizations today need to be very agile to ensure they remain viable within the context of the technology revolution. They devise new ways of doing business. They enter new markets. They launch new types of products or services. They restructure and reorganize themselves often. They participate in more and more M&A activity. And all this happens at a dizzying speed. Imagine then the consternation of Procurement functions that don\u2019t just need to keep up with these changes, but they need to do it in a sustainable and scalable manner!\n\n(4) Laser Focus on Cost Savings: With several cycles of economic ups and downs behind us in the last decade or so, businesses have started becoming much more hawkish and diligent about their costs. The obsession with focusing on revenue growth alone, has been decidedly replaced by acknowledging the need for being aggressive when it comes to cost controls as well. And this puts Procurement right up there on center-stage as a strategic cog-in-the-wheel for companies \u2014 they have their work cut out with very stringent savings targets."
    },
    {
        "url": "https://towardsdatascience.com/the-data-science-thought-process-df386ee7930a",
        "title": "The Data Science Thought Process \u2013",
        "text": "In my last post on preparing your data science resume and project portfolio, I was discussing about showcasing the thought process that one has, because besides the maths & statistics background, that to me is very important in assessing if one is suitable to be trained further or accepted into a data scientist role.\n\nIn a certain angle, data scientist most valuable skill is to use data to provide business value. It could be providing relevant insights to solve business challenges or reach business objectives or using machine learning models for better business decision making etc. To sum it all up, data scientist is a solution provider using organization\u2019s data as raw materials. And to be an effective solution provider, the thought process, seeking and bringing different resources in is very important, in my opinion (of course there is the execution part but that might be another discussion).\n\nSo I thought I will provide some tips on strengthening that thought process so that aspiring and current data scientist can provide more value to their employers and lead to better rewards (hopefully).\n\nI am an avid reader (or at least I think I am). Technical papers, especially those found in arxiv, are definitely part of the reading diet but I also read books on varied subjects such as sociology, psychology, economics, leaderships, biographies etc. I find that reading widely and drawing relations between the different knowledge and fields, helps to strengthen the thought process.\n\nThrough reading widely, some of the knowledge gained can be used in projects, such as understand certain nuances found in the data during EDA. It also helps to create certain hypothesis (which needs to be tested further), that can help in structuring a workable business strategy.\n\nReading widely, helps to spark off new ideas (INNOVATION!) on solving business challenges as well, putting ideas from different books together to create new synergies!\n\nThe main idea is to gain and relate the knowledge through reading and bring them into the thought process.\n\nI like to read up on Bloomberg Business Week, The Economist or similar periodicals. These are very good places to understand what is going on in the different industries, such as pharmaceutical, telcos, technology, banking and for countries, understanding the political and economics climate. It can be a story on how the major trends are exerting changes in the pharmaceutical industry for instances changes in the patent duration or how FRS 39/Basel III is affecting the banking industry.\n\nAll these help the data scientist, especially if they are working in the related industry, to be prepared in how they should work on the data and machine learning models, taking into account the major trends affecting the industry.\n\nHow does networking help in strengthening the thought process? Well, it is to gather from many people, how they solved their employer\u2019s business challenges. Understand what were the technical, data and organization-specific challenges when tackling the project and also if possible, understand how these challenges are tackled.\n\nAlso try to understand how they conduct their Exploratory Data Analysis and incorporate their best practices into your own.\n\nVery seldom, one can work on a project without any hiccups. So it is best to \u201clearn\u201d from other people\u2019s experience, be prepared for it or even take precautions.\n\nAgain the main idea here is to learn and have a broader view of how other people tackle their challenges and because you never know when you might come across something similar and you can adapt the solution to your own projects.\n\nThe broad idea is to learn from as many people, blogs (Medium, of course!), periodicals, books and websites on how to tackle different challenges. Working on a data science project involves many areas, components and teams thus challenges can come from anywhere, it is most important to be prepared for it so that we can solve them as they come and be able to continuously provide value through the organization\u2019s data.\n\nI wish all readers all the best in the data science journey! Keep learning!\n\nDo visit my other blog posts and LinkedIn profile."
    },
    {
        "url": "https://towardsdatascience.com/how-to-make-interesting-tableau-dashboards-using-gifs-182eab5fe354",
        "title": "How to Make Interesting Tableau Dashboards using GIFs",
        "text": "Visualizations can have a huge impact on how we perceive different things, especially data. Having data arranged in a structured manner in a tabular format is far better than having unstructured data; having a graphical representation of data in the form of charts is again better than having data in the form of tables. As the visualization improves, so does our understanding of the data and the context in which it is presented.\n\nLet\u2019s take an example and validate the above statement ourselves.\n\nTry to read the above table and understand what it is saying.\n\nOkay, let me help you with that.\n\nSo, the table provides the year-on-year trend of change in internet and mobile phone usage per capita. The overall trend has been positive and per capita usage for internet and mobile phone has increased over the years. However, to go a level further and understand the relative increase over the years, we will have to do some calculation and get some numbers prior to the analysis. However, if we were to present the above numbers in a better visual format, let\u2019s say a bar chart or a line chart, then it may aid our purpose. We will be able to see the overall movement of these numbers over the years and, further see the relative increase over the years. Let\u2019s have a look at the below chart and check out the difference.\n\nThe bar chart provides us with per capita mobile phone usage data and the line chart provides us with per capita internet usage data. The visual appeal of above chart is, I wouldn\u2019t shy away from saying, infinite times better than the plain numbers in a table. The above chart provides us with multiple insights, some of them are highlighted below:\n\nThere can be many other insights that can be derived from the above chart. That\u2019s the power of data visualization. The more interactive and dynamic it is, the better it is for users to derive insights from it.\n\nThe above two charts were created in Tableau, which is one of the most advanced data visualization tool available in the market. Tableau helps us in creating powerful interactive visualizations with simple drag-and-drop approach. Using Tableau, we will try to take our visualizations to the next level by converting static charts into more interactive forms, i.e., by converting them into GIFs. With a simple play button, user can see all the bar charts and line charts moving according to timeline. We will take a couple of examples and understand how to create GIFs in Tableau.\n\nWe will use the standard Tableau World Indicators in-built dashboard as an example. First, we will use the Internet and Mobile phone usage per capita data and understand how to create a GIF and see the impact it provides. We have seen the same data in two different forms in Figure 1 and Figure 2. To present the chart in a GIF format, first follow standard instructions to create a chart in the format you want. Standard chart may look like the one below.\n\nNow, to convert the standard image to GIF, all you have to do it move the Year field to Pages shelf in the top left corner. As soon as this is done, you will see a pages card in the right which will have a \u2018play\u2019 button. Just click on the play button and you would have made a GIF. Look at the pages card in the image below.\n\nSince, Tableau in its current version does not provide the flexibility to download GIFs, you will have to use third party tools which allows you to record the screen.\n\nLet\u2019s now break the above image into three sections (at different year points) and get an idea on how the entire GIF evolves.\n\nAbove image is the first step in the GIF (Year = 2000). Let\u2019s see second step at Year = 2005.\n\nNext, let\u2019s look at the image when Year = 2011.\n\nWhen you hit the play button, the image will move one year at a time. Using the pages card at the right side, you may choose different options such as speed of transition and specific year. If you click on the small arrow next to \u2018Show History\u2019, a pop-up will appear as shown below. Using this pop-up, you can change the coloring, fading of bars etc.\n\nAlso, if you don\u2019t want to keep the previous bars as the chart moves along the years, you may choose by selecting \u2018Highlighted\u2019 options instead of \u2018All\u2019 and the result will be as shown below.\n\nNow, let\u2019s look at another example to see how GIFs can be helpful in providing a trend of indicators over time on region maps. We will use the Tableau\u2019s in-built dashboard \u2014 World Indicators, but this time we will use a different dataset \u2014 Health Indicators.\n\nWe will first create a static region heat map for Average Birth Rate for different countries across Africa. The chart looks like below:\n\nNow, move the Year field to Pages pane and hit the play button and observe the change in color for different countries. As you will observe, the color of different countries changes as the Average Birth Rate changes. Notable difference comes for Algeria.\n\nIn the year 2000, Algeria is golden red in color which shows the Average Birth Rate is closer to 1.1% end (look at the gradient scale in the above image); while in the year 2012, the color for Algeria changes to light green showing the improvement in the Average Birth Rate spread across the years.\n\nThe Tableau sheets can then be moved to any Tableau dashboard and be used in presentations. This is a very powerful feature that Tableau provides and it can help us present data in a very appealing and interactive manner.\n\nEven though GIFs can be very useful in creating appealing visualizations, care should be taken while using them. They should mainly be used when you want to present a changing trend over a period, for example: change in GDP for a country over the years. Another example could be if you want to compare one indicator between two or more countries/regions and see the changing trend over the years. There can be many more areas where we can see the application of such GIFs, can you think of any? We would be happy to hear your views and experiences while creating GIFs in Tableau.\n\nThis article was contributed by Perceptive Analytics. Saneesh Veetil, Sinna Muthiah Meiyappan and Chaitanya Sagar contributed to this article.\n\nPerceptive Analytics provides Tableau Consulting, data analytics, business intelligence and reporting services to e-commerce, retail, healthcare and pharmaceutical industries. Our client roster includes Fortune 500 and NYSE listed companies in the USA and India."
    },
    {
        "url": "https://towardsdatascience.com/conditional-random-field-tutorial-in-pytorch-ca0d04499463",
        "title": "Conditional Random Field Tutorial in PyTorch \ud83d\udd25 \u2013",
        "text": "Let\u2019s say I have two identical dice, but one is fair and the other is loaded such that number 6 appears with 80% probability, while numbers 1\u20135 are equally likely with 4% probability. If I gave you a sequence of 15 rolls, could you predict which dice I used for each roll? A simple model would be to predict I used the biased dice whenever a 6 comes up and say I used the fair dice for all other numbers. In fact, if I was equally likely to use either dice for any roll, then this simple rule is the best you could do. But what if after using the fair dice, I have an 90% chance of using the biased dice on the next roll? If the next roll is a 3, your model will predict I used the fair dice when the biased dice is the more probable choice. We can verify this with Bayes\u2019 theorem:\n\nThe takeaway is that making the most likely choice at each time step is only a viable strategy when I\u2019m equally likely to use either dice. In the far more likely scenario that previous choices of dice affect what my future choices will be, you\u2019re going to have to take the interdependence of the rolls into account in order to be successful. A Conditional Random Field* (CRF) is a standard model for predicting the most likely sequence of labels that correspond to a sequence of inputs. There are plenty of tutorials on CRFs but the ones I\u2019ve seen fall into one of two camps: 1) all theory without showing how to implement or 2) code for a complex machine learning problem with little explanation of what\u2019s going on. I don\u2019t fault those authors for picking either theory or code. CRF\u2019s are a deep topic in a broader and deeper subject called probabilistic graphical models so covering theory and implementation in depth will take a book, not a blog post, but this makes learning about CRFs harder than it needs to be. My goal for this tutorial is to cover just enough theory so that you can dive into the resources in category 1 with an idea of what to expect and to show how to implement a CRF on a simple problem you can replicate on your own laptop. This will hopefully equip you with the intuition needed to adapt this simple toy CRF for a more complicated problem. Our theoretical discussion will be divided into three parts: 1) specifying model parameters, 2) how to estimate these parameters, and 3) using these parameters to make predictions. These three broad categories apply to any statistical model, even a simple logistic regression, so in that sense CRFs aren\u2019t anything special. But that doesn\u2019t mean that CRFs are as simple as logistic regression models. We\u2019ll see that things will get a bit more complicated once we tackle the fact we\u2019re making a sequence of predictions as opposed to a single prediction. In this simple problem, the only parameters we need to worry about are the costs associated with transitioning from one dice to the next in consecutive rolls. There are six numbers that we need to worry about and we\u2019ll store them in a 2x3 matrix called the transition matrix: The first column corresponds to transitions from the fair dice in the previous roll to the fair dice (value in row 1) and biased dice (value in row 2) in the current roll. So the first entry in the first column encodes the cost of predicting that I use the fair dice on the next roll given that I used the fair dice on the current roll. If the data show that I\u2019m unlikely to use fair dice in consecutive rolls, the model will learn this cost should be high and vice versa. The same logic applies to the second column. The second and third columns of the matrix assume we know which dice we used in the previous roll. Therefore we have to treat the first roll as a special case. We\u2019ll store the corresponding costs in the third column. Let\u2019s say I give you a set of rolls X and their corresponding dice labels Y. We will find the transition matrix T that minimizes the negative log likelihood over the training data. I\u2019ll show you what the likelihood and negative log likelihood look like for a single sequence of dice rolls. To get it for the entire data set, you\u2019d average over all the sequences.\n\nP(x_i | y_i) is the probability of observing a given dice roll given the current dice label. To give an example, P(x_i | y_i) = 1/6 if y_i = dice is fair. The other term, T(y_i | y_{i-1}), is the cost of having transitioned from the previous dice label to the current one. We can just read this cost off the transition matrix. Notice how in the denominator we\u2019re computing the sum over all possible sequences of labels y`. In a traditional logistic regression for a classification problem of two classes, we\u2019d have two terms in the denominator. But now we\u2019re dealing with sequences and for a sequence of length 15, there are a total of 2\u00b9\u2075 possible sequences of labels so the number of terms in the denominator is huge. The \u201csecret sauce\u201d of the CRF is that it exploits how the current dice label only depends on the previous one to compute that huge sum efficiently. This secret sauce algorithm is called the forward-backward algorithm*. Covering this in depth is out of the scope for this blog post but I\u2019ll point you to helpful resources below. Once we estimate our transition matrix, we can use it to find the most likely sequence of dice labels for a given sequence of dice rolls. The naive way to do this is to compute the likelihood for all possible sequences but this will be intractable for even sequences of moderate length. Just like we did for parameter estimation, we\u2019ll have to use a special algorithm to find the most likely sequence efficiently. This algorithm is closely related to the forward-backward algorithm and it\u2019s called the Viterbi algorithm. PyTorch is a deep learning library in Python built for training deep learning models. Although we\u2019re not doing deep learning, PyTorch\u2019s automatic differentiation library will help us train our CRF model via gradient descent without us having to compute any gradients by hand. This will save us a lot of work. Using PyTorch will force us to implement the forward part of the forward-backward algorithm and the Viterbi algorithms, which is more instructive than using a specialized CRF python package. Let\u2019s start by envisioning what the result needs to look like. We need a method for computing the log likelihood for an arbitrary sequence of rolls, given the dice labels. Here is one way it could look: This method does three main things: 1) maps the value on the dice to a likelihood, 2) computes the numerator of the log likelihood term, 3) computes the denominator of the log likelihood term. Let\u2019s first tackle the _data_to_likelihood method, which will help us do step 1. What we\u2019ll do is we\u2019ll create a matrix of dimension 6 x 2 where the first column is the likelihood of rolls 1\u20136 for the fair dice, and the second column is the likelihood of rolls 1\u20136 for the biased dice. This is what this matrix looks like for our problem: Now, if we see a roll of 4, we can just select the fourth row of the matrix. The first entry of that vector is the likelihood of a four for the fair dice (log(1/6)) and the second entry is the likelihood of a four for the biased dice (log(0.04)). This is what the code looks like: Next, we\u2019ll write the methods to compute the numerator and denominator of the log likelihood. That\u2019s it! We have all the code we need to start learning our transition matrix. But if we want to make predictions after training our model, we\u2019ll have to code the Viterbi algorithm: There\u2019s more to our implementation but I\u2019ve only included the big functions we discussed in the theory section. I evaluated the model on some data I simulated using the following probabilities: Check out the notebook I made to see how I generated the model and trained the CRF. The first thing we\u2019ll do is look at what the estimated transition matrix looks like. The model learned that I am more likely to roll the fair dice on the current roll if I used the fair dice on the previous roll (-1.38 < -0.87). The model also learned that I am more likely to use the fair dice after using the biased dice, but not by a lot (-0.59 < -0.41). The model assigns equal cost to both dice in the first roll (-0.51 ~ -0.54). Next, we\u2019ll see what the predictions looks like for a particular sequence of rolls: The model recognizes long sequences of 6\u2019s (these are the 5\u2019s since we\u2019re starting from 0) as coming from the biased dice, which makes sense. Notice that the model doesn\u2019t assign every 6 to the biased dice, though (eighth roll). This is because prior to that 6, we\u2019re pretty confident we\u2019re at the fair dice (we rolled a 2) and transitioning to the biased dice from the fair dice is less likely. I\u2019m ok with that mistake \u2014 I\u2019d say our model is successful! I\u2019ve shown you a little bit of the theory behind CRF\u2019s as well as how one can be implemented for a simple problem. There\u2019s certainly a lot more to them than I\u2019ve been able to cover here, so I encourage you to check out the sources I\u2019ve linked below. An Introduction to Conditional Random Fields: Overview of CRFs, Hidden Markov Models, as well as derivation of forward-backward and Viterbi algorithms. Using CRFs for named entity recognition in PyTorch: Inspiration for this post. Shows how a CRF can be applied to a more complex application in NLP. *To be precise, we\u2019re covering a linear-chain CRF, which is a special case of the CRF in which the sequences of inputs and outputs are arranged in a linear sequence. Like I said before, this topic is deep. *Since we\u2019re using PyTorch to compute gradients for us, we technically only need the forward part of the forward-backward algorithm ."
    },
    {
        "url": "https://towardsdatascience.com/extended-kalman-filter-ee9bd04ac5dc",
        "title": "Extended Kalman Filter \u2013",
        "text": "In my previous blog i have covered Kalman Filter . In this blog i will discuss on Extended filter and will see how it solves the problem of Kalman Filter.\n\nThis blog is a continuation of my previous blog on Kalman Filter, so if you have not read it kindly read it over here.\n\nExtended Kalman filter was introduce to solve the problem of non-linearity in Kalman filter . In real life there may be a lot of scenarios where the system may look in one direction and may take the measurement from another direction. This involved angles to solve these problems, resulting in non linear function which when fed to a Gaussian resulted in a non-Gaussian distribution. And we cannot apply Kalman filter on non-Gaussian distribution as it is senseless to compute the mean and variance of a non-Gaussian function.\n\nHow does it work ?\n\nExtended Kalman Filter makes the non linear function into linear function using Taylor Series , it helps in getting the linear approximation of a non linear function.\n\nIn mathematics, a Taylor series is a representation of a function as an infinite sum of terms that are calculated from the values of the function\u2019s derivatives at a single point. In simple terms we can say that in taylor series we take a point and perform bunch of derivative on it.\n\nWhen we perform a Taylor series for a polynomial then the result is also a polynomial therefore what EKF does is that it first evaluate the non-linear function at a mean which is the best approximate of the distribution and then estimate a line whose slope is around that mean .This slope is determined by the first order derivative of the Taylor expansion as the first order derivative gives a linear value. Therefore this method of getting the slope from the first derivative of the Taylor series is know as First order Taylor expansion.\n\nTaylor series is given by\n\nThe general formula for Taylor series equation of a non linear function f(x) at mean(\u03bc) is given by :\n\nSteps to follow to get the Taylor Expansion:\n\nLets try to understand this with an example:\n\nSuppose we have to find Taylor expansion of equation sin(x) at \u03bc = 0\n\nSo following the steps discussed above:\n\nWhen we talk about the effect of the EKF on the equations of KF its necessary to know the source of data . i.e for in case of Autonomous Vehicle we need to know whether the data on which we have to perform prediction, is coming from Lidar or Radar.\n\nSince a LIDAR provides us the distance in the form of Cartesian coordinate system and a RADAR provides us the distance and velocity in Polar coordinate system. The measurement vector for both of them is as follows:\n\nwhere px: x coordinate of object in Cartesian System\n\n\u03c1 : distance of the object from vehicle\n\nx-axis determines the location in which car is moving.\n\nTill now you might have understood that data provided by LIDAR are linear in nature while that of the radar are non linear . So the equations for the Lidar data will remain the same as that of Kalman Filter but for the Radar measurement it will be as follows:\n\nIt will remain same as that of KF irrespective of the device .\n\nh(x\u2032) : mapping of predicted Cartesian coordinate system to polar coordinate system. Here x\u2032 is the predicted values in the Cartesian system.\n\ny: Difference between the measured value and actual measurement.\n\nThese noise determine the correction parameter for the device. It is provided by the manufacturer of the device and remains constant throughout the cycle.\n\nAs we have already seen that to convert the non-linear function to linear we use the Taylor series expansion and takes it first derivative. This first order derivative is given by Jacobian Matrix as we are dealing with matrix in the equations.\n\nSince the state vector for radar consists of px,py,vx and vy .\n\nBy applying the derivative we will get the final value of Hj as follow:\n\nWe calculate the Kalman Gain by the simple formula i have shown in my blog on Kalman Filter . The only thing is that in matrix we cannot divide so here we are taking S\u207b\u00b9 .\n\nFinally we update are state vector and the covariance matrix and will move on to next prediction step to get the other values.\n\nIn extended Kalman filter the approximation was done based on a single point i.e means . This approximation may not be the best possible approximation and lead to poor performance. You can understand it by the diagram shown below:\n\nThis problem of extended Kalman filter was solve using Unscented Kalman Filter"
    },
    {
        "url": "https://towardsdatascience.com/machine-learning-and-the-law-of-falling-apples-f6a9cfc06ac3",
        "title": "Machine learning and the law of falling apples \u2013",
        "text": "Disclaimer: The purpose of this article is not to disparage machine learning in any shape or form. Machine learning is lovely, I make my living off it! The point is simply to explore the edges and try to see what lies beyond.\n\nImagine a young Isaac Newton sitting under a tree when he notices an apple fall. He thinks about it for a moment and realizes that he has never really seen an apple do anything else but fall straight down. They never go upwards or sideways.\n\nNow, had Newton known about machine learning and had the actual machines to do the learning, then this is how he might have gone about it. First, he could have set up a classification problem with three class labels \u201cdown\u201d, \u201cup\u201d and \u201csideways\u201d. Then he would collect data on the direction of falling apples. He would have noticed his dataset to be highly imbalanced. But, undaunted, he would have marshalled on and trained his classifier. If his classifier were any good it would predict \u201cdown\u201d as the direction of fall in most cases.\n\nHad he been even more enterprising he would have noticed that the time it takes for the apple to fall to the ground is larger for taller trees. To come up with a better model he would have measured the height of every apple tree he could find. And then he would stand under each one of them waiting for an apple to fall. In each case he would record the time it took for the apple to fall to the ground. After doing some exploratory data analysis he would have realized that he would be able to fit a better linear regression model if he used the square root of the height of the tree as a feature. Finally he would fit this linear regression model and gotten a very good fit.\n\nArmed with all these insights he would have formulated the \u201cLaw of falling apples\u201d: Apples almost always fall straight down and the time it takes for them to fall to the ground is approximately proportional to the square root of the height of the tree.\n\nThankfully for everyone involved, Newton was completely oblivious to machine learning. Instead, he went about it the old fashioned way. He thought hard about the issue and came to the conclusion that apples falling straight down is a manifestation of a deeper principle. This deep underlying principle affects not only apples falling from trees, but everything around us. It equally affects the earth and and the heavenly bodies. It affects everything in the universe. Newton formulated the law of universal gravitation.\n\nThe story of Newton formulating the law of gravitation after seeing an apple fall is probably apocryphal. It is, however, a very good illustration of what really makes science so powerful \u2014 its ability to generalize, the ability to find universal truths from limited data. At its core, scientific inquiry is predicated on a set of foundational conjectures regarding the nature of the universe. To a large extent machine learning derives its empirical methodology with science, while replacing human ingenuity, whenever possible, with computational muscle. But how far does this similarity go? To answer this question, let us play the game of analogies.\n\nThe fundamental conjecture of science is that there is order in the universe waiting to be discovered. Although this might sound trivial, without this core belief no scientific research is possible. In the case of science we do not stop to consider the importance of this conjecture because it has been validated over and over again. We simply take it for granted.\n\nBut, what about machine learning? Well, machine learning does not concern itself with the fate of the whole universe, but with data. Machine learning is effectively the art of of function approximation via inductive generalization, i.e., clever ways of \u201cguessing\u201d the form of a function based on data samples. The above statement is manifestly true for supervised learning. With a little thought and elaboration, it can also be seen to be true for reinforcement learning and unsupervised learning. (In the interest of simplicity, I will stay close to the language of supervised learning in the rest of the post).\n\nIn order to guess a function, one needs to assume that a function exists in the first place and a function is nothing but a codification of regularities. Thus, the first fundamental conjecture of machine learning is: it is very likely that observed data will contain regularities waiting to be discovered.\n\nOr in other words, given an input X and an output Y, there exists a function F such that\n\nUnlike science, the first conjecture of machine learning is not a given, but rather it needs to be validated on each and every data sample. If found to be untrue then machine learning is not of much use for that dataset.\n\nRegularities are useful because they help predict the unknown from the known. But in order to do so one needs to be able to express them in a language that is powerful enough. In the physical sciences, this language is that of mathematics. The key conjecture being that mathematics provides a sufficient basis for expressing and exploiting the regularities in physical phenomena. Once again, this might look like a trivial observation, but it is far from it. Without its validity much of the grand edifice on which most of modern science and technology rests on will come crashing down.\n\nThe language of machine learning is also a mathematical one, albeit of somewhat narrower scope. The underlying mathematical machinery behind machine learning is that of piecewise differentiable functions in vector spaces (roughly speaking, calculus and linear algebra). There are two very special properties of this machinery. First, it is possible to define the concept of \u201ccloseness\u201d and consequently that of a \u201cchange\u201d in a concrete manner in a vector space (by defining a distance). Second, for piecewise differentiable functions small changes lead to small effects. Together, these two properties are ultimately responsible for the enormous power of machine learning; its ability to generalize beyond observed data.\n\nTherefore, in order to successfully apply machine learning to any dataset we should be able to transform the data to a form that is amenable to its underlying machinery,\n\nwhere I and O are transformations to and from the original representation to one where the machinery can be applied (the feature space representation), and G is the function or the model that is built using the machinery in the feature space representation.\n\nThe properties mentioned above that make the feature space representation enormously powerful, also make it incredibly restricted. Not every dataset should be expected to have an appropriate feature space representation. However, most do, leading to the second fundamental conjecture of machine learning: if the observed data shows regularities then it is very likely that there exists a representation of the data where small changes give rise to small effects.\n\nThe act of transforming raw data into the feature space representation is called feature engineering. According to Andrew Ng \u2014 Coming up with features is dif\ufb01cult, time consuming, requires expert knowledge. \u201cApplied machine learning\u201d is basically feature engineering. The success of a machine learning task is severely dependent on being able to find the right transformations I and O. Very often they are lovingly handcrafted using a combination of deep domain knowledge and arcane witchcraft!\n\nDeep learning seeks to relieve this burden somewhat by making the process of feature engineering partially automated. Essentially, in deep learning, the transformations I and O are performed by the first and the last few layers of the deep neural network. Thus the mundane drudgery of nonlinear transformations is outsourced to machines while reserving human ingenuity for more impactful insights.\n\nWhile we are playing the game of analogies, we are bound to notice that in science there is one final fundamental conjecture. It is the conjecture that universal truths exist and that different phenomena are simply manifestations of those universal truths. It is this conjecture that allows science to generalize from a narrow set of observations to universal laws spanning a multitude of phenomena. To be clear, this conjecture alone does not automatically manifest those universal laws. One needs the genius of Newton to deduce the law of universal gravitation from observing falling apples. But, in the end, it is this conjecture that provides the basis for making those leaps of intuition, elevating science from being an exercise in stamp collecting to the engine of progress and enlightenment.\n\nIs is possible to make an analogous conjecture in machine learning? Certainly, machine learning does not have any grand designs of discovering universal truths. However, it can, and it must, have the ambition to break free from narrow domain walls. For sure, being able to identify cats in pictures after sifting through millions of pictures with cats, is useful. However, what would be much more useful is if one could use this data to draw some conclusions about how pictures are composed in general. Or, even better, if one could say something about the intentions or emotions of the photographers behind the pictures.\n\nNotice that this is a different kind of generalization. It is not the kind of generalization that necessarily aims to to be universal. But rather, it is the kind that is transferable. Transferable across domains \u2014 from the domain of cat pictures to the domain of visual composition or the domain of human emotion. But how do we find such transferable generalizations?\n\nWhat if the the feature space representations were not merely computational crutches, but encoded something deeper? What if the models in these representation (G) were not merely operational tools to connect inputs to outputs in this specific domain, but actually revealed underlying structural regularities spanning multiple domains?\n\nAs it turns out, these \u201cwhat ifs\u201d are not mere wishful thinking. There exists many situations where the observable data do have this feature of transferable generality. This vital observation underpins the fundamental premise of transfer learning. Thus, the third fundamental conjecture of machine learning is : (transfer learning) there exist situations where the observed data are manifestations of underlying (possibly probabilistic and approximate) laws.\n\nAs with the previous cases, conjecture alone is not sufficient to make progress. There are many questions that are yet unanswered. Which situations are amenable to transfer learning? How does one know if one has split F correctly between I, G and O? After all they are only unique up to a transformation. Is deep learning the only technique that can benefit from transfer learning?\n\nWe are only beginning to appreciate the potential of transfer learning in taking machine learning to the next frontier \u2014 cross domain generalization. According to Andrew Ng transfer learning will be the next driver of machine learning success. Such optimism is very well founded. Transfer learning provides machine learning with that elusive bridge to go from falling apples to the law of gravitation."
    },
    {
        "url": "https://towardsdatascience.com/up-and-running-with-pytorch-minibatching-dataloading-and-model-building-7c3fdacaca40",
        "title": "Up and running with PyTorch \u2014 minibatching, dataloading and model building",
        "text": "I have now experimented with several deep learning frameworks \u2014 TensorFlow, Keras, MxNet \u2014 but, PyTorch has recently become my tool of choice. This isn\u2019t because I think it is objectively better than other frameworks, but more that it feels pythonic, intuitive, and better suited to my style of learning and experimenting.\n\nThis post provides a tour around PyTorch with a specific focus on building a simple neural network (multilayer perceptron) to separate (i.e. classify) two classes in some toy data. My goal is to introduce some of PyTorch\u2019s basic building blocks, whilst also highlighting how deep learning can be used to learn non-linear functions. All of the code for this post is this github repo. This is more of a practical post, if you are looking for a tour of the inner workings of PyTorch I strongly recommend this post.\n\nTo follow along make sure you have PyTorch installed on your machine. Note that I am using version 0.3.1.post2 The next version of pytorch (0.4) will introduce some breaking changes (read about them here). Also worth keeping an eye out for the release of PyTorch 1.0, which aims to be \u201cproduction ready\u201d \u2014 I\u2019m very excited for this!.\n\nThe learning task for this post will be a binary classification problem \u2014 classifying points in half moon shapes. This is a simple task for a deep learning model, but it serves to highlight their ability to learn complex, non-linear functions.\n\nFor example, if we use a logistic regression to classify this data look what happens:\n\nDespite applying a softmax transformation to the predicted outputs (squeezing predicted output logits to sum to 1), the logistic regression is linear in its parameters and, therefore, struggles to learn non-linear relationships. We could use a more advanced ML model for this task, such as a random forest, but then we wouldn\u2019t have an excuse to play around with a neural network!\n\nBefore building the model, we will first create a custom data pre-processor and loader. In this example, the transformer will simply transform X and y from numpy arrays to torch tensors. We will then use the dataloader class to handle how data is passed through the model.\n\nIn this instance we will set-up a mini-batch routine. This means that during each epoch the model will train on small subsets (batches) of the data \u2014 that is, it will update its weights with respect to the loss associated with each mini-batch. This is generally a better approach than training on the full dataset each epoch. It is also advisable to use smaller batches \u2014 though this is a hyper parameter so do experiment!\n\nThe standard approach to defining a deep learning model with PyTorch is to encapsulate the network in a class. I quite like this approach because it ensures that all layers, data, and methods are accessible from a single object. The purpose of the class is to define the architecture of the network and to manage the forward pass of the data through it.\n\nThe typical approach is to define layers as variables. In this case we define a single layer network. The nn.Linear function requires input and output size. In the first layer input size is the number the features in the input data which in our contrived example is two, out features is the number of neurons the hidden layer.\n\nThe input to the output layer is the number of neurons in the previous layer and the output is the number of targets in the data \u2014 in this case two.\n\nWe then define a class method to manage the flow of data through the network. Here we call the layers on the data and also use apply the activation (from torch.nn.functional) on the hidden layer. Finally, we apply a sigmoid transformation on the output to squeeze the predicted values to sum to one.\n\nNext we need to define how the model learns. First we instantiate a model object from the class, we\u2019ll call this . Next we define the cost function \u2013 in this case binary cross entropy \u2013 see my previous post on log loss for more information. Finally we define our optimiser, . The optimiser will be optimising parameters of our model, therefore, the argument is simply the model parameters.\n\nNow we are ready to train the model. We will do this for 50 epochs. I have commented the code block to make it clear what is happening at each stage. We set up a for loop to iterate over the data (epochs) and with each epoch we loop over the mini batches of X and y stored in , which we defined previously. During each of these loops we make the input and target torch (note this step will not be necessary in the next release of pytorch because and \u200b will be merged) and then specify the forward and backward pass.\n\nIn the forward pass we use to model to predict y given X, calculate the loss (and accuracy). To so so we just pass the data through the model. In the backward bass we backproprogate the loss through the model and updates the weights according to the learning rate.\n\nWe can see that the loss decreases rapidly (the volatility can be explained by the mini-batches), which means our model is working \u2014 awesome! You can also see the non-linear decision regions learned by the model.\n\nAlso, check out the decision regions learned by the model:\n\nIt is pretty clear that the neural network can learn the non-linear nature of the data!\n\nThe purpose of this post was to show how to get up and running defining neural networks with pytorch. The model defined here is very simple, but the intention is to highlight the building blocks. I also didn\u2019\u2019t run a testing loop so we have no idea about the models test performance. In my next post I\u2019ll define a more complex pytorch model so stay tuned!"
    },
    {
        "url": "https://towardsdatascience.com/hyper-parameters-in-action-introducing-deepreplay-31132a7b9631",
        "title": "Hyper-parameters in Action! Introducing DeepReplay \u2013",
        "text": "In my previous post, I invited you to wonder what exactly is going on under the hood when you train a neural network. Then I investigated the role of activation functions, illustrating the effect they have on the feature space using plots and animations.\n\nNow, I invite you to play an active role on the investigation!\n\nIt turns out these plots and animations drew quite some attention. So I decided to organize my code and structure it into a proper Python package, so you can plot and animate your own Deep Learning models!\n\nHow do they look like, you ask? Well, if you haven\u2019t checked the original post yet, here it is a quick peek at it:\n\nSo, without further ado, I present you\u2026 DeepReplay!\n\nThe package is called DeepReplay because this is exactly what it allows you to do: REPLAY the process of training your Deep Learning Model, plotting and animating several aspects of it.\n\nThe process is simple enough, consisting of five steps:\n\nLet\u2019s go through each one of these steps!\n\nThe callback should be an instance of ReplayData.\n\nThe callback takes, as arguments, the model inputs (X and y), as well as the filename and group name where you want to store the collected training data.\n\nTwo things to keep in mind:\n\nLike I said, business as usual, nothing to see here\u2026 just don\u2019t forget to add your callback instance to the list of callbacks when fitting!\n\nSo, the part that gives the whole thing its name\u2026 time to replay it!\n\nIt should be straightforward enough: create an instance of Replay, providing the filename and the group name you chose in Step 1.\n\nThis is the step where things get interesting, actually. Just use Matplotlib to create a figure, as simple as the one in the example, or as complex as subplot2grid allows you to make it, and start attaching visualizations from your Replay object to the figure.\n\nThe example above builds a feature space based on the output of the layer named, suggestively, hidden.\n\nBut there are five types of visualizations available:\n\nFor this example, with a single visualization, you can use its plot and animate methods directly. These methods will return, respectively, a figure and an animation, which you can then save to a file.\n\nIf you decide to go with multiple simultaneous visualizations, there are two helper methods that return composed plots and animations, respectively: compose_plots and compose_animations.\n\nTo illustrate these methods, here is a gist that comes from the \u201ccanonical\u201d example I used in my original post. There are four visualizations and five plots (Probability Histogram has two plots, for negative and positive cases).\n\nThe animated GIF at the beginning of this post is actually the result of this composed animation!\n\nAt this point, you probably noticed that the two coolest visualizations, Feature Space and Decision Boundary, are limited to two dimensions.\n\nI plan on adding support for visualizations in three dimensions also, but most of datasets and models have either more inputs or hidden layers with many more units.\n\nSo, these are the options you have:\n\nWhat do we want to achieve? Since we can only do 2-dimensional plots, we want 2-dimensional outputs \u2014 simple enough.\n\nHow to get 2-dimensional outputs? Adding an extra hidden layer with two units, of course! OK, I know this is suboptimal, as it is actually modifying the model (did I mention this is a workaround?!). We can then use the outputs of this extra layer for plotting.\n\nYou can check either the Moons or the UCI Spambase notebooks, for examples on adding an extra hidden layer and plotting it.\n\nWhat are we doing with the model, anyway? By adding an extra hidden layer, we can think of our model as having two components: an encoder and a decoder. Let\u2019s dive just a bit deeper into those:\n\nLet me try to make it more clear with a network diagram:\n\nWhat do we have here? A 9-dimensional input, an original hidden layer with 5 units, an extra hidden layer with two units, its corresponding two outputs (features) and a single unit output layer.\n\nSo, what happens with the inputs along the way? Let\u2019s see:\n\nWhat does it all mean? It means that our model is also learning a latent space with two latent factors (f1 and f2) now! Fancy, uh?! Don\u2019t get intimidated by the fanciness of these terms, though\u2026 it basically means the model learned to best compress the information to only two features, given the task at hand \u2014 a binary classification.\n\nThis is the basic underlying principle of auto-encoders, the major difference being the fact that the auto-encoder\u2019s task is to reconstruct its inputs, not classify them in any way.\n\nI hope this post enticed you to try DeepReplay out :-)\n\nIf you come up with nice and cool visualizations for different datasets, or using different network architectures or hyper-parameters, please share it on the comments section. I am considering starting a Gallery page, if there is enough interest in it.\n\nFor more information about the DeepReplay package, like installation, documentation, examples and notebooks (which you can play with using Google Colab), please go to my GitHub repository:\n\nIf you found this post useful, leave a clap (or many, if you REALLY liked it!), so others can find it too :-)\n\nIf you have any thoughts, comments or questions, please leave a comment below or contact me on Twitter."
    },
    {
        "url": "https://towardsdatascience.com/artificial-intelligence-for-music-videos-c5ad14e643db",
        "title": "Artificial Intelligence for Music Videos \u2013",
        "text": "I am doing a video analysis A.I. project for a client, and I want to share with you a super fun idea that popped into my mind tonight while I was waiting for stuff to render. I really don\u2019t have time to be writing this, but I feel like I have to. Some ideas are just too awesome to not stay up all night programming. Let\u2019s get an A.I. system to outline people in a music video.\n\nThere have been some great object detection articles on Towards Data Science, and so I don\u2019t want to post duplicates of the same cool stuff. Instead of detecting the location and label of an object, lets draw lines around the object instead, using COCO and Mask_RCNN. Doing it on a music video is just extra cool.\n\nLet\u2019s mess with Jain\u2019s Makeba video. I removed the audio track and kept only the first 2000 video frames, just to make it extra clear that this article is making fair use of the artist\u2019s work for educational purposes. Enjoy her music video on YouTube by clicking here.\n\nHere is the result of our A.I. analysis of the video:\n\nI kind of love how it finds people in billboards and in the background. It\u2019s really subtle. I also like how the merge of the black and white girls faces was so smooth that it faked out the A.I. into thinking it is actually one person. Here below are some cool segments of the video I thought were noteworthy. Enjoy!\n\nI have to add one more fun project to this post.\n\nData science is hard work. When we are all sitting around the office, and our model keeps overfitting, or not compiling, we need some way to get back into the success mindset. Our office culture is pretty whimsical, and so when data science is getting us down, or when we have an unexpected triumph, we slam the emergency party button. Similar products are on the market, but we wanted to make our own from scratch. Not as a product. Just for fun.\n\nWe 3D printed the parts and Matt put together the relay, raspberry pi, and other parts and wired it all up.\n\nHere is our emergency party button demo:\n\nWe (mostly Mathieu Lemay) have been working on this project for like 2 years in our spare time. It is the side project that just won\u2019t die. Matt added a song randomizer, relays, speakers, and really every cool bell and whistle you can imagine. It feels really good to slam that button and trigger the lights and music.\n\nAnd so, in conclusion, artificial intelligence can be used to analyze video and extract a lot of information. And when it\u2019s all too much, hit the emergency party button!\n\nIf you liked this article on A.I. applied to music videos, press the follow button, and have a look at some of my most read past articles, like \u201cHow to Price an AI Project\u201d and \u201cHow to Hire an AI Consultant.\u201d\n\nOther articles you may enjoy:"
    },
    {
        "url": "https://towardsdatascience.com/building-neural-networks-in-f-part-1-a2832ae972e6",
        "title": "Building Neural Networks in F# \u2014 Part 1 \u2013",
        "text": "The main objective here is to demonstrate a working neural network which can be built to be as deep or wide as required. We will use MSE as last time to calculate our overall cost as this simplifies the calculation. What makes neural networks different from any arbitrary layered simple perceptrons are activation functions which helps neural networks learn non-linearity in the data-set. There are many different activation functions out there and each one of them has its merits but in our case, the hyperbolic tangent will suffice. Finally, we will use the MathNet library for most of our data-structures and calculations. This is essentially numpy for F# (but admittedly a lot less straight-forward).\n\nThe key idea in functional programming is to break your main function into smaller, simpler tasks which are easier to implement, debug and comprehend. Keeping this in mind we will first create a few helper functions that will be used during forward and backward propagation.\n\nLet\u2019s begin with defining some custom types for our functions. Having a good type system is one of the key ingredients in functional programming and among other benefits it makes your code look cleaner and less prone to errors. The main types to focus on are \u2018Layer\u2019 and \u2018NodesVect\u2019. Each Layer object can be thought of a labelled tuple of a weight matrix and an activation function that gets applied to the output of that layer. wMatx will hold the weights for each connection from the previous layer to the next layer (i_max = m (rows), j_max = n (cols)). Each NodesVect will consist of two vectors. sVect will hold the output of any arbitrary layer (and the input vector) before its passed through an activation function. xVect simply holds the value of sVect after it has been passed through the activation function. We need both of these for back-propagation so we can\u2019t just store xVect.\n\nThe image below summarizes these types in mathematical form. One thing to note regarding biases: we will store the weightings in the weight matrix but because they are only forward connected (i.e. their node value is always = 1), we will append these \u2018on the fly\u2019 to sVect or xVect only when required.\n\nThe activation functions are defined here as discriminated-unions. These are much similar to enums but a lot more flexible and they allow us to pass the activation function list to the network generator by just using keywords rather than actual function names. getActFn takes in as input a specific activation function\u2019s \u2018keyword\u2019 and a float and returns the transformed value.\n\nlastLayerDeriv is a function to calculate the error-delta for the very last layer used for back-propagation. Pointwise-multiply is used here because we will be getting the derivatives for an output of any arbitrary dimension so our outputs are now vectors.\n\nTo be able to work with any number of layers and any number of nodes within each layer, we will create a network generator that will simply take in the number of inputs, outputs and hidden nodes in the network (excluding biases) and the activation function types as two lists and output a Result monad. This will either return an error string or a Network type object. We will only use the Result monad to print errors if the node and activation function list doesn\u2019t meet the required specification (and that\u2019s why you see so many matches!). This function is pretty much self-explanatory.\n\nThe forward pass is usually conceptually simple to understand for neural networks. In a nutshell it involves setting the values of nodes in the next layer as the weighted sum of the nodes and bias in the previous layer and passing that result through a non-linear activation function.\n\nIn our implementation of fwdProp we will use List.fold which is an efficient implementation of recursively iterating through a list. What\u2019s different from the diagram above is that our accumulator would not be a single element but a list of xVectAndsVect and Layer types in reverse order. We return these in reverse order because F# lists are implemented as singly-linked lists so appending to the top is O(1) (and O(n) to the bottom). This also comes to our favour because back-propagation by definition would work in reverse order!\n\nList.fold allows us to set an initial state and these will be the input vector pair (this is the special case when sVect == xVect) and an empty list to store weights in reverse order. The transpose of the weight matrix (n*m) multiplied by the x-vector (m*1) will give the s-vector (n*1) for the next layer (as shown in the mathematical form above). Giving special care to biases, a \u20181.0\u2019 is appended to the x-vector in the current layer but the way the network is designed, it will still be required for the next layer (because nothing feeds into a bias node). Finally a simple activation function mapping is performed on each element in s-vector to generate the x-vector for the next layer.\n\nIts time to get serious. The back-propagation derivation is well-explained in many sources online. We will use the stochastic-gradient descent approach and hence some of the equations might look slightly different. The full list of equations can be found in the lecture notes mentioned earlier. It is a good idea to have that in mind before going through the code.\n\nWe begin with calculating the error-deltas (which describe the contribution of the final error attributed to each of the unactivated outputs) as a vector. We do this calculation for the very last layer using the helper function described earlier. Once we get this value we now need to calculate the error-delta vectors for all the layers. Strictly speaking a layer can be classified as such if something feeds into it. Thus, the inputs are not classified as a layer and it makes no sense to calculate error deltas for them. The previous layers\u2019 error-deltas is calculated using the current layer\u2019s weight-matrix and previous layer\u2019s (or input\u2019s) s-vector. This can be easily done using a custom tail-recursive function.\n\nOnce we have a list of error-delta vectors in reverse order, we need to multiply the activated outputs (starting from the second last layer) with the error-deltas to produce a dE/dW matrix \u2014 which describes the error contribution by each weight and will be used to update the weights and thus allow the machine to \u2018learn\u2019 from its mistakes. The weights are simply updated by subtracting a factor (learning-rate) of dE/dW from the original weight-matrix. To perform all these operations on a per-layer basis we use List.map3. This is a built-in F# function and it allows us to iterate over 3 lists (weight matrices, x vectors & error-delta vectors) simultaneously. The result of this mapping will produce updated weights in the correct original order which can then be directly returned by the back-propagation function.\n\nA few things to note here. The x vectors, s vectors and w matrices are all passed in reverse order to the recursive function. \u2018Head\u2019 simply returns the first element in a list and the \u2018Tail\u2019 returns all but the first element. We remove the first row in the weight matrix because it corresponds to the bias node and there is no contribution from that node to any of the weights behind it i.e. no weights feeding into the bias node. Iterating over xAndSLstRev.Tail is equivalent to iterating over each previous layer\u2019s output.\n\nThe layerListUpdater function requires an extra 1.0 appended to the x-vector because this is the previous layer\u2019s activated output and here a bias is connected to a weight component in the current layer i.e. the weight-matrix has m-rows and the 0th row corresponds to bias weightings.\n\nGive yourself a pat on the back for making it so far! You can now use these functions to create any arbitrary network from a simple top-level description such as: . Once you extract the Network object from the output, you just need to pass this through a pipeline ( ) and you will have your new set of weights.\n\nIn the next part, we will make use of these and some more \u2018helper\u2019 functions in order to generate training data from a non-trivial function and make the network actually do some real work. So stay tuned and if you liked this article, please leave a \ud83d\udc4f!"
    },
    {
        "url": "https://towardsdatascience.com/gradient-descent-and-cryptocurrency-995693ddae6b",
        "title": "Gradient Descent and Cryptocurrency \u2013",
        "text": "I have been thinking of an interesting way to talk about some of the early things I learned through the fast.ai course. I was on the verge of just running through the concept of stochastic gradient descent (SGD) with restarts with a quick article, but I do enjoy articles that use analogies. Analogies serve as another way to look at a concept and further cement it inside of that brain of yours.\n\nSo today, I will provide you all with a nice connection between a twist on SGD and an aspect about the wonderful world of cryptocurrency for those fanatics out there.\n\nThere are several great articles on SGD in Towards Data Science, and those who aren\u2019t familiar with it or need a quick recap \u2014 I would suggest searching and reading those. I found a couple here and here.\n\nThe overall idea is to change the value of the weights in your neural network in order to reach a local minimum. This minimum is defined by a function that you are aiming to optimize. The learning rate is how fast you are moving toward that minimum point. If you learn too quickly, you might miss the minimum. If you learn too slowly, the time to solve the problem may take too long. Data science is all about trade-offs.\n\nIn the fast.ai class, we learned early on about SGD with restarts. This technique helps the burden on the model to find the best path in the first go-around. You can think of the \u2018restart\u2019 method as a way to look around the search space to find a better path than the first go-around. Here\u2019s a picture to help visualize this.\n\nIn the left picture, the model is going through the SGD process once. With that one journey, the model has found the best solution to the best of its abilities and constraints. In the right picture, the restart function exploits the SGD by running it multiple times. As you can see, there is more opportunity to reach a better local minimum.\n\nThis improvement can be the difference between 99% and 99.5% accuracy in a model! Plus, the course makes this function very accessible, so I recommend checking it out.\n\nNow on to the crypto world.\n\nSome of you make think of this as a stretch as an analogy, but I really enjoy merging ideas together. So, during one of my drives home, I was listening to this podcast about a company called Multicoin Capital. They invest in crypto teams, and they had a lot of interesting things to say about the current state of the space.\n\nOne of the topics that was brought up was the framework they use when looking at companies with differing smart contract protocols. The words smart contract protocol doesn\u2019t matter in this discussion, so don\u2019t worry too much about it. They were highlighting their thesis in which they believe there will be multiple winners in the smart contract space because there are several dimensions in which there could be local maximums. They argue that currently, the world has only seen one local maximum which is Ethereum and there are definitely more that will be found.\n\nTo break that down a bit, think of the picture above (or scroll up to it). The terrain of that function is very hilly and bumpy \u2014 there are deep troughs and there are points of high peaks. In the case of investing, the goal is to find the high peaks (not the local troughs when we think about ML).\n\nSo in their world, they are searching and searching through this weird land and trying to find those companies that exploit different features and technologies such that a new local optimum can be realized. These investors are the \u2018SGD with Restarts\u2019 trying to jump through the function to find the local maximum (i.e. the companies with the best chance of winning)."
    },
    {
        "url": "https://towardsdatascience.com/bmw-machine-learning-weekly-week-9-d996c486dbb",
        "title": "BMW Machine Learning Weekly \u2014 Week 9 \u2013",
        "text": "Controlling your gadgets by talking to them is so 2018. In the future, you will not even have to move your lips. A prototype device called AlterEgo , created by Arnav Kapur, a 23-year old MIT Media Lab graduate student, is already making this possible. With Kapur\u2019s device \u2014 a 3-D-printed plastic doodad that looks kind of like a skinny white banana attached to the side of his head \u2014 he can flip through TV channels, change the colors of lightbulbs, make expert chess moves, solve complicated arithmetic problems, and order a pizza, all without saying a word or lifting a finger. \u201cI do feel like a cyborg, but in the best sense possible,\u201d he says of his experience with the device, which he built as a research project in the past year. \n\nAlterEgo picks up on tiny electrical signals produced by small movements of our facial and neck muscles when silently reading or talking to oneself. Kapur sees AlterEgo as a sort of antidote to the increasing anxiety sourcing from AI: his device shows how AI can help augment rather than replace humans.\n\nThough they are not that common yet, self-driving cars are moving toward consumer use, with companies like Google\u2019s Waymo testing them out on public roads. And these cars are likely to inflate the problem of motion sickness, which is caused when a person\u2019s eyes and inner ears send conflicting signals to the brain: the ear detects the motion of the automobile, but the eye sees the stationary surroundings of the interior. A startup called ClearMotion, the world\u2019s first proactive ride system, transforms the way your car rides and handles: it does for motion what noise-cancelling does for noise, mitigating road roughness within a fraction of a second. Relying on a combination of software and hardware, ClearMotion predicts the road and enables cars to react, giving passengers and drivers unmatched comfort, handling and stability.\n\nMany sections of the Chinese wall are in remote or hard-to-reach areas, and some have fallen into disrepair. MIT Technology Review reported that Intel and the China Foundation for Cultural Heritage Conservation have teamed up to deploy drones to scan and capture 3-D images of the structure. A detailed model of the nearly 700-year-old Jiankou portion of the wall, in the mountains north of Beijing, will be created and used to identify sections that are most in need of restoration. The use of drones is especially helpful to facilitate regular maintenance and inspection in areas such as the Jiankou stretch of the wall, which is notoriously steep and densely vegetated terrain."
    },
    {
        "url": "https://towardsdatascience.com/constructing-contour-enhanced-funnel-plots-for-meta-analysis-6434cc8e51d0",
        "title": "Constructing contour-enhanced funnel plots for meta-analysis",
        "text": "Meta-analyses are used to synthesize bodies of research and can carry considerable weight when it comes to directing policy. However, a big limitation with this method is that the studies included in a meta-analysis can be biased.\n\nPublication bias is a well-known source of bias. For instance, researchers might shelve studies that aren\u2019t statistically significant, as journals are less likely to publish these kind of results. Researchers might also use questionable research practices \u2014 also known as p-hacking \u2014 to nudge an effect across the line to statistical significance.\n\nConsequently, the risk of publication bias needs to be considered when performing a meta-analysis.\n\nTwo interrelated approaches are typically used to assess the risk of publication bias in meta-analysis. First, the risk of publication bias is visualised by constructing a funnel plot which visualizes a measure of effect size against a measure variance (e.g., standard error), with the funnel centred on the summary effect size.\n\nLet\u2019s consider the two funnel plots below in Figure 1.\n\nFigure 1A seems to be fairly symmetrical, with roughly the same number of studies either size of the summary effect size (i.e., the vertical line in the middle of the funnel). Studies with greater variance have a larger spread around the summary effect size, near the bottom of the plot. In contrast, Figure 1B is not symmetrical. It seems that the studies with more variance (i.e., studies with fewer participants) only fall to the right of the summary effect size. Many researchers would conclude that Figure 1B is indicative of publication bias, due to the clear asymmetry.\n\nEgger\u2019s regression test is often used as an objective measure of funnel plot asymmetry, as it assesses the relationship between the effect size a measure of sampling variance. A statistically significant effect is indicative of funnel plot asymmetry.\n\nNow let\u2019s consider the two funnel plots in figure 2, each with ten effect sizes.\n\nBoth plots demonstrate asymmetry, but note the differences in observed outcomes (i.e., effect sizes). The effect sizes in figure 2A range from 0.2 to 0.65, whereas the effect sizes in figure 2B range from -0.1 to 0.35.\n\nFor a given range of effect sizes and variances, it\u2019s possible to calculate statistical significance at for any combination of these two variables. Consequently, you can visualise a set of significance thresholds on a traditional funnel plot by constructing what\u2019s called a contour-enhanced funnel plot.\n\nThe funnel plots in Figure 3 are a duplicate of the plots in Figure 2, except key areas of statistical significance have been superimposed on the funnel, and the plot is now centred at zero. The red zones show effects between p = .1 and p = .05, and the orange zones show effects between p = .05 and p = .01. Effects in the white zone are greater than p = .1 and effects in the grey zones are smaller than p = .01.\n\nRunning Egger\u2019s regression test on both of these plots yield identical outcomes (z = 2.78, p = 0.01), which are suggestive of asymmetry. However, there\u2019s clear pattern in Figure 3A of studies falling into the statistical significance channel on the right of the funnel. Figure 3B displays clear asymmetry, but none of the studies are statistically significant.\n\nFigure 3 illustrates that funnel plots and Egger\u2019s regression tests only assess the risk of small study bias, insofar that smaller studies tend to have greater variance. Sure, an asymmetrical plot might be due to publication bias, but there could be other reasons that can better explain funnel plot asymmetry. For instance, smaller studies with greater variance are associated with poor study design.\n\nHere\u2019s how to make these plots for your own meta-analyses, using R.\n\nThe metafor R package provides a straightforward means of constructing highly customisable contour-enhanced funnel plots.\n\nThe following R script will generate a standard funnel plot and three different contour-enhanced funnel plots.\n\nFigure 4A is a standard funnel plot. Figure 4B is a contour enhanced funnel plot \u2014 note that the vertical reference line is now at zero (instead of the summary effect size, like Figure 4A). Figure 4C has added a little colour, and Figure 4D has adjusted the contour lines to a single contour between p = .05 and p = .01.\n\nOne of the many benefits of using R for your analysis is that you can easily share the analysis script with your manuscript. If you\u2019re new to R, you should check out my step-by-step tutorial paper to performing your own correlational meta-analysis using R. I also put together a companion video, if that\u2019s more your thing.\n\nOpen materials are usually promoted for the purpose of other scientists to reproduce your analysis. However, you\u2019ll quickly learn that one of biggest beneficiaries of open materials is your future-self, for when you need to revisit analyses.\n\nContour-enhanced funnel plots provide a handy visualisation of the pattern of statistical significance reported in a body of research. While they\u2019re a useful addition to traditional funnel plots, which only visualise small-study bias, they cannot be used to objectively assess the risk of p-hacking, or the degree of effect size inflation in a given field. Despite these limitations, funnel plots can still provide a valuable addition to meta-analyses, given the ease at which they can be put together."
    },
    {
        "url": "https://towardsdatascience.com/6-reasons-why-data-visualisation-projects-fail-1ea7a56d7602",
        "title": "6 Reasons why Data Visualisation projects Fail \u2013",
        "text": "Inspite of the tremendous promise of data visualisation, and the discipline being mainstream for about a decade, it is anything but mature. With a boatload of visualisation tools at disposal and fancy data scientists to play with them, impactful use of data visualisation is still a rarity in enterprises.\n\nThanks to sustained advances by practitioners and marketing blitzkrieg of the top players, visualisation has reached the deep shores of organisations. The investment and awareness notwithstanding, Business ROI from visualisation initiatives or long-term adoption of tools is hard to come by.\n\nAt times, one wonders what makes the visual display of information so tough.\n\nCan we look at the key failure points in visualisation projects? Or, as a visualisation practitioner would you like some funny reflections of shared frustrations? Read on for a sampling of our experience at Gramener, from close to a decade of implementing visual intelligence at enterprises. Lets also discuss how these pitfalls can be tackled.\n\nEnd users are often not directly engaged while defining needs for visualisation projects. One often hears the project sponsor or manager say that they \u2018exactly know what the users want\u2019. Either the user\u2019s time is not easily available (CXOs..) or there are too many of them (Ops staff). And they are conveniently ignored with a naive assumption that others can cover for them.\n\nThis is a prime reason why visual dashboards often go unused after rollout. If just the listing of common asks or definition of KPIs were sufficient, this challenge woudn\u2019t arise. Whats really needed is maping of user stories, and hearing how users approach business problems. This is the user\u2019s practical wisdom that can\u2019t be transferred, and which is closely linked to actionability.\n\nIts critical to onboard the end users and gather their nuanced business perspectives so that it can be built into dashboards. Build the user persona through interviews, map the user journey by gentle probing, and jointly sketch out the as-is business scenarios. Its also helpful to list the questions that will be answered by the visualisation, and clarify on the ones that will not be.\n\nWhen proposed with various competing options for visualisation, how often have you heard the answer, \u2018All of the above\u2019? Users develop an acute sense of insecurity when asked to prioritise, and fear that a future scenario may go unaddressed. If its the first visualisation project for an enterprise, the urge to fit in all possible bells and whistles is heightened.\n\nSponsors miss to see that the more you dump into an application, the lesser it will get used. While one gets a false sense of satisfaction by checking all boxes, the cognitive load could get so high that users simply stop using it altogether. When it comes to prioritisation, the most knowledgeable users may not have the right perspective to take hard calls, or the gumption to bite the bullet.\n\nIts important to play a consultative role and help whittle down the feature list to the most critical. While screen space is technically unlimited, its useful to impose constraints on data density, such as \u2018nothing more than a non-scrollable screenful\u2019. Onboard stakeholders who know the priority, who can take hard decisions and also champion the many battles needed to convince other users.\n\nIt can get outright bizarre when the need for data is questioned in a data visualisation project. One can consider themselves lucky if they have not been told, \u201cBuild the dashboard first, and plug in data directly at Go-live\u201d. Yes, there are challenges in cleaning data and preparing the feeds, but designing dashboards sans any data is like putting a cart before the horse.\n\nRetrofitting data is the root cause for ending up with non-actionable dashboards or weird-looking charts. Without exploratory analysis, charts could be skewed by outliers or worse, end up without patterns. Choice of charts is also driven by data. For example, revenue distribution of upto 4 products can be a stacked bar, but for more, charts like treemap are relevant.\n\nAs part of project planning, its critical to account for data upfront. While getting the header rows is a useful start, full data is essential before critical design decisions can be taken. Clients must be educated that data is indeed in the critical path of visualisation, and that data insights drive design decisions.\n\nAt times people fall so much in love with a chart, that they fatefully try extending this relationship beyond the scope of cool visualisation examples. This leads to unproductive force-fitting of charts into the solution. The compromises made for this adjustment can wreak havoc on the entire project.\n\nThose who demand exotic or 3D charts even when usecases don\u2019t support it, are doing so for their own satisfaction, and end up alienating users. It's not a buffet where one can pick an assortment of fancy charts to serve up on the platter. One might have a crush on the Sankey chart or Chord diagram, but it would be a disaster to give these charts to a non-data savvy audience.\n\nThe choice of chart is a science and there are robust disciplines like grammar of graphics that govern it. Factors to pick a chart include: type of representation (distribution, trend..), data points (few, more..), user\u2019s role (operational, strategic..), user\u2019s data familiarity (analyst, business user..) etc. One must actively educate users on the rationale and explain with examples.\n\nWhen designing navigation and interactivity, a common request is to \u201cEnable deep drill-in to the last level, and make everything clickable\u201d. If dumping the entire world into a single screen is a common fantasy, making it all clickable is closely related. When pushed to prioritise features within a screen, a common excuse is for users to try and hide entire dashboards behind unrelated clicks.\n\nBurying a hundred clicks into a single screen turns visualisations into an easter egg hunt. The user may never know where useful information may be hidden in the dashboard. In most applications, over 90% of clicks are never used, and it's just the 10% that natively fall into a user\u2019s workflow that matter.\n\nRich UI doesn\u2019t mean many clicks, it means just the right and intuitively placed ones. It may be useful to impose some guidelines, say, no more than 8 clicks per screen. Data stories can be equally powerful in the static format, so carefully question the interactivity needed. Users will be thankful for this call.\n\nWhile all of the points covered so far can be rationalised, the same cannot be said about color. Statements such as \u201cSomething seems missing in the dashboard\u201d or \u201cThis visual misses the wow effect\u201d gives practitioners goosebumps. Whip us if you have to, but don\u2019t leave the feedback hanging.\n\nEach user has their choice of colors and they can get pretty opinionated. Unfortunately, this can have a huge bearing on the application\u2019s acceptance. And color is not just about look-and-feel. We learnt this painful lesson on a project that had users with red-green color blindness. Our rich dashboard with RGB palette showed up as a single blob of colour, making it unusable.\n\nColor theory is more an art than science, though there are standard guidelines to handle the aesthetic, functional and social aspects. Its best to go with the user persona and application requirements, rather than trying to please everyone. One must also take the effort to articulate choices and help resolve disconnect, since most users are unable to explain their colour preferences.\n\nWhile we\u2019ve seen the 6 critical points of failure in data visualisation projects, they also happen to fall in the six key stages of visualisation engagements. Hence, getting them right by avoiding the pitfalls can tilt scales towards overall success of the initiative.\n\nVisualization should be seen as a medium of story telling using data. A visual story is a perfect blend of art and science. Practitioners must hone their skills to fuse the right aesthetic ingredients with scientific elements. This creates an output that is relevant for users, solves a specific business challenge and delivers ROI for enterprises."
    },
    {
        "url": "https://towardsdatascience.com/automold-specialized-augmentation-library-for-autonomous-vehicles-1d085ed1f578",
        "title": "Automold- specialized augmentation library for Autonomous vehicles",
        "text": "It was not long into Udacity\u2019s Self driving car nano-degree course when I realized that besides normally followed augmentation techniques, self-driven cars also require specialized augmentation. A self driven car is one of the most complex challenges of AI and also different from other challenges in many aspects. It not only sees the world in day and night but also with various weather changes. It is way different for a car to drive in snow than on regular summer roads. To make cars safer than before and generalize them to all aspects of roads it is necessary to train them in all possible environments.\n\nI searched the abyss of the Internet but couldn\u2019t find concrete ways to create such scenarios artificially. Till now the only and also more robust way to train vehicles, is to train them on actual roads. But roads are not accessible to all genius minds out there who want to train their Car\u2019s CNN to drive gracefully in all conditions.\n\nThus I decided to create Automold. This library is written in python and uses OpenCV and Numpy underneath. Automold helps to augment the images to have various seasons. Whether you want to add a little more sun or rain, fog or snow, or maybe some speed, gravels and autumn leaves, Automold helps you to achieve that without getting you into the hassle of writing the whole code yourself.\n\nBelow are some of the images I will test Automold on.\n\nAdding snow to the images can really help an Autonomous vehicle engineer to make the car\u2019s understand, how is it to drive in snow. Snowy and rainy roads pose a great threat while driving. Even humans find it really difficult to identify paths and thus training CNNs to identify routes is cumbersome.\n\nHere are some images with fresh snow and rain generated by Automold.\n\nAlso the amount of rain and snow is configurable. There are three rain types to choose from \u2018drizzle\u2019, \u2018heavy\u2019 and \u2018torrential\u2019. If you don\u2019t want to do too many configurations just set the rain type and you are good to go. If you want to make the images more or less shady you can darken or brighten the images by setting the respective coefficients or maybe go in for random values.\n\nThis doesn\u2019t requires getting into OpenCV color channels and modifying pixel values. Also you don\u2019t need to loop through you image list and call the function on each image. Whether it\u2019s a single image or whole list of images just pass it to the functions. The whole thing is as easy as:\n\nApart from this if you want to introduce shadows or gravels on the road, Automold helps you to do so instantly. However at the same time allows you to do various configurations. Defining region of interest or specifying how many gravel patches to introduce can be really handy at times. If you don\u2019t want to configure, you can go with the default values.\n\nIntroducing Fog or sun flare is also very easy and yet configurable. Fog intensity, Sun flare source position, flare color, angle etc. can be set to create desired effects.\n\nAlso there are two new additions to the library which allow us to add speed motion and autumn to the images. Motion blur is actually a thing, which happens when you drive fast. Distant positions on the roads stay distinctly visible but blur away as they come close.\n\nmaking the leaves red and orange might not be the best way to describe autumn but still is the closest way to artificially reproduce the effect. Although I am still thinking of a better and robust way to do so. So there are many functions at your disposal which can help you to augment the images specifically for roads. But isn\u2019t there an even better way to do augmentation and not call these functions manually? Well, Yes !! You can do so just by typing this:\n\nThe function applies a bunch of augmentations on the input images and reduces the whole augmentation code to just a single line.\n\nYou can find the documentation and the library code along with other Autonomous vehicle projects on my Github profile and for more articles by me visit Medium or Wordpress."
    },
    {
        "url": "https://towardsdatascience.com/choosing-the-right-metric-for-evaluating-machine-learning-models-part-2-86d5649a5428",
        "title": "Choosing the Right Metric for Evaluating Machine Learning Models \u2014 Part 2",
        "text": "In the first blog, we discussed some important metrics used in regression, their pros and cons, and use cases. This part will focus on commonly used metrics in classification, why should we prefer some over others with context. Let\u2019s first understand the basic terminology used in classification problems before going through the pros and cons of each method. You can skip this section if you are already familiar with the terminology. Recall or Sensitivity or TPR (True Positive Rate): Number of items correctly identified as positive out of total true positives- TP/(TP+FN) Specificity or TNR (True Negative Rate): Number of items correctly identified as negative out of total true negatives- TN/(TN+FP) Precision: Number of items correctly identified as positive out of total items identified as positive- TP/(TP+FP) False Positive Rate or Type I Error: Number of items wrongly identified as positive out of total true negatives- FP/(FP+TN) False Negative Rate or Type II Error: Number of items wrongly identified as negative out of total items identified as negative- FN/(FN+TP) F1 Score: It is a harmonic mean of precision and recall given by- \n\nF1 = 2*Precision*Recall/(Precision + Recall) The probabilistic interpretation of ROC-AUC score is that if you randomly choose a positive case and a negative case, the probability that the positive case outranks the negative case according to the classifier is given by the AUC. Here, rank is determined according to order by predicted values. Mathematically, it is calculated by area under curve of sensitivity (TPR) vs. \n\nFPR(1-specificity). Ideally, we would like to have high sensitivity & high specificity, but in real-world scenarios, there is always a tradeoff between sensitivity & specificity. Some important characteristics of ROC-AUC are- The value can range from 0 to 1. However auc score of a random classifier for balanced data is 0.5 ROC-AUC score is independent of the threshold set for classification because it only considers the rank of each prediction and not its absolute value. The same is not true for F1 score which needs a threshold value in case of probabilities output Log-loss is a measurement of accuracy that incorporates the idea of probabilistic confidence given by following expression for binary class: It takes into account the uncertainty of your prediction based on how much it varies from the actual label. In the worst case, let\u2019s say you predicted 0.5 for all the observations. So log-loss will become -log(0.5) = 0.69. Hence, we can say that anything above 0.6 is a very poor model considering the actual probabilities.\n\nConsider Case 1 (Balanced Data), it looks like model 1 is doing a better job in predicting the absolute probabilities whereas model 2 is working best in ranking observations according to their true labels. Let\u2019s verify with the actual score: If you consider log-loss, Model 2 is worst giving a high value of log-loss because the absolute probabilities have big difference from actual labels. But this is in complete disagreement with F1 & AUC score, according to which Model 2 has 100% accuracy. Also, you would like to note that with different thresholds, F1 score is changing, and preferring model 1 over model 2 for default threshold of 0.5. Inferences drawn from the above example (balanced):\n\n- If you care for absolute probabilistic difference, go with log-loss\n\n- If you care only for the final class prediction and you don\u2019t want to tune threshold, go with AUC score\n\n-F1 score is sensitive to threshold and you would want to tune it first before comparing the models How each of them deals with class imbalance?\n\nThe only difference in the two models is their prediction for observation 13 & 14. Model 1 is doing a better job in classifying observation 13 (label 0) whereas Model 2 is doing better in classifying observation 14 (label 1). The goal is to see which model actually captures the difference in classifying the imbalanced class better (class with few observations, here it is label 1). In problems like fraud detection/spam mail detection, where positive labels are few, we would like our model to predict positive classes correctly and hence we will sometime prefer those model who are able to classify these positive labels Clearly log-loss is failing in this case because according to log-loss both the models are performing equally. This is because log-loss function is symmetric and does not differentiate between classes . Both F1 score and ROC-AUC score is doing better in preferring model 2 over model 1. So we can use both these methods for class imbalance. But we will have to dig further to see how differently they treat class imbalance.\n\nIn the previous example, we saw that there were few positive labels. In the second example, there were few negative labels. Let\u2019s see how F1 score & ROC-AUC differentiate between these two cases. ROC-AUC score handled the case of few negative labels in the same way as it handled the case of few positive labels. An interesting thing to note here is that F1 score is pretty much same for both Model 3 & Model 4 because positive labels are large in number and it cares only for the misclassification of positive labels. Inferences drawn from above example:\n\n- If you care for a class which is smaller in number independent of the fact whether it is positive or negative, go for ROC-AUC score. When will you prefer F1 over ROC-AUC? When you have a small positive class, then F1 score makes more sense. This is the common problem in fraud detection where positive labels are few. We can understand this statement with the following example. We can see that model (1) predicts 5 positives out of 100 true positives in a dataset of size 10K observations, while another model (2) predicts 90 positives out of 100 true positives. Clearly, model (2) is doing a much better job than model (1) in this case. Let\u2019s see if both F1 score & ROC-AUC score are able to capture that difference Yes, the difference in F1 score reflects the model performance. ROC-AUC gives a decent score to model 1 as well which is nota good indicator of its performance. Hence we should be careful while picking roc-auc for imbalanced datasets. Which metric should you use for multi-classification? We have further three types of non-binary classification: Multi-Class: classification task with more than two classes such that the input is to be classified into one, and only one of these classes. Example: classify a set of images of fruits into any one of these categories \u2014 apples, bananas, and oranges. Multi-labels: classifying a sample into a set of target labels. Example: tagging a blog into one or more topics like technology, religion, politics etc. Labels are isolated and their relations are not considered important. Hierarchical: each category can be grouped together with similar categories, creating meta-classes, which in turn can be grouped again until we reach the root level (set containing all data). Examples include text classification & species classification. For more details, refer this blog. In this blog, we will cover only the first category.\n\nAs you can see in the above table, we have broadly two types of metrics- micro-average & macro-average, we will discuss the pros and cons of each. Most commonly used metrics for multi-classes are F1 score, Average Accuracy, Log-loss. There is yet no well-developed ROC-AUC score for multi-class. Log-loss for multi-class is defined as: - In Micro-average method, you sum up the individual true positives, false positives, and false negatives of the system for different sets and the apply them to get the statistics.\n\n- In Macro-average, you take the average of the precision and recall of the system on different sets Micro-average is preferable if there is a class imbalance problem."
    },
    {
        "url": "https://towardsdatascience.com/3-ways-non-profits-use-emerging-technology-to-further-donor-dollars-and-how-you-can-too-6e4d02ee2569",
        "title": "3 Ways Non-Profits Use Emerging Technology to Further Donor Dollars \u2014 And How You Can, Too",
        "text": "A U.S.-based non-profit organization wants to send clean-burning stoves to rural Guatemalan heads-of-households. The idea is for recipients to use the stoves to start small businesses in support of their families. But what if one of those stoves breaks down?\n\nIxo helps non-profits collect, measure, evaluate, value, and tokenize verifiable impact. Their application of the Internet of Things (IoT) sensors, smart contracts, and blockchain help non-profits serve those in need in real-time, no matter their location. Via IoT sensors, for example, sending brands can monitor product performance from thousands of miles away, troubleshoot problems even before they arise, and then offer instructions to fix them.\n\nFurther, ixo pairs blockchain and smart contracts with IoT sensors to gather data regarding carbon emissions, then issue carbon tokens in real-time \u2014 something that, pre-emerging technologies, may have taken a year to process. And, for transparency, blockchain authenticates the IoT sensors so no tampering of data goes unnoticed.\n\n\u201cAs a result of these technologies, there\u2019s greater trust between parties, while reducing evaluation costs,\u201d says Fennie Wang, partnership and regulatory lead of the ixo foundation, a nonprofit organization that uses technology to create social impact.\n\nBrands Further Donor Dollars By Matching One Technology to One Use Case at a Time\n\nThough ixo matches myriad emerging technologies to non-profit problems, this approach can be overwhelming for many organizations. So, others are taking a one-case-to-one-technology approach to serve more with less. Learn 3 emerging-technology use cases for non-profits to serve more people with fewer donations:\n\n1. Machine Learning (ML) allows brands to safely cut IT infrastructure costs.\n\nTwenty-two people die daily waiting for an organ. One decision to be an organ, tissue, and eye donor potentially saves 8 lives and enhances 75.\n\nAs a result, Donate Life Month leads to thousands saved as people use social media and other platforms to raise awareness and offer information about organ-donor registration. In turn, supporting organizations rally around those promoting it and news organizations tell stories of those impacted.\n\nIt\u2019s a month of charity, empathy, and lives saved. But, just one technical glitch could down websites where registration and donation opportunities are available. This would mean lives lost and less impactful donations as cause-related organizations struggle to make up for lost user-generated promotional opportunities.\n\n\u201cBecause the non-profit sector is so dependent on seasonality, it affects them even more than most industries,\u201d says Dylan Max, growth marketing lead at FogLogic. \u201cTo protect themselves from fluctuating demands, many larger non-profit organizations over-provision their IT infrastructure. This keeps them safe but is highly inefficient and costly.\u201d\n\nHe says that ML is the answer. While monitoring system interactions and changes, it pinpoints potential issues, then raises red flags to prevent them. In the end, organizations cut costs on over-the-top infrastructure and can rest assured that prevention and correction measures will happen in time.\n\nGender, location, conflict, familial socioeconomics, disability, and local and national conflict mean 61 million primary school children across the globe have no education access. Pencils of Promise (PoP) is a non-profit that raises money to build public schools in rural Guatemala, Ghana, Nicaragua, and Laos.\n\nAnd while many non-profit organizations rely on long-distance trips to show investors where donations go, Pencils of Promise turned to VR. Using VR, investors experience projects from across the globe. And, in doing so, money that would pay travel costs goes directly toward serving people.\n\n\u201cThere is no other medium that allows an immersive, personal experience quite like VR,\u201d says Michael Dougherty, CEO of Pencils of Promise. \u201cOur [VR] film takes supporters inside a rural community in Ghana to experience the transformational impact of education.\u201d\n\nThough VR technologies are expensive, by carefully matching them to a use case, organizations stand to gain greater support for causes and, thereby, offset the cost of producing and offering immersive experiences. Via a 90-second VR film, Pencils of Promise raised a total $1.9 million for school construction.\n\nWhen Opal, a Vervet Monkey, was found by the Centre for the Rehabilitation of Wildlife (CROW) clinical team in Durban, South Africa, she was just 6 days old. The team found her still clinging to her mother, who had been killed by a car. Today, the CROW clinical team searches for a sponsor to cover Opal\u2019s food, lodging, medical and rehabilitation costs.\n\nAnd Opal\u2019s future sponsors can rest assured that CROW focuses on efficient donation usage. A year ago, CROW became a client of Sun Exchange, a blockchain company that helps cut CROW\u2019s electricity costs by 10\u201340 percent. When asked how blockchain cut CROW\u2019s electricity costs, Abe Cambridge, founder and CEO of Sun Exchange explained:\n\n\u201cNGO\u2019s in underdeveloped markets are using expensive and dirty fossil fuels for electricity, and in those instances, solar energy is drastically cheaper to use but also much more expensive to implement. So we can come in and implement a fully-funded, fully-operational solar energy system that meets all the NGO requirements and they won\u2019t have to pay a penny upfront whatsoever.\u201d\n\nUsing blockchain, anyone from around the globe can join the Sun Exchange platform, and collectively purchase a target number of solar panels to be leased onto the NGO via Sun Exchange, who has them installed on the NGOs rooftop. From there, the NGO pays a rental lease to solar panel owners, which is collected and sent back to the solar panel owners quickly and securely using cryptocurrency blockchain payments.\n\n\u201cSending payments using new blockchain based digital currencies such as Bitcoin directly to projects is a far more efficient and elegant way of doing it because the sender of the money knows where that money is going specifically, and it takes minutes, not days,\u201d says Abraham.\n\nWith no middlemen, NGOs don\u2019t pay extra international-transactions costs and, using a blockchain-based cryptocurrency, money doesn\u2019t go to currency-exchange fees. Furthermore, investors are given confidence to get involved in projects across borders because they can view where their money is going on CROW\u2019s solar-power online blockchain ledger.\n\nWhen it comes to complex emerging technologies, don\u2019t overthink it.\n\nThe vast capabilities of today\u2019s VR, ML, blockchain, smart contract, and IoT technologies may have some organizations overwhelmed. But, by matching technologies to just one of their costliest use cases, non-profit organizations can cut costs, allowing donor dollars to serve more.\n\nNon-profits affected by seasonality protect their most lucrative donation times via ML. Some cut travel costs via VR experiences. And, still others cut electricity costs via blockchain.\n\nSun Exchange makes it even simpler. Instead of implementing in-house technologies, they offer non-profits technologies-as-a-service. Doing so allows organizations to cut costs in their path to greater efficiency.\n\nTo serve more with less, the starting path is clear: match one use case to one technology. Partner with tech firms to cut implementation costs. And, if you don\u2019t know where to start in identifying your organization\u2019s use case, look to tech-firm expertise to pinpoint the use case with the greatest potential for cutting overhead costs. Your donors and those you serve will thank you!"
    },
    {
        "url": "https://towardsdatascience.com/an-intro-to-kalman-filters-for-autonomous-vehicles-f43dd2e2004b",
        "title": "An intro to Kalman Filters for Autonomous Vehicles \u2013",
        "text": "I have recently completed my Udacity Term2 of Self Driving Car Nanodegree Program and would like to share my views on one of the interesting and cool topic that i came across with \u2018Kalman Filter\u2019 . Hope this blog gives you a clear understanding of what it is . enjoy learning :)\n\nAn autonomous vehicle consists of various device through which it collects data and perform an action . Following image show the location of few of the important components used in the vehicle.\n\nData used by the Kalman filter comes from LIDAR and RADAR . So for now will only focus on these two deivces.\n\nWe can use Kalman Filter to make an educated guess , about what the system is going to do next in any place where we have uncertain information about some dynamic system . In case of Autonomous Vehicle Kalman Filter can be used to predict the next set of actions that the car in-front of our autonomous vehicle is going to take based upon the data our vehicle receives . It is an iterative process that uses two step predict and update.\n\nIt is mostly suitable for these type of operations because of the following factors :\n\nI will give you an overview of how all this work followed by an example that will help you understand the math behind it.\n\nKalman Filter is an iterative process that follows two step predict and update\n\nIn this step the Kalman Filter predict the new values from the initial value and then predict the uncertainity/error/variance in our prediction according to the various process noise present in the system.\n\nIn autonomous vehicle process noise can be understood by taking a simple example of a car moving infront of our vehicle. Our model will assume the car to move with a constant velocity due to zero acceleration but in reality it will have acceleration i.e it speed will fluctuate from time to time. This change in acceleration of that car is the uncertainity/error/variance and we introduce it to our system using process noise.\n\nIn this step we take the actual measured value from the devices of the system . In case of autonomous vehicle these devices can be radar or Lidar .We then calculate the difference between the predicted value and the measured value and then decide which value to keep i.e predicted value or measured value by calculating the Kalman Gain. Based upon the decision made by the kalman gain we calculate the new value and and new uncertainty/error/variance .\n\nThis output from the update step is again fed back to the prediction step and the process continues till the difference between the predicted value and the measured value tend to convert to zero.This calculated value will be the prediction/educated guess done by the Kalman Filter.\n\nKalman Gain : It determines whether our predicted or measured value is close to the actual value . Its value ranges from 0 to 1 . If its value is near to 0 then it means predicted value is close to the actual value or if the value is near to 1 then it means final measured value is close to the actual value. Its value ranges from 0 to 1 because it uses the uncertainity/errors in the predicted and the measured value and is represented by the simple formula as show below.\n\nK = Error In Prediction / (Error in Prediction + Error in Measurement)\n\nIn this we will try to understand how the Kalman filter is used in the self driving car based upon the information we have till now .\n\nLets assume a 2D environment in which we have a car and a pedestrian moving by as shown below:\n\np_x and p_y determine the 2D position and v_x and v_y determine the 2D velocity of the pedestrian . This can be represented as x = (p_x,p_y,v_x,v_y). We have to predict the 2D position(p_x\u2032,p_y\u2019) and 2D velocity (v_x\u2032,v_y\u2032) of the pedestrian i.e x\u2032\n\nSince the state vector only models the position and velocity we need to also have the uncertainity/noise so as to model the acceleration, this is done by the process covariance matrix P.\n\nPrediction step :calculates the predicted value x\u2032 and predicted error P\u2032 also know as Process Covariance Matrix by using the following formula:\n\nIt is used to transform the state vector matrix from one form to another . i.e suppose the state of a vehicle is given by position p and velocity v and is not accelerating at time t.\n\nAfter a time particular time say t+1 the new state vector X\u2019 will be\n\nin matrix this can be shown as below:\n\nthen the F matrix will be given by:\n\nIt determines the change in the control input vector \u03bc due to external force(longitudnal or latitudinal force) and internal force(gravity,friction etc.). In context to autonomous vehicle we cannot model the external forces as it varies from one region to region and internal forces as it varies from one model of the car to another. So mostly B.\u03bc=0 .\n\nIt is nothing but determines random noise that might be present in the system. It is added to make the prediction bit accurate.\n\nIt gives uncertainty in the object\u2019s position when predicting location. The model assumes velocity is constant between time intervals, but in reality we know that an object\u2019s velocity can change due to acceleration. The model includes this uncertainty via the process noise.\n\nUpdate Step : A quick recap in update step we do the following:"
    },
    {
        "url": "https://towardsdatascience.com/beyond-the-hype-the-value-of-machine-learning-and-ai-artificial-intelligence-for-businesses-892128f12dd7",
        "title": "Beyond the Hype: The Value of Machine Learning and AI (Artificial Intelligence) for Businesses\u2026",
        "text": "Picture this: Your marketing software provides actionable guidance on writing innovative, personalized content that will steadily improve conversion rates. Your employees are more engaged and happier, spending less time working on tedious, repetitive tasks such as summarizing articles, tagging content and adding descriptive metadata to images in the CMS. Instead, they are working on creative marketing campaigns and producing fresh, well-received content. Your teams are more productive, translating to company-wide efficiency.\n\nThis scenario is an emerging reality thanks to the practical application of AI in digital marketing. There is no shortage of hype describing how AI will fundamentally change our work and lives in the future. When it comes to AI, it can be hard to separate fact from fiction, and in many cases marketing buzz from practical applications. To understand how AI-powered technologies can impact your business, it\u2019s first essential to understand how the technology is being used today to deliver value.\n\nMarketing technology has been evolving rapidly over the past decade. We have seen Content Management Systems transform into full-featured Customer Experience Platforms. Alongside this platform growth, we have seen an explosion of marketing point solutions. This sheer number of marketing systems has led to a subsequent focus on feature and data interoperability. If you have an email system, a CMS, and a CRM, it\u2019s now much easier to pull that data together, and then to find correlations and insights that can help your business. At the same time computing has advanced to the point where the algorithms and computing power needed to run neural networks or crunch machine learning models are available to any business. There is a confluence here of these two pieces \u2014 the affordability of computing power intersecting with massive amounts of data.\n\nTop tech companies including Microsoft, IBM, Alphabet (Google\u2019s parent), and Amazon (just to name a few), are in a race to develop publicly-available cognitive APIs. This category of intelligent services is collectively referred to as Machine Learning as a Service (MLaaS). It\u2019s worth noting that according to Stratistics MRC (global market research company), the MLaaS market is expected to grow to 7.6 billion dollars by 2023. These companies invest in large teams of top computer scientists, linguists, and data scientists, and the work they do provides massive value to all of their customers, allowing other companies to tap into this expertise without the cost of developing the tools themselves.\n\nThe massive tech companies are not only racing to be first-to-market to provide these services, but they also have tremendous use for these technologies internally for their operations, not to overlook the immense potential of not-yet-realized applications and enterprises. This continuing trend of ubiquitous and inexpensive cognitive services is spurring adoption, allowing software providers, systems integrators, and internal development teams to infuse advanced AI-powered features into their software for a fraction of the investment it would have taken even five years ago.\n\nWhile I\u2019m guilty of using ML and AI interchangeably, it\u2019s important to note the distinctions between them. AI is a general classification of programming machines to display characteristics of intelligent behavior or to \u201cthink.\u201d Machine learning is a subset of AI in which a system automatically learns and improves with access to data and without explicit programming. The more data provided, the more the system improves performing specific tasks.\n\nMachine learning and AI more broadly have the potential to provide exponential value to businesses over the long-term, but there is no lack of benefits in the short-term. Any organization with departments of content authors who produce large volumes of content (publishing, media, B2B, nonprofits) can benefit from the efficiency of being able to automate a variety of frequent tasks. In the content management space, this includes automated tagging of images and text or the creation of text summaries.\n\nAs far back as several years ago, several media giants including the Associated Press (AP), Fox News, and Yahoo were already using AI-powered software to generate stories and recaps. Though none of this works entirely without human involvement, the tasks AI-powered software can cut down on eliminates employee time spent on such work, availing them to take on higher-value tasks. AI is best viewed as a tool for enhancing human capabilities instead of replacing humans for the time being.\n\nAnother area where machine learning and AI are compelling is in the automation of analytical activities such as segmentation, optimization, and predictive modeling. The race to deliver this capability in off- the-shelf products and services offers the potential of unlocking customer insights that would not have been previously possible without a small army of data scientists.\n\nRecommendation engines are a great example of predictive analytics and one which we are all familiar with through the casual use of popular services. Companies like Amazon and Netflix invest heavily and train machine learning and AI models to make surprisingly relevant and accurate predictions. They have no shortage of data to pull from, given the number of variables that can be tagged, and the various dimensions that can be categorized.\n\nNetflix, for example, can make recommendations based on the description of the content itself (content-based filtering) but also on a user\u2019s previously viewed titles in relation to his or her similarity to other users (collaborative filtering). The combination of these two models known as the hybrid recommendation system is prevalent, and in the case of Netflix, allows the company to provide their customers with personalized content recommendations.\n\nOne downside of these technologies being so easy to adopt is that it becomes easier to fall into the trap of selecting a technology in search of a problem. It\u2019s essential to focus first on the business and use case that will derive the most benefit, and then enlist the right technology to solve that problem. The list below describes the categories of machine learning services that are ready for use today, and potential business use cases that they are particularly well suited to solve.\n\nThe burgeoning field of computer vision includes image, object, and facial recognition. It\u2019s possible to use inexpensive APIs to derive information such as the topic, a text summary, recognized faces, emotion or sentiment, inappropriate or lewd content, or the sex and age of individuals in the photo. If your digital media library has thousands or tens of thousands of images, this can significantly ease manual maintenance of the library, and also provide more helpful ways to search your date when retrieving the content. For example, you could easily find photos of a daytime city street with happy people walking down it.\n\nNatural Language Processing (NLP) is a branch of AI in which systems are developed to understand, interpret, and manipulate human language. Deriving concept or intent of a sentence when accounting for the ambiguities of language is a complicated problem. While this field has existed for over 40 years, factors such as a demand for stronger customer experience through human-machine interaction, advances in computing power, and access to large sets of language data has resulted in a massive boon to the field. Today this area is practically applied to voice interfaces (Alexa), chat interfaces (chatbots), and text mining applications like social media sentiment analysis or comment moderation.\n\nMany of our clients have vast reserves of content that have been meticulously tagged by humans for decades, which is a perfect data set to train a new predictive model. Right now, we are teaching Watson to auto-tag new articles based on a library of previously tagged content. We are also using off-the-shelf models from Microsoft and others to tag content with the sentiment, tone, grade-level, and other aspects relevant to content writing. This application is especially suitable for media or publishing clients.\n\nGiven the vast stores of data many companies have, machine learning and AI-powered services offer a powerful capability to surface actionable insights. By feeding data describing your content or products along with analytics collected from numerous sources and touchpoints with customers, it\u2019s possible to mine the data for correlations and patterns. The system scours the data to discover new customer segments, valuable customer journeys, or recommend content that is more likely to lead to the desired conversion. Amazon, Microsoft, and IBM are deploying a quickly-expanding toolset that allows businesses to integrate with this technology without having to learn and master complex ML algorithms. With the right data and a moderate amount of knowledge about machine learning, you can start building recommendation systems powered by similar technologies that Netflix or Amazon use to personalize content.\n\nIt\u2019s worth mentioning that every time you make an API call, it costs a fraction of a cent. Large companies are providing these cognitive services at little cost as part of their enhanced suite of cloud offerings. For the most part, you are not required to sign up for lengthy contract terms, meaning that the barrier to entry for using these technologies is extremely low.\n\nWhile there is a great deal of hype around the application of AI for improving customer experience and marketing programs, hopefully, this article has helped to show there are many practical uses of this technology available today. We\u2019re taking small, measurable steps right now, so clients can see the positive impact of these solutions for their specific business challenges. We have focused on the use of readily available and cost-effective cognitive services and machine learning APIs that clients can use to automate routine tasks, so their teams can focus on higher-value work like personalized campaigns and content.\n\nIn the next post, we\u2019ll be covering how all of this applies explicitly to challenges we see in the digital marketing space and why we\u2019re keen on helping our clients invest in this technology at this time.\n\nCorey Caplette is Velir\u2019s CTO and is responsible for guiding the technology vision and execution that has earned Velir a reputation as a trusted technology partner. With 15 years of experience in digital, he has built a reputation of bringing a thoughtful and strategic approach to complex technical problems with particular expertise in software development, systems architecture, enterprise content management, and marketing technology integration. Corey oversees Velir\u2019s Solution Architecture, Development, Production Support, Managed Services and IT teams. Corey joined us from Fidelity Investments where he worked as a technologist in the Research and Development group for the Fidelity Center of Applied Technology.\n\nWe are a fully-integrated digital agency based right outside of Boston, in Somerville MA. With a team of over 130 digital professionals, we are best-known for our digital work for some of the world\u2019s largest and most influential non-profit organizations, membership associations, and Fortune 500 enterprises. Our capabilities in marketing technology and data visualization are recognized as the best in the industry, stemming from a 17-year focus on digital strategy, creative design, and web engineering and application development. We have completed complex implementations and large-scale builds for brands including Bayer, the Robert Wood Johnson Foundation, the Metropolitan Museum of Art, AARP, Yale, and Informa. We have been honored with the recognition of \u201cSmall Business of the Year\u201d by both the Greater Boston Chamber of Commerce and the Somerville Chamber of Commerce, and have offices in Boston and Washington DC. For more information, please visit www.velir.com."
    },
    {
        "url": "https://towardsdatascience.com/speeding-up-convolutional-neural-networks-240beac5e30f",
        "title": "Speeding up Convolutional Neural Networks \u2013",
        "text": "Before diving into this method, be aware that it\u2019s extremely dependent upon how the Separable Convolutions where implemented in a given framework. As far as I am concerned, TensorFlow might have some specific optimizations for this method while for other backends, like Caffe, CNTK or PyTorch it is unclear.\n\nThe idea is that instead of convolving jointly across all channels of an image, you run a separate 2D convolution on each channel with a depth of . The intermediate channels get concatenated together, and mapped to using a 1x1 convolution.[5] This way one ends up with significantly fewer parameters to train.[2]\n\nIt\u2019s not so simple tho. Beware that Separable Convolutions sometimes aren\u2019t training. In such cases, modify the depth multiplier from 1 to 4 or 8. Also note that these are not that efficient on small datasets, like CIFAR 10, moreover on MNIST. Another thing to keep in mind, don\u2019t use Separable Convolutions in early stages of the network.\n\nThe factorization scheme showed above work well in practice, but are quite simple. They work but are by far not the limit of what\u2019s possible. There are numerous works, including [3] by V. Lebedev et al. that show us different tensor decomposition schemes that drastically decrease the number of parameters, hence the number of required computations.\n\nInspired by [1] here\u2019s a code snippet of how to do CP-Decomposition in Keras:\n\nIt doesn\u2019t work, regretfully, but it gives you the intuition of how it should look like in code. Btw, the image at the top of the article is the graphical explanation of how CP-Decomposition works.\n\nShould be noted such schemes as TensorTrain decomposition and Tucker. For PyTorch and NumPy there\u2019s a great library called Tensorly that does all the low-level implementation for you. In TensorFlow there\u2019s nothing close to it, still, there is an implementation of TensorTrain aka TT scheme, here.\n\nThe full code is currently available as a Colaboratory notebook with a Tesla K80 GPU accelerator. Make yourself a copy and have fun tinkering around with the code.\n\nIf you\u2019re reading this, I\u2019d like to thank you and hope all of the above written will be of great help for you, as it was for me. Let me know what are your thoughs about it in the comments section. Your feedback is valuable for me.\n\nAlso, don\u2019t forget to clap if you liked the article \ud83d\ude0f or even follow me for more articles like this.\n\n[1] https://medium.com/@krishnatejakrothapalli/hi-rain-4e76039423e2\n\n[2] F. Chollet, Xception: Deep Learning with Depthwise Separable Convolutions, https://arxiv.org/abs/1610.02357v2\n\n[3] V. Lebedev et al, Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition, https://arxiv.org/abs/1412.6553\n\n[4] C. Szegedy et al, Rethinking the Inception Architecture for Computer Vision, https://arxiv.org/pdf/1512.00567v1.pdf\n\n[5] https://stackoverflow.com/questions/37092037/tensorflow-what-does-tf-nn-separable-conv2d-do#37092986\n\n[6] S. Zagoruyko and N. Komodakis, Wide Residual Networks, https://arxiv.org/pdf/1605.07146v1.pdf"
    },
    {
        "url": "https://towardsdatascience.com/infographic-marketing-10-small-things-that-can-make-a-big-difference-92f18951f54a",
        "title": "Infographic Marketing: 10 Small Things That Can Make a Big Difference",
        "text": "Humans are visual creatures. That could probably explain why guestographics have become one of the most effective, viral marketing techniques employed by digital marketers of late.\n\nSo, what\u2019s a guestographic? Brian Dean of Backlinko defines guestographics as a combination of three powerful digital marketing tactics: infographics, guest posting and link-building. He even calls it one of his all-time \u201cfavorite\u201d SEO strategies.\n\nAs an SEO strategy, using guestographics involves repurposing and republishing your infographic as a guest post on authoritative sites \u2014 hence, the term \u201cguestographics.\u201d And by simply writing unique intros to go along with your guestographics, you can get these visuals published on multiple sites with minimal effort.\n\nNeed some inspiration and tips for executing your own guestographics campaign? Here are five case studies of guestographics done right.\n\nYou can view the visual summary of this post below or click here to read a detailed explanation of each tip for promoting guestographics.\n\nYou can imagine how challenging it is to generate buzz for a topic as boring as pest control. But SEO strategist Mike Bonadio did it via a cleverly strategized guestographic campaign, and his unlikely success was so interesting that it was published on Backlinko.\n\nBelow are the results of his campaign:\n\nHere is a partial screenshot of the guestographic he used:\n\nAnd here\u2019s a brief overview of how Mike did it:\n\nWhen Mike launched his guestographics promotion, he tested two types of outreach. One was a soft-sell \u201cfeeler\u201d outreach, in which he gauged the receiver\u2019s interest before sending his infographic over.\n\nThe other was a more direct pitch, in which he immediately shared his infographic with the receiver.\n\nThe \u201cfeeler\u201d email got a 40% response rate compared to the direct pitch email, which only got 16%, so it was good to know for future campaigns.\n\nTo prospects who responded favorably, Mike sent an email which contained a link to the guestographic and an offer to write a customized intro to go with it.\n\nHere\u2019s how Mike used branded anchor text for his guestographics intro:\n\nSince then, Mike\u2019s infographic has gone viral and shared on sites like Flipboard and Pinterest. Needless to say, his pest control company client definitely got their much-needed traffic boost, and all this from a well thought-out guestographics strategy!\n\nWhen Perrin Carrell of HerePup decided to launch a pet blog, he knew he was entering a populated niche already dominated by blogs such as PetMD.com. Still, he persevered.\n\nBy executing a guestographics campaign, Perrin got a significant leg up, which led to the following results:\n\nBelow is a partial screenshot of the infographic:\n\nAnd here\u2019s how Perrin used guestographics to get ahead of his competitors:\n\nIn Perrin\u2019s case, he identified that the top content could be much improved with a lot more research and design, so he set about creating even better content than the ones he found from more influential sources.\n\nAs a result of adding more engaging media in the form of a visually stunning infographic and promoting it to relevant sites, Perrin was able to break through and stand out from his more established competitors.\n\nControversy can be great publicity. Take this guestographic case study featured on the Ahrefs blog, for example.\n\nThe infographic, titled \u201cWhat Happens One Hour After Drinking A Can Of Coke,\u201d was originally published on The Renegade Pharmacist. It was based on an 11-year-old article by Wade Meredith about how Coke can wreak havoc on your system within an hour of drinking it.\n\nBelow is a screenshot of the infographic:\n\nHere\u2019s how well this infographic performed:\n\nSo, how did they do it?\n\nApparently, this infographic has the perfect combination of the following ingredients which resulted in it going viral according to Ahrefs:\n\nHere\u2019s how each attribute, in combination with others, influenced the results:\n\nWordstream has successfully launched several infographics, such as \u201cGoogle\u2019s Biggest Flops & Failures\u201d and their most recent infographic titled \u201cWhere\u2019s Google Making Its Money: Top 20 Most Expensive Keywords in Google Adwords Advertising.\u201d\n\nWordstream wrote a case study which outlined how they were able to achieve success with their infographics. It is worth noting that this was made back in 2010, but there is a lot that you can learn from it. Here are the amazing results of their guestographic campaign:\n\nAlthough Wordstream has admitted that infographics do not have the same viral effect as they did in 2010, a quick search of their latest blog post containing the \u201cWhere\u2019s Google Making Its Money\u201d guestographic still displays these awesome results:\n\nWhen known online marketers preach about an SEO strategy, you can bet that it works \u2014 most of the time.\n\nIn one case study, Neil published an infographic on his blog that, at first, barely received any attention.\n\nBelow is a screenshot of said graphic:\n\nAccording to Neil, his infographic did not get much traffic when he first published it on his blog.\n\nBut when he contributed it to Entrepreneur Magazine, that\u2019s when the magic happened. His guestographic ended up generating over 12K shares!\n\nWhat Neil learned from reviving his underrated guestographic:\n\nSo there you have it: four great case studies that prove the power of guestographics.\n\nAs you can see, guestographics are one of the best strategies that you can execute to give your site a much-needed boost!\n\nAnxious to get on the bandwagon, but don\u2019t have the resources? Then, I recommend you to try out Visme which lets you make infographics in minutes and with no design skills required. Great, right?\n\nLack inspiration? Then go ahead and check this massive list of 101 best Infographic Examples on 19 Different Subjects.\n\nSo, how has guestographics helped your blog to grow? Let me know in the comments below!"
    },
    {
        "url": "https://towardsdatascience.com/back-to-basics-deriving-back-propagation-on-simple-rnn-lstm-feat-aidan-gomez-c7f286ba973d",
        "title": "[ Back to Basics ] Deriving Back Propagation on simple RNN/LSTM (feat. Aidan Gomez)",
        "text": "There are two things to note here\u2026.\n\n1. Green Line \u2192 Please remember the derivative of Tanh() can be rewritten as such. If you do not remember why, please scroll up.\n\n2. I did not perform derivative respect to i(2), and f(2). The reason is because those two terms have very similar derivative structure with a(2). I\u2019ll try to explain why in details below.\n\nIf we observe how state(t) is calculated we can see that the terms a(t), i(t), f(t) and state(t-1) are involved. So when we take the derivative respect to the variable a(t), we can know that it would be very similar to taking derivative respect to i(t). However, there is one term that we need to take a more deeper look into and that is o(t). Since, the term gets used after we calculate state(t) the derivative is also different.\n\nWe can clearly see that there are some difference between the derivative equation when compared to a(2).\n\nWith those ideas in-mind we can see that deriving back-propagation at time stamp 2 is not that hard, since it is the most outer layer of our LSTM."
    },
    {
        "url": "https://towardsdatascience.com/how-to-build-a-gesture-controlled-web-based-game-using-tensorflow-object-detection-api-587fb7e0f907",
        "title": "How to build a Gesture Controlled Web based Game using Tensorflow Object Detection Api",
        "text": "Control the game paddle by waving your hand in from of a web cam.\n\nWith the TensorFlow object detection api, we have seen examples where models are trained to detect custom objects in images (e.g. detecting hands, toys, racoons, mac n cheese). Naturally, an interesting next step is to explore how these models can be deployed in real world use cases \u2014 for example, interaction design.\n\nIn this post, I cover a basic body-as-input interaction example where real time results from a hand tracking model (web cam stream as input) is mapped to the controls of a web-based game (Skyfall). The system demonstrates how the integration of a fairly accurate, light weight hand detection model can be used to track player hands and enable realtime body-as-input interactions.\n\nWant to try it out? Project code is available on Github.\n\nUsing parts of the human body as input has the benefit of being always available as the user is not required to carry any secondary device. Importantly, appropriating parts of the human body for gesture based interaction has been shown to improve user experience [2] and overall engagement [1]. While the idea of body as input is not entirely new, existing approaches which leverage computer vision, wearables and sensors (kinect, wii, [5]) etc sometimes suffer from accuracy challenges, are not always portable and can be challenging to integrate with 3rd party software. Advances in light-weight deep neural networks (DNNs), specifically models for object detection (see [3]) and key point extraction (see [4]) hold promise in addressing these issues and furthering the goal of always available (body as) input. These models allow us track the human body with good accuracy using 2D images and with the benefit of easy integration with a range of applications and devices (desktop, web, mobile). While tracking from 2D images does not give us much depth information, it is still surprisingly valuable in building interactions as shown in the Skyfall game example.\n\nSkyfall is a simple web based game created using planck.js \u2014 a 2D physics engine. The play mechanism for SkyFall is simple. 3 types of balls fall from the top of the screen in random order \u2014 white balls (worth 10 points), green balls (worth 10 points) and red balls (worth -10 points). Players earn points by moving a paddle to catch the good balls (white and green balls) and avoid bad balls (red balls). In the example below, the player can control the paddle by moving the mouse or by touch (drag) on a mobile device.\n\nThis is a pretty simple and fun game. However, we can make it even more engaging by allowing the user control the paddle using their body (hand). The goal is to accomplish this by detecting hand position using web cam video stream\u2014 no additional sensors or wearables.\n\nTo add gesture interaction, we replace the mouse controls above with a system that maps the movement of the players hand to the game paddle position. In the current implementation, a python application (app.py), detects the player\u2019s hand using the TensorFlow object detection api, and streams hand coordinates to the game interface \u2014 a web application served using FLASK \u2014 over websockets.\n\nIn a previous post, I covered how to build a real-time hand detector using the Tensorflow Object detection api. Please see the blog post to learn more on how the hand tracking model is built. For any errors or issues related to loading the hand model, please see the hand tracking Github repo and issues. This example follows a similar approach where a multi threaded python app reads web cam video feed and outputs bounding boxes for each hand detected.\n\nNote that hand detection is done on a frame-by-frame basis and the system does not automatically track hand across frames. However, this type of inter-frame tracking is useful as it can enable multiple user interaction where we need to track a hand across frames (think a bunch of friends waving their hands or some other common object, each controlling their own paddle). To this end, the current implementation includes naive euclidean distance based tracking where hands seen in similar positions across frames are assigned the same id.\n\nOnce each hand in the frame is detected (and a tracking id assigned), the hand coordinates are then sent to a web socket server which sends it out to connected clients.\n\nThe game interface connects to the web socket server and listens for hand detection data. Each detected hand is used to generate a paddle, and the coordinate of the hand in the video frame is used to relatively position the paddle on the game screen.\n\nThere are several limitations with the current implementation \u2014 so contributions, pull requests are most welcome!\n\nHand Detector Improvement\n\nThis entails collecting additional training data and leveraging data augmentation strategies to improve the hand detector. This is important as the entire interaction (and user experience) depends on accurate and robust hand tracking (false positives, false negatives make for bad UX).\n\nThe current implementation uses a simple euclidean based metric to track hands across frames (hand in current frame is identified based on its distance from hands in previous frames). With several overlapping hands, things can get complicated\u2014 a more robust tracking algorithm is required. Perhaps integrating a fast tracking algorithm from OpenCV or other sources \u2026\n\nTensorflowjs implementation \n\nConduct some experiments with a TensorFlowjs implementation that allows the entire interaction to be prototyped completely in the browser!!! Tensorflowjs brings so many benefits \u2014 easy deployment (no python or Tensorflow installation), no websocket servers and clients, easy reproducibility, more potential users \u2026\n\nThere are existing projects that apply machine learning models in designing interactions. A common example is the use of LSTMs to autocomplete text and create fast email replies (see the Smart Reply paper by Google), and the more recent experiments where image classification models are used as game controls (see Teachable Machines and other Tensorflowjs demos by Google Pair). This work builds on these trends by exploring the use of object detection models in creating interactions.\n\nAs AI algorithms continue to mature (accuracy, speed), there is potential to leverage these advances in building better interactions. This could be generative interfaces that tailor content to the user or predict their intended interaction, models that enable new types of vision-based interactions, conversational UI etc. Across these areas, it is increasingly important to study the mechanics of such interactions, rigorously test these interactions and and create design patterns that inform the use of AI models as first class citizens in interaction design.\n\nGot feedback, comments, want to collaborate? Feel free to reach out \u2014 twitter, linkedin.\n\nNote: An earlier version of Skyfall was submitted in January 2018 to the NVIDIA Jetson Challenge.\n\n[1] Shafer, D. M., Carbonara, C. P., and Popova, L. 2011. Spatial presence and perceived reality as predictors of motion-based video game enjoyment. Presence: Teleoperators and Virtual Environments 20(6) 591\u2013619.\n\n[2] Birk, M., and Mandryk, R. L. 2013. Control your game-self: effects of controller type on enjoyment, motivation, and personality in game. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems \u2014 CHI \u201913 685\u2013694.\n\n[3] Huang, J., Rathod, V., Sun, C., Zhu, M., Korattikara, A., Fathi, A., Fischer, I., Wojna, Z., Song, Y., Guadarrama, S., and Murphy, K. 2017. Speed/accuracy trade-offs for modern convolutional object detectors. CVPR\n\n[5] Harrison, C., Tan, D., and Morris, D. 2010. Skinput: appropriating the body as an input surface. Proceedings of the 28th international conference on Human factors in computing systems \u2014 CHI \u201910 453."
    },
    {
        "url": "https://towardsdatascience.com/is-it-high-time-for-data-democratization-2e4e16a5356e",
        "title": "Is it High Time for Data Democratization? \u2013",
        "text": "One of the ways to achieve a competitive edge is by making data increasingly accessible. This is as opposed to having data analysts and scientists being the only ones with the ability to understand company data. At the end of the day, these specialists do not come cheap.\n\nAs would be expected, there are both advantages and disadvantages that come with data democratization. These will be discussed below.\n\nFor quite some time, many shared a conservative view on data management, believing that rigorous analysis of data should be left to digital analysts and data scientists. Strict governance would also be applied to the gathering, interpretation, and dissemination of data. This way, accountability would reside within the data science group tasked with handling the data.\n\nThere are various reasons for people to believe that access to information should be restricted. For instance, they may think that it would ensure tighter control over potentially risky data such as profiles, confidential records, and information that gives firms a distinct reputational or competitive advantage. Additionally, it is thought that restrictions ensure greater scrutiny when making recommendations since data specialists analyze problems using advanced modeling techniques and multiple data sets.\n\nData democratization allows access to multiple data sources and data consuming groups thereby presenting multiple versions of the \u2018truth,\u2019 a factor that may lead to loss of confidence and fewer recommendations. Access to data by individuals in different teams within a firm can also lead to expensive redundant functions. Privacy concerns also arise since less governance may lead to the violation of privacy as well as regulation controls.\n\nOver the recent past, however, companies have realized the importance of having a collaborative and holistic approach to data. This means, everybody within the organization should have an understanding of the data enough to make thought-out decisions. Additionally, proponents of data democratization believe that accountability resides with the user of the data.\n\nAdvancements in the management and access of data such as data democratization increase users\u2019 ability to manage, transform, and interpret the data. Those with access to the said data also get a chance to add their input, providing for consumption and analysis by different analytics and business intelligence platforms. Moreover, the distribution of information to individuals in different working teams promotes entrepreneurial spirit and can be particularly beneficial for small and medium-sized enterprises since they are more agile and less regulated than large enterprises.\n\nDemocratization can also help data-savvy stakeholders in SMBs to reduce their dependence on centralized analysis teams. Through access to data, SMBs can also identify potentially profitable business opportunities that they may have been missing. Additionally, they can drive down the costs of customer acquisition and make fact-based investment decisions, by making use of the data to effectively eliminate guesswork.\n\nThere have been numerous advancements in technology designed to allow non-specialists understand data much better. These innovations are also in place to help make big data more manageable. Here, we are talking about cloud storage, data federation software, and data visualization software.\n\nCloud storage is one of the tools that have aided the adaptation of data democratization. With a central location accessible online, all authorized users may use the data stored there to make informed decisions. Although there are security risks that are associated with cloud services, they can be minimized with encryption protocols.\n\nData democratization is also greatly facilitated by data federation software. This software aids in the combination of data from different sources, and storing them in a virtual database. The data may then be used to for analysis of business intelligence and others. Instead of having data in the virtual database created, there is metadata concerning the data and where it is located.\n\nData visualization software can be used to retrieve and manipulate data by individuals who do not understand technical details. It converts textual and numeric data into tables, figures, and visual charts. For instance, an enterprise can use the software to present visitor activity.\n\nDecisions within an organization are directly influenced by the data at hand. Having business analysts, marketers, and executives among other employees go through the data specialists has been the norm over the years. As it is, there are those who believe that nothing should change while the rest believe it is time that data accessibility was heightened. With all the software solutions that propel data democratization, it\u2019s highly probable that the latter will win."
    },
    {
        "url": "https://towardsdatascience.com/why-data-science-careers-are-here-to-stay-43b08995cf2f",
        "title": "Why Data Science Careers Are Here to Stay \u2013",
        "text": "Why Data Science Careers Are Here to Stay\n\nWhen people either ponder the career paths that interest them initially or think about moving into another field, they often look at statistics related to earnings, the health of the job market and the likelihood of ongoing opportunities for work.\n\nStatistics released in 2017 by IBM indicate there is a substantial demand for data scientists, and that the desire for them will climb 28 percent by 2020. The most job openings will exist in the finance and insurance industries.\n\nFor several reasons, data scientist jobs have taken off in the past couple of years.\n\nEven better for people in the job market with data science training, the momentum shows no signs of slowing.\n\nEvery time a person posts a status update on Facebook, tags a friend on Twitter, searches for something on Google or shares snapshots on Instagram, they add to the vast amount of data that exists, and that\u2019s just for starters.\n\nIt\u2019s understandable, then, that companies are not content with merely collecting data.\n\nThey want to study it and learn the context surrounding it to enjoy a greater understanding of their customers. Data scientists can help them achieve those things.\n\nHowever, analysts point to the prominent shortage of qualified data science candidates. Statistics indicate it takes up to eight days longer to fill some data-related positions compared to others.\n\nA lack of people available to fill roles will only become more evident as companies start paying more attention to data.\n\nThe people who decide to get the training and experience required for data science positions now should be able to tap into the skills shortage and assert themselves as ready to fill it.\n\nWhen companies fail to look for trends within data continuously, they may miss out on chances to boost their profits. Even worse, they could make mistakes that hinder profitability and damage reputations.\n\nData scientists can help business leaders become increasingly proactive in connecting data trends to the possibilities for boosted profits. Often, it\u2019s helpful if the people who work with data have intimate understandings of the companies for which they work.\n\nThat\u2019s why many companies hire data scientists from within. A PwC study from January 2017 shows that 59 percent of companies want data scientist candidates to have college degrees and at least three years of experience.\n\nThey also look for competent people who are willing to be flexible and expand their existing skills.\n\nRemaining profitable requires companies to display an ability to adapt, and the same is true for the employees working for those businesses.\n\nThe data scientists who are the most successful in this competitive field will be those who can adjust to change while always keeping an eye on the factors that determine a company\u2019s profits.\n\nProfitability is a goal that organizations have regardless of short-term trends or brief changes in focus.\n\nThus, when companies hire individuals who can examine data to keep them profitable, those people do not have to worry that their jobs might become obsolete.\n\nGlassdoor chose Data Scientist as its top career for 2018 due to factors like the $110,000 median base salary and higher than average job satisfaction levels compared to other fields.\n\nConcerning career choices, the IT field as a whole is a sector with exciting options that are characterized by ongoing job growth and attractive salaries.\n\nAs well-known companies advertise that they want to add more data scientists to their teams and let candidates know they\u2019ll be playing crucial roles in business forecasting, people looking for jobs now or soon will feel even more assured that the data science field offers promising career paths.\n\nIt\u2019s also important to realize that a person who wishes to work in computer science has numerous ways to grow their career. For example, if an individual has a passion for something \u2014 whether it\u2019s farming or artificial intelligence \u2014 that person can look for ways to apply data analysis to that interest.\n\nNot so long ago, some industries weren\u2019t even considering using data science to achieve their aims. Now, representatives from most of those sectors realize there\u2019s a place for analysis and want data scientists to help them find it.\n\nMoreover, even people who don\u2019t major in computing frequently end up working in a related role eventually.\n\nIf job candidates show they\u2019re willing to be versatile, they\u2019ll have better chances of working in positions they love that provide lucrative salaries.\n\nThere\u2019s a reality that applies to every industry: It\u2019s that forecasting capabilities equip companies to stop relying on gut instincts and refer to collective information instead.\n\nWhen that happens, business leaders can feel more confident in the worth of their forecasts, making it easier for stakeholders to trust them and invest in their establishments."
    },
    {
        "url": "https://towardsdatascience.com/are-we-aware-of-the-opioid-crisis-a-data-exploration-of-opioid-abuse-part-1-b6d3e6d8b062",
        "title": "Are We Aware of the Opioid Crisis? A Data Exploration of Opioid Abuse \u2014 Part 1",
        "text": "Living in Canada, one can often here warnings of opioid overdose and the importance of packing Naloxone kits as a part of your First Aid kit on the radio or on TV. In fact, the effects of the opioid crisis are getting so severe that one can also view advertisements and warning around different social media platforms when browsing the internet. Naloxone is the main drug that will reverse the effects of an opioid overdose such that the individuals does not have to worry about reaching the emergency room too late. Although, what exactly are opioids and is the opioid crisis really a \u201ccrisis\u201d?\n\nOpioids are a group of drugs that are painkillers for severe pain. Some well-known opioids are morphine, codeine, heroin, and fentanyl that is rising in its popularity. When looking at how the science of these painkillers work, an opiate is like any foreign chemical substance would affect the brain. The opioid \u2014 let\u2019s say morphine for example, travels through the bloodstream and into the brain where is attaches to receptors on the brain that would be responsible for emitting a chemical signal to the brain\u2019s reward center \u2014 or a section of your brain known as the Nucleus Accumbens. When the reward center releases dopamine, the \u201cpleasure\u201d signal, we have a positive reaction to what is happening in our body. Although, this is where the problem with opioids comes into play. Drugs such as morphine, fentanyl, and heroin, will release dopamine in excess amounts, more than what is needed, to help us survive or provide that small amount of pleasure. With repeated usage of opioids, it becomes easier for someone to want repetitive dopamine release, and this is where an opioid addiction may form. In addition to this, individuals who have a predisposition to depression, and anxiety are more likely to become addicted to opioids. It is often found that people who take opioids and do not have a history of depression, anxiety or other mental illness, are less likely to become addicted to prescription opioids.\n\nPart of opioid crisis blame is given to the over usage of prescription drugs that are being given out to unsuspecting patients. In order to strive for a pain free post-surgery experience, individuals may not have known that they may get addicted to some opioids.\n\nDrugs such as fentanyl and heroin are so potent that they have the strength to not only cause physical side effects but mental ones as well. Individuals who abuse these drugs have dedicated their life to finding opiates for the cheapest price. While users know that there is no positive effect in their life, individuals still insist on buying and using these drug. This epidemic, which had started in Southern Ontario, has slowly progressed to major cities such as Mississauga and Toronto. Opioids are the most powerful killers in Ontario today, it is important to understand them and make progress to try to get them off the streets of Ontario.\n\nRegarding our neighbours to the south, in 2017, U.S. President Donald Trump declared opioid abuse a national crisis. With many areas around America being affected by this crisis. In East Liverpool, Ohio, many neighbourhoods are overrun with opioid pills being sold like candy. Individuals exclaim that sitting on their porch is now impossible as they may run into someone either selling or using drugs right in front of them.\n\nFigure 1 shows us a significant increase in opioid usage, from American data, taken from DataUSA.io. A simple analysis done in Microsoft Excel shows us that the rate of opioid related deaths is increasing at a rate of 0.56 per year with the highest increases starting in 2009.\n\nMoving on, I then proceeded to explore states at random and settled at three which which could give us some insight into regions that are most affected. Firstly, since I had done research on an article based out of Ohio, I made sure to choose Ohio, then New York as it is quite diverse and densely populated, and lastly Massachusetts as a random choice.\n\nFigure 2 shows us the average death rate for the three states. We can see that Massachusetts has the highest death rate average among the three states. On average, in an age adjusted population, around 11 individuals are dying per year in this state. Why is that so? The National Review has some insight into this data.\n\nIn early 2018, it is reported that almost one in four individuals from Massachusetts personally lost someone due to a fatal opioid overdose. Residents of Massachusetts are so aware of the problem that is occurring in their state, that 7 in 10 residents find this crisis to be a \u201cvery serious problem\u201d \u2014 out-weighting concerns about tax reform or health-care costs. One of the main concerns is that this opioid epidemic is due to an over prescription of painkillers by doctors and hospitals, and another part is that the motivation for a quick recovery has decreased significantly. Since opioid addiction is co-morbid with depression, mental health awareness that comes post surgery may be decreased.\n\nSeeing an upward trend in the opioid overdose rate in USA, we then shift our focus to North of the border \u2014Canada. What do we see with this opioid trends? Are the growing at a similar rate? Is there an increase or decrease in opioid related deaths and hospitalizations in Canada?\n\nAccording to the Canadian Institute for Health Information site, around 16 Canadians are hospitalized every day for opioid poisoning. While the overall population death rate was unavailable \u2014 hospitalization rate gives us an insight on how many people are affected by an overuse of opioids on a daily basis.\n\nThe Canadian Institute for Health Information site articulates that seniors had some of the highest rate of usage of opioids, while the rates in young adults is rapidly growing as well. One of the main offenders is prescriptions drugs that are either used for too long after a diagnosis, or, prescriptions drugs not being returned back to the pharmacy to avoid younger children from using them.\n\nAs Figure 3 shows us, the rate of opioid poisoning is increases at a rate of 0.59 per year. Around 59% of Canada\u2019s population is being hospitalized for opioid related poisoning. It is unclear whether this is due to recreational drug use or prescription drugs.\n\nOne of the most surprising observations that we can make from this data is it\u2019s similarity to the American trends. While the parameters are different, we can still see an increase in both countries that opioid abuse and poisoning is in fact increasing, almost at a very similar rate as well. In the following articles, it would be beneficial to understand whether these statistics are significant in any way, and if we can make any prediction using this model.\n\nFinally, similarly to the American data-set, I proceeded to then choose three provinces at random, and as well the territories that were represented as a single group (due to the low population).Figure 4 shows us the average hospitalization rate in each province. We find that British Columbia has the highest rate of opioid poisoning, with Alberta and Ontario following suite. BC sits at a hospitalization rate of 18.85 individuals per 100 000 people on average, while Ontario is at 11.58, and Alberta and the Territories is at 15.9 and 17.03 respectively.\n\nNext, let us look at some demographic information about the opioid growth male vs. female, and how age is a factor.\n\nFirstly, we see from Figure 5 that the highest opioid poisoning is in individuals over 65+. This goes back to the point that when opioids are prescribed for something such as pain management and/or post-surgery recovery, people do not often realize how easy it can be to become addicted to something that is seemingly helpful. We see an over prescription of opioid drugs to seniors, and then shifting our focus, we also see a rapid increase in opioid usage of youths age 15\u201324 at a rate of 0.7964 per year. Research says that this is in part due to recreational drug use by youths due to opioid prescriptions that are not kept in a safe space, improper disposal of drugs, and addiction due to mental health issues in the population.\n\nFigure 6 articulates the gender demographic of opioid usage in Canada. We see similar increase trends in both male and female users. Although, it can be observed the females are more likely to be hospitalized due to opioid poisoning. The prediction here is that female users are more likely to seek out help and go into medical care for their symptoms than males are. Although, around 2016\u20132017, we see a converging in the male and female cases we see in the the demographic. Perhaps, due to the high volume of opioid users, more and more individuals have no choice but to go into hospital. More research and analysis will hopefully tell us more.\n\nIn part two of this series, I will be taking a closer look at data regarding Ontario and Massachusetts data. The choice of choosing Ontario is biased as I live in that province, however, we can see from the data above that it would be beneficial to also examine British Columbia and Alberta with a closer look.\n\nThe next article will look at the statistics more in depth, and try to predict how the opioid trend looks if we go the path that we are on at the moment. This is where I will try to apply python predictive modelling to the data, and aim to view the data from a more narrow perspective. I hope that you will join me, as this topic is very important and and the same time affects many people in North America.\n\nThank you for reading and see you in part two."
    },
    {
        "url": "https://towardsdatascience.com/generating-pixelated-images-from-segmentation-masks-using-conditional-adversarial-networks-with-6479c304ea5f",
        "title": "Generating Pixelated Images from Segmentation Masks using Conditional Adversarial Networks with\u2026",
        "text": "Red Line \u2192 How each layer is composed of it is made up of Convolution Batch Normalization and finally Relu()\n\nSo from above image we can already see that each of the layer for this network is composed of three operations, and when implemented in Tensorflow it would look something like below.\n\nThe reason why there is a if condition for batch normalization is due to the fact that the first layer for generator does not have a batch normalization layer."
    },
    {
        "url": "https://towardsdatascience.com/whats-new-in-deep-learning-research-microsoft-wants-to-use-generative-adversarial-networks-with-1838f0b15741",
        "title": "What\u2019s New in Deep Learning Research: Microsoft Wants to Use Generative Adversarial Networks with\u2026",
        "text": "Generative models are the subdiscipline of deep learning that focuses on the creation of statically accurate target data. In deep learning research, we can find different groups of generative models such as Boltzmann Machines or Directive Generative Nets. However, the popularity prize when comes to generative models in the last few years undoubtedly goes to generative adversarial networks(GANs). In just a few years, GANs have established themselves as the go to models for mission critical deep learning scenarios such as image generation.\n\nDespite its popularity, most GAN techniques today are only effective when working with continuous data. This is because GAN models, as normally formulated, rely on the generated samples being completely differentiable and thus do not work very well for discrete data. Recently, researchers from Microsoft proposed a new technique called Boundary-Seeking-GANs that enables the training of GAN models using discrete data.\n\nFor all the buzz around GANs, the generative method is a relative new technique which was proposed by deep learning Ian Goodfellow in a research paper in 2014. The main idea behind GANs is to create networks that involve two fundamental models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake.\n\nGANs are very effective when working with continuous data because the composition of the generator and discriminator is fully differentiable which beans that both networks can be trained using an algorithm like back-propagation. However, this is not true in the case of discreate data because those distributions have zero gradient nearly everywhere (and is otherwise infinite), so it is not possible to use back-propagation alone to train the generator.\n\nNow that we understand the challenges of using GAN models with discrete data the next obvious question is why is this relevant at all? It turns out that discreate GAN models that key to a large number of natural language processing scenarios such as machine translation or caption generation. Similarly, discrete generation is used in other domains such as image segmentation as it should avoid the overfitting challenges of traditional models.\n\nMicrosoft\u2019s idea behind Boundary-Seeking GANs is to introduce a new policy gradient that works effectively with discreate data. As mentioned before, GANs only work when the value function is completely differentiable. With discreate data, the gradients that would otherwise be used to train the generator of discrete variables are zero almost everywhere, so it is impossible to train the generator directly using the value function. However, what happens if we change the gradient?\n\nBoundary-Seeking GANs simply introduces a dual gradient method that works effectively with discrete data distributions. Initially, Boundary-Seeking GANs use a policy gradient based on the KL-divergence which uses the importance weights. The technique then combines that gradient with a lower-variance gradient which defines a unique reward signal for each z and prove this can be used to solve our original problem. The result is a GAN model in which the discriminator can be used to formulate importance weights which provide policy gradients for the generator.\n\nThe Microsoft team put the Boundary-Seeking GAN method to test with different discrete data generation scenarios. One of those experiments used the famous MNIST and CelebA datasets. In the case of the MNIST experiment, the results quantitatively outperform competing methods such as WGAN-GP. The following figure shows how the algorithm was able to produce realistic and highly variable generated handwritten digits.\n\nSimilarly, in the CelebA experiment, the generator trained as a BGAN produced reasonably realistic images which resemble the original dataset well and with good diversity"
    },
    {
        "url": "https://towardsdatascience.com/the-4-recommendation-engines-that-can-predict-your-movie-tastes-109dc4e10c52",
        "title": "The 4 Recommendation Engines That Can Predict Your Movie Tastes",
        "text": "The 4 Recommendation Engines That Can Predict Your Movie Tastes\n\nRecommendation systems are used not only for movies, but on multiple other products and services like Amazon (Books, Items), Pandora/Spotify (Music), Google (News, Search), YouTube (Videos) etc.\n\nAn example of recommendation system is such as this:\n\nHave you ever had to answer this question at least once when you came home from work? As for me \u2014 yes, and more than once. From Netflix to Hulu, the need to build robust movie recommendation systems is extremely important given the huge demand for personalized content of modern consumers.\n\nIn this post, I will show you how to implement the 4 different movie recommendation approaches and evaluate them to see which one has the best performance.\n\nThe dataset that I\u2019m working with is MovieLens, one of the most common datasets that is available on the internet for building a Recommender System. The version of the dataset that I\u2019m working with (1M) contains 1,000,209 anonymous ratings of approximately 3,900 movies made by 6,040 MovieLens users who joined MovieLens in 2000.\n\nAfter processing the data and doing some exploratory analysis, here are the most interesting features of this dataset:\n\nBeautiful, isn\u2019t it? I can recognize that there are a lot of movie franchises in this dataset, as evidenced by words like II and III\u2026 In addition to that, Day, Love, Life, Time, Night, Man, Dead, American are among the most commonly occurring words.\n\nIt appears that users are quite generous in their ratings. The mean rating is 3.58 on a scale of 5. Half the movies have a rating of 4 and 5. I personally think that a 5-level rating skill wasn\u2019t a good indicator as people could have different rating styles (i.e. person A could always use 4 for an average movie, whereas person B only gives 4 out for their favorites). Each user rated at least 20 movies, so I doubt the distribution could be caused just by chance variance in the quality of movies.\n\nHere\u2019s another word-cloud of the movie genres:\n\nThe top 5 genres are, in that respect order: Drama, Comedy, Action, Thriller, and Romance.\n\nNow let\u2019s move on to explore the 4 recommendation systems that can be used. Here they are, in respective order of presentation:\n\nThe Content-Based Recommender relies on the similarity of the items being recommended. The basic idea is that if you like an item, then you will also like a \u201csimilar\u201d item. It generally works well when it\u2019s easy to determine the context/properties of each item.\n\nA content based recommender works with data that the user provides, either explicitly movie ratings for the MovieLens dataset. Based on that data, a user profile is generated, which is then used to make suggestions to the user. As the user provides more inputs or takes actions on the recommendations, the engine becomes more and more accurate.\n\nThe concepts of Term Frequency (TF) and Inverse Document Frequency (IDF) are used in information retrieval systems and also content based filtering mechanisms (such as a content based recommender). They are used to determine the relative importance of a document / article / news item / movie etc.\n\nTF is simply the frequency of a word in a document. IDF is the inverse of the document frequency among the whole corpus of documents. TF-IDF is used mainly because of two reasons: Suppose we search for \u201cthe results of latest European Socccer games\u201d on Google. It is certain that \u201cthe\u201d will occur more frequently than \u201csoccer games\u201d but the relative importance of soccer gamesis higher than the search query point of view. In such cases, TF-IDF weighting negates the effect of high frequency words in determining the importance of an item (document).\n\nBelow is the equation to calculate the TF-IDF score:\n\nAfter calculating TF-IDF scores, how do we determine which items are closer to each other, rather closer to the user profile? This is accomplished using the Vector Space Model which computes the proximity based on the angle between the vectors. In this model, each item is stored as a vector of its attributes (which are also vectors) in an n-dimensional space and the angles between the vectors are calculated to determine the similarity between the vectors. Next, the user profile vectors are also created based on his actions on previous attributes of items and the similarity between an item and a user is also determined in a similar way.\n\nSentence 2 is more likely to be using Term 2 than using Term 1. Vice-versa for Sentence 1. The method of calculating this relative measure is calculated by taking the cosine of the angle between the sentences and the terms. The ultimate reason behind using cosine is that the value of cosine will increase with decreasing value of the angle between which signifies more similarity. The vectors are length normalized after which they become vectors of length 1 and then the cosine calculation is simply the sum-product of vectors.\n\nWith all that math in mind, I am going to build a Content-Based Recommendation Engine that computes similarity between movies based on movie genres. It will suggest movies that are most similar to a particular movie based on its genre.\n\nI do not have a quantitative metric to judge the machine\u2019s performance so this will have to be done qualitatively. In order to do so, I\u2019ll use TfidfVectorizer function from scikit-learn, which transforms text to feature vectors that can be used as input to estimator.\n\nI will be using the Cosine Similarity to calculate a numeric quantity that denotes the similarity between two movies. Since I have used the TF-IDF Vectorizer, calculating the Dot Product will directly give me the Cosine Similarity Score. Therefore, I will use sklearn\u2019s linear_kernel instead of cosine_similarities since it is much faster.\n\nI now have a pairwise cosine similarity matrix for all the movies in the dataset. The next step is to write a function that returns the 20 most similar movies based on the cosine similarity score.\n\nLet\u2019s try and get the top recommendations for a few movies and see how good the recommendations are.\n\nAs you can see, I have quite a decent list of recommendation for Good Will Hunting (Drama), Toy Story (Animation, Children\u2019s, Comedy), and Saving Private Ryan (Action, Thriller, War).\n\nOverall, here are the pros of using content-based recommendation:\n\nHowever, there are some cons of using this approach:\n\nThe Collaborative Filtering Recommender is entirely based on the past behavior and not on the context. More specifically, it is based on the similarity in preferences, tastes and choices of two users. It analyses how similar the tastes of one user is to another and makes recommendations on the basis of that.\n\nFor instance, if user A likes movies 1, 2, 3 and user B likes movies 2,3,4, then they have similar interests and A should like movie 4 and B should like movie 1. This makes it one of the most commonly used algorithm as it is not dependent on any additional information.\n\nIn general, collaborative filtering is the workhorse of recommender engines. The algorithm has a very interesting property of being able to do feature learning on its own, which means that it can start to learn for itself what features to use.\n\nThere are 2 main types of memory-based collaborative filtering algorithms:\n\nIn either scenario, we builds a similarity matrix. For user-user collaborative filtering, the user-similarity matrix will consist of some distance metrics that measure the similarity between any two pairs of users. Likewise, the item-similarity matrix will measure the similarity between any two pairs of items.\n\nThere are 3 distance similarity metrics that are usually used in collaborative filtering:\n\nDue to the limited computing power in my laptop, I will build the recommender system using only a subset of the ratings. In particular, I will take a random sample of 20,000 ratings (2%) from the 1M ratings.\n\nI use the scikit-learn library to split the dataset into testing and training. Cross_validation.train_test_split shuffles and splits the data into two datasets according to the percentage of test examples, which here is 0.2.\n\nNow I need to create a user-item matrix. Since I have splitted the data into testing and training, I need to create two matrices. The training matrix contains 80% of the ratings and the testing matrix contains 20% of the ratings.\n\nNow I use the pairwise_distances function from sklearn to calculate the Pearson Correlation Coefficient. This method provides a safe way to take a distance matrix as input, while preserving compatibility with many other algorithms that take a vector array.\n\nWith the similarity matrix in hand, I can now predict the ratings that were not included with the data. Using these predictions, I can then compare them with the test data to attempt to validate the quality of our recommender model.\n\nThere are many evaluation metrics but one of the most popular metric used to evaluate accuracy of predicted ratings is Root Mean Squared Error (RMSE). I will use the mean_square_error (MSE) function from sklearn, where the RMSE is just the square root of MSE. I\u2019ll use the scikit-learn\u2019s mean squared error function as my validation metric. Comparing user- and item-based collaborative filtering, it looks like user-based collaborative filtering gives a better result.\n\nRMSE of training of model is a metric which measure how much the signal and the noise is explained by the model. I noticed that my RMSE is quite big. I suppose I might have overfitted the training data.\n\nOverall, Memory-based Collaborative Filtering is easy to implement and produce reasonable prediction quality. However, there are some drawback of this approach:\n\nNote: The complete code for content-based and memory-based collaborative filtering can be found in this Jupyter Notebook.\n\nIn the previous attempt, I have used memory-based collaborative filtering to make movie recommendations from users\u2019 ratings data. I can only try them on a very small data sample (20,000 ratings), and ended up getting pretty high Root Mean Squared Error (bad recommendations). Memory-based collaborative filtering approaches that compute distance relationships between items or users have these two major issues:\n\nThus I\u2019d need to apply Dimensionality Reduction technique to derive the tastes and preferences from the raw data, otherwise known as doing low-rank matrix factorization. Why reduce dimensions?\n\nModel-based Collaborative Filtering is based on matrix factorization (MF)which has received greater exposure, mainly as an unsupervised learning method for latent variable decomposition and dimensionality reduction. Matrix factorization is widely used for recommender systems where it can deal better with scalability and sparsity than Memory-based CF:\n\nA well-known matrix factorization method is Singular value decomposition (SVD). At a high level, SVD is an algorithm that decomposes a matrix A into the best lower rank (i.e. smaller/simpler) approximation of the original matrix A. Mathematically, it decomposes A into a two unitary matrices and a diagonal matrix:\n\nwhere A is the input data matrix (users\u2019s ratings), U is the left singular vectors (user \u201cfeatures\u201d matrix), Sum is the diagonal matrix of singular values (essentially weights/strengths of each concept), and V^T is the right singular vectors (movie \u201cfeatures\u201d matrix). U and V^T are column orthonormal, and represent different things: U represents how much users \u201clike\u201d each feature and V^T represents how relevant each feature is to each movie.\n\nTo get the lower rank approximation, I take these matrices and keep only the top k features, which can be thought of as the underlying tastes and preferences vectors.\n\nScipy and Numpy both have functions to do the singular value decomposition. I\u2019m going to use the Scipy function svds because it let\u2019s me choose how many latent factors I want to use to approximate the original ratings matrix (instead of having to truncate it after).\n\nAs I\u2019m going to leverage matrix multiplication to get predictions, I\u2019ll convert the Sum (now are values) to the diagonal matrix form.\n\nI now have everything I need to make movie ratings predictions for every user. I can do it all at once by following the math and matrix multiply U, Sum, and V^T back to get the rank k = 50 approximation of A.\n\nBut first, I need to add the user means back to get the actual star ratings prediction.\n\nWith the predictions matrix for every user, I can build a function to recommend movies for any user. I return the list of movies the user has already rated, for the sake of comparison.\n\nNow I write a function to return the movies with the highest predicted rating that the specified user hasn\u2019t already rated.\n\nInstead of doing evaluation manually like the last time, I will use the Surprise library that provided various ready-to-use powerful prediction algorithms including (SVD) to evaluate its RMSE (Root Mean Squared Error) on the MovieLens dataset. It is a Python Scikit-Learn\u2019s building and analyzing recommender systems.\n\nI get a mean Root Mean Square Error of 0.8736 which is pretty good.\n\nLet\u2019s try to recommend 20 movies for user with ID 1310.\n\nThese look like pretty good recommendations. It\u2019s good to see that, although I didn\u2019t actually use the genre of the movie as a feature, the truncated matrix factorization features \u201cpicked up\u201d on the underlying tastes and preferences of the user. I\u2019ve recommended some comedy, drama, and romance movies \u2014 all of which were genres of some of this user\u2019s top rated movies.\n\nNote: The complete code for SVD Matrix Factorization can be found in this Jupyter Notebook.\n\nThe idea of using deep learning is similar to that of Model-Based Matrix Factorization. In matrix factorization, we decompose our original sparse matrix into product of 2 low rank orthogonal matrices. For deep learning implementation, we don\u2019t need them to be orthogonal, we want our model to learn the values of embedding matrix itself. The user latent features and movie latent features are looked up from the embedding matrices for specific movie-user combination. These are the input values for further linear and non-linear layers. We can pass this input to multiple relu, linear or sigmoid layers and learn the corresponding weights by any optimization algorithm (Adam, SGD, etc.).\n\nHere are the main components of my neural network:\n\nThis code is based on the approach outlined in Alkahest\u2019s blog post Collaborative Filtering in Keras.\n\nI then compile the model using Mean Squared Error (MSE) as the loss function and the AdaMax learning algorithm.\n\nNow I need to train the model. This step will be the most-time consuming one. In my particular case, for our dataset with nearly 1 million ratings, almost 6,000 users and 4,000 movies, I trained the model in roughly 6 minutes per epoch (30 epochs ~ 3 hours) inside my MacBook Laptop CPU. I spitted the training and validation data with ratio of 90/10.\n\nThe next step is to actually predict the ratings a random user will give to a random movie. Below I apply the freshly trained deep learning model for all the users and all the movies, using 100 dimensional embeddings for each.\n\nHere I define the function to predict user\u2019s rating of unrated items.\n\nDuring the training process above, I saved the model weights each time the validation loss has improved. Thus, I can use that value to calculate the best validation Root Mean Square Error.\n\nThe best validation loss is 0.7424 at epoch 17. Taking the square root of that number, I got the RMSE value of 0.8616, which is better than the RMSE from the SVD Model (0.8736).\n\nHere I make a recommendation list of unrated 20 movies sorted by prediction value for user ID 2000. Let\u2019s see it.\n\nThis model performed better than all the approaches I attempted before (content-based, user-item similarity collaborative filtering, SVD). I can certainly improve this model\u2019s performance by making it deeper with more linear and non-linear layers.\n\nNote: The complete code for Deep Learning Model can be found in this Jupyter Notebook.\n\nRecommendation Engine is your companion and advisor to help you make the right choices by providing you tailored options and creating a personalized experience for you. It is beyond any doubt that recommendation engines are getting popular and critical in the new age of things. It is going to be in your best interest to learn to use them for businesses to be more competitive and consumers to be more efficient.\n\nI hope that this post has been helpful for you to learn about the 4 different approaches to build your own movie recommendation system. You can view all the source code in my GitHub repo at this link (https://github.com/khanhnamle1994/movielens). Let me know if you have any questions or suggestions on improvement!\n\nIf you enjoyed this piece, I\u2019d love it if you hit the clap button \ud83d\udc4f so others might stumble upon it. You can find my own code on GitHub, and more of my writing and projects at https://jameskle.com/. You can also follow me on Twitter, email me directly or find me on LinkedIn."
    },
    {
        "url": "https://towardsdatascience.com/facial-emotion-detection-using-ai-use-cases-248b932200d6",
        "title": "Facial Emotion Detection using AI: Use-Cases \u2013",
        "text": "Sentiment Analysis is already widely used by different companies to gauge consumer mood towards their product or brand in the digital world. However, in offline world users are also interacting with the brands and products in retail stores, showrooms, etc. and solutions to measure user\u2019s reaction automatically under such settings has remained a challenging task. Emotion Detection from facial expressions using AI can be a viable alternative to automatically measure consumer\u2019s engagement with their content and brands.\n\nAt ParallelDots, we have combined the science of psychology, human expressions and artificial intelligence to recognize different emotions on an individual\u2019s face automatically. Our facial emotion detection algorithm can identify seven different type of emotional states in real-time.\n\nIn this post, we will discuss how such a technology can be used to solve a variety of real-world use-cases effectively.\n\nCar Manufacturers around the world are increasingly focusing on making cars more personal and safe for us to drive. In their pursuit to build more smart car features, it makes sense for makers to use AI to help them understand the human emotions. Using facial emotion detection smart cars can alert the driver when he is feeling drowsy.\n\nThe US Department of Transportation claims that driving-related errors cause around 95% of fatal road accidents. Facial Emotion Detection can find subtle changes in facial micro-expressions that precedes drowsiness and send personalized alerts to the driver asking him to stop for a coffee break, change music or temperature.\n\nA candidate-interviewer interaction is susceptible to many categories of judgment and subjectivity. Such subjectivity makes it hard to determine whether candidate\u2019s personality is a good fit for the job. Identifying what a candidate is trying to say is out of our hands because of the multiple layers of language interpretation, cognitive biases, and context that lie in between. That\u2019s where AI comes in, which can measure candidate\u2019s facial expressions to capture their moods and further assess their personality traits.\n\nNotably, Unilever is already starting to incorporate this technology into their recruitment process. With this technology, a recruiter will be able to know, say, the overall confidence level of an interviewee and make a decision about whether or not this candidate will be able to perform well at a client-facing job. Similarly, it will be possible to find whether the candidate is honestly replying to all the questions by measuring the change in emotions during his responses and correlating it the vast amount of knowledge available in this area.\n\nEmployee morale can also be perceived using this technology by holding and recording interactions on the job. As an HR tool, it can help not only in devising recruiting strategies but also in designing HR policies that bring about best performance from employees.\n\nVideo games are designed keeping in mind a specific target audience. Each video game aims to evoke a particular behavior and set of emotions from the users. During the testing phase, users are asked to play the game for a given period and their feedback is incorporated to make the final product. Using facial emotion detection can aid in understanding which emotions a user is going through in real-time as he is playing without analyzing the complete video manually.\n\nSuch product feedback can be taken by analyzing a live feed of the user and detecting his facial emotions. While feelings of frustration and anger are commonly experienced in advanced video games, making use of facial emotion detection will help understand which emotions are experienced at what points in the game. It is also possible that some unexpected or undesirable emotions are observed during the game. Taking feedback from the user has experienced the game can be inefficient. This is because it can often be difficult to put an experience into words. Moreover, users may be unable to remember what exactly they went through emotionally across different parts of the game. Facial Emotion detection is a practical means of going beyond the spoken or written feedback and appreciating what the user is experiencing. When feedback is taken in this format, it becomes genuinely non-intrusive when it comes to user experience. At the same time, such feedback is more reliable than other forms.\n\nTraditionally, market research companies have employed verbal methods such as surveys to find the consumers wants and needs. However, such methods assume that consumers can formulate their preferences verbally and the stated preferences correspond to future actions which may not always be right.\n\nAnother popular approach in market research industry is to employ behavioral methods that observe user\u2019s reaction while interacting with a brand or product. Such methods are considered more objective than verbal methods. Behavioral methods use video feeds of users interacting with the product which are then analyzed manually to observer their reactions and emotions. However, such techniques can quickly become very labor intensive as the sample size increase. Facial Emotion Recognition can come to the rescue by allowing market research companies to measure moment-by-moment facial expressions of emotions (facial coding) automatically and aggregate the results.\n\nDetecting emotions with technology is a challenging task, yet one where machine learning algorithms have shown great promise. Using ParallelDots\u2019 Facial Emotion Detection API, customers can process images, and videos in real-time for monitoring video feeds or automating video analytics, thus saving costs and making life better for their users. The API is priced on a Pay-As-You-Go model allowing you to test out the technology before scaling up.\n\nFacial Emotion detection is only a subset of what visual intelligence could do to analyze videos and images automatically. Click here to check facial emotion in your picture.\n\nCheck here to know more about different use cases of Virality Detection API which can be used to enhance image quality.\n\nWe hope you liked the article. Please Sign Up for a free ParallelDots account to start your AI journey. You can also check the demos of ParallelDots APIs here."
    },
    {
        "url": "https://towardsdatascience.com/my-first-data-scientist-internship-7f7aa2ee4040",
        "title": "My First Data Scientist Internship \u2013",
        "text": "Quantum Inventions specializes in providing mobility intelligence to consumer, corporations and governments by leveraging on its integrated suite of mobility applications. enterprise logistics and analytics platforms. I was the first data scientist intern to join the R&D and analytics team.\n\nWithin the first few days, I was introduced to the amazing colleagues, various traffic terms in the industry, and the ongoing exciting projects. One of the things that I liked the most about my internship was the trust and freedom given to me as an intern to choose the project that I was interested in and just went all-in for it!\n\nTo my surprise, I realized that I was the one who was pioneering the project as no one has done it before. When nobody has done something before, research comes in and this is where I was grateful for, despite the uncertainties and difficulties. Why? Simple because I had the opportunity to experience the real data science workflow (if not all) from scratch.\n\nAllow me to briefly list down the workflow that I have gone through as these are what that has built my foundation in Data Science. And I hope you will find it useful in some ways. \ud83d\ude0a\n\nThe project chosen was about Short Term Freeway Travel Time Prediction. However, like I said, asking the right questions is very important for a Data Scientist. A lot of questions were raised to really understand the real business problem before the project was finalized, be it data sources available, the end goals of the project (even after I left) etc. Essentially, our objective was to predict travel time for a freeway in Singapore N minutes ahead more accurate than the current baseline estimation.\n\nExcited with the new project, I started collecting data sources from database and colleagues (basically walking around the office to ask questions on data sources). Collecting the right data source is similar to the case where you are scraping data from different websites for data preprocessing later. It is so important that it could affect the accuracy of the models that you are building in the later stage.\n\nReal world data is dirty. We can\u2019t expect a nicely formatted and clean data as provided by Kaggle. Therefore, data preprocessing (other people might call it data munging or data cleaning) is so crucial that I can\u2019t stress enough how important it is. It is the most important stage as it could occupy 40%-70% of the whole workflow, just to clean the data to be fed to your models.\n\nOne of the things that I like about data science is that you have to be honest to yourself. When you don\u2019t know what you don\u2019t know, and you think the data preprocessed is already clean enough and ready to feed to your models, therein lies a risk of building the correct models with the wrong data. In order words, always try to question yourself if the data is technically correct with the domain knowledge that you have, ask scrutinize the data with stringent threshold to check for any other outliers, missing or inconsistent data in the whole datasets.\n\nI was particularly careful about this after I made a mistake of feeding the models with the wrong data, just because of a simple flaw in one of the preprocessing steps.\n\nAfter some research, I proposed four models to be used in our project, which were Support Vector Regression (SVR), Multilayer Perceptron (MLP), Long Short Term Memory (LSTM), and State Space Neural Networks (SSNN). For the sake of brevity, you can find detailed explanation of each model on various websites.\n\nBuilding different models from scratch was a steep learning curve for me as a person who was still learning from MOOCs and textbooks. Fortunately, Scikit-learn and Keras (with Tensorflow backend) came to my rescue as they are easy to learn for fast models prototyping and implementation in Python. In addition, I also learned how to optimize the models and fine-tuned the hyperparameters for each model using several techniques.\n\nTo evaluate the performance of each model, I used mainly a few metrics:\n\nAt this stage, Steps 3\u20135 were repeated (interchangeably) until the best model was determined that could outperform the baseline estimation."
    },
    {
        "url": "https://towardsdatascience.com/a-philosophical-look-at-data-analytics-exploring-questions-of-right-and-wrong-42c78d2ba3cb",
        "title": "A Philosophical Look at Data Analytics: Exploring Questions of Right and Wrong",
        "text": "A Philosophical Look at Data Analytics: Exploring Questions of Right and Wrong\n\nI recently came across the above quote by Edward Snowden and it got me thinking, particularly in the context of the raging debate around the Cambridge Analytica \u2014 Facebook controversy, and the rights and wrongs of how personal data is used. As a practitioner, the questions that come to my mind are quite existential in nature and they boil down to one thing \u2014 how do we even begin to draw the line when it comes to personal data usage? There are many interconnected aspects that need to be seen in conjunction, so let me break this down a little.\n\n(1) The Big Fat Elephant in the Room: Data Privacy\n\nFirst things first \u2014 \u2018data privacy\u2019 is very different from \u2018data analysis/usage\u2019. It\u2019s critical to make this distinction, especially because much of the current discourse around using data almost always uses them interchangeably.\n\nSo what is the difference?\n\nObviously, it would be simplistic to assume that the two aren\u2019t linked. But the point I\u2019m making is that undesirable conduct around either of them are independent of each other. For example, you could get data through fair means (without flouting data privacy protocols) and yet use it towards a questionable end. Similarly, you could get data surreptitiously, paying no heed to data privacy, but use it for perfectly harmless analyses.\n\nComing back to the recent debates, I believe that the biggest wrong that happened was around data privacy itself. Sensitive data about individuals, their preferences and their behaviours was not protected by the likes of Facebook and this vulnerability was exploited by the likes of those that mined this data. This is a pretty black and white crime, without too much grey in there, unlike many other aspects of data usage. There are a few things to note here:\n\nWhat\u2019s the solution? Like most other matters of law, this will probably require a combination of tighter, better defined and better understood laws, coupled with a better understanding of risks by consumers. People will get smarter about what they put out there. Just like even though laws exist against burglary, people still lock their houses.\n\nThis world of data proliferation is still too new. Things are still developing. Controls will too.\n\nFor me this is the aspect that requires much more introspection and gives rise to several questions around what\u2019s acceptable and what\u2019s not. Data can be used to create many inferences. Those inferences in turn can be used to identify what is the best way to really touch a chord with any specific person.\n\nAdvertisers have been doing it for centuries \u2014 sure they did it for groups of people rather that individuals, but they did try to push the right buttons to move these target individuals along a path that they may or may not have chosen themselves.\n\nOf course, if the desired outcome is achieved through misrepresentation or lies then one would squarely be in the wrong. However, if data is being used only to identify which message would resonate most with whom, then why should that be considered wrong?\n\n(3) The Question of Domains \u2014 Are Some Holier than Others?\n\nAnother thing that I have been having a hard time reconciling with is the furore around the fact that Cambridge Analytica\u2019s work impacted political results. That\u2019s true of course, but why is it as relevant as it seems to have become is what I don\u2019t understand.\n\nFor years companies have been analyzing data (including personal data) and using it to make many business decisions, including customer targeting, but until now no one raised an eyebrow. On the contrary there was a race on to see who did it better, cheaper, more effectively. Now, all of a sudden, because the customer targeting is happening around political candidates rather than brand A vs. brand B, why has the same data analytics, using the same methods, become a monster that everyone is running scared of?\n\nYou could argue that political choices should not be based on data driven targeting of individuals but only the election platforms of the candidates. Sure. But the same reasoning can be extended to everything \u2014 individuals should pick bank A vs. bank B based on both banks\u2019 products and services, not based on targeted advertising by the banks. Why is it ethically wrong in case of politics and not in the case of banks and retail brands and airlines and insurance and telecom companies?\n\nDon\u2019t get me wrong \u2014 I am not trying to comment on whether or not using targeted data analytics in politics is right or wrong. I\u2019m just saying the rules need to be at least somewhat consistent."
    },
    {
        "url": "https://towardsdatascience.com/why-machine-learning-on-the-edge-92fac32105e6",
        "title": "Why Machine Learning on The Edge? \u2013",
        "text": "Why Machine Learning on The Edge?\n\nSoftware engineering can be fun, especially when working toward a common goal with like-minded people. Ever since we started the uTensor project, a microcontrollers (MCUs) artificial intelligent framework, many have asked us: why bother with edge computing on MCUs? Aren\u2019t the cloud and application processors enough for building IoT systems? Thoughtful questions, indeed. I will attempt to show our motivation for the project here, hopefully, you would find them interesting too.\n\nMCUs are very low-cost tiny computational devices. They often found in the heart of IoT edge devices. With 15 billion MCUs shipped a year, these chips are everywhere. Their low-energy consumption means they can run for months on coin-cell batteries and require no heatsinks. Their simplicity helps to reduce the overall cost of the system.\n\nThe computational power of MCUs has been increasing over the past decades. However, in most IoT applications, they do nothing more than shuffling data from sensors to the cloud. MCUs are typically clocked at hundreds of MHz and packaged with hundreds of KB of RAM. Given the clock-speed and RAM capacity, forwarding data is a cakewalk. In fact, the MCUs are idle most of the time. Let\u2019s illustrate this below:\n\nThe area of the graph above shows the computational budget of the MCU. Green areas indicate when the MCU is busy, this can include:\n\nThe blue areas represent the idle, untapped potential. Imagine millions of such devices deployed in the real world, that is collectively a lot of unutilized computational power.\n\nWhat if we can tap into this power? Can we do more on the edge? It turns out, AI fits perfectly here. Let\u2019s look at some ways we can apply AI on the edge:\n\nSimple image classification, gesture recognition, acoustic detection and motion analysis can be done on the edge device. Because only the final result is transmitted, we can minimize delay, improve privacy and conserve the bandwidth in IoT systems. The image on the left shows the classic hand-written-digit dataset, MNIST, in a projected space.\n\nUsing machine learning and other signal processing algorithms, different off-the-shelf sensors can be combined into a synthetic sensor. These type of sensors are capable of detecting complex events. These sensors are lower cost and more energy efficient compare to camera based systems. A good example of super sensor can be found here.\n\nDevices can make continuous improvements after they are deployed in the field. Google\u2019s Gboard uses a technique called federated learning, that involves every device collecting data and making individual improvements. These individual improvements are aggregated on a central service and every device is then updated with the combined result.\n\nNeural networks can be partitioned such that some layers are evaluated on the device and the rest in the cloud. This enables the balancing of workload and latency. The initial layers of a network can be viewed as feature-abstraction functions. As information propagates through the network, they abstract into high-level features. These high-level features take up much less room than the original data, as a result, making them much easier to transmit over the network. IoT communication technologies, such as Lora and NB-IoT have very limited payload size. Feature-extraction helps to pack the most relevant information in limited payloads.\n\nIn the bandwidth example above, the neural network is distributed between device and cloud. In some cases, it is possible to repurpose the network for a completely different application by just changing the layers in the cloud.\n\nThe application logic in the cloud is fairly easy to change. This hot-swapping of the network layer enables the same devices to be used for different applications. The practice of modifying part of the network to perform different tasks is an example of transfer learning.\n\nComplementary to the bandwidth and transfer learning examples above, with careful engineering, an approximation of the original data can be reconstructed from the features extracted from the data. This may allow edge devices to generate complex outputs with minimal input from the cloud, as well as having applications in data decompression.\n\nAI could help edge devices to be smarter, improve privacy and bandwidth usage. Though, at the time of writing, there is no known framework that deploys Tensorflow models on MCUs. We created uTensor hoping to catalyze edge computing\u2019s development.\n\nIt may still take time before low-power and low-cost AI hardware is as common as MCUs. In addition, as deep learning algorithms are rapidly changing, it makes sense to have a flexible software framework to keep up with AI/machine-learning research.\n\nuTensor will continue to take advantage of the latest software and hardware advancements for example, CMSIS-NN, Arm\u2019s Cortex-M machine learning APIs. These will be integrated into uTensor to ensure the best performance possible on the Arm\u2019s hardware. Developers and researchers will be able to easily test their latest ideas with uTensor, like new algorithms, distributed computing or RTLs.\n\nWe hope this project brings anyone who is interested in the field together. After all, collaboration is the key to success at the cutting edge.\n\nFollow me on Medium for upcoming uTensor articles."
    },
    {
        "url": "https://towardsdatascience.com/russian-fake-tweets-visualized-6f73f767695",
        "title": "Russian Fake Tweets Visualized \u2013",
        "text": "With the fervor of the Presidential election being skewed by Russian probing\u2019s as well as the notorious Facebook / Cambridge Analytics scandal still topping daily domestic headlines, it became clear to us that \u201cfake news\u201d and Russian users are still prevalent, yet vague concepts. Who are these fake users disguising as? How are these fake users fooling people? How are they influencing people? Therefore, with our backgrounds in natural language processing, data visualization, and interest in the combination of technology and politics, it was only natural to examine the fake Russian users Tweet data with Python and Plotly. In particular, we analyzed:\n\nCase Study of Most Successful (or Unsuccessful) Users\n\nNBC News published the database of more than 200,000 tweets that Twitter has tied to \u201cmalicious activity\u201d from Russia-linked accounts during the 2016 U.S. presidential election. These accounts, working in concert a part of large networked, pushed hundreds of thousands of inflammatory tweets. The data can be found here.\n\nNow that we have the data, let\u2019s dive into the analysis:\n\nWho are these fake users?\n\nIf we look at the face value of our fake users, we can see that there are three distinct categories of fake account names. The first combination consists of American sounding first names such as \u201cChris,\u201d \u201cRick,\u201d or \u201cJeremy\u201d combined with American sounding last names such as \u201cGreen,\u201d \u201cRoberts,\u201d or \u201cCox.\u201d The second combination consists of formal sounding news sources such as \u201cNew [York Times]\u201d or \u201cAtlanta Today.\u201d Finally, the third combination consists of purely foreign names. From this, we can see that it\u2019s often difficult to tell which accounts are fake based off the name alone as it could be any average Joe, news site, or unrecognizable name.\n\nWhat are their profile descriptions?\n\nA unique part of creating Twitter accounts is the option to include a short description of who you are. Users often post their summaries and ideologies for others to see. Specifically, the above graph shows one topic in common with the fake accounts \u2014 religion. By using words like \u201cGod,\u201d \u201cInGodWeTrust,\u201d and \u201cGodBlessAmerica,\u201d the fake accounts become relatable to a large group of people. Other bios include black lives matters topics, official news sounding descriptions (such as \u201csports,\u201d \u201cweather,\u201d and \u201cofficial\u201d), and foreign topics. Therefore, by quickly, if not instantly, relating to this fake user, users are more likely to follow or agree with the fake accounts tweets. More on this similarity attraction phenomenon can be read here.\n\nWhen were they made?\n\nOf the 454 accounts deemed to be fake Russian accounts, we can see the creation of the fake accounts started in 2009 and reached its peak of creations in 2013 before slowly lessening all the way to the start of 2017. Interestingly, this means the majority of fake accounts were created years before the actual 2016 Presidential election, perhaps to cause strife and division amongst US readers ahead of the election.\n\nWhere are they from?\n\nWe took a look at where the fake users \u201coriginated\u201d from and discovered that of the 454 values, approximately half the values were missing. Of the 287 locations listed, 124 were listed as some form of \u201cUnited States\u201d, 68 were listed as a large metropolitan cities in the United States (ie: San Francisco, New York, Atlanta, Los Angeles), and the remaining 37 values were in foreign countries, and the remaining 58 values were imaginary like \u201clocated at the corner of happy and healthy\u201d or \u201cthe block down the street.\u201d Therefore, since the data was majority missing and most likely fake, we opted from analyzing the data further.\n\nHow influential are they?\n\nOn a high level, we can see the number of Tweets increasing with the number of followers. This makes sense as these fake accounts are leveraging their popularity on social media to reach out and influence more individuals. One such notable fake user is the infamous Jenna Abrams account, whose racist, controversial, and fake tweets were at one point covered in mainstream media. At this point, it\u2019d be safe to say these tweets definitely were influential on the popular mass.\n\nWhen are they posting?\n\nFrom the above heat map, we can see that the fake users are predominantly posting on Sundays and Tuesdays in the later months of the year (ie: August, September, October, November, and December). Therefore, we can expect this is not by chance; that the fake users understand their content reaches more individuals on weekends rather than weekdays in the later half of the year when the election and other big events occur.\n\nWhat are they saying?\n\nSimilar to the descriptions of the fake users, we took a look at the topics covered within the actual tweet content. In the above graph, we can see that the Black Lives Matters and other racial subject matters were one such topic the Russian accounts targeted with words such as \u201cpolice,\u201d \u201cblacklivesmatter,\u201d \u201ccrime,\u201d and references to the shooting in San Bernardino, particularly about the perpetrator being of minority descent. Other topics covered by the users consisted of Hillary Clinton\u2019s private email server, ISIS, pro-Trump slogans, slanderings of the election debates, and school shootings. These poignant and popular events were a particularly easy topic the fake accounts could add their propaganda-opinion to, as individuals were already livid and divided on how to feel in reaction to these events.\n\nCase Study \u2014 in depth look at top 20 users\n\nWe know now what the prototypical fake account looks like, but what is the anatomy of a successful (or perhaps unsuccessful) Russian fake account? How are they gaining attention? From Figure (scatterplot), we see a positive correlation between number of followers and number of Tweets. Therefore, the following graphs examine their tweeting behavior in terms of velocity, sentiment, and subjectivity over time of the top 20 followed fake Russian accounts.\n\nIn terms of pure tweet volume, we can see a trend of the fake accounts being almost nonexistent until around June 2016, at which point the volume of Tweets increases dramatically \u2014 reaching its apex in October 2016. The tweets then tumble down in volume after November 2016 (election month), with one last resurgence around December 2016 before going back to an almost inactive state. This worrying trend shows the opportunistic behavior of the fake accounts, tweeting at the most tense and vital points of the election fervor.\n\nThe following graph is the average sentiment and subjectivity of the tweets made by the top 20 followed users. In the context of tweets, sentiment is defined as an attitude, thought, or judgement. In particular, tweets can be analyzed by using the Textblob package for their sentiment, with a score of -1 for a negative sentiment tweet of a +1 for positive sentiment tweets. Lastly, subjectivity in the context of tweets is often seen as how opinionated the given tweet is. In Textblob, the score ranges from 0 for very objective and 1 for heavily subjective or opinionated.\n\nUsing the power of hindsight and the Wikipedia of US current events, we can see what some notable spikes are related to:\n\nAugust 4, 2016: spike of hash tag #obamaswishlist which were posts about fanciful and perceived hypocritical items Obama \u201cwanted\u201d\n\nAugust 17, 2016: spike of the hash tag #trumpsfavoriteheadline which are Tweets about sardonic headlines that Donald Trump would endorse\n\nSeptember 28, 2016: #ihavearighttoknow movement by fake accounts to know what Hillary Clinton\u2019s emails were\n\nOctober 5, 2016: #ruinadinnerinonephrase was actually seen as both politically-backed and non-politically-backed with some referencing it to Hillary Clinton while others made memes out of the hashtag\n\nOctober 17, 2016: #makemehateyouinonephrase, another hash tag movement that was seen as either part of a meme culture or part of the political systems\n\nNovember 14, 2016: #reallifemagicspells used in reference with black lives matters and Trump\u2019s family\n\nDecember 7, 2016: #idrunforpresidentif \u201cI\u2019d known I needed literally zero experience\u201d and other sardonic comments about the presidential election\n\nPerhaps coincidentally or not, the initial spikes were all related to fake accounts simultaneously using hashtags to mock presidents and presidential candidates. The tweets were clearly politically-based with the name drops to actual candidates. However, as time progressed, the distinguishing factor between these tweets became less obvious, as the fake accounts used actual popular hashtags that were not clearly political. Additionally, the tweets seemed aimed at all candidates rather than one particular candidate until Trump was actually elected \u2014 at which point all attacks were directed at Trump.\n\nFrom our analysis, we learned that the fake accounts disguise themselves as (1) average Americans, (2) news sites with metropolitan names, or (3) international names that describe themselves with relatable topics such as political and religious beliefs; they achieved their objective of influencing Twitter users by posting subjective and polar tweets at opportunistic times such as the weekends when scandals and large announcements occurred. Finally, they grew sentient of their obvious posts by subtlely joining trending hashtags and injecting propaganda within it.\n\nThank you for taking the time to read our analysis of Russian Fake Tweets. Leave a like or comment if you can now talk about fake tweets without sounding fake, think there\u2019s any other things we should analyze, or have any opinions. Feel free to follow or connect with Stephen and/or David on LinkedIn as we\u2019ll be posting more interesting articles and tutorials!"
    },
    {
        "url": "https://towardsdatascience.com/building-a-linear-regression-with-pyspark-and-mllib-d065c3ba246a",
        "title": "Building A Linear Regression with PySpark and MLlib",
        "text": "Apache Spark has become one of the most commonly used and supported open-source tools for machine learning and data science.\n\nIn this post, I\u2019ll help you get started using Apache Spark\u2019s spark.ml Linear Regression for predicting Boston housing prices. Our data is from the Kaggle competition: Housing Values in Suburbs of Boston. For each house observation, we have the following information:\n\nZN \u2014 proportion of residential land zoned for lots over 25,000 sq.ft.\n\nDIS \u2014 weighted mean of distances to five Boston employment centres.\n\nBLACK \u2014 1000(Bk \u2014 0.63)\u00b2 where Bk is the proportion of blacks by town.\n\nMV \u2014 median value of owner-occupied homes in $1000s. This is the target variable.\n\nThe input data set contains data about details of various houses. Based on the information provided, the goal is to come up with a model to predict median value of a given house in the area.\n\nScatter matrix is a great way to roughly determine if we have a linear correlation between multiple independent variables.\n\nIt\u2019s hard to see. Let\u2019s find correlation between independent variables and target variable.\n\nThe correlation coefficient ranges from \u20131 to 1. When it is close to 1, it means that there is a strong positive correlation; for example, the median value tends to go up when the number of rooms goes up. When the coefficient is close to \u20131, it means that there is a strong negative correlation; the median value tends to go down when the percentage of the lower status of the population goes up. Finally, coefficients close to zero mean that there is no linear correlation.\n\nWe are going to keep all the variables, for now.\n\nPrepare data for Machine Learning. And we need two columns only \u2014 features and label(\u201cMV\u201d):\n\nSummarize the model over the training set and print out some metrics:\n\nRMSE measures the differences between predicted values by the model and the actual values. However, RMSE alone is meaningless until we compare with the actual \u201cMV\u201d value, such as mean, min and max. After such comparison, our RMSE looks pretty good.\n\nR squared at 0.74 indicates that in our model, approximate 74% of the variability in \u201cMV\u201d can be explained using the model. This is in align with the result from Scikit-Learn. It is not bad. However, we must be cautious that the performance on the training set may not a good approximation of the performance on the test set.\n\nSure enough, we achieved worse RMSE and R squared on the test set.\n\nUsing our Linear Regression model to make some predictions:\n\nGradient-boosted tree regression performed the best on our data.\n\nSource code can be found on Github. I am happy to hear any feedback or questions."
    },
    {
        "url": "https://towardsdatascience.com/dimensionality-reduction-by-stacking-pca-and-t-sne-420d9fcfab54",
        "title": "Dimensionality Reduction in Machine Learning by stacking PCA and t-SNE",
        "text": "We\u2019ll use the Database of Faces for this task:\n\nThe dataset has 10 images each of 40 different individuals, and the above gives image paths to each of the 400 images.\n\nCreate the dataset we need:\n\nThe images are 112x92, and the above code flattens the images to use each pixel as a feature \u2014 giving us a set 10304 features for each image \u2014 which means our dataset now has 10304 dimensions.\n\nPull out the to identify individuals:\n\nLet\u2019s plot the data above and quickly see how these images look like:\n\nUsing the function above, \u2014 gives us:\n\nCreate a color for each of the 40 individuals:\n\nDefine a function which takes 2 arguments: & , and returns the resulting data:\n\nIf we use the above with our dataset and pass , what we get back is 3-dimensional dataset from a 10304-dimensional dataset we had.\n\nLet\u2019s see this in a 3D plot:\n\nNothing special \u2014 each of the color above corresponds to a unique individual, and looking at the plot above, its hard to see a clear separation between any of them.\n\nGiven how they are implemented, PCA is the recommended technique when you have a very large number of dimensions and t-SNE shines when the number of dimensions are small (eg- 50)\n\nWe 1st use to bring the dimensions down to 50, and then use further to bring it down to a 3-dimensional dataset \u2014 which we finally plot.\n\nNow we are talking \u2014 there seem to be definite clusters for each of the individuals."
    },
    {
        "url": "https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-understanding-kl-divergence-2b382ca2b2a8",
        "title": "Light on Math Machine Learning: Intuitive Guide to Understanding KL Divergence",
        "text": "I\u2019m starting a new series of blog articles following a beginner friendly approach to understanding some of the challenging concepts in machine learning. To start with, we will start with KL divergence.\n\nFirst of all let us build some ground rules. We will define few things we need to know like the back of our hands to understand KL divergence.\n\nBy distribution we refer to different things such as data distributions or probability distributions. Here we are interested in probability distributions. Imagine you draw two axis (that is, X and Y) on a paper, I like to imagine a distribution as a thread dropped between the two axis; X and Y. X represents different values you are interested in obtaining probabilities for. Y represents the probability of observing some value on the X axis (that is, y=p(x)). I visualize this below.\n\nThis is a continuous probability distribution. For example think of axis X as the height or a human and Y as the probability of finding a person with that height.\n\nIf you want to make this probability distribution discrete, you cut this thread to fixed length pieces and turn the pieces in such a way that they are horizontal. And then create rectangles connecting the edges of each piece of thread and the x-axis. That is a discrete probability distribution.\n\nFor a discrete probability distribution, an event is you observing X taking some value (e.g. X=1). Let us call P(X=1) probability of the event X=1. In continuous space you can think of this as a range of values (e.g. 0.95< X<1.05). Note that the definition of an event is not restricted to the values it takes on the x-axis. However we can move forward considering only that.\n\nTo continue from this point onwards, I will be humbly using the example found in this blog post [1]. It is a great post explaining the KL divergence, but felt some of the intricacies in the explanation can be explained in more detail. All right let\u2019s get into it.\n\nSo the gist of the problem that is being solved in [1] is that, we\u2019re a group of scientists visiting the vast outer-space and we discovered some space worms. These space worms have varying number of teeth. Now we need to send this information back to earth. But sending information from space to earth is expensive. So we need need to represent this information with a minimum amount of information. A great way to do this is, instead of recording individual numbers, we draw a plot where X axis is different numbers of teeth that has been observed (0,1,2,\u2026, etc.) and make Y axis the probability of seeing a worm with x many teeth (that is, number of worms with x teeth / total number of worms). We have converted our observations to a distribution.\n\nThis distribution is efficient than sending information about individual worms. But we can do better. We can represent this distribution with a known distribution (e.g. uniform, binomial, normal, etc.). For example, if we represent the true distribution with a uniform distribution, we only need to send two pieces of information to recover true data; the uniform probability and the number of worms. But how do we know which distribution explains the true distribution better? Well that\u2019s where the KL divergence comes in.\n\nSo we could use the KL divergence to make sure that we matched the true distribution with some simple-to-explain and well-known distribution well.\n\nTo be able to check numerical correctness, let us change probability values to more human friendly values (compared to the values used in [1]). We will assume the following. Say we have 100 worms. And we have following types of worms in following amounts.\n\nQuick sanity check! Let\u2019s ensure that the values add up to 100 and probability add up to 1.0.\n\nHere\u2019s what it looks visually.\n\nNow that out of the way, let us first try to model this distribution with a uniform distribution. A uniform distribution has only a single parameter; the uniform probability; the probability of a given event happening.\n\nThis is what the uniform distribution and the true distribution side-by-side looks like.\n\nLet us keep this result aside and we will model the true distribution with another type of distributions.\n\nYou are probably familiar with the binomial probability through calculating probabilities of a coin landing on it\u2019s head. We can extend the same concept to our problem. For a coin you have two possible outputs and assuming the probability of the coin landing on its head is p and you run this experiment for n trials, the probability getting k successes is given by,\n\nLet\u2019s take a side trip and understand each term in the binomial distribution and see if they make sense. The first term is p^k. We want to get k successes, where the probability of a single success is p. Then the probability of getting k successes is p^k. Remember that we\u2019re running the experiment for n trials. Therefore, there\u2019s going to be n-k failed trials, with a failure probability of (1-p). So the probability of getting k successes is the joint probability of p^k (1-p)^{n-k}. Our work doesn\u2019t end here. There are different permutations the k trials can take place within the n trials. The number of different permutations k elements to be arranged within n spaces is given by,\n\nMultiplying all these together gives us the binomial probability of k successes.\n\nWe can also define a mean and a variance for a binomial distribution. These are given by,\n\nWhat does the mean reflect? Mean is the expected (average) number of successes you get if you run n trials. If each trial has a success probability of p it make sense to say you will get np trials if you run n trials. Next what does the variance represent. It represents how much the true number of success trials to deviate from the mean value. To understand the variance, let us assume n=1. Then the equation is, variance=p(1-p). You have the highest variance when p=0.5 (when it is equally likely to get heads and tail) and lowest when p=1 or p=0 (when for sure you\u2019re getting head/tail).\n\nNow with a solid understanding about the binomial distribution, let us spiral back to the problem at our hands. Let us first calculate the expected number of teeth for the worms. It would be,\n\nWith mean known, we can calculate p where,\n\nmean = np\n\n 5.44 = 10p\n\n p = 0.544\n\nNote than n is the maximum number of teeth observed from the population of worms. You might ask why we did not choose n to be the total number of worms (that is 100) or total number of events (that is 11). We will soon see the reason. With that, we can define probabilities of any number of teeth as follows.\n\nFrom the perspective of the coin flip, this is like asking,\n\nFormally, we calculate the probability pk^{bi} for all different values of k. Here k becomes the number of teeth we would like to observe. And pk^{bi} is the binomial probabilities for the k th bin of teeth (that is, 0 teeth, 1 tooth, etc.). So when we calculate them as follows,\n\nThis is what a comparison between the true distribution and the binomial distribution looks like.\n\nOkey, turn back and reflect on what we did so far. First we understood the problem we want to solve. The problem is to send statistics of teeth of a certain type of space worms across the space with minimal effort. For that we thought of representing the true statistics of worms with some known distribution, so we can just send the parameter of that distribution instead of true statistics. We looked at two types of distributions and came up with the following statistics.\n\n\n\n* Uniform distribution \u2014 with probability of 0.0909\n\n* Binomial distribution \u2014 with n=10, p=0.544 and k taking different values between 0 to 10\n\nNow let\u2019s visualize everything in one place\n\nNow with all these fancy calculations, we need a way to measure the matching between each approximated distribution and the true distribution. This is important, so that, when we send the information across, we can have a peace of mind without worrying about the question \u201cdid I choose correctly?\u201d for the rest of our lives.\n\nThis is where the KL divergence comes in. KL divergence is formally defined as follows.\n\nHere q(x) is the approximation and p(x) is the true distribution we\u2019re interested in matching q(x) to. Intuitively this measures the how much a given arbitrary distribution is away from the true distribution. If two distributions perfectly match, D_{KL} (p||q) = 0 otherwise it can take values between 0 and \u221e. Lower the KL divergence value, the better we have matched the true distribution with our approximation.\n\nLet\u2019s look at the KL divergence piece by piece. First take the log(p(x_i)/q(x_i)) component. What happens if q(x_i) is higher than p(x_i)? Then this component will produce a negative value (because log of less than 1 values are negative). On the other hand if q(x_i) is always smaller than p(x_i) this component will produce positive values. This will be zero only if p(x_i)=q(x_i). Then to make this an expected value, you weight the log component with p(x_i). This means that, matching areas where p(x_i) has higher probability is more important than matching areas with low p(x_i) probability.\n\nIntuitively it makes sense to give priority to correctly match the truly highly probable events in the approximation. Mathematically, this allows you to automatically ignore the areas of the distribution that falls outside of the support (support is the full length on the x axis used by a distribution) of the true distribution. Additionally this avoid calculating log(0) that will come up if you try to compute the log component for any area that falls outside of the support of the true distribution.\n\nLet us now compute the KL divergence for each of the approximate distributions we came up with. First let\u2019s take the uniform distribution.\n\nNow for the binomial distribution we get,\n\nLet\u2019s just play around with the KL divergence now. First we will see how the KL divergence changes when the success probability of the binomial distribution changes. Unfortunately we cannot do the same with the uniform distribution because we cannot change the probability as n is fixed.\n\nYou can see that as we are moving away from our choice (red dot), the KL divergence rapidly increases. In fact, if you print some of the KL divergence values small \u0394 amount away from our choice, you will see that our choice of the success probability gives the minimum KL divergence.\n\nNow we arrive to the end of our discussion about KL divergence.\n\nNow we have some solid results, though the uniform distribution appears to be simple and very uninformative where the binomial distribution carries more subtlety, the uniform distribution matches the true distribution better than the binomial distribution. To be honest, this result actually took me by surprise. Because I expected the binomial to model the true distribution better. Therefore, this teaches us the important less of why we should not trust our instincts alone!\n\nYou can have more fun around playing with the KL divergence to understand KL divergence better. You can read more about this in my blog post.\n\nNote: Please go and checkout my website as I post more machine learning stuff there as well."
    },
    {
        "url": "https://towardsdatascience.com/elbow-clustering-for-artificial-intelligence-be9c641d9cf8",
        "title": "Elbow Clustering for Artificial Intelligence \u2013",
        "text": "Clustering is the process of taking a pile of unsorted stuff (your dataset) and breaking it into piles (i.e. clusters) according to the similarity of the stuff. This is an unsupervised process (no training examples needed). Perhaps the most common way of doing this is with k-means, and I will introduce a different way to do it with a Gaussian Mixture Model (GMM) and some other stuff on top of that.\n\nOur example today will be separating 2 kinds of picture in a dataset, where you don\u2019t really have a label or a classifier for either of these \u201cthings\u201d in your dataset. You can see it works by just looking at the result, as we will see. I want to start off by making it clear that this method does not always work, and so it takes some fine tuning.\n\nThe first thing you need to remember to do at each major step in your artificial intelligence project is to LOOK AT YOUR DATA WITH YOUR EYES. I can\u2019t emphasize strongly enough that things with k-means can go stupidly wrong if you don\u2019t see what it is doing.\n\nPicking up where we left off in my article on image dataset collection, let\u2019s do clustering of images by perceptual hashing with k-means and GMM. If you just want to find what pictures are similar to others, there is an example right in the imagehash code to do that. What we are trying to do here is a bit more: we want to separate the data into categories (even though we don\u2019t know what the categories are). On GitHub there is an example of this type of library licensed only for academic use.\n\nWe immediately run into a problem, regardless of the library we pick: How many clusters do we want to end up with? This is not given to you by the clustering library. You need to decide, and deciding the number of clusters should be based on some logic. The way we are supposed to do this is using \u2026 the elbow method!\n\nThe elbow method is a weird name for a simple idea. Keep adding clusters until you see diminishing returns, and then stop. With k-means this means starting with 2 means and then 3 means, and so on until k. Same idea with GMM. When we see an elbow in the graph of explained variance versus cluster count, we back up and select the number of clusters where we see the elbow. You can see in the image below how the first two dots are the arm, the dot at 4 is the elbow, and the forearm is the points at 5 through 8.\n\nI know: We computer people should stop naming stuff right now. Turing machine, Round robin, k-maps, it just doesn\u2019t stop.\n\nSee the figure on page 148 of \u201cPython Machine Learning\u201d for some more insight on how some components of a dataset can contain more variance than others (PCA). The elbow method is just as valid when deciding when to stop training during early stopping, but in that case the data is typically much noisier. Basically, we stop when it looks like it\u2019s not worth adding effort.\n\nWhy is this elbow method thing not well known? Well, I guess it is kind of subjective, and so people who want to know an exact number are weirded out. Also, we tend to have a number of clusters in mind when we cluster things, and often for stupid reasons, but it\u2019s a fact. For example, I prefer to have a number of clusters that my tiny brain understands (e.g. a small number), rather than the number that the math tells me I really need. Also, I have had cases where the number of clusters is limited by some outside influence like hardware restrictions. There may be 299,945 trading pattern types, but if my time-limited space-limited neural network on GPU can only analyze 2,048 pattern types in a useful amount of time, then I guess we\u2019re going with 2,048 clusters. There are many examples in the research literature using the elbow method. We will see in our example that the elbow method is not the only game in town.\n\nLet\u2019s move on to doing the thing we came here to do: Cluster pictures with the elbow method using perceptual hashes! Let\u2019s sort images from a standard image dataset to see what sub-types we end up with. The goal here is to sort the images without using the ground truth labels to do the sorting (i.e. clustering). The thing we are about to do with k-means is unsupervised. It just looks at pictures and sorts them into piles. The reason for using a dataset with labels is just to help visualize after the fact what the sorting process did to the dataset. In reality you would look with your eyes at the pictures in each cluster and get an intuitive feel for what it did.\n\nIn our example today we will use the \u201cFaces\u201d and \u201cairplanes\u201d subfolders of the Caltech101 dataset. We mix the pictures like a deck of cards and put them in a big pile. Now imagine this is the output of some image scraping job.\n\nFor the leaf dataset from the last article, we end up with is 4102 rows \u00d7 2 columns, telling us exactly what the hash is for each image. About 20% of the original 5,000+ images would not generate a hash, usually because the file somehow arrived damaged during the scraping process. Let\u2019s get a better sense of what just happened\u2026.\n\nWe need to unwrap this sneaky thing we are calling the hash. We can compute the difference between images by subtracting the hash of two different images, but something odd pops up at this point. Even though we are able to use the subtraction operator on this hash number, it is actually NOT doing a subtract. Prove this to yourself by simply adding hashes instead of subtracting, and it will not so gracefully fail to compile. So, what is this hash thing we get at the output? Well, it\u2019s a field of boolean values for each image. Clearly the characters in the hash string are hexidecimal, and so printing one element reveals the true nature of the hash hiding underneath this pretty string: It\u2019s a bunch of binary numbers.\n\nSo, what is the subtract operation really doing when we \u201ccompare\u201d hashes? It is doing an XOR and counting the number of bits that were not the same. Put another way, the \u201cdifference\u201d between images is quantified as the count of the number of differences in the hash of the two images. We can prove it to ourselves using this handy little script\u2026\n\nWe see the following output when we run this script:\n\nAnd so, this was quite sneaky, but now we know that the distance between hashes is not an orderable property. It changes based on which 2 images we use as inputs. What we really are saying is that we compute the hamming distance. In scipy land, this metric is #8 in this list of distance metrics.\n\nNow, to do the clustering! We have 4 bit hex characters in our hash string, and there are 16 of the characters in each hash. That gives us 64 dimensions by which to compare images. We need to update our pandas dataframe to include 64 more columns (1 per hash feature).\n\nWe end up with the 64 new columns we wanted.\n\nUsing the code from this post, we can obtain some nice graphs for the full dataset of images, including the percentage of explained variance.\n\nWe in the figure above that at around 8 or 9 clusters we start to lose steam. Also, since most of the variance is not explained by the model, it is not a super useful predictor.\n\nSimilar to the perceptual hash figure, we see in the figure above that at around 8 or 9 clusters we start to lose steam. Much more of the variance is explained by this method than the perceptual hash in the previous figure (the top line is 40% instead of 14%). We also see a nicer elbow in this figure. It is still not a super useful predictor, but better than before.\n\nNow that we have some answers, what does it all mean? What images got clustered into each pile? Well, there are only some constrained cases where the clustering works well. Let\u2019s look at the pile of 2 kinds of images we mixed together in the beginning of the article (airplances and faces) and see if the clustering was able to unmix the two image types. Let\u2019s compare the GMM unmixing with k-means, to see which is a better unsupervised \u201cunmixer\u201d for this problem.\n\nFirst, here is the code for K-means:\n\nThe results look like this:\n\nThere were 800 airplane pictures and 435 faces pictures.\n\nAs we can see in the above pie charts, with 2 clusters the airplanes and faces were not separated well. We can pick 2 clusters because we know there are 2 things to separate out into piles. Stuff from both classes ended up mostly in cluster 0.With GMM we see a different story.\n\nLet\u2019s have a look first at the code for GMM, and then see the results.\n\nAnd now the results from a few example runs:\n\nSo these results are a lot nicer. GMM separates the files into a folder with mostly faces in the images and another folder with mostly airplanes in the images.\n\nWe can cherry pick the results by running GMM a few times to see when the split looks good (by visual inspection of the resulting image datasets). We can also label some subset of the data and using the code above we can just look at pie charts to see how the split worked. That approach needs less data than a full blown image classifier would. There are 50 other ways of doing this as well, but count this as way 51.\n\nI have provided you with the code to play around with, and hopefully this will help you wrangle your scraped images a bit better. On a large number of classes this gets super messy, but I do have a word of advice on that. It often happens that with a lot of classes the garbage and duplicate images cluster together and help you to then delete blobs of garbage images. You can then re-cluster and see what happens. K-means sometimes works, and in this case GMM was a bit better. You never know. Basically try lots of stuff and you will know right away when it is working.\n\nAnd so, in conclusion, perceptual hashes can be used to crunch images down to a bunch of binary dimensions upon which k-means, GMM, or other unsupervised methods can arrange images into categories.\n\nIf you liked this article on clustering images with k-means and the elbow method, have a look at some of my most read past articles, like \u201cHow to Price an AI Project\u201d and \u201cHow to Hire an AI Consultant.\u201d In addition to business-related articles, I also have prepared articles on other issues faced by companies looking to adopt deep machine learning, like \u201cMachine learning without cloud or APIs.\u201d\n\nOther articles you may enjoy:"
    },
    {
        "url": "https://towardsdatascience.com/ai-market-place-is-not-what-you-are-looking-for-in-the-telecommunication-industry-c1308d3ab379",
        "title": "AI market place is not what you are looking for (in the telecommunication industry).",
        "text": "In a far away land was the kingdom of Kadana. Kadana was a vast country with few inhabitants. The fact that in the warmest days of summer, temperature was seldom above -273\u00b0C was probably a reason for it. The land was cold, but people were warm.\n\nIn Kadana there was 3 major telecom operators: B311, Steven\u2019s and Telkad. There were also 3 regional ones: Northlink, Southlink and Audiotron. Many neighboring kingdoms also had telecom operators, some a lot bigger than the ones in Kadana. Dollartel, Southtel, Purpletel, we\u2019re all big players and many more competed in that environment.\n\nIt was a time of excitement. A new technology called AI was becoming popular in other fields and the telecommunications operators wanted to get the benefits as well. Before going further in our story, it can be of interest to understand a little bit what this AI technology is all about. Without going into too much details, let\u2019s just say that traditionally if you wanted a computer to do something for you, you had to feed him a program handcrafted with passion by software developer. The AI promise was that from now on, you could feed a computer with a ton of data about what you want to be done and it would figure out the specific conditions and provide the proper output without (much) of programming. For those aware of AI this looks like an overly simplistic (if not outright false) summary of the technology, but let\u2019s keep it that way for now\u2026\n\nGoing back to the telecommunication world, somebody with nice ideas decided to create Akut05. Akut05 was a new product combining the idea of a marketplace with the technology of AI. Cool! The benefit of a market place as demonstrated by the Apple App Store or Google Play, combined with the power of AI.\n\nThis is so interesting, I too want to get into that party, and I immediately create my company, TheLoneNut.ai. So now I need to create a nice AI model that I could sell on the Akut05 marketplace platform.\n\nWell, let not be so fast\u2026 You see, AI models are built from data as I said before. What data will I use? That\u2019s just a small hurdle for TheLoneNut.ai company\u2026 we go out, talk with operators. Nobody knows TheLoneNut.ai, it\u2019s a new company, so let\u2019s start with local operators. B311, Steven\u2019s and Telkad all think we are too small a player to give us access to their data. After all, their data is a treasure trove they should benefit from, why would they give us access to it. We then go to smaller regional players and Northlink has some interests. They are small and cannot invest massively in a data science team to build nice models, so with proper NDA, they agree to give us access to their data in counterpart, they will have access to our model on Akut05 with substantial rebate.\n\nGood! We need to start somewhere. I\u2019ll skip all the adventures along the way of getting the data, preparing it and building a model\u2026 but let me tell you that was full of adventures. We deploy a nice model in an Akut05 store and it works wonderfully\u2026 for awhile. After some time, the subscribers from Northlink change a bit their behavior, and Northlink see that our model does not respond properly anymore. How do they figure? I have no idea, since Akut05 does not provide with any real model monitoring capabilities besides the regular \u201ccloud\u201d monitoring metics. More alarming, we see 1-star reviews pouring in from B311, Steven\u2019s and Telkad who tried our model and got from the get go poor results. And there is nothing we can do about it because after all we never got deals with those big names to access their data. A few weeks later, having discounted the model to Northlink and getting only bad press from all other operators, TheLoneNut.ai bankrupt and we never hear from it again. The same happens to a lot of other small model developers who tried their hand at it, and in no time the Akut05 store is empty of any valuable model.\n\nSo contrary to an App Store, a Model Store is generally a bad idea. To get a model right (assuming you can) you need data. This data needs to come from representative examples of what you want the model to apply to. But it easy, we just need all the operator to agree to share the data! Well, if you don\u2019t see the irony, then good luck. But this is a nice story, lets put aside the irony. All the operators in our story decide to make their data available to any model developers on the Akut05 platform. What else could go wrong.\n\nLet us think about a model that use the monthly payment a subscriber pays to the operator. In Kadana this amount is provided in the data pool as $KAD, and it works fine for all Kadanian operators. Dollartel tries it out and (not) surprisingly it fails miserably. You see, in the market of Dollartel, the money in use is not the $KAD, but some other currency\u2026 The model builder, even if he has data from Dollartel may have to do \u201clocal\u201d adjustments. Can a model still provide good money to the model builder if the market is small and fractured i.e. needs special care being taken? Otherwise you\u2019ll get 1-star review and again disappear after a short while.\n\nOk, so the Akut05 is not a good idea for independent model builders. Maybe it can still be used by Purpletel which is a big telecom operator which can hire a great number of data scientists. But in that case, if its their data scientist who will do the job, why would they share their data? If they don\u2019t share their data and hire their own data scientists, why would they need a market place in the first place?\n\nIndependent model builders can\u2019t find their worth from a model market place, operators can\u2019t either\u2026 can the telecom manufacturer make money there? Well, why would it more valuable than for an independent model builder? Maybe it could get easier access to data, but the prerogatives are basically the same and it wouldn\u2019t be a winning market either I bet.\n\nWell, therefore a market place for AI is not what you are looking for\u2026 In a next post I\u2019ll try to say a little bit about what you should be looking for in the telecom sector when it comes to AI.\n\nFor sure this story is an oversimplification of the issue, still, I think we can get the point. You have a different view? Please feel free to share it in the comments below so we can all learn from a nice discussion!"
    },
    {
        "url": "https://towardsdatascience.com/if-your-files-are-saved-only-on-your-laptop-they-might-as-well-not-exist-29f3503750d5",
        "title": "If your files are saved only on your laptop they might as well not exist!",
        "text": "Last week, as I was working on one of my three final graduate course projects, my laptop decided it was a good time to give out. I spent a futile 15 minutes resetting the battery and holding down the power button trying to get a response, but to no avail: my laptop was done for good.\n\nAt this point a year ago, I would have been sobbing uncontrollably, my semester wreaked in the final week. However, this time, I set down my laptop, walked to the school library, logged onto a computer, downloaded my files from Google Drive where they had been synced up until the minute my laptop went dark, and was working on my final projects within 30 minutes. All in all, thanks to automatic back-ups, instead of losing an entire semester, I lost two lines of one report.\n\nThis near-tragedy illustrates two points that anyone who does any work on a computer must keep in mind:\n\nHaving witnessed firsthand the destruction wreaked on fellow students by computer failures, I finally installed Google Drive backup and sync a few months ago. This is one of a number of services that will run in the background on your computer, saving all files (or select ones you choose) to the cloud where you can access them from any computer in the world.\n\nThere\u2019s a saying I\u2019m quite fond of known as the Rule of Two: \u201cTwo is one and one is none.\u201d (I first heard this from CGPGrey on the Cortex Podcast). What these means is that if you only have one of some necessity, it might as well not exist because you will probably inevitably lose it. When it comes to file storage, if your files are located only on your laptop, they might as well not be saved at all for how vulnerable you are.\n\nSigning up for a back-up service tends to be one of those things that everyone says they will get around to but never actually implements (I was in this group for a long time). However, having proper back-ups should be a necessity before you start doing any work you don\u2019t want to lose!\n\nIn today\u2019s world of ridiculously cheap storage, there is no excuse for not having multiple copies of your files available in the cloud. For students, you might have free unlimited storage through Google Drive which means you can store anything you want (I haven\u2019t tested the limits of unlimited, but have friends with multiple terabytes saved). For everyone else, 100 GB of storage is only $2/month on Google Drive and other options are equally reasonable. I know the moment my screen went blank I would gladly have paid $100 / year to know that my files were safe.\n\nThe exact backup route you choose does not matter, but what is important is ensuring that your files live in multiple places (usb sticks are better than nothing, but cloud backup is the best option). I prefer automatic syncing because humans are fallible. With a service that syncs your files by itself, there is little risk you will get in the way of technology helping you out! This is one area where you should be content to let acomputer think for you. (Along the same lines, if a program offers the option to auto-save files, make sure to enable it to save as often as possible!)\n\nWhenever I see the little Google Drive sync icon whirring away on my laptop, I take a moment to appreciate the wonders of automatic file backups. The next time my laptop inevitably fails, I know that it will be only a minor, recoverable inconvenience. Can you say the same?\n\nI welcome feedback and discussion and can be reached on Twitter @koehrsen_will."
    },
    {
        "url": "https://towardsdatascience.com/data-science-for-startups-introduction-80d022a18aec",
        "title": "Data Science for Startups: Introduction \u2013",
        "text": "I recently changed industries and joined a startup company where I\u2019m responsible for building up a data science discipline. While we already had a solid data pipeline in place when I joined, we didn\u2019t have processes in place for reproducible analysis, scaling up models, and performing experiments. The goal of this series of blog posts is to provide an overview of how to build a data science platform from scratch for a startup, providing real examples using Google Cloud Platform (GCP) that readers can try out themselves.\n\nThis series is intended for data scientists and analysts that want to move beyond the model training stage, and build data pipelines and data products that can be impactful for an organization. However, it could also be useful for other disciplines that want a better understanding of how to work with data scientists to run experiments and build data products. It is intended for readers with programming experience, and will include code examples primarily in R and Java.\n\nOne of the first questions to ask when hiring a data scientist for your startup is\n\n how will data science improve our product? At Windfall Data, our product is data, and therefore the goal of data science aligns well with the goal of the company, to build the most accurate model for estimating net worth. At other organizations, such as a mobile gaming company, the answer may not be so direct, and data science may be more useful for understanding how to run the business rather than improve products. However, in these early stages it\u2019s usually beneficial to start collecting data about customer behavior, so that you can improve products in the future.\n\nSome of the benefits of using data science at a start up are:\n\nMany organizations get stuck on the first two or three steps, and do not utilize the full potential of data science. The goal of this series of blog posts is to show how managed services can be used for small teams to move beyond data pipelines for just calculating run-the-business metrics, and transition to an organization where data science provides key input for product development.\n\nHere are the topics I am planning to cover for this blog series. As I write new sections, I may add or move around sections. Please provide comments at the end of this posts if there are other topics that you feel should be covered.\n\nThroughout the series, I\u2019ll be presenting code examples built on Google Cloud Platform. I choose this cloud option, because GCP provides a number of managed services that make it possible for small teams to build data pipelines, productize predictive models, and utilize deep learning. It\u2019s also possible to sign up for a free trial with GCP and get $300 in credits. This should cover most of the topics presented in this series, but it will quickly expire if your goal is to dive into deep learning on the cloud.\n\nFor programming languages, I\u2019ll be using R for scripting and Java for production, as well as SQL for working with data in BigQuery. I\u2019ll also present other tools such as AirFlow and Shiny. Some experience with R and Java is recommended, since I won\u2019t be covering the basics of these languages."
    },
    {
        "url": "https://towardsdatascience.com/scrape-data-and-build-a-webapp-in-r-using-rvest-and-shiny-f20d84dd1b74",
        "title": "Scrape Data and Build a Webapp in R Using Rvest and Shiny",
        "text": "Now that we have our functions built, time to build the shiny dashboard that will display our data to the world.\n\nA shiny webapp will allow us to build an interactive dashboard that we will let Rstudio host for us with their servers. They have a free plan so anyone can easily get started with it. From the Rstudio documentation:\n\nAt this point I highly recommend skimming through the official documentation referenced above to get yourself familiar with basic Shiny concepts.\n\nWithout getting too much in the weeds, let\u2019s begin by building the UI object.\n\nIn the UI object we will lay out our dashboard. When I was building this I went off a pencil wireframe sketch. To help you visualize the end result here is a screenshot of the finished product:\n\nThe shinydashboard structure allows for us to have a sidebar and a dashboard area where we can add boxes in rows and columns. The black area above is the sidebar, and everything to the right of that is the body. Now for the UI code.\n\nGoing down the line, we assign the dashboardPage function to the UI and then add the parts that we need.\n\nThe sidebar is a good place to put documentation or filters (which we don\u2019t cover in this tutorial). You\u2019ll notice that you can pass certain html tags and classes to your text. H5 is just a tag that defines the text as the 5th-level heading in a document, which is usually the 5th-largest text.\n\nIn the body section we build out the main items in the dashboard. There are a few key parts of the body section that actually output the data.\n\nThese two code chunks output the two purple boxes at the top of our dashboard. The two output ID\u2019s, \u201ctop.coin\u201d and \u201ctop.name\u201d are referencing data that is being output in the server function that we will go over later.\n\nSame with the data table and plot. Table is the output ID that will tie to a server function below, and plot also ties to the server.\n\nNext on the agenda is defining our server function. The server function is where values are computed and then read and plotted or displayed by the UI function.\n\nRemember the functions we defined at the beginning? We\u2019re going to use them now. We\u2019re also going to add another concept: reactive expressions. These allow our shiny dashboard to update at regular intervals or based on user input. For this dashboard, we are asking the program to run our functions every 60 seconds, which updates the dashboard with the most recent values from the website we are scraping.\n\nRemember the output ID table that we defined above? We\u2019re going to reference that in the function above. Notice that we used the liveish_data reactive function and not our original function from the very beginning.\n\nNext, we plot a simple barplot using ggplot2.\n\nFinally, we plot our two infoboxes using the reactive expressions defined in the beginning of the server function.\n\nThe very last part of the script combines our UI with our server and deploys the app.\n\nAt this point, you should have a running app appear in a window in Rstudio.\n\nHit the open in browser button to see an un-jumbled version of the app; the actual version you will see when it\u2019s deployed."
    },
    {
        "url": "https://towardsdatascience.com/random-forest-mystery-revealed-69ca18b82ff5",
        "title": "Random Forest \u2014 Mystery Revealed \u2013",
        "text": "Selecting the \u2018right\u2019 machine learning algorithm for your application is one of the many challenges of appropriately applying machine learning in the real world. The right algorithm can sometimes be the one which is the most simple, the most robust and the most easily understood by your end users. Random Forest will regularly tick all of these boxes but, as others have written, is often overlooked. Hopefully this post provides you with a solid base understanding of how the algorithm works so you are more confident when conducting your own research or applying within tools like R or Python.\n\nRandom Forest is a supervised learning algorithm commonly applied in both classification and regression situations. As a brief introduction, the algorithm is an ensemble model, creating a \u2018forest\u2019 of many decision \u2018trees\u2019 with the number of trees defined by the user. Each decision tree is created based on a subset of attributes (columns) and observations (rows) from the original data set. These trees are grown using the training data set and applied to the test data set. The final classification returned by the model is the one which matches the classifications provided by the greatest number of individual decision trees.\n\nAs random forest is simply the classification returned by the largest number of trees, this blog will focus on decision trees themselves which are at the core of the algorithm.\n\nA decision tree is a flow-type structure where each attribute in the data is represented as an \u2018internal node\u2019, each \u2018branch\u2019 represents the outcome of a test, and the \u2018leaves\u2019 represent the classification made. Each decision tree within the algorithm is created using a different, \u2018random\u2019 subset of attributes and observations from the original training data set.\n\nThis next section can be confusing (it was for me at least!) so hopefully my explanation is clear. Decision trees are created in Disjunctive Normal Form where the decision is an \u2018OR\u2018 of \u2018ANDs\u2018. This can alternatively be phrased as a dis-junction of one or more conjunctions of one or more literals. [2] The decision tree example above shows the first disjunction as Salary above \u2018OR\u2019 below $60k. This is also the internal node in terms of the structure of the tree. The branch representing \u2018No\u2019 is an outcome of this salary test, where \u2018Reject\u2019 is the classification. The second disjunction is work experience greater than 2 years. We again have a Reject classification, but in this instance we also have an Approve. To achieve the approve classification, Work Experience must be above 2 years AND (conjunction) the outcome of the salary > $60k test must be true/Yes. A short video has been embedded below which discussed disjunctive statements. Wikipedia also has an article on logical disjunction which includes venn diagrams to highlight the \u2018OR\u2019 property.\n\nAs mentioned, each level or dis-junction within the decision tree represents an attribute in the random data segment used for the individual decision tree. The ordering of these attributes is important when considering both the accuracy of the final model as well as the computational effort associated with the tree. In other words, attributes should be ordered in such a way to provide the most efficient and informative structure to classify future observations. In decisions trees, attributes are prioritised in terms of \u2018information gain\u2019 as a result of a dis-junction on an attribute.\n\nThe information gain from a child internal node is equal to the entropy (level of disorder) of the parent node minus the weighted average entropy of the child node. Weighting is based on the number of classifications made via each branch. Entropy is zero when there is perfect separation in the classification, i.e. all \u2018True\u2019 outcomes result in a \u2018Good\u2019 classification and all \u2018False\u2019 outcomes result in a \u2018Bad\u2019 classification. Entropy of \u20181\u2019 (highest possible entropy) occurs when both the \u2018True\u2019 and \u2018False\u2019 outcomes return equal numbers of \u2018Good\u2019 and \u2018Bad\u2019 classifications\u2019. The second example provides no information gain as there is effectively a random split of classifications between the two possible outcomes and the tree has gained no information. This would therefore be a poor attribute to use as the \u2018root\u2019 of the tree.\n\nEntropy is a term which relates to the order or disorder in a system. High entropy relates to high disorder and vice versa. Below are two short videos from Udacity covering the concept of entropy and information gain. They form part of a free series which provides an introduction into the topic of machine learning.\n\nTo keep this post short I will end before covering concepts including \u2018pruning\u2019 of decision trees and over fitting which goes hand in hand with the pruning concept. I will cover both of these in a later post but hopefully the summary to this point has pulled back the curtains on the powerful Random Forest algorithm."
    },
    {
        "url": "https://towardsdatascience.com/how-to-ace-the-in-person-data-science-interview-584ca11df08a",
        "title": "How to Ace the In Person Data Science Interview \u2013",
        "text": "I\u2019ve written previously about my recent job hunt, but this article is solely devoted to the in-person interview. That full-day, try to razzle-dazzle em\u2019, cross your fingers and hope you\u2019re well prepared for what gets thrown at you. After attending a ton of these interviews, I\u2019ve found that they tend to follow some pretty standard schedules.\n\nYou may meet with 3\u20137 different people, and throughout the span of meeting with these different people, you\u2019ll probably cover:\n\nI\u2019ve mentioned this before when talking about phone screens. The way I approach this never changes. People just want to hear that you can speak to who you are and what you\u2019re doing. Mine was some variation of:\n\nI am a Data Scientist with 8 years of experience using statistical methods and analysis to solve business problems across various industries. I\u2019m skilled in SQL, model building in R, and I\u2019m currently learning Python.\n\nAlmost every company I spoke with asked questions that should be answered in the STAR format. The most prevalent STAR questions I\u2019ve seen in Data Science interviews are:\n\nThe goal here is to concisely and clearly explain the Situation, Task, Action and Result.\n\nMy response to the \u201ctechnical results\u201d questions would go something like this:\n\nVistaprint is a company that sells marketing materials for small businesses online (always give context, the interviewer may not be familiar with the company). I had the opportunity to do a customer behavioral segmentation using k-means. This involved creating 54 variables, standardizing the data, plenty of analysis, etc. When it was time to share my results with stakeholders, I had really taken this information up a level and built out the story. Instead of talking about the methodology, I spoke to who the customer segments were and how their behaviors were different. I also stressed that this segmentation was actionable! We could identify these customers in our database, develop campaigns to target them, and I gave examples of specific campaigns we might try. This is an example of when I explained technical results to non-technical stakeholders. (always restate the question afterwards).\n\nFor me, these questions required some preparation time. I gave some real thought to my best examples from my experience, and practiced saying the answer. This time paid-off. I was asked these same questions over and over throughout my interviewing.\n\nThis is when the interviewer has you stand at the whiteboard an answer some SQL questions. If a job description asks for SQL, this is fair game. In most scenarios, they\u2019ll tape a couple pieces of paper up on the whiteboard. One will be a table with (for example) ids and names (lets call this NamesTable), the other paper might include ids, dates, and purchases (lets call this PurchasesTable). You get the idea, you\u2019re about to write SQL queries to answer their questions.\n\nThey\u2019ll ask a series of questions such as:\n\nAs mentioned in my previous article. I was asked FizzBuzz two days in a row by two different companies. A possible way to write the solution in Python (just took a screenshot of my computer) is below:\n\nThe coding problem will most likely involve some loops, logic statements and may have you define a function. If a specific language is mentioned on the job description, they may want to see the answer in that language. The hiring manager wants to be sure that when you say you can code, you at least have some basic programming knowledge.\n\nI\u2019ve been asked about all the methods I mention on my resume at one point or another (regression, classification, time-series analysis, MVT testing, etc). I don\u2019t mention my thesis from my Master\u2019s Degree on my resume, but casually referenced it when asked if I had previously had experience with Bayesian methods. The interviewer followed up with a question on the prior distributions used in my thesis.\n\nI had finished my thesis 9 years ago, couldn\u2019t remember the priors and told him I\u2019d need to follow up. I did follow up and sent him the answer to his question. They did offer me a job, but it\u2019s not a scenario you want to find yourself in. If you are going to reference something, be able to speak to it. Even if it means refreshing your memory by looking at wikipedia ahead of the interview. Things on your resume and projects you mention should be a home run.\n\nSome basic questions will be asked to make sure that you have an understanding of how numbers work. The question may require you to draw a graph or use some algebra to get at an answer, and it\u2019ll show that you have some business context and can explain what is going on. Questions around changes in conversion, average sale price, why is revenue down in this scenario? What model would you choose in this scenario? Typically I\u2019m asked two or three questions of this type.\n\nI was asked a probability question at one interview. They asked what the expected value was of rolling a fair die. I was then asked if the die was weighted in a certain way, what would the expected value of that die be. I wasn\u2019t allowed to use a calculator.\n\nHonestly, I used the question above to try and get at whether you needed to work 60 hours a week and work on the weekends to be someone who stood out. I pretty frequently work on the weekends because I enjoy what I do, I wouldn\u2019t enjoy it if it was expected.\n\nReally, I like to get this question out of the way during the phone screen. I\u2019m not personally interested in working for a SAS shop, so I\u2019d want to know that upfront. My favorite response to this question is \u201cyou can use whatever open source tools you\u2019d like as long as it\u2019s appropriate for the problem.\u201d\n\nThis is your opportunity to let them tell you if there is anything that you haven\u2019t covered yet, or that they might be concerned about. You don\u2019t want to leave an interview with them feeling like they didn\u2019t get EVERYTHING they needed to make a decision on whether or not to hire you.\n\nI also ask about the reporting structure, and I certainly ask about what type of projects I\u2019d be working on soon after starting (if that is not already clear).\n\nI wish you so much success in your data science interviews. Hopefully you meet a lot of great people, and have a positive experience. After each interview, remember to send your thank you notes! If you do not receive an offer, or do not accept an offer from a given company, still go on LinkedIn and send them connection requests. You never know when timing might be better in the future and your paths might cross.\n\nIf you liked this article, visit my website! Here"
    },
    {
        "url": "https://towardsdatascience.com/cvpr-2014-paper-summary-the-secrets-of-salient-object-segmentation-9c777babdc5",
        "title": "[ CVPR 2014 / Paper Summary ] The Secrets of Salient Object Segmentation",
        "text": "This section of the topic is huge, mainly because it is one of the core contribution of this paper. And I have to say, I don\u2019t know in details of all of the statistical analysis they did, however, once I research about them. I will surely make another blog post.\n\nPsycho physical experiments on the PASCALS data-set\n\nHere the authors have performed some experiments to gather ground truth data for fixation Prediction on PASCAL 2010 data set.\n\nEvaluating dataset consistency\n\nTo compare the level of agreement among different labelers (from previous experiment) the authors have done some extensive analysis to know the Inter-subject consistency. (For both salient object segmentation as well as fixation prediction). And one interesting fact the authors found was\u2026.. (shown below)\n\nBenchmarking\n\nHere the authors have compared many state of the art algorithms that performed salient object segmentation and found out that when the algorithm was not trained on FT dataset their performances decreased significantly.\n\nDataset design bias\n\nThe authors really went all out in this section, they performed many statistical analysis such as comparing Local color contrast, Global color contrast, Local gPB boundary strength, and Object size. And they have summed the founding into one paragraph\n\nBasically, in FT data set there is a strong contrast between the object that we want to segment and the back ground image of that object. That makes it easier for the model to learn how to segment an object, but fails to generalize well.\n\nFixations and F-measure\n\nHere the author discuss the effect of center bias problem, and the methods of many state of the art algorithms to counteract the center bias problems. For example, in AWS and SIG they benchmark their algorithm performance in s-AUC which cancels out the center bias problem."
    },
    {
        "url": "https://towardsdatascience.com/web-scraping-regular-expressions-and-data-visualization-doing-it-all-in-python-37a1aade7924",
        "title": "Web Scraping, Regular Expressions, and Data Visualization: Doing it all in Python",
        "text": "While most data used in classes and textbooks just appears ready-to-use in a clean format, in reality, the world does not play so nice. Getting data usually means getting our hands dirty, in this case pulling (also known as scraping) data from the web. Python has great tools for doing this, namely the library for retrieving content from a webpage, and (BeautifulSoup) for extracting the relevant information.\n\nThese two libraries are often used together in the following manner: first, we make a GET request to a website. Then, we create a Beautiful Soup object from the content that is returned and parse it using several methods.\n\nThe resulting soup object is quite intimidating:\n\nOur data is in there somewhere, but we need to extract it. To select our table from the soup, we need to find the right CSS selectors. One way to do this is by going to the webpage and inspecting the element. In this case, we can also just look at the soup and see that our table resides under a HTML tag with the attribute . Using this info and the method of our soup object, we can pull out the main article content.\n\nThis returns another soup object which is not quite specific enough. To select the table, we need to find the tag (see above image). We also want to deal with only the text in the table, so we use the attribute of the soup.\n\nWe now have the exact text of the table as a string, but clearly is it not of much use to us yet! To extract specific parts of a text string, we need to move on to regular expressions. I don\u2019t have space in this article (nor do I have the experience!) to completely explain regular expressions, so here I only give a brief overview and show the results. I\u2019m still learning myself, and I have found the only way to get better is practice. Feel free to go over this notebook for some practice, and check out the Python documentation to get started (documentation is usually dry but extremely helpful).\n\nThe basic idea of regular expressions is we define a pattern (the \u201cregular expression\u201d or \u201cregex\u201d) that we want to match in a text string and then search in the string to return matches. Some of these patterns look pretty strange because they contain both the content we want to match and special characters that change how the pattern is interpreted. Regular expressions come up all the time when parsing string information and are a vital tool to learn at least at a basic level!\n\nThere are 3 pieces of info we need to extract from the text table:\n\nFirst up is the name. In this regular expression, I make use of the fact that each name is at the start of a line and ends with a comma. The code below creates a regular expression pattern, and then searches through the string to find all occurrences of the pattern:\n\nLike I said, the pattern is pretty complex, but it does exactly what we want! Don\u2019t worry about the details of the pattern, but just think about the general process: first define a pattern, and then search a string to find the pattern.\n\nWe repeat the procedure with the colleges and the salary:\n\nUnfortunately the salary is in a format that no computer would understand as numbers. Fortunately, this gives us a chance to practice using a Python list comprehension to convert the string salaries into numbers. The following code illustrates how to use string slicing, , and , all within a list comprehension to achieve the results we want:\n\nWe apply this transformation to our salaries and finally have the all info we want. Let\u2019s put everything into a dataframe. At this point, I manually insert the information for my college (CWRU) because it was not in the main table. It\u2019s important to know when it\u2019s more efficient to do things by hand rather than writing a complicated program (although this whole article kind of goes against this point!).\n\nThis project is indicative of data science because the majority of time was spent collecting and formatting the data. However, now that we have a clean dataset, we get to make some plots! We can use both and to visualize the data.\n\nIf we aren\u2019t too concerned about aesthetics, we can use the built in dataframe plot method to quickly show results:\n\nTo get a better plot we have to do some work. Plotting code in Python, like regular expressions, can be a little complex, and it takes some practice to get used to. Mostly, I learn by building on answers on sites like Stack Overflow or by reading official documentation.\n\nAfter a bit of work, we get the following plot (see notebook for the details):\n\nMuch better, but this still doesn\u2019t answer my original question! To show how much students are paying for 5 minutes of their president\u2019s time we can convert salaries into $ / five minutes assuming 2000 work hours per year.\n\nThis is not necessarily a publication-worthy plot, but it\u2019s a nice way to wrap up a small project.\n\nThe most effective way to learn technical skills is by doing. While this whole project could have been done manually inserting values into Excel, I like to take the long view and think about how the skills learned here will help in the future. The process of learning is more important than the final result, and in this project we were able to see how to use 3 critical skills for data science:\n\nNow, get out there and start your own project and remember: it doesn\u2019t have to be world-changing to be worthwhile.\n\nI welcome feedback and discussion and can be reached on Twitter @koehrsen_will."
    },
    {
        "url": "https://towardsdatascience.com/word-morphing-9f87ee577775",
        "title": "Word Morphing \u2013",
        "text": "In this post I\u2019ll describe how I employed word2vec\u2019s embeddings and A* search algorithm to morph between words.\n\nIn order to perform word morphing, we\u2019ll define a graph G where the set of nodes N represent words, and there\u2019s some non negative weight function f:N\u00d7N\u2192\u211d. Given a start word S and an end word E, our goal is to find a path within the graph which minimizes the sum of weights induced by f:\n\nUsually when people talk about word morphing they mean searching for a path between S and E where there\u2019s an edge only between words such that one can be achieved from the other by changing one letter, as can be seen here. In this case, f(n\u2081,n\u2082) is 1 when such a change exists, and \u221e otherwise.\n\nIn this post I\u2019ll show you how to morph between semantically similar words, i.e. f will be related to semantics. Here\u2019s an example to illustrate the difference between the two approaches: given S=tooth, E=light, the approach of changing one character at a time can result in\n\nwhile the semantics approach which will be defined in this post results in\n\nYou can find the entire code here."
    },
    {
        "url": "https://towardsdatascience.com/an-overview-of-business-metrics-for-data-driven-companies-b0f698710da1",
        "title": "An Overview of Business Metrics for Data-Driven Companies",
        "text": "As I am further into the data science journey, I had realized that the data-driven companies need people to have business intelligence and communication skills equally (or more than) data analysis / Machine Learning skills. One does not imply the other.\n\nWith this kind of basic business knowledge, we would be able to analyze and explore the data better and readily map it to possible increase in revenue, profitability or reducing the risk.\n\nThis is an example of not data driven metric\n\nThanks a ton to Daniel Egger: the one who taught the course business metrics on Coursera.\n\nThe reason why I refrain from writing about core machine learning skills or project is, there are already enough number of articles on various platforms, which are very informative. AnalyticsVidhya and MachineLearningMastery almost cover everything needed for a beginner to start and excel at developing skill-set. TowardsDataScience covers State of the Art techniques. Kaggle provides a platform to practice. I don\u2019t want to be repetitive. Instead I decided to touch upon a few rarely discussed topics in data science community. Hope this provides value and motivates aspiring data scientists to educate themselves on the business side.\n\nYou can follow me on Twitter too.\n\nI do paid consulting too. If you want help with your projects, or writing posts, you can contact me through LinkedIn"
    },
    {
        "url": "https://towardsdatascience.com/machine-learning-with-swift-for-tensorflow-9167df128912",
        "title": "Understanding Swift for TensorFlow \u2013",
        "text": "Swift for TensorFlow was introduced by Chris Lattner at TensorFlow Dev Summit 2018. On April 27, 2018 Google team has made its first release to public community on their GitHub repository. But Swift for TensorFlow is still in its infancy stage. And it seems to be too early for developers/researchers to use it in their projects. If you are still interested in trying it out then install Swift for TensorFlow\u2019s snapshot from Swift Official website.\n\nIn this article I\u2019ll focus on explaining the following topics:\n\nNote: One major thing to note about Swift for TensorFlow is that it is a define-by-run framework. This means that although Swift for TensorFlow is creating graphs in the backend [like TensorFlow] but you don\u2019t have to create sessions for executing these graphs. This approach is similar to eager-execution as in TensorFlow.\n\nWith Swift for TensorFlow we can use Python APIs in the most Pythonic way. To access a Python APIs one has to into the program as in the following example code snippet.\n\nAnd Swift for TensorFlow also has a new type called which exhibits the complete dynamic type system behavior of Python in Swift without affecting the behavior of other types in Swift.\n\nFor more information please refer to official Python Interoperability documentation.\n\nSwift for TensorFlow has built-in support for computing gradients of functions with respect to other variables. This functionality has been directly incorporated into Swift\u2019s compiler for optimized behavior. It supports two differential functions: and . Although it didn\u2019t work for me \ud83d\ude29 (it\u2019s too early) but the documentation says the following syntax to follow. This code snippet is obtained from official documentation.\n\nCurrently only automatic reverse differentiation is allowed and forward differentiation is in discussions.\n\nAs a simple example we will create a instance on which we apply some operations using TensorFlow.\n\nIn above code used the basic operator which adds the to itself in a loop. This becomes possible because of Swift\u2019s Advanced Operators feature that overloads for instances.\n\nMoving on to the neural network\u2014true reason for the popularity of machine learning in this era. In this section we will teach our 3-layer fully connected feed-forward neural network to predict the digit in images from MNIST dataset. To begin with TensorFlow in Swift in your Swift file first.\n\nNote: The training code can be found here if someone\u2019s impatient.\n\nOur neural network will have 3 layers:\n\nThe affine transformation is basically the dot product of data with weights and then the addition of biases further followed by the element-wise application of activation function. Following is the equation for affine transformation of input data x.\n\nHere, O(.) is output function, f(.) is activation function (sigmoid in our case), W is weight matrix, b is bias vector, and \u2022 represents dot product.\n\nWe use sigmoid activation function because it squashes the values to the range [0, 1] restricting the output range and thus provides the probability of possible digit in image at output layer.\n\nLet\u2019s read the dataset and construct instances of these images and labels. We have to create objects because this is what a TensorFlow model (neural network) allows to flow through it hence its name.\n\nNext we create 2 weight matrices, 2 bias vectors in terms of TensorFlow\u2019s type as following.\n\nThe training loop is the block of code where the neural network does its learning. We pass the images and labels through the network (forward pass). Then computes the errors in prediction and then back propagates them to compute the gradients of trainable parameters. Next we descent these parameters using their corresponding gradients with the help of learning rate. Finally the loss is computed giving the notion of how far we are from the images\u2019 true labels. Each of these steps are described as follows.\n\nAs already discussed above the input image pixel values go through affine tranformation. Here the values are dot product with weights and then add the biases which further goes through sigmoidal activation function (applied element-wise).\n\nOne thing to note here is the Swift uses \u2297 unicode to represent dot product which shows that how cool Swift language actually is! Frankly I really love \u2665\ufe0f this programming language.\n\nThe backward pass computes the error between predictions and true labels. These errors are then back-propagated through the network computing the gradients of learnable parameters.\n\nNow we descent the parameters with their gradients and the learning rate which decides the velocity with which the neural network learns it parameters so as to predict true-er values when next time the input image is fed to it.\n\nWe update the loss value to see that how close we are to true labels so as to predict the digit images more correctly next time.\n\nLet us now print our loss which tells that how well we perform on learning to recognize digit images from our training set. Lower the loss better is our network at recognition task.\n\nIn this article we learned about Swift for TensorFlow and how easy is it to use it since Swift is much similar to Python and looks like scripting language but is pretty fast. We saw that Swift for TensorFlow allows us to use Python APIs and also that Swift\u2019s compiler has been deeply optimized to built-in support for automatic differentiation which is something very important for machine learning tasks. We also saw how to use TensorFlow with Swift and we created our instances and played with them a little (using basic operator ). Finally we trained our 3-layer neural network to solve the traditional problem of recognizing digit images.\n\nIt seems like the name should\u2019ve been TensorFlow for Swift instead of Swift for TensorFlow. It\u2019s not the case because actually Swift\u2019s compiler has been modified to support TensorFlow therefore Swift doesn\u2019t only act as wrapper around Python libraries and TensorFlow but more like machine learning language now. To keep the workflow consistent throughout the machine learning and data science community (since Python is heavily used) it also allows access to Python APIs in Pythonic way and also has a new type for Python\u2019s dynamic system type behavior of instances.\n\nAs last words, Swift for TensorFlow is being built by Google so it is very likely that it will become famous in the coming times. Also it is trying to take best capabilities of original TensorFlow implementation such as eager-execution.\n\nIf you find this article useful/knowledgeable please hit some claps \ud83d\udc4f so that others can find it too or you may also share it on social networks. You may also drop a comment below if you find some error in my explanation (maybe I explained it wrongly) or if something isn\u2019t clear to you from this article."
    },
    {
        "url": "https://towardsdatascience.com/stochastic-weight-averaging-a-new-way-to-get-state-of-the-art-results-in-deep-learning-c639ccf36a",
        "title": "Stochastic Weight Averaging \u2014 a New Way to Get State of the Art Results in Deep Learning",
        "text": "In this article, I will discuss two interesting recent papers that provide an easy way to improve performance of any given neural network by using a smart way to ensemble. They are:\n\nAdditional prerequisite reading that will make context of this post much more easy to understand:\n\nTraditional ensembling combines several different models and makes them predict on the same input. Then some way of averaging is used to determine the final prediction of the ensemble. It can be simple voting, an average or even another model that learns to predict correct value or label based on the inputs of models in the ensemble. Ridge regression is one particular way of combining several predictions which is used by Kaggle-winning machine learning practitioners.\n\nWhen applied in deep learning, ensembling can be used to combine predictions of several neural networks to produce one final prediction. Usually it is a good idea to use neural networks of different architectures in an ensemble, because they will likely make mistakes on different training samples and therefore the benefit of ensembling will be larger.\n\nHowever, you can also ensemble models with the same architecture and it will give surprisingly good results. One very cool trick exploiting this approach was proposed in the snapshot ensembling paper. The authors take weights snapshot while training the same network and then after training create an ensemble of nets with the same architecture but different weights. This allows to improve test performance, and it is a very cheap way too because you just train one model once, just saving weights from time to time.\n\nYou can refer to this awesome post for more details. If you aren\u2019t yet using cyclical learning rates, then you definitely should, as it becomes the standard state-of-the art training technique that is very simple, not computationally heavy and provides significant gains at almost no additional cost.\n\nAll of the examples above are ensembles in the model space, because they combine several models and then use models\u2019 predictions to produce the final prediction.\n\nIn the paper that I am discussing in this post, however, the authors propose to use a novel ensembling in the weights space. This method produces an ensemble by combining weights of the same network at different stages of training and then uses this model with combined weights to make predictions. There are 2 benefits from this approach:\n\nLet\u2019s see how it works. But first we need to understand some important facts about loss surfaces and generalizable solutions.\n\nThe first important insight is that a trained network is a point in multidimensional weight space. For a given architecture, each distinct combination of network weights produces a separate model. Since there are infinitely many combinations of weights for any given architecture, there will be infinitely many solutions. The goal of training of a neural network is to find a particular solution (point in the weight space) that will provide low value of the loss function both on training and testing data sets.\n\nDuring training, by changing weights, training algorithm changes the network and travel in the weight space. Gradient descent algorithm travels on a loss plane in this space where plane elevation is given by the value of the loss function.\n\nIt is very hard to visualize and understand the geometry of multidimensional weight space. At the same time, it is very important to understand it because stochastic gradient descent essentially traverses a loss surface in this highly multidimensional space during training and tries to find a good solution \u2014 a \u201cpoint\u201d on the loss surface where loss value is low. It is known that such surfaces have many local optima. But it turns out that not all of them are equally good.\n\nOne metric that can distinguish a good solution from a bad one is its flatness. The idea being that training data set and testing data set will produce similar but not exactly the same loss surfaces. You can imagine that a test surface will be shifted a bit relative to the train surface. For a narrow solution, during test time, a point that gave low loss can have a large loss because of this shift. This means that this \u201cnarrow\u201d solution did not generalize well \u2014 training loss is low, while testing loss is large. On the other hand, for a \u201cwide\u201d and flat solution, this shift will lead to training and testing loss being close to each other.\n\nI explained the difference between narrow and wide solutions because the new method which is the focus of this post leads to nice and wide solutions.\n\nInitially, SGD will make a big jump in the weight space. Then, as the learning rate gets smaller due to cosine annealing, SGD will converge to some local solution and the algorithm will take a \u201csnapshot\u201d of the model by adding it to the ensemble. Then the rate is reset to high value again and SGD takes a large jump again before converging to some different local solution.\n\nCycle length in the snapshot ensembling approach is 20 to 40 epochs. The idea of long learning rate cycles is to be able to find sufficiently different models in the weight space. If the models are too similar, then predictions of the separate networks in the ensemble will be too close and the benefit of ensembling will be negligible.\n\nSnapshot ensembling works really well and improves model performance, but Fast Geometric Ensembling works even better.\n\nFast geometric ensembling is very similar to snapshot ensembling, but is has some distinguishing features. It uses linear piecewise cyclical learning rate schedule instead of cosine. Secondly, the cycle length in FGE is much shorter \u2014 only 2 to 4 epochs per cycle. At first intuition, the short cycle is wrong because the models at the end of each cycle will be close to each other and therefore ensembling them will not give any benefits. However, as the authors discovered, because there exist connected paths of low loss between sufficiently different models, it is possible to travel along those paths in small steps and the models encountered along will be different enough to allow ensembling them with good results. Thus, FGE shows improvement compared to snapshot ensembles and it takes smaller steps to find the model (which makes it faster to train)."
    },
    {
        "url": "https://towardsdatascience.com/installing-opencv-on-raspberry-pi-3-b-46ab17a9fc5a",
        "title": "Installing OpenCV on Raspberry Pi 3 B \u2013",
        "text": "I am using the B version, purchased on Amazon, with the following specs:\n\nThere is a newer version (\u201cB+\u201d) there now and it should work the same way, but some steps (such as compiling OpenCV, which took almost 2 hours on my device) should be faster.\n\nAlso, as expected, compiling OpenCV made the fanless CPU overheat 30 min into the process \u2014 so I had to place a powerful fan next to it, until I receive a case with proper cooling.\n\nStep 1: make sure you have the latest version of OS\n\nCurrent OS version is Raspbian Stretch (April 2018). You can either do a clean install from SD (follow the instructions listed here), or upgrade your existing version.\n\nTo upgrade, open (as sudo) the files /etc/apt/sources.list and /etc/apt/sources.list.d/raspi.list in your favorite editor, and change all occurences of your current distro name (e.g. \u201cjessie\u201d) to \u201cstretch\u201d. Then, open a terminal and run the update:\n\nMake sure SSH is enabled. Change the default password, too!\n\nSome of my favorite utilities on Linux are screen (to keep processes running if your terminal session is dropped) and htop (performance monitoring) \u2014 these may already be pre-installed:\n\nStep 3: free up 1GB+ by ditching Wolfram and Libreoffice\n\nIt\u2019s unlikely that you will need these two packages on your computer vision box, so:\n\nWe need this in order to enable Python bindings in Open CV:\n\nI am using version 3.4.1 of OpenCV. You can check the Releases section of the official site (or Github) to see what the current build is. If your desired version is different, update the commands and paths below accordingly.\n\nDownload and unzip OpenCV 3.4.1 and its experimental modules (those are stored in the opencv_contrib repository):\n\nThese are the lowest-level tools for managing Python packages.\n\nModify your ~/.profile file to include the following lines:\n\nThis is what my file looks like:\n\n\u2026 or, if you want to use Python 2.7 instead of Python 3:\n\nThese are the basic commands for working with virtualenvwarapper:\n\nIn our case, we can activate a virtualenv called \u201ccv\u201d:\n\nNow that you are inside your virtual environment (as evidenced by the \u201c(cv)\u201d prefix in your terminal window), let\u2019s install some additional packages for data analysis \u2014 numpy and scipy:\n\nNote: this will take a long, long, long time. Took almost 2 hours on my device. Also, your Raspberry Pi will overheat without proper cooling.\n\nAgain, I am using version 3.4.1 of OpenCV. If you aren\u2019t \u2014 update your paths accordingly:\n\nNow, get yourself a glass of beer, and prepare for the final step \u2014 compiling. To speed things up, temporarily increase the swap file size in your /etc/dphys-swapfile by changing CONF_SWAPSIZE from 100 to 1024:\n\nTo avoid rebooting in order for these changes to take effect, simply restart the swap service:\n\nThere\u2019s a detailed guide on how to compile OpenCV with packages here:\n\nWhat I did (using all 4 CPU cores):\n\n\u2026 then reboot the system \u2014 and you should be good to go!\n\nIf you are getting an error (ImportError: No module named \u2018cv2\u2019), the library may be named incorrectly:\n\nFix it by renaming the library file to \u201ccv2.so\u201d:"
    },
    {
        "url": "https://towardsdatascience.com/weekly-selection-apr-27-2018-80e65734fd9d",
        "title": "Weekly Selection \u2014 Apr 27, 2018 \u2013",
        "text": "Transfer Learning is the reuse of a pre-trained model on a new problem. It is currently very popular in the field of Deep Learning because it enables you to train Deep Neural Networks with comparatively little data.\n\nThis tutorial is part one of a two part series about Restricted Boltzmann Machines, a powerful deep learning architecture for collaborative filtering. In this part I introduce the theory behind Restricted Boltzmann Machines."
    },
    {
        "url": "https://towardsdatascience.com/detecting-breast-cancer-with-a-deep-learning-10a20ff229e7",
        "title": "Detecting Breast Cancer with Deep Learning \u2013",
        "text": "Invasive ductal carcinoma (IDC) also known as infiltrating ductal carcinoma is most common type of breast cancer. American Cancer Society estimates more than 180,000 women in the United States find out they have invasive breast cancer every year. Most of these cancers are diagnosed with IDC.\n\nAccurately identifying and categorizing breast cancer subtype is an important task. Automated methods based on AI can significantly save time and reduce error.\n\nIn this article I will be building a WideResNet based neural network to categorize slide images to two classes one that contains breast cancer and other that don\u2019t using the Deep Learning Studio.\n\nIf you are not familiar with Deep Learning take a look at this :)\n\nAnd if you want to know more about Deep Cognition see this:\n\nDataset for this problem has been collected by researcher at Case Western Reserve University in Cleveland, Ohio. Original dataset is available here (Edit: the original link is not working anymore, download from Kaggle). This dataset is preprocessed by nice people at Kaggle that was used as starting point in our work.\n\nThere are 162 whole mount slides images available in the dataset. These slides have been scanned at 40x resolution. Finally, those slides then are divided 275,215 50x50 pixel patches. Then one label of 0 or 1 is assigned to each of these patches. For patches that include IDC has label of 1 and patches that don\u2019t include IDC have label of 0.\n\nResNet architecture that uses residual connections have been very successful at image classification tasks. WideResNet architecture has shown that similar performance can be achieved with much less depth as small as 16 layers deep. This helps in solving various problems associated with very deep ResNets like exploding/vanishing gradients and degradation.\n\nUsing the great information in the blogs by Vincent Fung and Apil Tamang we can get some intuiton about what a ResNet is actually doing.\n\nThe core idea of ResNet is introducing a so-called \u201cidentity shortcut connection\u201d that skips one or more layers.\n\nThe original authors of the paper hypothesized that letting the stacked layers fit a residual mapping is easier than letting them directly fit the desired underlaying mapping. This indicates that the deeper model should not produce a training error higher than its shallower counterparts.\n\nBecause of its compelling results, ResNet quickly became one of the most popular architectures in various computer vision tasks.\n\nNow a WideResNet exist for a reason: each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train.\n\nTo tackle these problems Zagoruyko and Komodakis conducted a detailed experimental study on the architecture of ResNet blocks (published in 2016), based on which they proposed a novel architecture where we decrease depth and increase width of residual networks. They called them Wide Residual Networks.\n\nNow we will show step by step process of solving this problem using WideResNet architecture. We are using Deep Learning Studio that allows us to build neural network rapidly without need to worry about coding, syntax and dataset ingestion.\n\nAfter you log in to Deep Learning Studio that is either running locally or in cloud click on + button to create a new project.\n\nWe then setup dataset for this project in \u201cData\u201d tab. Usually 80% \u2014 20% is a good split between training and validation but you can use other setting if you prefer. Also don\u2019t forget to set Load Dataset in Memory to \u201cFull dataset\u201d if your machine has enough RAM to load full dataset in RAM.\n\nYou can create a neural network as shown below by dragging and dropping the layers.\n\nMake sure to set WideResNet 100% trainable from the properties on the right side. Also first Dense layer (Dense_3) should have 20 or so neurons with ReLU as activation function. Final Dense layer (Dense_1) should have output dimension as 1 and activation as sigmoid. Reason of this is because we have setup this problem as a regression instead of classification. If the regression output is below 0.5 then we can say that input belongs to class 0 (no IDC cancer), or else it has IDC cancer.\n\nHyperparameters that we used are shown below. Feel free to change and experiment with them.\n\nFinally, you can start the training from Training Tab and monitor the progress with training dashboard.\n\nOnce you complete your training you can check the results in results tab. We achieved more than 85% accuracy in matter of couple of hours on a K80 GPU that costs about $0.90 per hour.\n\nWith Deep Learning Studio deployment as a webapp or REST API is child\u2019s play can be done using deploy tab as shown below.\n\nDeployed model can be accessed as WebApp or REST API as shown below:\n\nSo you can see that a Deep Learning model can be built in minutes and deployed in seconds with Deep Learning Studio. Such power will enable many developers to tackle complex problem without worrying about coding, API etc.\n\nI\u2019ll repeat here what I said in the \u201cDeep Learning made easy with Deep Cognition\u201d blog about the \u201cBlack-Box problem\u201d:\n\nThanks to Deep Cognition for helping me build this article :)"
    },
    {
        "url": "https://towardsdatascience.com/how-i-got-in-the-top-1-on-kaggle-79ddd7c07f1c",
        "title": "How I got in the top 1 % on Kaggle. \u2013",
        "text": "I participated in Santander Customer Satisfaction challenge, ran on Kaggle for 2 months and got into top 1%. Here, I would be discussing my approach to this problem.\n\nCustomer Satisfaction is one of the prime motive of every company. From frontline support teams to C-suites, customer satisfaction is a key measure of success. Unhappy customers don\u2019t stick around. What\u2019s more, unhappy customers rarely voice their dissatisfaction before leaving.\n\nSantander Bank floated a problem statement to help them identify dissatisfied customers early in their relationship. Doing so would allow Santander to take proactive steps to improve a customer\u2019s happiness before it\u2019s too late.\n\nIn this competition, data was some hundreds of anonymized features to predict if a customer is satisfied or dissatisfied with their banking experience. The \u201cTARGET\u201d column is the variable to predict. It equals 1 for unsatisfied customers and 0 for satisfied customers.\n\nLet peek into the dataset:\n\nYou will find that unhappy customers are little under 4 %, which implies that the dataset is very unbalanced. Handling unbalanced dataset it another challenge that has to be tackled in this challenge.\n\nNot much was done. Here we are converting non-numeric data into numeric using one hot encoding. Also NaN are filled with the mean of the series.\n\nI used XGBoost. XGBoost is the leading model for working with standard tabular data (the type of data you store in Pandas DataFrames, as opposed to data like images and videos). XGBoost models dominate many Kaggle competitions.\n\nXGBoost is an implementation of the Gradient Boosted Decision Trees algorithm.\n\nThe implementation of the algorithm is such that the compute time and memory resources are very efficient. A design goal was to make the best use of available resources to train the model. Some key algorithm implementation features include:\n\nXGBoost has a few parameters that can change accuracy and speed of your model significantly.\n\nn_estimators specifies how many times to go through the model. Too large a value can causes overfitting, which is accurate predictions on training data, but inaccurate predictions on new data, which means your model is not generalising instead mugging up the training dataset and in this challenge this happened with many participant as in contest ranking and final ranking were having huge difference. You can experiment with your dataset to find the ideal. Typical values range from 100\u20131000, though this depends a lot on the learning rate discussed below.\n\nThe argument early_stopping_rounds offers a way to automatically find the ideal value. Early stopping causes the model to stop iterating when the validation score stops improving, even if we aren\u2019t at the hard stop for n_estimators. It\u2019s smart to set a high value for n_estimators and then use early_stopping_rounds to find the optimal time to stop iterating.\n\nIn general, a small learning rate (and large number of estimators) will yield more accurate XGBoost models, though it will also take the model longer to train since it does more iterations through the cycle.\n\nMy final score was 0.840968 which fetched me 48th rank out of 5123 team participated."
    },
    {
        "url": "https://towardsdatascience.com/highlights-from-the-trinity-mirror-data-unit-this-week-e9e811cf62d9",
        "title": "Highlights from the Trinity Mirror Data Unit this week",
        "text": "Okay, it\u2019s actually a fortnight of highlights. I was visiting my spiritual home of the East Midlands last Friday, and didn\u2019t have time for a round-up.\n\nTerrible, in a word. The Home Office recorded crime figures released this week presented a grim picture pretty much everywhere. Violent crime, possession of knives and other weapons, burglary \u2014 all going up, and going up quickly.\n\nClaire Miller and Deb Aru did a great job going through the data for each police force and writing up their findings for titles across the group, resulting in (depressing but necessary) coverage like this:\n\nTo pick out just a few lines they found: Greater Manchester now has the highest crime rate in the country; crime rose 40 per cent in Durham in a single year, more than anywhere else; and both Durham and Northumbria now how crime rates which are higher than London.\n\nOne thing this story showed is the value of a single data journalist (or team of journalists) looking at datasets for all areas. It gives them a really good bird\u2019s-eye view of the situation; a context that you might not have if you were just looking at one particular police force.\n\nMost people know the detection rates for bike thefts are pretty poor. But how poor? And are there particular hotspots?\n\nRob Grant decided to analyse the data on a hyper-local level to find out.\n\nIn the Foleshill ward of Coventry, there were 64 thefts in 2017.\n\nHow many suspects were ever identified?\n\nIt was the same in other places: Edgbaston in Birmingham, for example, where there were 99 bike thefts and no suspects identified.\n\nNow obviously this is not good news. But it is worth pointing out that we only know such things because open police data in the UK is good, and improving all the time. There\u2019s a big difference between publishing (say) annual datasets at a force-wide level (as in the case above), and publishing fine-grained details of individual crimes. Our government and police do both, and while we might not like the stories that emerge, we should be grateful for the opportunity to find them. American crime data is better, but many countries have it much, much worse.\n\nTrinity Mirror hosted a Google Survey, in conjunction with other regional press groups, about Brexit. It got more than 200,000 responses which is, I believe, a record for a survey of this sort.\n\nNow you can argue about the value of such surveys: it is entirely reasonable to point out that they are not demographically weighted in the same way as an opinion poll, and you can (indeed should) set that against the scale of responses.\n\nIn any case, our role was to pick out lines and find a way of presenting the findings to anyone who wanted them, whether within the Trinity Mirror group or not.\n\nClaire did this by accompanying her copy with a public Tableau visualisation which was never meant to be embedded in our sites, but rather to make the data available to journalists. You can try it out here.\n\nThe findings of the survey? People want to be in the Single Market. They are deeply concerned about Brexit negotiations. They fear Brexit will make us worse off.\n\nBut they would probably still vote for it anyway.\n\nThere are various things you could take from all this. What I take from it is that what drives opposition isn\u2019t ignorance about the potential economic risks, but a belief in the whole \u201ctake back control of our laws/borders/immigration levels\u201d argument that trumps pretty much any other considerations.\n\nWhat follows from this? Well, if you are hoping to persuade a Brexit-voter to switch allegiance by piling up economic evidence, or evidence of botched negotiations, you might as well bang your head against a brick wall.\n\nThings that we know audiences like: (i) fine-grained data giving a really local picture; (ii) postcode-search interactives allowing people to personalise the story; (iii) stories about kids getting fat.\n\nNew data was published this week showing the proportion of schoolchildren who were either overweight, or clinically obese, broken down to council-ward level.\n\nAnnie Gouk analysed this data and found Camberwell Green in Southwark, London, had the biggest problem (literally). Some 33 per cent of 10- and 11-year-olds there are obese, and a further 51 per cent are overweight.\n\nShocking for the people of Camberwell Green, true. But most people aren\u2019t Camberwell Green people, and don\u2019t care too much about those who are.\n\nFor everyone else, then, why not give them a way of getting their own figures, at a glance? Suddenly you\u2019ve increased the number of people who might be interested by a factor of hundreds.\n\nYou can try the gadget out on any version of the story we wrote, e.g. this one.\n\nAnnie provided regional stories based on analysis of the latest figures for foodbank use; the Manchester Evening News\u2019 Charlotte Cox incorporated these into a wider feature about the people who actually use foodbanks. It\u2019s a really great read and a really good example of how local centres can built on our work:\n\nWe also wrote about rising numbers of people being hospitalised because of wounds from firearms (e.g. in Birmingham and West London), and broke the story of record-breaking numbers of assaults in prisons, at a local level, a full day before you\u2019ll have seen the national story.\n\nOur sport stories included this one about Fulham potentially setting a most unwelcome record in the Championship this year.\n\nFinally, we were super-happy to win the international print innovation award at #newsawards18 for the daily and weekly data-based print pages we do for titles across the group. Hurrah for Marianna Longo, Alice Cachia, Kelly Leung and coding genius Carlos Novoa!\n\nWe are also delighted to have four nominations at this year\u2019s Regional Press Awards. These are possibly the most important to us, as we were set up to serve the regional titles in the Trinity Mirror group, and to support regional journalism more generally.\n\nWe\u2019re shortlisted as a team for the overall digital award (we won it a couple of years ago and were shortlisted again last year) and the award for digital initiative of the year.\n\nMarianna Longo is shortlisted for designer of the year while Annie is up for specialist reporter of the year.\n\nThat last one is a big development for the data journalism industry as a whole, I reckon. It\u2019s brilliant to see a full-time data journalist being recognised for their investigative work alongside the best in their profession, and a potential breakthrough moment for all of us who believe data journalism is mainstream journalism.\n\nYou can read more about Annie\u2019s work here."
    },
    {
        "url": "https://towardsdatascience.com/this-is-what-i-really-do-as-a-data-scientist-d637ed747ef9",
        "title": "This is what I really do as a Data Scientist \u2013",
        "text": "This is what I really do as a Data Scientist\n\nData Science is getting very popular and many people are trying to jump into the bandwagon, and this is GREAT. But many assume that data science, machine learning, plug any other buzzword here, is to plug data to some Sckit-Learn libraries. Here is what the actual job is.\n\nTo bring you into context, the following is happening after the data was collected. Don\u2019t get me wrong, I don\u2019t think it should be considered a simple step, but I would like to focus on data pre-processing and normalization.\n\nIf you followed my blogs, you probably realized that I work a lot in the Machine 2 Machine field. Recently at work I was trying to cluster machine together based on their behaviour, aka their data consumption.\n\nThese features report the usage download and upload of specific categories of services, e.g. Social Media, Audio, etc. For this, let\u2019s assume that we are looking at a counter that looks at how much connection was established to AWS (Amazon Web Services).\n\nIf we take the data as is and decide to pull clusters out of it, we get something like this:\n\nNote, this an LDA 2 axis representation of the data. The LDA as a \u00b1 90% representation factor. Even if it\u2019s not perfect the clustering is close to useless. At this point, some I suggest using this or that algorithm and to tune the hyper parameters and this is exactly the worst idea.\n\nNow, let\u2019s work a bit. What does the data look like. Advise you should spend a lot of time before typing some from sklearn.cluster import KMeans. We will look at one specific feature for this example but, keep in mind that most of them were similar.\n\nHere is the AWS Counter (Not really but we will pretend).\n\nAll the data seems to be 0, but if you look in detail, there is value going up to 3e+7. How on earth do you expect to create any meaningful distance with this. Even if you scale it between 0 and 1 most of the data will be between 0 and 0.0000005 or something like this, hence the distance won\u2019t get any better.\n\nIf we look at all the non-zero data, we have something more interesting:\n\nThis start to look like a LogNormal distribution. Now how can we normalize it. At least a bit. So a colleague of mine, Tanguy had a very clever idea on how to convert that lognormal Distribution. The Box Cox transform. This transform tries to \u201cnormalize\u201d as much as possible a different distribution including LogNormal.\n\nThis transform tries to minimize the lamda value in this formula (Picture from Wikipedia) :\n\nIn our case, there is so much zeros that lamda would be lower to something like this: (Note: You need to get only positive value so 1 was added to all values)\n\nIn this picture you can see the small bump at around 9 and it contains most of the non-zero data. At this point we already have something better, order of magnitude better in terms of distances but still it could be improved.\n\nLet\u2019s bring back the context of this data again. We are trying to categorized machine behaviour. In the world of machine to machine there is a lot of information hidden in this. The machine CAN use AWS. It sounds funny but it\u2019s very important here. These machines are usually programmed to do a very specific task, e.g. Report weather, show adds, etc. Their task is usually hard-coded and they don\u2019t randomly start going on Facebook or something. Just the fact that they can use a service (like AWS) is a lot of information. So my plan was set, I will leave the zeros at zeros and then give the range 0.5 to 1 for the rest of this data. Now how to express the rest of the data\u2026 Box Cox transform. I will do the transform on the only non-zero devices.\n\nSo here is what it looks. On the left you can see all the data transformed like explained in a 0\u20131 space. On the right, you have a zoomed graphic of the section between 0.5 and 1.\n\nEven if the left graph does not seem that better, I can assure you that for the algorithm it\u2019s a world of differences.\n\nSo, after pre-processing the data, we ran the clustering algorithm on it, and without any customization we got that results.\n\nI find people are a bit \u201ctrigger happy\u201d on the algorithms and jump too quickly to the model building. Some say that you don\u2019t even have to understand the math behind and I disagree, you need to understand at least the fundamental of an algo. At lease enough to understand how to prepare the data for it. As examples in K-Means it\u2019s all about the distance between point, and when you have this type of range (10e7) you cannot expect the algo to find subtitle patterns, it will be blinded by the range.\n\nSo please, look at your data again and again. To the point you feel you can separate it by hand, then let fancy algorithms to the rest of the work."
    },
    {
        "url": "https://towardsdatascience.com/deep-learning-meets-physics-restricted-boltzmann-machines-part-i-6df5c4918c15",
        "title": "Deep Learning meets Physics: Restricted Boltzmann Machines Part I",
        "text": "In my opinion RBMs have one of the easiest architectures of all neural networks. As it can be seen in Fig.1. a RBM consists out of one input/visible layer (v1,\u2026,v6), one hidden layer (h1, h2) and corresponding biases vectors Bias a and Bias b. The absence of an output layer is apparent. But as it can be seen later an output layer wont be needed since the predictions are made differently as in regular feedforward neural networks.\n\nEnergy is a term that may not be associated with deep learning in the first place. Rather is energy a quantitative property of physics. E.g. gravitational energy describes the potential energy a body with mass has in relation to another massive object due to gravity. Yet some deep learning architectures use the idea of energy as a metric for measurement of the models quality.\n\nOne purpose of deep learning models is to encode dependencies between variables. The capturing of dependencies happen through associating of a scalar energy to each configuration of the variables, which serves as a measure of compatibility. A high energy means a bad compatibility. An energy based model model tries always to minimize a predefined energy function. The energy function for the RBMs is defined as:\n\nAs it can be noticed the value of the energy function depends on the configurations of visible/input states, hidden states, weights and biases. The training of RBM consists in finding of parameters for given input values so that the energy reaches a minimum.\n\nRestricted Boltzmann Machines are probabilistic. As opposed to assigning discrete values the model assigns probabilities. At each point in time the RBM is in a certain state. The state refers to the values of neurons in the visible and hidden layers v and h. The probability that a certain state of v and h can be observed is given by the following joint distribution:\n\nHere Z is called the \u2018partition function\u2019 that is the summation over all possible pairs of visible and hidden vectors.\n\nThis is the point where Restricted Boltzmann Machines meets Physics for the second time. The joint distribution is known in Physics as the Boltzmann Distribution which gives the probability that a particle can be observed in the state with the energy E. As in Physics we assign a probability to observe a state of v and h, that depends on the overall energy of the model. Unfortunately it is very difficult to calculate the joint probability due to the huge number of possible combination of v and h in the partition function Z. Much easier is the calculation of the conditional probabilities of state h given the state v and conditional probabilities of state v given the state h:\n\nIt should be noticed beforehand (before demonstrating this fact on practical example) that each neuron in a RBM can only exist in a binary state of 0 or 1. The most interesting factor is the probability that a hidden or visible layer neuron is in the state 1 \u2014 hence activated. Given an input vector v the probability for a single hidden neuron j being activated is:\n\nHere is \u03c3 the Sigmoid function. This equation is derived by applying the Bayes Rule to Eq.3 and a lot of expanding which will be not covered here.\n\nAnalogous the probability that a binary state of a visible neuron i is set to 1 is:"
    },
    {
        "url": "https://towardsdatascience.com/the-unscented-kalman-filter-anything-ekf-can-do-i-can-do-it-better-ce7c773cf88d",
        "title": "The Unscented Kalman Filter: Anything EKF can do I can do it better!",
        "text": "I have just completed my Term 2 of Udacity Self Driving Car Nanodegree. I wrote about Kalman Filter and Extended Kalman Filter. Today we will look at another member of Kalman Filter Family: The Unscented Kalman Filter. So, if you read my last two posts you would be knowing my colleague Larry by now.\n\nSummary: \n\nKalman Filter: It is a tool to predict values using a bunch of mathematical equations under the assumptions that our data is in the form of Gaussian Distribution and we apply linear equations to that Gaussian distribution.\n\nExtended Kalman Filter: In real world, we have non linear equations, because we may be predicting in one direction while our sensor is taking reading in some other direction, so it involves angles and sine cosine functions which are non linear. So EKF takes helps of Taylor Series (and Jacobian Matrix further) to linearly approximate a non linear function around the mean of the Gaussian and then predict the values.\n\nLarry: I know about Kalman Filter and Extended Kalman Filter, now what? I know the reason why Kalman Filter failed in real life and the need of Extended Kalman Filter. Now why Unscented Kalman Filter?\n\nMe: Performance.\n\nLarry: Performance? How come?\n\nMe: How many points we took in EKF to approximate a new linear function from non linear function?\n\nLarry: 1 point, that is the mean of the Gaussian.\n\nMe: Correct, so is there a better way to linearize?\n\nLarry: What do you mean?\n\nMe: Have a look below what happened in EKF:\n\nWe have just one point to approximate the Gaussian. So, is there a better way to linearize?\n\nLarry: If I would have known that, I would not be talking to you. Tell me!\n\nMe: What do you think will give us a better approximation? Suppose we have two scenarios to reach from a Source Gaussian to an Approximated Gaussian-:\n\nScenario 1: We have one point (say mean)and we approximate around one point.\n\nScenario 2: We have a bunch of points including the mean and we approximate around those multiple points.\n\nLarry: My Intuition says if we have multiple points as in case of scenario 2, we will have a better approximation!\n\nMe: Congrats! You now know the Unscented Kalman Filter.\n\nLarry: So in that case why don\u2019t you consider all the points in source Gaussian and then transform and then approximate?\n\nMe: That will take a lot of computational power and resources, so it may be the most trivial solution but it is not optimal.\n\nLarry: Oh!! So how do we go about choosing the right number of points?\n\nMe: So in Unscented Kalman Filter we have a concept of Sigma Points. We take some points on source Gaussian and map them on target Gaussian after passing points through some non linear function and then we calculate the new mean and variance of transformed Gaussian.\n\nIt can be very difficult to transform whole state distribution through a non linear function but it is very easy to transform some individual points of the state distribution, these individual points are sigma points. These sigma points are the representatives of whole distribution.\n\nHere the main difference from EKF is that in EKF we take only one point i.e. mean and approximate, but in UKF we take a bunch of points called sigma points and approximate with a fact that more the number of points, more precise our approximation will be!\n\nLarry: Great! Got it! Its so simple.\n\nMe: Well, that\u2019s not the case, in addition to sigma points, these points also have weights, so these are weighted sigma points."
    },
    {
        "url": "https://towardsdatascience.com/technical-analysis-library-to-financial-datasets-with-pandas-python-4b2b390d3543",
        "title": "Technical Analysis library to financial datasets with Python Pandas",
        "text": "Technical Analysis library to financial datasets with Python Pandas\n\nDuring the last months at Lecrin Technologies, we have been studying some financial time series such as predict bitcoin price or different challenges proposed by Numer.ai, Two Sigma Investment or G-Research. Giving that said, we have decided to develop a technical analysis library in python based on the Pandas library. You can find the library at:\n\nThis new library is oriented to do \u201cFeature Engineering\u201d from typical financial datasets that typically include columns such as \u201cTimestamp\u201d, \u201cOpen\u201d, \u201cHigh\u201d, \u201cLow\u201d, \u201cClose\u201d and \u201cVolume\u201d. This library will be used by data scientifics that want to resolve Machine Learning problems using Python technology stack for data science (Pandas, Scikit-Learn, XGBoost, LightGBM, Keras, TensorFlow, etc).\n\nAt this moment, these tools are getting good results to predict almost anything, but they are not working correctly when are used to face financial problems. They are not working correctly because the rows in the dataset only contains information about a specific period of time (e.g. 6 hours or one day) which is not sufficient to generate good predictions with the current models. To improve the predictions, we need to provide more information (features) to the dataset as the current models get better results when more information is provided.\n\nTechnical Analysis is focused on providing new information from the past to forecast the direction of price. By adding the information generated by different indicators for the different variables (\u201cVolume\u201d, \u201cVolatility\u201d, \u201cTrend\u201d, \u201cMomentum\u201d, etc), we can improve the quality of the original dataset.\n\nNow, we will explain two examples in detail:\n\nThe Bollinger Bands are used to analyze the volatility of the price for an asset in a specific period of time. There are 3 bands, the Middle Band (MB) is the average of the price in the last n periods, the Upper (UB) and Lower Bands (LB) are equal to the middle band, but adding and subtracting x times the standard deviation. The normal parameters that are being used are n = 20 periods and x = 2. So:\n\nIn the library, the closing price variable is converted to 5 new features. Apart from the 3 Bollinger Bands, we generate another 2 indicators that will indicate when the closing value has a value higher than the Upper Bollinger Band or lower than the Lower Bollinger Band. Therefore, these two characteristics will be 0 except when the closing value get out of these bands, which will be 1.\n\nIf we take a look at the image 2, when the closing wave (blue) surpasses the upper or lower bands, there are sudden changes in the price, and it is usually a good idea to sell when it is higher than the Upper Band and to buy when it is lower than the Lower Band.\n\nMoving Average Convergence Divergence is a trading indicator that focuses on exponential moving average (EMA). To calculate it we use:\n\nThe theory tells us that when the MACD curve (blue) is smaller than the MACD_Signal (orange) or when the MACD difference (green curve which represents the difference between MACD_Signal and MACD curve) has a value lower than 0, the price trend will be bearish. On the contrary, it indicates a price increase.\n\nAt this moment, the library has implemented 25 indicators:\n\nThese indicators result in 48 features. The developers can set a lot of input parameters such as the size of windows, different constants or smart automatic fill NaN values generated in the methods.\n\nWe have uploaded a first stable version of the library to GitHub and it can be installed by using \u201cpip\u201d. The library is in continue development so we will be including more indicators, features, documentation, etc. Please, let us know about any comment, contribution or feedback."
    },
    {
        "url": "https://towardsdatascience.com/image-datasets-for-artificial-intelligence-bbb12615edd7",
        "title": "Image Datasets for Artificial Intelligence \u2013",
        "text": "A.I. algorithms and how they are plugged into each other is an art that is becoming known through university courses, online training, and literally by people watching YouTube videos. Artificial Intelligence is open source, and it should be. What you can do to protect your company from competition is build proprietary datasets.\n\nThere are plenty of datasets open to the public. For example, Kaggle, and other corporate or academic datasets, and many federal and municipal data sources. We use these for lots of projects, but so can everyone else. What you want is to build something \u201cspecial\u201d that I don\u2019t have. For example, you can\u2019t beat Google at search because they know what people search for and you don\u2019t. The advantage is the size and depth of their dataset, rather than market share alone.\n\nWe often come across requirements to build an artificial intelligence solution where the client needs an image dataset to move forward. The client often has no dataset of images to start off with. They can\u2019t simply use an off-the-shelf solution or API, because the typical 1000 objects in the off-the-shelf convolutional networks are not as broad as one would hope, and also a classifier that differentiates between 2 classes can be a lot more powerful than one with 1000. There are simply fewer chances to make the wrong prediction when the number of \u201coutput classes\u201d (types of things the system can see) is small, and so these specialized models tend to work well.\n\nWhat I want to walk you through today is one way that we build up these custom image datasets. Let\u2019s talk about the case where there are only 2 classes: infected leaves and healthy leaves. The idea is to use A.I. to distinguish between healthy and sickly leaves in a field somewhere.\n\nTo start, we install images-scraper and nodejs, and we limit the images we will scrape to non-https URLs. We make sure the URL has \u2018.jpg\u2019 on the end, and that is is well formed in general. What we need next is a set of keywords to scrape, and so we use the following keyword list to start off with:\n\nFrom here we generate combinations of the keyword list with the base keyword. For example:\n\nNext, we pass each of the combinations into a separate nodejs thread. The scraper collects the scraped images into a base directory with subfolders for each keyword combination. We then run scripts to remove duplicate and empty files.\n\nHere is a full example of how we scrape the images for infected leaves.\n\nAt this point we have 2 folders, one containing thousands of images of healthy leaves (and a lot of junk) and the other containing thousands of images of infected leaves (and a lot more junk). The next task is to browse the images by hand, and delete the images that are not relevant to leaves (a baby holding an umbrella), and then go through the images again, and remove images that are the wrong type (a drawing of a leaf, a 3D render of a leaf, etc). Finally, the human operator combs through the images and adds as much effort as they feel they need for a first pass. In later stages we may choose to crop images, or do other image cleanup. At this stage the goal is simply to make sure that bad data does not filter into the training data."
    },
    {
        "url": "https://towardsdatascience.com/computer-vision-for-lane-finding-24ea77f25209",
        "title": "Computer Vision for Lane Finding \u2013",
        "text": "Advanced computer vision techniques to identify lane lines from a video camera feed mounted on a car.\n\nCode for this project can be found on: Github.\n\nThis article can also be found on my website here.\n\nIn the fourth project of the udacity self-driving car engineer course , I applied computer vision techniques to detect lane lines on a road captured using a camera mounted at the front of a car. The Open Source Computer Vision Library (OpenCV) library was heavily utilized during this project.\n\nThe project consisted of the following stages:\n\nThe output from the camera is a video, which in essence is a time-series of images. Due to the nature of photographic lenses, images captured using pinhole camera models are prone to radial distortions which result in inconsistent magnification depending on the object\u2019s distance from the optical axis.\n\nThe following is an example image from OpenCV showcasing the two main types of radial distortions:\n\nIn order to correctly detect the lane lines in the image, we first need to correct for radial distortion.\n\nComputer vision researchers have come up with a way to correct this radial distortion. The camera to be calibrated is used to capture images of checkerboard patterns, where all the white and black boxes in the pattern are of the same size. If the camera suffers from distortions, the captured images will incorrectly show the measurements of the checkerboard.\n\nTo correct the effects of distortion, the corners of the checkerboard are identified and deviations from the expected checkerboard measurements are used to calculate the distortion coefficients. These coefficients are then used to remove radial distortion from any image captured using that camera.\n\nIn the diagram above, the leftmost image shows the original distorted image, the rightmost image shows the corners drawn on top of the distorted image, and the middle image shows the resultant undistorted image after camera calibration.\n\nThe OpenCV functions findChessboardCorners and calibrateCamera were used to achieve the above camera calibration process.\n\nNow that we\u2019ve calibrated our camera, I tested the results on actual footage from the car video. The below image shows the result of camera calibration:\n\nWith the undistorted images at hand, we now return to the main objective of detecting the lane lines on the road. One way to separate and detect objects in an image is to use colour transforms and gradients to generate a filtered-down thresholded binary image.\n\nFor color transforms, I experimented with three color spaces in order to find out which one is best at filtering the pixels representing the lane line on the road. Three colour spaces were tested:\n\nAfter some experimentation, I concluded that the b channel of the LAB colorspace and the L channel of the LUV colour space are the best combination for detecting the lane lines on the road.\n\nThe Sobel gradient filter was also considered. An image gradient measures the directional intensity of the colour change. Sobel is a type of gradient filter that uses Gaussian smoothing and differentiation operations to reduce the effects of noise.\n\nWe can now distinguish lane lines in the image, but the task of figuring out the exact angle/direction of the lane is difficult using the default camera view. In the default camera perspective, objects further away from the camera appear smaller and the lane lines appear to converge the further they are from the car, which isn\u2019t a true representation of the real world.\n\nOne way to fix this perspective distortion is to transform the perspective of the image such that we are looking at the image from above, also known as birds-eye view.\n\nOpenCV provides functions getPerspectiveTransform and warpPerspective, which can be used to apply a perspective transformation to a segment in the image. Firstly, we pick the area in the image we would like to apply the transformation to. In the image below, I\u2019ve picked the lane line segment in front of the car:\n\nThen, we choose the points representing the destination space we would like to transform the segment to, in our case any rectangle would suffice. The function will then return a 3x3 transformation matrix which can be used to warp any segment into our chosen perspective using the warpPerspective function.\n\nThe following image shows lanes lines from two different segments of the road with the perspective transformation successfully applied:\n\nNotice how it is now much easier to determine the curvature of the lane line!\n\nWe are now finally ready to fully detect the lane lines! As a start, we apply the binary thresholding disucssed in the Image Preprocessing section to the perspective transformed lane line segment. We now have an image where the white pixels represent parts of the lane line we are trying to detect.\n\nNext, we need to find a good starting point to look for pixels belonging to the left lane line and pixels belonging to the right lane line. One approach is to generate a histogram of the lane line pixels in the image. The histogram should have two peaks each representing one of the lane lines, where the left peak is for the left lane line and the right peak is for the right lane line. The image below shows two example histograms generated from two binary images:\n\nThe locations of the two peaks are then used as a starting point to search for pixels belonging to each lane line. We employ a sliding window search technique that starts from the bottom and iteratively scans all the way to the top of the image, adding detected pixels to a list. If a sufficient number of pixels is detected in a window, the next window will be centred around their mean position, that way we are following the path of the pixels throughout the image.\n\nAfter we\u2019ve detected the pixels belonging to each lane line, we then fit a polynomial through the points, generating a smooth line which acts as our best approximation of where the lane line is.\n\nThe image below shows the sliding window technique in action, with the polynomial fit through the detected lane pixels (red for left lane pixels and blue for right lane pixels):\n\nBelow is another view of the sliding window search technique, with the search area highlighted and filled:\n\nFinally, using the location of the two detected lane lines and the assumption that the camera is located at the centre of the image, we then calculate the position of the car relative to the lane. Scale measurements to convert from pixels to meters have been calculated using the resolution of the image.\n\nFurthermore, using the scale measurements, we can also calculate the curvature of the lane by fitting a new polynomial to the world space, then calculating the radii of curvature. The radius of curvature of the lane is then just the average of the two radii. Below image shows curve radius and centre offset for the two lane lines (detection not visible in image):\n\nWith our lane line detection pipeline complete, we now visualise our detection by projecting a filled polygon onto the video where we believe the lane line boundaries are, as per the below image:\n\nThe pipeline was run on a video of the car driving. Below is a video demonstrating the pipeline in action:"
    },
    {
        "url": "https://towardsdatascience.com/optimal-coupon-targeting-for-grocery-items-an-instacart-case-study-128e8d169c7c",
        "title": "Optimal Coupon Targeting for Grocery Items: an Instacart Case Study",
        "text": "I set up a relational, SQL-database on AWS to contain all data and then used Python\u2019s Pandas on a powerful AWS instance to perform the analyses.\n\nFor this project I used the Instacart data as available on kaggle.com . The dataset contains information about 3m+ orders through Instacart in the form of .csv files.\n\nSo, what did I do to find the users in this sweet spot? And what do we do with this information?\n\nIn order to do so, I looked into the likelihood of an individual user reordering a product included in a past order. More specifically, I wanted to find those Instacart users who might be on the tipping point of reordering a product, and to incentivize them to reorder by sending them targeted coupons. In order to identify these users, I built a machine learning model to assign probabilities to a product being reordered by specific Instacart users in their next order, based on their past behavior. I would then use these probabilities as input for deciding which group to target with coupons for these items, to have maximum \u2018persuasive\u2019 effect.\n\nFor a recent project I set out to develop an optimal coupon targeting strategy for stores offering their products through Instacart, to increase their sales, and, ultimately, profit.\n\nWhere the order_products_train table consists of the latest cart ordered by users, and thus containing the information about whether or not the product was reordered in the latest cart. This represents our target variable.\n\nGiven that not all products purchased by a user at some point will be reordered, I had to take care of the class imbalance that was present in the data, not to get a predictor that was biased to predicting \u2018no reorder\u2019. To do this, I made use of oversampling after making an 80/20 train-test split.\n\nWorking with the original data and features, I did a lot of feature engineering and by combining or extracting meaning from the original features to make the most of the available data. In total I created 55+ features to more accurately estimate the probability a user will reorder a product that has been ordered by that user in the past.\n\nThe features created belonged to one of the following three categories:\n\nGiven the nature of the problem, being estimating the likelihood a specific users reorders a specific product, the latter category turned out to be most relevant to the prediction.\n\nSome of the User and User * Product features I created that proved explanatory were:\n\nAs for the modeling, I used three types of models: Logistic Regression, Naive Bayes, and Random Forest Regression.\n\nGiven the objective to get relative likelihoods among users reordering a specific product, rather than 0\u20131 predictions about reorders, I did not use regular the common scoring method for this dataset, being F1. Instead, I used ROC-AUC, which combines specificity and precision, giving me an indication of the probability that a randomly chosen reordered product by a user will be ranked higher by the model than a non-reordered product by a user.\n\nOf these three models, the Random Forest Classifier performed best, both on the validation set, and on the test set, with a test AUC of 0.824. This means that the model will rank a randomly chosen positive instance (reordered) higher than a randomly chosen negative one (non-reordered) 82.4% of the time. Given time, I plan on doing some more tweaking to the model to see if I could get a higher AUC, but for me, this was primarily an exercise in feature engineering and working with large, relational, data.\n\nThe results of this model then serve as input to the marketing department of the various suppliers on Instacart. In practice, it would come down to the store selecting a specific product, and for that product looking at the probabilities for each user of reordering that particular product. The store would then select the users in the middle, say, 20% of the ranked probabilities of reorder for that product, see what kind of coupon would make most sense and would result in the highest increase in profits, taking into account various factors such as the conversion rate. E.g. offering a coupon reduces revenue per product sold with that coupon, but might offset that by a quantitative increase in number of products sold if the coupon has the intended effect of increasing sales.\n\nThe end product is a dashboard in which business decision makers in the stores can find the product, assess the number of \u2018target\u2019 users for each product. The dashboard might look something like this:"
    },
    {
        "url": "https://towardsdatascience.com/analyzing-a-dataset-the-step1-of-machine-learning-which-often-gets-overlooked-415ca65ed2f1",
        "title": "Analyzing a dataset \u2014 the Step1 of Machine Learning which often gets overlooked",
        "text": "Unlike traditional programming, in Machine Learning one can spend months on a project with no results to show.\n\nAnd eventually drift into the common \u2018it\u2019s a black box\u2019 mentality.\n\nIn this post, we\u2019ll try to outline with an example, the 1st step which every ML project must take \u2014 studying/analyzing your data."
    },
    {
        "url": "https://towardsdatascience.com/a-review-of-nvidia-gtc-2018-conference-new-gpus-deep-learning-acceleration-data-augmentation-d6d4f638bcda",
        "title": "A Review of NVIDIA GTC 2018 Conference \u2014 New GPUs, Deep Learning Acceleration, Data Augmentation\u2026",
        "text": "A selection of the technical talks I attended are detailed below.\n\nI found this talk to be interesting as it provided practical advice on ways to satiate the data hungry demands of supervised deep learning. The speaker begins with the premise that human annotation of data is an expensive process and proposes four approaches they use in their workflow to address this.\n\nWeb scraping: Approaches to efficiently scrape labelled data from websites and social networks. \n\nWeakly Supervised Methods: Given a small dataset labelled by experts, we we can learn labelling strategies and apply this in labelling larger datasets.\n\nData Transformations: We can augment datasets by generating additional examples using simple linear transformations \u2014 e.g cropping, shifting, color casting, lens distortion, vignetting, random backgrounds etc. Example of a library for this sort of transformation is imageAug.\n\nData Synthesis: We can generate texturized CAD models as training data, we can add specific features to data such as adding glasses to facial images, and altogether synthesizing new images using GANs.\n\nMore can be found on the presenter\u2019s slides here.\n\nResearchers from NVIDIA demonstrated some early work detecting drowsiness in drivers. The authors train a scaled down VGG16 model, and augment their training dataset using synthetic data generated from 3D face models. For classification they rely on predicted eye pitch angle over a time period.\n\nGenerative Design, Autodesk: In this talk, the presenter discussed some interesting ways in which evolutionary algorithms were used in generating CAD designs. Given a design challenge, the goal is usually to balance the cost (of materials) and performance. To this end, they have experimented with evolutionary algorithms that generate design candidates, while optimizing on parameters such cost/performance/manufacturing method etc and use automated stress tests (FEA analysis) as part of feedback. A specific example was given where an evolutionary algorithm came up with a high performance (and unusual looking) part of a motorbike.\n\nA.I. Disrupting the Future of Content Creation for Games \u2014 Eric Risser, Artomatix\n\nThis talk focused on how AI accelerated workflows can be applied to aspects of the media industry (e.g. movies, video games). The presenter reveals that the video game industry spends 61% of its budget on artistic content generation \u2014 main character as well as background. Much of this efforts include a manual workflow. Creative or generative AI offers opportunities to improve this, across areas such as Texture Synthesis, Material Enhancement, Hybridization and Style Transfer. This includes methods that enable artists paint with structure, example based workflow (scanning real world objects and improving with AI) and photogrammetry. AI can also help with recycling old content e.g. up-res video. An industry use case was given with IKEA being able to easily scan 3D models of products which were then used in websites (studies showed having 3D models led to 20% higher sales on websites). See more details on the presenters company blog.\n\nGrowing Generative Models \u2014 Samuli Laine et Al\n\nResearchers from NVIDIA presented some interested work on how to generate high resolutions images using Generative Adversarial Networks (GANs). Their approach addresses a known problem with GANs (mode collapse), speeding up and stabilizing the training process. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, and adding new layers that model increasingly fine details as training progresses. They highlight the potential of this work in generating assets for games and films and conditioning the GAN to determine output (e.g. male or female faces). More details can be found in their paper.\n\nAnother interesting talk looked at how ML can be used to address some issues in security \u2014 detecting domain generation algorithms. Domain Generation Algorithms DGAs, are algorithms seen in various families of malware that are used to periodically generate a large number of domain names that can be used as rendezvous points with their command and control servers. They are used by hackers to communicate and exfiltrate data from networks, designed to circumvent traditional cyber defenses and have been extremely successful. They cite a recent paper \u201cInline DGA Detection with Deep Networks\u201d."
    },
    {
        "url": "https://towardsdatascience.com/going-beyond-with-agile-data-science-fcff5aaa9f0c",
        "title": "Going Beyond with Agile Data Science \u2013",
        "text": "Going Beyond with Agile Data Science Seeing beyond what my eyes are seeing is something I apply to every aspect in my life. In this article I\u2019ll show how Agile Data Science is helping me in this endeavour.\n\nWe live in a complex world, we live a cultural world. That means that we can\u2019t choose being culture-less creatures. We are born and that\u2019s it, you are already there. You can\u2019t choose your status, parents, city and more. Then we learn, from life and from school, our friends, our family.\n\nThat process creates a way of seeing things, a way of thinking about the world, about ourselves and what surrounds us. Our senses open us to the world, we hear, we feel, we taste, we see. When I mention \u201cseeing beyond\u201d or something like that I mean the combination of all of our senses, what we perceive from every angle.\n\nWe get the data from the \u201coutside world\u201d and our body and brain analyze the raw data we got, and then we \u201cinterpret\u201d things.\n\nWhat is this \u201cinterpretation\u201d? Just what we\u2019ve learned about how to react, think, feel and understand from the information we are getting. When we are understanding we are decoding the parts that forms this complex thing, and transforming the raw data we got in the beginning into something useful and simple.\n\nWe do this by modeling. This is the process of understanding the \u201creality\u201d, the world around us, but creating a higher level prototype that will describe the things we are seeing, hearing and feeling, but it\u2019s a representative thing, not the \u201cactual\u201d or \u201creal\u201d thing.\n\nWe don\u2019t stay there. Our model of the world, or a process, has another part. What we think that particular thing means and how we feel about it.\n\nWhen I was younger I had a lot of prejudice about lots and lots of things. Judging things and people before even meeting them. And I don\u2019t think I was alone in this.\n\nWe are used to jump to conclusions really fast, not analyzing every side of things. We are use to see what our eyes are seeing and \u201ctrusting our gut\u201d.\n\nSadly the common sense that reigns in our culture is Aristotelian and Medieval (\u00c9tudes d\u2019histoire de la pens\u00e9e scientifique \u2014 Alexander Koyr\u00e9). That means that intuition fails a lot of times when trying to understand the world, also this \u201ccommon sense\u201d comes sometimes with judgement, something that creates a bias in the way we see things.\n\nGoing and seeing beyond in this context means going a step forward, putting your judgment, common sense and intuition aside and really analyzing a situation. We should be doing this for everything that happens around us, question ourselves if the thing you are doing, thinking and perceiving is actually correct. This is something very close to the Cartesian Doubt.\n\nSo what does Data Science have to do with any of this? Actually going further, and beyond our common sense and intuition is the only way of solving complex business problems.\n\nIn a world full of intuitive models, disruption and advancement come from going beyond, using data to understand what can not be seen with the naked eye or with an \u201cexpert look\u201d.\n\nThe process of Agile Data Science proposed by Russell Jurney is an amazing way of understanding how and why Data Science together with agility helps us going beyond, seeing more and solving problems in a creative way.\n\nThe manifesto for Agile Data Science leads us to this. Iterating, over and over again, rethinking the business process and needs, experimenting a lot, listening what the data has to say, understanding and encouraging the business to understand that the data\u2019s opinion must always be included in product discussions, finding a critical path to solve the problem and then organizing the team around completing it, and going further, letting the models solve the problems, of course using our expertise to help them, but not biasing them.\n\nFrom understanding the business and its needs to the deployment of the solution we need to take a look at the bigger picture, from above, from below, from the side. Emptying our minds from the intuition that everyone can add to the solution, and trust what our models are saying about the process and understanding how it is solving the problem."
    },
    {
        "url": "https://towardsdatascience.com/wth-does-a-neural-network-even-learn-a-newcomers-dilemma-bd8d1bbbed89",
        "title": "\u201cWTH does a neural network even learn??\u201d \u2014 a newcomer\u2019s dilemma",
        "text": "A simple, clear bird\u2019s eye view of what neural networks learn \u2014 they learn \u201cincreasingly more complex concepts\u201d.\n\nDoesn\u2019t that feel familiar? Isn\u2019t that how we learn anything at all?\n\nFor instance, let\u2019s consider how we, as kids, probably learnt to recognise objects and animals \u2014\n\nSo, neural networks learn like we do!\n\nIt almost eases the mind to believe that we have this intangible sort of.. man-made \u201cthing\u201d that is analogous to the mind itself! It is especially appealing to someone who has just begun his/her Deep Learning journey.\n\nBut NO. A neural network\u2019s learning is NOT ANALOGOUS to our own. Almost all the credible guides and \u2018starters packs\u2019 on the subject of deep learning come with a warning, something along the lines of:\n\n..and that\u2019s where all the confusion begins!\n\nI think this was mostly because of the way in which most of the tutorials and beginner level books approach the subject.\n\nLet\u2019s see how Michael Nielsen describes what the hidden neurons are doing in his book \u2014 Neural Networks and Deep Learning:\n\nHe, like many others, uses the analogy between neural networks and the human mind to try to explain a neural networks. The way lines and edges make loops, which then help in recognising some digits is what we would think of doing. Many other tutorials try to use a similar analogy to explain what it means to build a hierarchy of knowledge.\n\nI have to say that because of this analogy, I understand neural nets better.\n\nBut it is one of the paradoxes, that the very analogy that makes a difficult concept intelligible to the masses, can also create an illusion of knowledge among them.\n\nReaders need to understand that it is just an analogy. Nothing more, nothing less. They need to understand that every simple analogy needs to be followed by more rigorous, seemingly difficult explanations.\n\nNow don\u2019t get me wrong. I am deeply thankful to Michael Nielsen for writing this book. It is one of the best books on the subject out there. He is careful in mentioning that this is \u201cjust for the sake of argument\u201d.\n\nBut I took it to mean this \u2014 \n\nMaybe, the network won\u2019t use the same exact pieces. Maybe, it will figure out some other pieces and join them in some other way to recognise the digits. But the essence will be the same. Right? I mean each of those pieces has to be some kind of an edge or a line or some loopy structure. After all, it doesn\u2019t seem like there are other possibilities if you want to build a hierarchical structure to solve the problem of recognising digits.\n\nAs I gained a better intuition about them and how they work, I understood that this view is obviously wrong. It hit me..\n\nBeing able to identify a loop is essential for us humans to write digits- an 8 is two loops joined end-to-end, a 9 is loop with a tail under it and a 6 is loop with a tail up top. But when it comes to recognising digits in an image, features like loops seem difficult and infeasible for a neural network (Remember, I\u2019m talking about your vanilla neutral networks or MLPs here).\n\nI know its just a lot of \u201chand-wavy\u201d reasoning but I think it is enough to convince. Probably, the edges and all the other hand-engineered features will face similar problems.\n\nI had no clue about the answer or how to find it until 3blue1brown released a set of videos about neural networks. It was Grant Sanderson\u2019s take at explaining the subject to newcomers. Maybe even he felt that there were some missing pieces in the explanation by other people and that he could address them in his tutorials.\n\nAnd boy, did he!\n\nGrant Sanderson of 3blue1brown, who uses a structure with 2 hidden layers, says \u2014\n\nThe very loops and edges that we ruled out above.\n\nThey were not looking for loops or edges or anything even remotely close! They were looking for.. well something inexplicable.. some strange patterns that can be confused for random noise!\n\nI found those weight matrix images (in the above screenshot) really fascinating. I thought of them as a Lego puzzle.\n\nThe weight matrix images were like the elementary Lego blocks and my task was to figure out a way to arrange them together so that I could create all 10 digits. This idea was inspired from the excerpt of Neural Networks and Deep Learning that I posted above. There we saw how we could assemble a 0 using hand-made features like edges and curves. So, I thought that, maybe, we could do the same with the features that the neural network actually found good.\n\nAll I needed was those weight matrix images that were used in 3blue1brown\u2019s video. Now the problem was that Grant had put only 7 images in the video. So, I was gonna have to generate them on my own and create my very own set of Lego blocks!\n\nI imported the code used in Michael Nielsen\u2019s book to a Jupyter notebook. Then, I extended the class in there to include the methods that would help me visualise the weight matrices.\n\nOne pixel for every connection in the network. One image for each neuron showing how much it \u2018likes\u2019(colour: blue) or \u2018dislikes\u2019(colour: red) the previous layer neurons.\n\nSo, if I was to look at the image belonging to one of the neurons in the hidden layer, it would be like a heat map showing one feature, one basic Lego block that will be used to recognise digits. Blue pixels would represent connections that it \u201clikes\u201d whereas red ones would represent the connections that it \u201cdislikes\u201d.\n\nNotice that we will have 30 different types of basic Lego blocks for our Lego puzzle here because that\u2019s the size of our hidden layer.\n\nAnd.. here\u2019s what they look like! \u2014"
    },
    {
        "url": "https://towardsdatascience.com/bmw-machine-learning-weekly-week-8-149ca5108899",
        "title": "BMW Machine Learning Weekly \u2014 Week 8 \u2013",
        "text": "Google\u2019s latest contribution to AI Experiments lets you talk to books: the project called \u201cTalk to Books\u201d lets you quite literally converse with a ML-trained algorithm that surfaces answers to questions with relevant passages from human-written texts. It lets you make a statement or ask a question. The tool then finds a sentence in books that respond to the question or statement made, without a dependence on keyword matching. Another (far more interactive and fun) tool is \u201cSemantris,\u201d a word-association game powered by ML. Semantris basically tests your word association abilities with the same software that powers Talk to Books. It ranks and scores the words on-screen based on how well they correspond to the answers you put in. For instance, if you are given the word \u201cbed\u201d at the top of a collection of 10 words, you might think to type \u201csleep\u201d as a response. Semantris will then rank the 10 words and give you points based on how well it thinks the semantic relationship between bed and sleep is in comparison to the relationship between \u201cbed\u201d and every other word in the list. Past AI experiments from Google include \u201cQuick, Draw!\u201d, a neural network that learns how to recognize your doodling, or the \u201cInfinite Drum Machine,\u201d which lets you create beats using sounds from the everyday world.\n\nAI researchers at UC Berkeley and the University of British Columbia have created virtual characters capable of imitating the way a person performs martial arts, parkour, and acrobatics, practicing moves relentlessly until they get them just right. The work could transform the way video games and movies are made. Instead of planning a character\u2019s actions in excruciating detail, animators might feed real footage into a program and have their characters master them through practice. Such a character could be dropped into a scene and left to perform the actions. The virtual characters developed by the AI researcher use an AI technique known as reinforcement learning, where a virtual character experiments with its motions and receives positive reinforcement each time it gets a little closer to the motions of the expert. Reinforcement learning implies that the same algorithm can be used to train a character to do a backflip or a moonwalk.\n\nOne Concern, a startup company in California is using ML to advise fire departments about how to plan for and respond to earthquakes. The aim of One Concern is to reduce the guesswork in the planning process for disaster response. Using data about homes, buildings, the material they are made of, when they were built, and how likely they are to collapse when the ground starts shaking, as well as data about the natural environment and live weather data, predictions are made about what would happen if an earthquake occurred in a particular area. The model is then tested in areas where past earthquakes have caused damage.\n\nThe European Commission outlined a European approach to boost investment and set ethical guidelines for AI. It proposed a three-pronged approach to prevent \u201cbrain drain\u201d to other, non-European competition, and believes that coordinated European action is required for the EU to be at the forefront of AI development. The three goals are defined to i) boost the financial support for AI and encourage uptake by the public and private sectors, ii) prepare for socio-economic changes brought about by AI and iii) ensure an appropriate ethical and legal framework concerning AI."
    },
    {
        "url": "https://towardsdatascience.com/build-your-first-deep-learning-classifier-using-tensorflow-dog-breed-example-964ed0689430",
        "title": "Build Your First Deep Learning Classifier using TensorFlow: Dog Breed Example",
        "text": "In this article, I will present several techniques for you to make your first steps towards developing an algorithm that could be used for a classic image classification problem: detecting dog breed from an image.\n\nBy the end of this article, we\u2019ll have developed code that will accept any user-supplied image as input and return an estimate of the dog\u2019s breed. Also, if a human is detected, the algorithm will provide an estimate of the dog breed that is most resembling.\n\nN.B. This project was completed as part of Udacity\u2019s Machine Learning Nanodegree (GitHub repo).\n\nConvolutional neural networks (also refered to as CNN or ConvNet) are a class of deep neural networks that have seen widespread adoption in a number of computer vision and visual imagery applications.\n\nA famous case of CNN application was detailed in this research paper by a Stanford research team in which they demonstrated classification of skin lesions using a single CNN. The Neural Network was trained from images using only pixels and disease labels as inputs.\n\nConvolutional Neural Networks consist of multiple layers designed to require relatively little pre-processing compared to other image classification algorithms.\n\nThey learn by using filters and applying them to the images. The algorithm takes a small square (or \u2018window\u2019) and starts applying it over the image. Each filter allows the CNN to identify certain patterns in the image. The CNN looks for parts of the image where a filter matches the contents of the image.\n\nThe first few layers of the network may detect simple features like lines, circles, edges. In each layer, the network is able to combine these findings and continually learn more complex concepts as we go deeper and deeper into the layers of the Neural Network.\n\nThe overall architecture of a CNN consists of an input layer, hidden layer(s), and an output layer. They are several types of layers, for e.g. Convolutional, Activation, Pooling, Dropout, Dense, and SoftMax layer.\n\nThe Convolutional Layer (or Conv layer) is at the core of what makes a Convolutional Neural Network. The Conv layer consists of a set of filters. Every filter can be considered as a small square (with a fixed width and height) which extends through the full depth of the input volume.\n\nDuring each pass, the filter \u2018convolves \u2018across the width and height of the input volume. This process results in a 2-dimensional activation map that gives the responses of that filter at every spatial position.\n\nTo avoid over-fitting, Pooling layers are used to apply non-linear downsampling on activation maps. In other words, Pooling Layers are aggressive at discarding information but can be useful if used appropriately. A Pooling layer would often follow one or two Conv Layers in CNN architecture.\n\nDropout Layers are also used to reduce over-fitting, by randomly ignore certain activations functions, while Dense Layers are fully connected layers and often come at the end of the Neural Network.\n\nThe output of the layers and of the neural network are processed using an activation function, which is a node that is added to the hideen layers and to the output layer.\n\nYou\u2019ll often find that the ReLu activation function is used in hidden layers, while the final layer typically consists of a SoftMax activation function. The idea is that by stacking layers of linear and non-linear functions, we can detect a large range of patters and accurately predict a label for a given image.\n\nSoftMax is often found in the final layer which acts as basically a normalizer and produces a discrete probability distribution vector, which is great for us as the CNN\u2019s output we want is a probability that an image corresponds to a particular class.\n\nWhen it comes to model evaluation and performance assessment, a loss function is chosen. In CNNs for image classification, the categorial cross-entropy is often chosen (in a nutshell: it corresponds to -log(error)). There are several methods to minimise the error using Gradient Descent \u2014 in this article, we\u2019ll rely on \u201crmsprop\u201d, which adaptive learning rate method, as an optimizer with accuracy as a metric.\n\nTo build our algorithm, we\u2019ll be using TensorFlow, Keras (neural networks API running on top of TensorFlow), and OpenCV (computer vision library).\n\nTraining and testing datasets were also available on-hand when completing this project (see GitHub repo).\n\nTo detect whether the image supplied is a human face, we\u2019ll use one of OpenCV\u2019s Face Detection algorithm. Before using any of the face detectors, it is standard procedure to convert the images to grayscale. Below, the function executes the classifier stored in and takes the grayscale image as a parameter.\n\nTo detect whether the image supplied contains a face of a dog, we\u2019ll use a pre-trained ResNet-50 model using the ImageNet dataset which can classify an object from one of 1000 categories. Given an image, this pre-trained ResNet-50 model returns a prediction for the object that is contained in the image.\n\nWhen using TensorFlow as backend, Keras CNNs require a 4D array as input. The function below takes a string-valued file path to a color image as input, resizes it to a square image that is 224x224 pixels, and returns a 4D array (referred to as a \u2018tensor\u2019) suitable for supplying to a Keras CNN.\n\nAlso, all pre-trained models have the additional normalization step that the mean pixel must be subtracted from every pixel in each image. This is implemented in the imported function .\n\nAs shown in the code above, for the final prediction we obtain an integer corresponding to the model\u2019s predicted object class by taking the argmax of the predicted probability vector, which we can identify with an object category through the use of the ImageNet labels dictionary.\n\nNow that we have functions for detecting humans and dogs in images, we need a way to predict breed from images. In this section, we will create a CNN that classifies dog breeds.\n\nTo reduce training time without sacrificing accuracy, we\u2019ll be training a CNN using Transfer Learning \u2014 which is a method that allows us to use Networks that have been pre-trained on a large dataset. By keeping the early layers and only training newly added layers, we are able to tap into the knowledge gained by the pre-trained algorithm and use it for our application.\n\nKeras includes several pre-trained deep learning models that can be used for prediction, feature extraction, and fine-tuning.\n\nAs previously mentioned, the ResNet-50 model output is going to be our input layer \u2014 called the bottleneck features. In the code block below, we extract the bottleneck features corresponding to the train, test, and validation sets by running the following.\n\nWe\u2019ll set up our model architecture such that the last convolutional output of ResNet-50 is fed as input to our model. We only add a Global Average Pooling layer and a Fully Connected layer, where the latter contains one node for each dog category and has a Softmax activation function.\n\nAs we can see in the above code\u2019s output, we end up with a Neural Network with 272,517 parameters!\n\nNow, we can use the CNN to test how well it identifies breed within our test dataset of dog images. To fine-tune the model, we go through 20 iterations (or \u2018epochs\u2019) in which the model\u2019s hyper-parameters are fine-tuned to reduce the loss function (categorial cross-entropy) which is optimised using RMS Prop.\n\nProvided with a testing set, the algorithm scored a testing accuracy of 80%. Not bad at all!\n\nNow that we have the algorithm, let\u2019s write a function that takes an image path as input and returns the dog breed that is predicted by our model.\n\nNow, we can write a function that takes accepts a file path to an image and first determines whether the image contains a human, dog, or neither.\n\nIf a dog is detected in the image, return the predicted breed. If a human is detected in the image, return the resembling dog breed. If neither is detected in the image, provide output that indicates an error.\n\nWe are ready to take the algorithm for a spin! Let\u2019s test the algorithm on a few sample images:\n\nThese predictions look accurate to me!\n\nOn a final note, I noted that that the algorithm is prone to errors unless it\u2019s a clear facing shot with very little noise/extra information on the image. Hence, we need to make the algorithm more robust to noise. Image Augmentation may help be useful to solve this \u2014 which is a topic I will address in a future article."
    },
    {
        "url": "https://towardsdatascience.com/how-i-trained-a-language-detection-ai-in-20-minutes-with-a-97-accuracy-fdeca0fb7724",
        "title": "How I trained a language detection AI in 20 minutes with a 97% accuracy",
        "text": "This story is a step-by-step guide to how I built a language detection model using machine learning (that ended up being 97% accurate) in under 20 minutes.\n\nLanguage detection is a great use case for machine learning, more specifically, for text classification. Given some text from an e-mail, news article, output of speech-to-text capabilities, or anywhere else, a language detection model will tell you what language it is in.\n\nThis is a great way to quickly categorize and sort information, and apply additional layers of workflows that are language specific. For example, if you want to apply spell checking to a Word document, you first have to pick the correct language for the dictionary being used. Otherwise you\u2019re going to find the spell checker to be quite wrong.\n\nOther use cases might include routing e-mails to the right geographically located customer service department, applying the correct subtitles or closed-captioning to a video, or applying some other language-specific text classification to the text you\u2019re analyzing.\n\nOk, you get it, language detection is really useful, let\u2019s move on to how I did it so quickly.\n\nIt\u2019s basically a .csv with samples of English, French and Spanish. My goal was to see if I might train a machine learning model to understand the difference between those languages and then, given some new text, predict the language it was in.\n\nSo the first thing I did was spin up Classificationbox, a machine learning model builder that runs in a Docker container and has a simple API. This took less than a minute.\n\nThen I downloaded this handy tool that makes it really easy to train Classificationbox with text files on your computer. This took another minute or so.\n\nThe next step was to convert the CSV into text files so that I might easily train Classificationbox.\n\nHere is some not-great Go code I wrote in case you\u2019re interested, if not, please skip to the next step.\n\nAfter running this script, I had folders on my hard drive named for the different languages. Inside each folder were text files with the language samples. It took me about 10 minutes to write the script and run it.\n\nNow comes the fun part. I made sure Classificationbox was up and running, then I ran on the parent directory of the language folders. It took about 3 seconds to:\n\nThese were my results:\n\n97% ! That\u2019s pretty good for only spending 20 minutes on training a language detection machine learning model.\n\nOne important thing to note is that my classes were not balanced. I had different numbers of samples for each class which does not adhere to the best practices for training a model. Ideally, I would have the exact same number of examples in each class.\n\nThe point is, machine learning benefits best from experimentation. I strongly encourage everyone to give it try using Machine Box or any other tools. I hope I was able to demonstrate just how easy it is to create your own machine learning / classification model given a good data set.\n\nMachine Box puts state of the art machine learning capabilities into Docker containers so developers like you can easily incorporate natural language processing, facial detection, object recognition, etc. into your own apps very quickly.\n\nThe boxes are built for scale, so when your app really takes off just add more boxes horizontally, to infinity and beyond. Oh, and it\u2019s way cheaper than any of the cloud services (and they might be better)\u2026 and your data doesn\u2019t leave your infrastructure."
    }
]