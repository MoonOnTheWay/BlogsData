[
    {
        "url": "https://deeplearning4j.org/overview", 
        "text": "Deeplearning4j is a Java-based toolkit for building, training and deploying deep neural networks, the regressions and KNN.\n\nDeeplearning4j has the following sub-projects.\n\nIngesting, cleaning, joining, scaling, normalizing and transforming data are jobs that must be done in any sort of data analysis. This work may not be exciting, but it\u2019s a precondition of deep learning. DataVec is our toolkit for that process. We give data scientists and developers tools to turn raw data such as images, video, audio, text and time series into feature vectors for neural nets.\n\nThe DataVec Github repo is here. Here is how the repo is organized.\n\nThere are DataVec examples in our examples repo on Github.\n\nA descriptive summary of many of the examples is here.\n\nHere is the DataVec JavaDoc.\n\nNeural Networks process multi-dimensional arrays of numerical data. Getting your data from a CSV file, or a directory of images, to be serialized into numeric arrays is the job of DataVec. DataVec is an ETL tool specifically built for machine learning pipelines.\n\nBase class for reading and serializing data. RecordReaders ingest your data input and return a List of Serializable objects (Writables).\n\nThe labels for data input may be based on the directory where the image is stored.\n\nDeeplearning4j is where you design your neural networks. It is a domain specific language (DSL) for configuring neural networks.\n\nThe Deeplearning4j Github repo is here. Here\u2019s how the repo is organized.\n\nHere is the Deeplearning4j JavaDoc here.\n\nThere are Deeplearning4j examples in the Github repository here.\n\nA descriptive summary of many of the examples is here.\n\nIf you have worked with the Python Library Keras and would like to import a trained model, or a model configuration, into Deeplearning4j, please see our model import feature.\n\nThe Model Import is actually part of DeepLearning4J, but it is worth its own section. Github folder is here.\n\nWe will add examples here\n\nHere\u2019s a video showing how to import of a Keras model to DL4J:\n\nND4J is the numerical computing library that underpins Deeplearning4j. It is a tensor library, the JVM\u2019s answer to Numpy.\n\nHere is the ND4J Github repo. ND4J is a DSL for handling n-dimensional arrays (NDArrays), also known as tensors.\n\nHere is the ND4J JavaDoc.\n\nYou may not use some of these classes directly, but when you configure a neural network. Behind the scenes, the configurations you set for , , and are all done in ND4J.\n\nScalNet is Keras for Scala. It\u2019s a Scala wrapper for Deeplearning4j that can run Spark on multi-GPUs.\n\nRL4J is a library and environment for reinforcement learning on the JVM. It includes Deep Q learning, A3C and other algorithms implemented in Java and integrated with DL4J and ND4J.\n\nArbiter helps you search the hyperparameter space to find the best tuning and architecture for a neural net. This is important because finding the right architecture and hyperparamters is a very large combinatorial problem. The winning ImageNet teams at corporate labs like Microsoft are searching through hyperparameters to surface 150-layer networks like ResNet. Arbiter includes grid search, random search, some Bayesian methods, as well as model evaluation tools.\n\nHere is the Arbiter Github repository.\n\nFor people just getting started with deep learning, the following tutorials and videos provide an easy entrance to the fundamental ideas of deep neural networks:", 
        "title": "What Is DeepLearning4j? - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/quickstart", 
        "text": "This is everything you need to run DL4J examples and begin your own projects.\n\nWe recommend that you join our Gitter Live Chat. Gitter is where you can request help and give feedback, but please do use this guide before asking questions we\u2019ve answered below. If you are new to deep learning, we\u2019ve included a road map for beginners with links to courses, readings and other resources. If you need an end to end tutorial to get started (including setup), then please go to our getting started.\n\nDeeplearning4j is a domain-specific language to configure deep neural networks, which are made of multiple layers. Everything starts with a , which organizes those layers and their hyperparameters.\n\nHyperparameters are variables that determine how a neural network learns. They include how many times to update the weights of the model, how to initialize those weights, which activation function to attach to the nodes, which optimization algorithm to use, and how fast the model should learn. This is what one configuration would look like:\n\nWith Deeplearning4j, you add a layer by calling on the , specifying its place in the order of layers (the zero-indexed layer below is the input layer), the number of input and output nodes, and , as well as the type: .\n\nOnce you\u2019ve configured your net, you train the model with .\n\nYou should have these installed to use this QuickStart guide. DL4J targets professional Java developers who are familiar with production deployments, IDEs and automated build tools. Working with DL4J will be easiest if you already have experience with these.\n\nIf you are new to Java or unfamiliar with these tools, read the details below for help with installation and setup. Otherwise, skip to DL4J Examples.\n\nIf you don\u2019t have Java 1.7 or later, download the current Java Development Kit (JDK) here. To check if you have a compatible version of Java installed, use the following command:\n\nPlease make sure you have a 64-Bit version of java installed, as you will see an error telling you if you decide to try to use a 32-Bit version instead.\n\nMaven is a dependency management and automated build tool for Java projects. It works well with IDEs such as IntelliJ and lets you install DL4J project libraries easily. Install or update Maven to the latest release following their instructions for your system. To check if you have the most recent version of Maven installed, enter the following:\n\nIf you are working on a Mac, you can simply enter the following into the command line:\n\nMaven is widely used among Java developers and it\u2019s pretty much mandatory for working with DL4J. If you come from a different background, and Maven is new to you, check out Apache\u2019s Maven overview and our introduction to Maven for non-Java programmers, which includes some additional troubleshooting tips. Other build tools such as Ivy and Gradle can also work, but we support Maven best.\n\nAn Integrated Development Environment (IDE) allows you to work with our API and configure neural networks in a few steps. We strongly recommend using IntelliJ, which communicates with Maven to handle dependencies. The community edition of IntelliJ is free.\n\nThere are other popular IDEs such as Eclipse and Netbeans. However, IntelliJ is preferred, and using it will make finding help on Gitter Live Chat easier if you need it.\n\nInstall the latest version of Git. If you already have Git, you can update to the latest version using Git itself:\n\nTo run DL4J in your own projects, we highly recommend using Maven for Java users, or a tool such as SBT for Scala. The basic set of dependencies and their versions are shown below. This includes:\n\nEvery Maven project has a POM file. Here is how the POM file should appear when you run your examples.\n\nWithin IntelliJ, you will need to choose the first Deeplearning4j example you\u2019re going to run. We suggest , as you will almost immediately see the network classify two groups of data in our UI. The file on Github can be found here.\n\nTo run the example, right click on it and select the green button in the drop-down menu. You will see, in IntelliJ\u2019s bottom window, a series of scores. The rightmost number is the error score for the network\u2019s classifications. If your network is learning, then that number will decrease over time with each batch it processes. At the end, this window will tell you how accurate your neural-network model has become:\n\nIn another window, a graph will appear, showing you how the multilayer perceptron (MLP) has classified the data in the example. It will look like this:\n\nCongratulations! You just trained your first neural network with Deeplearning4j. Now, why don\u2019t you try our next tutorial: MNIST for Beginners, where you\u2019ll learn how to classify images.\n\nQ: I\u2019m using a 64-Bit Java on Windows and still get the error\n\nA: You may have incompatible DLLs on your PATH. To tell DL4J to ignore those, you have to add the following as a VM parameter (Run -> Edit Configurations -> VM Options in IntelliJ):\n\nQ: SPARK ISSUES I am running the examples and having issues with the Spark based examples such as distributed training or datavec transform options.\n\nA: You may be missing some dependencies that Spark requires. See this Stack Overflow discussion for a discussion of potential dependency issues. Windows users may need the winutils.exe from Hadoop.\n\nDownload winutils.exe from https://github.com/steveloughran/winutils and put it into the null/bin/winutils.exe (or create a hadoop folder and add that to HADOOP_HOME)\n\nWindows users might be seeing something like:\n\nIf that is the issue, see this page. In this case replace with \u201cNd4jCpu\u201d.\n\nWe recommend and use Maven and Intellij. If you prefer Eclipse and dislike Maven here is a nice blog post to walk you through an Eclipse configuration.\n\nDeeplearning4j is a framework that lets you pick and choose with everything available from the beginning. We\u2019re not Tensorflow (a low-level numerical computing library with automatic differentiation) or Pytorch. For more details, please see our deep learning library compsheet. Deeplearning4j has several subprojects that make it easy-ish to build end-to-end applications.\n\nIf you\u2019d like to deploy models to production, you might like our model import from Keras.\n\nDeeplearning4j has several submodules. These range from a visualization UI to distributed training on Spark. For an overview of these modules, please look at the Deeplearning4j examples on Github.\n\nTo get started with a simple desktop app, you need two things: An nd4j backend and . For more code, see the simpler examples submodule.\n\nIf you want a flexible deep-learning API, there are two ways to go. You can use nd4j standalone See our nd4j examples or the computation graph API.\n\nIf you want distributed training on Spark, you can see our Spark page Keep in mind that we cannot setup Spark for you. If you want to set up distributed Spark and GPUs, that is largely up to you. Deeplearning4j simply deploys as a JAR file on an existing Spark cluster.\n\nIf you want Spark with GPUs, we recommend Spark with Mesos.\n\nIf you want to deploy on mobile, you can see our Android page.\n\nWe deploy optimized code for various hardware architectures natively. We use C++ based for loops just like everybody else. For that, please see our C++ framework libnd4j.\n\nDeeplearning4j has two other notable components:\n\nOverall, Deeplearning4j is meant to be an end-to-end platform for building real applications. Not just a tensor library with automatic differentiation. If you want that, that\u2019s in ND4J and it\u2019s called samediff. Samediff is still in alpha, but if you want to take a crack at contributing, please come in to our live chat on Gitter.\n\nLastly, if you are benchmarking Deeplearnin4j, please consider coming in to our live chat and asking for tips. Deeplearning4j has all the knobs but some may not work as the Python frameworks to do. You have to build Deeplearning4j from source for some applications.", 
        "title": "Quick Start Guide for Deeplearning4j - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/gettingstarted", 
        "text": "This page builds on the instructions in the Quick Start Guide, and provides additional details and some troubleshooting steps. Seriously, go and read that page first before you proceed with this. It\u2019s the easy way to start with DL4J.\n\nThis is a multistep install. We highly recommend you join our Gitter Live Chat if you have questions or feedback, so we can walk you through it. If you\u2019re feeling anti-social or brashly independent, you\u2019re still invited to lurk and learn. In addition, if you are utterly new to deep learning, we\u2019ve got a road map of what to learn when you\u2019re starting out.\n\nAfter following the steps in the Quick Start Guide, please read the following:\n\nThis section provides a more comprehensive version of the steps contained in the quickstart guide.\n\nIn IntelliJ, it is simply sufficient to import the examples as described in the quickstart guide. In order to use the example in Eclipse, an additional step is required.\n\nAfter running a , run the following command in your command line:\n\nThis will create an Eclipse project that you can then import.\n\nAfter many years using Eclipse, we recommend IntelliJ, which has a similar interface. Eclipse\u2019s monolithic architecture has a tendency to cause strange errors in our code and others\u2019.\n\nIf you use Eclipse, you will need to install the Maven plugin for Eclipse: eclipse.org/m2e/.\n\nMichael Depies has written this guide to installing Deeplearning4j on Eclipse.\n\nIf you just want to get the examples running within Eclipse and without using command line at all you can also directly checkout the project from Eclipse\u2019s built-in Source Control Management (SCM):\n\nYou might get some errors in the pom.xml validator, ignore those for now.\n\nFind the file called XorExample.java in the org.deeplearning4j.examples.feedforward.xor packege in the src/main/java folder of the dl4j-examples project. Right-click and \u201cRun as Java-Application\u201d:\n\nYou should see an output like this:\n\nAs of 0.9.0 (or 0.8.1-SNAPSHOT), Deeplearning4j has a new native model zoo that can be accessed and instantiated directly from DL4J. Gone are the days of copying model configs from Github. The model zoo also includes pretrained weights for different datasets that are downloaded automatically and checked for integrity. \ud83d\ude80\n\nSetting up a pretrained model with weights trained on ImageNet are as easy as:\n\nLearn more about the new zoo here.\n\nnote When building or rebuilding from source please see Building Locally for complete instructions.\n\nIf you are working in a managed environment like Databricks, Domino or Sense.io, you\u2019ll need to take an additional step. After you\u2019ve followed the local setup above, just run\n\nin the command line from within the examples directory. Then you can upload the JAR file to the managed environment you\u2019ve chosen.\n\nNeural net weights are initialized randomly, which means the model begins learning from a different position in the weight space each time, which may lead it to different local optima. Users seeking reproducible results will need to use the same random weights, which they must initialize before the model is created. They can reinitialize with the same random weight using the following method:\n\nOur Scala version is here This is a port of keras to scala (a work in progress).\n\nIf you install Deeplearning4j on an AWS server with a Linux OS, you may want to use the command line to run your first examples, rather than relying on an IDE. In that case, run the git clones and mvn clean installs according to the instruction above. With the installs completed, you can run an actual example with one line of code in the command line. The line will vary depending on the repo version and the specific example you choose.\n\nAnd here is a concrete example, to show you roughly what your command should look like:\n\nThat is, there are two wild cards that will change as we update and you go through the examples:\n\nTo make changes to the examples from the command line and run that changed file, you could, for example, tweak MLPBackpropIrisExample in src/main/java/org/deeplearning4j/multilayer and then maven-build the examples again.\n\nTo make best use of your hardware see this page for CPU optimizations.\n\nTo make best use of your GPU\u2019s, see this page to configure our data processing Library ND4J for GPU\u2019s.\n\nIn order to get started building neural nets, checkout the Neural Nets Overview for more information.\n\nTake a look at the MNIST tutorial to get running quickly, and check out our guide for restricted Boltzmann machines to understand the basic mechanics of deep-belief networks.\n\nFollow the ND4J Getting Started instructions to start a new project and include necessary POM dependencies.\n\nFor people just getting started with deep learning, the following tutorials and videos provide an easy entrance to the fundamental ideas of deep neural networks:", 
        "title": "Full Installation of Deeplearning4j - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/quickref", 
        "text": "Deeplearning4j (and related projects) have a lot of functionality. The goal of this page is to summarize this functionality so users know what exists, and where to find more information.\n\nOutput layers: usable only as the last layer in a network. Loss functions are set here.\n\nGraph vertex: use with ComputationGraph. Similar to layers, vertices usually don\u2019t have any parameters, and may support multiple inputs.\n\nAn InputPreProcessor is a simple class/interface that operates on the input to a layer. That is, a preprocessor is attached to a layer, and performs some operation on the input, before passing the layer to the output. Preprocessors also handle backpropagation - i.e., the preprocessing operations are generally differentiable.\n\nNote that in many cases (such as the XtoYPreProcessor classes), users won\u2019t need to (and shouldn\u2019t) add these manually, and can instead just use or similar, which whill infer and add the preprocessors as required.\n\nIterationListener: can be attached to a model, and are called during training, once after every iteration (i.e., after each parameter update). TrainingListener: extends IterationListener. Has a number of additional methods are called at different stages of training - i.e., after forward pass, after gradient calculation, at the start/end of each epoch, etc.\n\nNeither type (iteration/training) are called outside of training (i.e., during output or feed-forward methods)\n\nDL4J has a number of classes for evaluating the performance of a network, against a test set. Different evaluation classes are suitable for different types of networks.\n\nMultiLayerNetwork and ComputationGraph can be saved using the ModelSerializer class - and specifically the , and methods.\n\nAs of current master (but not 0.9.1) and have been added. These use ModelSerializer internally. Similar save/load methods have also been added for ComputationGraph.\n\nNetworks can be trained further after saving and loading: however, be sure to load the \u2018updater\u2019 (i.e., the historical state for updaters like momentum, ). If no futher training is required, the updater state can be ommitted to save disk space and memory.\n\nMost Normalizers (implementing the ND4J interface) can also be added to a model using the method.\n\nNote that the format used for models in DL4J is .zip: it\u2019s possible to open/extract these files using programs supporting the zip format.\n\nThis section lists the various configuration options that Deeplearning4j supports.\n\nActivation functions can be defined in one of two ways: (a) By passing an Activation enumeration value to the configuration - for example, (b) By passing an IActivation instance - for example,\n\nNote that Deeplearning4j supports custom activation functions, which can be defined by extending BaseActivationFunction\n\nWeight initialization refers to the method by which the initial parameters for a new network should be set.\n\nWeight initialization are usually defined using the WeightInit enumeration.\n\nCustom weight initializations can be specified using for example. As for master (but not 0.9.1 release) is also possible, which is equivalent to the previous approach.\n\nAvailable weight initializations. Not again that not all are available in the 0.9.1 release:\n\nAn \u2018updater\u2019 in DL4J is a class that takes raw gradients and modifies them to become updates. These updates will then be applied to the network parameters. The CS231n course notes have a good explanation of some of these updaters.\n\nAll updaters that support a learning rate also support learning rate schedules (the Nesterov momentum updater also supports a momentum schedule). Learning rate schedules can be specified either based on the number of iterations, or the number of epochs that have elapsed. Dropout (see below) can also make use of the schedules listed here.\n\nConfigure using, for example: You can plot/inspect the learning rate that will be used at any point by calling on the schedule object you have created.\n\nNote that custom schedules can be created by implementing the ISchedule interface.\n\nL1 and L2 regularization can easily be added to a network via the configuration: . Note that must be enabled on 0.9.1 also (this option has been removed after 0.9.1 was released).\n\nL1 and L2 regularization is applied by default on the weight parameters only. That is, .l1 and .l2 will not impact bias parameters - these can be regularized using .\n\nAll dropout types are applied at training time only. They are not applied at test time.\n\nNote that (as of current master - but not 0.9.1) the dropout parameters can also be specified according to any of the schedule classes mentioned in the Learning Rate Schedules section.\n\nAs per dropout, dropconnect / weight noise is applied only at training time\n\nConstraints are deterministic limitations that are placed on a model\u2019s parameters at the end of each iteration (after the parameter update has occurred). They can be thought of as a type of regularization.\n\nDataSetIterator is an abstraction that DL4J uses to iterate over minibatches of data, used for training. DataSetIterator returns DataSet objects, which are minibatches, and support a maximum of 1 input and 1 output array (INDArray).\n\nMultiDataSetIterator is similar to DataSetIterator, but returns MultiDataSet objects, which can have as many input and output arrays arrays as required for the network.\n\nThese iterators download their data as required. The actual datasets they return are not customizable.\n\nThe iterators in this subsection are used with user-provided data.\n\nND4J provides a number of classes for performing data normalization. These are implemented as DataSetPreProcessors. The basic pattern for normalization:\n\nIn general, you should fit only on the training data, and do and with the same/single normalizer that has been fit on the training data only.\n\nNote that where appropriate (NormalizerStandardize, NormalizerMinMaxScaler) statistics such as mean/standard-deviation/min/max are shared across time (for time series) and across image x/y locations (but not depth/channels - for image data).\n\nDeeplearning4j has classes/utilities for performing transfer learning - i.e., taking an existing network, and modifying some of the layers (optionally freezing others so their parameters don\u2019t change). For example, an image classifier could be trained on ImageNet, then applied to a new/different dataset. Both MultiLayerNetwork and ComputationGraph can be used with transfer learning - frequently starting from a pre-trained model from the model zoo (see next section), though any MultiLayerNetwork/ComputationGraph can be used.\n\nThe main class for transfer learning is TransferLearning. This class has a builder pattern that can be used to add/remove layers, freeze layers, etc. FineTuneConfiguration can be used here to specify the learning rate and other settings for the non-frozen layers.\n\nDeeplearning4j provides a \u2018model zoo\u2019 - a set of pretrained models that can be downloaded and used either as-is (for image classification, for example) or often for transfer learning.\n\n*Note: Trained Keras models (not provided by DL4J) may also be imported, using Deeplearning4j\u2019s Keras model import functionality.\n\n[TODO - This section: remains to be written]\n\nModels created and trained in Keras (both Keras 1 and 2) can be imported to DL4J.\n\nFor further details, see the following pages:", 
        "title": "Deeplearning4j Quick Reference: Layers, Functionality and Classes - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/buildinglocally", 
        "text": "NOTE: MOST USERS SHOULD USE THE RELEASES ON MAVEN CENTRAL AS PER THE QUICK START GUIDE, AND NOT BUILD FROM SOURCE\n\nUnless you have a very good reason to build from source (such as developing new features - excluding custom layers, custom activation functions, custom loss functions, etc - all of which can be added without modifying DL4J directly) then you shouldn\u2019t build from source. Building from source can be quite complex, with no benefit in a lot of cases.\n\nFor those developers and engineers who prefer to use the most up-to-date version of Deeplearning4j or fork and build their own version, these instructions will walk you through building and installing Deeplearning4j. The preferred installation destination is to your machine\u2019s local maven repository. If you are not using the master branch, you can modify these steps as needed (i.e.: switching GIT branches and modifying the script).\n\nBuilding locally requires that you build the entire Deeplearning4j stack which includes:\n\nNote that Deeplearning4j is designed to work on most platforms (Windows, OS X, and Linux) and is also includes multiple \u201cflavors\u201d depending on the computing architecture you choose to utilize. This includes CPU (OpenBLAS, MKL, ATLAS) and GPU (CUDA). The DL4J stack also supports x86 and PowerPC architectures.\n\nYour local machine will require some essential software and environment variables set before you try to build and install the DL4J stack. Depending on your platform and the version of your operating system, the instructions may vary in getting them to work. This software includes:\n\nUbuntu Assuming you are using Ubuntu as your flavor of Linux and you are running as a non-root user, follow these steps to install prerequisite software:\n\nHomebrew is the accepted method of installing prerequisite software. Assuming you have Homebrew installed locally, follow these steps to install your necessary tools.\n\nFirst, before using Homebrew we need to ensure an up-to-date version of Xcode is installed (it is used as a primary compiler):\n\nNote: You can not use clang. You also can not use a new version of gcc. If you have a newer version of gcc, please switch versions with this link\n\nlibnd4j depends on some Unix utilities for compilation. So in order to compile it you will need to install Msys2.\n\nAfter you have setup Msys2 by following their instructions, you will have to install some additional development packages. Start the msys2 shell and setup the dev environment with:\n\nThis will install the needed dependencies for use in the msys2 shell.\n\nYou will also need to setup your PATH environment variable to include (or where ever you have decided to install msys2). If you have IntelliJ (or another IDE) open, you will have to restart it before this change takes effect for applications started through them. If you don\u2019t, you probably will see a \u201cCan\u2019t find dependent libraries\u201d error.\n\nOnce you have installed the prerequisite tools, you can now install the required architectures for your platform.\n\nOf all the existing architectures available for CPU, Intel MKL is currently the fastest. However, it requires some \u201coverhead\u201d before you actually install it.\n\nUbuntu Assuming you are using Ubuntu, you can install OpenBLAS via:\n\nYou will also need to ensure that (or any other home directory for OpenBLAS) is on your . In order to get OpenBLAS to work with Apache Spark, you will also need to do the following:\n\nCentOS Enter the following in your terminal (or ssh session) as a root user:\n\nAfter that, you should see a lot of activity and installs on the terminal. To verify that you have, for example, gcc, enter this line:\n\nFor more complete instructions, go here.\n\nYou can install OpenBLAS on OS X with Home Science:\n\nAn OpenBLAS package is available for . You can install it using the command.\n\nUbuntu An apt package is available for ATLAS on Ubuntu:\n\nCentOS You can install ATLAS on CentOS using:\n\nInstalling ATLAS on OS X is a somewhat complicated and lengthy process. However, the following commands will work on most machines:\n\nDetailed instructions for installing GPU architectures such as CUDA can be found here.\n\nThe CUDA Backend has some additional requirements before it can be built:\n\nIn order to build the CUDA backend you will have to setup some more environment variables first, by calling . But first, set the system environment variable to , so all of the variables that sets up, are passed to the mingw shell.\n\nIf you are building Deeplearning4j through an IDE such as IntelliJ, you will need to install certain plugins to ensure your IDE renders code highlighting appropriately. You will need to install a plugin for Lombok:\n\nIf you want to work on ScalNet, the Scala API, or on certain modules such as the DL4J UI, you will need to ensure your IDE has Scala support installed and available to you.\n\nDeeplearning4j uses a separate repository that contains all resources necessary for testing. This is to keep the central DL4J repository lightweight and avoid large blobs in the GIT history. If you wish to run tests in the DL4J stack:\n\nBefore running the DL4J stack build script, you must ensure certain environment variables are defined before running your build. These are outlined below depending on your architecture.\n\nYou will need to know the exact path of the directory where you are running the DL4J build script (you are encouraged to use a clean empty directory). Otherwise, your build will fail. Once you determine this path, add to the end of that path and export it to your local environment. This will look like:\n\nYou can link with MKL either at build time, or at runtime with binaries initially linked with another BLAS implementation such as OpenBLAS. To build against MKL, simply add the path containing (or on Windows), say , to the environment variable on Linux (or on Windows) and build like before. On Linux though, to make sure it uses the correct version of OpenMP, we also might need to set these environment variables:\n\nWhen libnd4j cannot be rebuilt, we can use the MKL libraries after the facts and get them loaded instead of OpenBLAS at runtime, but things are a bit trickier. Please additionally follow the instructions below.\n\nYou can use the build-dl4j-stack.sh script from the deeplearning4j repository to build the whole deeplearning4j stack from source: libndj4, ndj4, datavec, deeplearning4j. It clones the DL4J stack, builds each repository, and installs them locally to Maven. This script will work on both Linux and OS X platforms.\n\nOK, now read the following section carefully.\n\nUse the build script below for CPU architectures:\n\nMake sure to read this if you are on OS X (ensure gcc 5.x is setup and you aren\u2019t using clang): https://github.com/deeplearning4j/deeplearning4j/issues/2668\n\nIf you are using a GPU backend, use this instead:\n\nYou can speed up your CUDA builds by using the flag as explained in the libndj4 README.\n\nFor Scala users, you can pass your binary version for Spark compatibility:\n\nThe build script passes all options and flags to the libnd4j script. All flags used for those script can be passed via .\n\nIf you prefer, you can build each piece in the DL4J stack by hand. The procedure for each piece of software is essentially:\n\nThe overall procedure looks like the following commands below, with the exception that libnd4j\u2019s accepts parameters based on the backend you are building for. You need to follow these instructions in the order they\u2019re given. If you don\u2019t, you\u2019ll run into errors. The GPU-specific instructions below have been commented out, but should be substituted for the CPU-specific commands when building for a GPU backend.\n\nOnce you\u2019ve installed the DL4J stack to your local maven repository, you can now include it in your build tool\u2019s dependencies. Follow the typical Getting Started instructions for Deeplearning4j, and appropriately replace versions with the SNAPSHOT version currently on the master POM.\n\nNote that some build tools such as Gradle and SBT don\u2019t properly pull in platform-specific binaries. You can follow instructions here for setting up your favorite build tool.\n\nIf you encounter issues while building locally, the Deeplearning4j Early Adopters Channel is a channel dedicated to assisting with build issues and other source problems. Please reach out on Gitter for help.", 
        "title": "Building the DL4J Stack Locally - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/maven", 
        "text": "Maven is the most commonly used build automation tool for Java programmers. While there is no Python tool that matches Maven feature for feature, it is analogous to a package management system like pip in Python, or PyBuilder, or Distutils.\n\nIt is also the single most convenient way to get up and running with Deeplearning4j, which offers a Scala API whose syntax will strike many Python programmers as eerily familiar, while offering them powerful concurrent features.\n\nAs a build automation tool, Maven compiles source to byte code and links object files to executables, library files etc. Its deliverable is a JAR file, created from Java source, as well as resources for deployment.\n\nMaven dynamically downloads Java libraries and Maven plug-ins from Maven Central Repository which are specified in an XML file that stores a Project Object Model, which you\u2019ll find in the file POM.xml.\n\nTo quote Maven: The Complete Reference:\n\nLike Deeplearning4j, Maven relies on convention over configuration, which means that it provides default values that allow it to run without the programmer having to specify each parameter for each new project.\n\nIf you have both IntelliJ and Maven installed, IntelliJ will allow you to choose Maven when creating a new project in the IDE, and will then take you through the wizard (we comment more thoroughly on the process in our Getting Started page). That is, you can make the build happen from within IntelliJ, without going anywhere else.\n\nAlternatively, you can use Maven from your project\u2019s root directory in the command prompt to freshly install it:\n\nThe above command instructs maven to clean any directories of compiled files before running install. This makes sure that the build is a clean build from scratch.\n\nSeveral useful books have been written about Apache Maven. They are available on the website of Sonatype, the company that supports the open-source project.", 
        "title": "Maven for Python Programmers - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/buildtools", 
        "text": "While we encourage Deeplearning4j, ND4J and DataVec users to employ Maven, it\u2019s worthwhile documenting how to configure build files for other tools, like Ivy, Gradle and SBT \u2013 particularly since Google prefers Gradle over Maven for Android projects.\n\nThe instructions below apply to all DL4J and ND4J submodules, such as deeplearning4j-api, deeplearning4j-scaleout, and ND4J backends. You can find the latest version of any project or submodule on Maven Central. As of January 2017, the latest version is . Building from source, the latest version is .\n\nYou can use Deeplearning4j with Maven by adding the following to your POM.xml:\n\nNote that deeplearning4J will have dependencies on nd4J and DataVec, for a working example of proper Maven configurations please see our examples\n\nYou can use lombok with ivy by adding the following to your ivy.xml:\n\nYou can use Deeplearning4j with SBT by adding the following to your build.sbt:\n\nYou can use Deeplearning4j with Gradle by adding the following to your build.gradle in the dependencies block:\n\nClojure programmers may want to use Leiningen or Boot to work with Maven. A Leiningen tutorial is here.\n\nNOTE: You\u2019ll still need to download ND4J, DataVec and Deeplearning4j, or doubleclick on the their respective JAR files file downloaded by Maven / Ivy / Gradle, to install them in your Eclipse installation.\n\nBackends and other dependencies are explained on the ND4J website.", 
        "title": "Configuring Automated Build Tools - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/overview", 
        "text": "Deeplearning4j is a Java-based toolkit for building, training and deploying deep neural networks, the regressions and KNN.\n\nDeeplearning4j has the following sub-projects.\n\nIngesting, cleaning, joining, scaling, normalizing and transforming data are jobs that must be done in any sort of data analysis. This work may not be exciting, but it\u2019s a precondition of deep learning. DataVec is our toolkit for that process. We give data scientists and developers tools to turn raw data such as images, video, audio, text and time series into feature vectors for neural nets.\n\nThe DataVec Github repo is here. Here is how the repo is organized.\n\nThere are DataVec examples in our examples repo on Github.\n\nA descriptive summary of many of the examples is here.\n\nHere is the DataVec JavaDoc.\n\nNeural Networks process multi-dimensional arrays of numerical data. Getting your data from a CSV file, or a directory of images, to be serialized into numeric arrays is the job of DataVec. DataVec is an ETL tool specifically built for machine learning pipelines.\n\nBase class for reading and serializing data. RecordReaders ingest your data input and return a List of Serializable objects (Writables).\n\nThe labels for data input may be based on the directory where the image is stored.\n\nDeeplearning4j is where you design your neural networks. It is a domain specific language (DSL) for configuring neural networks.\n\nThe Deeplearning4j Github repo is here. Here\u2019s how the repo is organized.\n\nHere is the Deeplearning4j JavaDoc here.\n\nThere are Deeplearning4j examples in the Github repository here.\n\nA descriptive summary of many of the examples is here.\n\nIf you have worked with the Python Library Keras and would like to import a trained model, or a model configuration, into Deeplearning4j, please see our model import feature.\n\nThe Model Import is actually part of DeepLearning4J, but it is worth its own section. Github folder is here.\n\nWe will add examples here\n\nHere\u2019s a video showing how to import of a Keras model to DL4J:\n\nND4J is the numerical computing library that underpins Deeplearning4j. It is a tensor library, the JVM\u2019s answer to Numpy.\n\nHere is the ND4J Github repo. ND4J is a DSL for handling n-dimensional arrays (NDArrays), also known as tensors.\n\nHere is the ND4J JavaDoc.\n\nYou may not use some of these classes directly, but when you configure a neural network. Behind the scenes, the configurations you set for , , and are all done in ND4J.\n\nScalNet is Keras for Scala. It\u2019s a Scala wrapper for Deeplearning4j that can run Spark on multi-GPUs.\n\nRL4J is a library and environment for reinforcement learning on the JVM. It includes Deep Q learning, A3C and other algorithms implemented in Java and integrated with DL4J and ND4J.\n\nArbiter helps you search the hyperparameter space to find the best tuning and architecture for a neural net. This is important because finding the right architecture and hyperparamters is a very large combinatorial problem. The winning ImageNet teams at corporate labs like Microsoft are searching through hyperparameters to surface 150-layer networks like ResNet. Arbiter includes grid search, random search, some Bayesian methods, as well as model evaluation tools.\n\nHere is the Arbiter Github repository.\n\nFor people just getting started with deep learning, the following tutorials and videos provide an easy entrance to the fundamental ideas of deep neural networks:", 
        "title": "What Is DeepLearning4j? - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/examples-tour", 
        "text": "Deeplearning4j\u2019s Github repository has many examples to cover its functionality. The Quick Start Guide shows you how to set up Intellij and clone the repository. This page provides an overview of some of those examples.\n\nMost of the examples make use of DataVec, a toolkit for preprocessing and clearning data through normalization, standardization, search and replace, column shuffles and vectorization. Reading raw data and transforming it into a DataSet object for your Neural Network is often the first step toward training that network. If you\u2019re unfamiliar with DataVec, here is a description and some links to useful examples.\n\nThis example takes the canonical Iris dataset of the flower species of the same name, whose relevant measurements are sepal length, sepal width, petal length and petal width. It builds a Spark RDD from the relatively small dataset and runs an analysis against it.\n\nThis example loads data into a Spark RDD. All DataVec transform operations use Spark RDDs. Here, we use DataVec to filter data, apply time transformations and remove columns.\n\nThis example shows the print Schema tools that are useful to visualize and to ensure that the code for the transform is behaving as expected.\n\nYou may need to join datasets before passing to a neural network. You can do that in DataVec, and this example shows you how.\n\nThis is an example of parsing log data using DataVec. The obvious use cases are cybersecurity and customer relationship management.\n\nThis example is from the video below, which demonstrates the ParentPathLabelGenerator and ImagePreProcessing scaler.\n\nThis example demonstrates preprocessing features available in DataVec.\n\nDataMeta data tracking - i.e. seeing where data for each example comes from - is useful when tracking down malformed data that causes errors and other issues. This example demostrates the functionality in the RecordMetaData class.\n\nTo build a neural net, you will use either or . Both options work using a Builder interface. A few highlights from the examples are described below.\n\nMNIST is the \u201cHello World\u201d of deep learning. Simple, straightforward, and focussed on image recognition, a task that Neural Networks do well.\n\nThis is a Single Layer Perceptron for recognizing digits. Note that this pulls the images from a binary package containing the dataset, a rather special case for data ingestion.\n\nA two-layer perceptron for MNIST, showing there is more than one useful network for a given dataset.\n\nData flows through feed-forward neural networks in a single pass from input via hidden layers to output.\n\nThese networks can be used for a wide range of tasks depending on they are configured. Along with image classification over MNIST data, this directory has examples demonstrating regression, classification, and anomoly detection.\n\nConvolutional Neural Networks are mainly used for image recognition, although they apply to sound and text as well.\n\nThis example can be run using either LeNet or AlexNet.\n\nTraining a network over a large volume of training data takes time. Fortunately, you can save a trained model and load the model for later training or inference.\n\nThis demonstrates saving and loading a network build using the class ComputationGraph.\n\nDemonstrates saving and loading a Neural Network built with the class MultiLayerNetwork.\n\nOur video series shows code that includes saving and loading models, as well as inference.\n\nDo you need to add a Loss Function that is not available or prebuilt yet? Check out these examples.\n\nDo you need to add a layer with features that aren\u2019t available in DeepLearning4J core? This example show where to begin.\n\nNeural Networks for NLP? We have those, too.\n\nGlobal Vectors for Word Representation are useful for detecting relationships between words.\n\nA vectorized representation of words. Described here\n\nOne way to represent sentences is as a sequence of words.\n\nt-Distributed Stochastic Neighbor Embedding (t-SNE) is useful for data visualization. We include an example in the NLP section since word similarity visualization is a common use.\n\nRecurrent Neural Networks are useful for processing time series data or other sequentially fed data like video.\n\nThe examples folder for Recurrent Neural Networks has the following:\n\nTakes the complete works of Shakespeare as a sequence of characters and Trains a Neural Net to generate \u201cShakespeare\u201d one character at a time.\n\nThis example trains a neural network to do addition.\n\nThis example trains a neural network to perform various math operations.\n\nA publicly available dataset of time series data of six classes, cyclic, up-trending, etc. Example of an RNN learning to classify the time series.\n\nHow do autonomous vehicles distinguish between a pedestrian, a stop sign and a green light? A complex neural net using Convolutional and Recurrent layers is trained on a set of training videos. The trained network is passed live onboard video and decisions based on object detection from the Neural Net determine the vehicles actions.\n\nThis example is similar, but simplified. It combines convolutional, max pooling, dense (feed forward) and recurrent (LSTM) layers to classify frames in a video.\n\nThis sentiment analysis example classifies sentiment as positive or negative using word vectors and a Recurrent Neural Network.\n\nDeepLearning4j supports using a Spark Cluster for network training. Here are the examples.\n\nThis is an example of a Multi-Layer Perceptron training on the Mnist data set of handwritten digits. Show me the code\n\nND4J is a tensor processing library. It can be thought of as Numpy for the JVM. Neural Networks work by processing and updating MultiDimensional arrays of numeric values. In a typical Neural Net application you use DataVec to ingest and convert the data to numeric. Classes used would be RecordReader. Once you need to pass data into a Neural Network, you typically use RecordReaderDataSetIterator. RecordReaderDataSetIterator returns a DataSet object. DataSet consists of an NDArray of the input features and an NDArray of the labels.\n\nThe learning algorithms and loss functions are executed as ND4J operations.\n\nThis is a directory with examples for creating and manipulating NDArrays.\n\nDeep learning algorithms have learned to play Space Invaders and Doom using reinforcement learning. DeepLearning4J/RL4J examples of Reinforcement Learning are available here:", 
        "title": "Survey of DeepLearning4j Examples - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/benchmark", 
        "text": "Total training time is always ETL plus computation. That is, both the data pipeline and the matrix manipulations determine how long a neural network takes to train on a dataset.\n\nWhen programmers familiar with Python try to run benchmarks comparing Deeplearning4j to well-known Python frameworks, they usually end up comparing ETL + computation on DL4J to just computation on the Python framework. That is, they\u2019re comparing apples to oranges. We\u2019ll explain how to optimize several parameters below.\n\nThe JVM has knobs to tune, and if you know how to tune them, you can make it a very fast environment for deep learning. There are several things to keep in mind on the JVM. You need to:\n\nUsers have to reconfigure their JVMs themselves, including setting the heap space. We can\u2019t give it to you preconfigured, but we can show you how to do it. Here are the two most important knobs for heap space.\n\nYou can set these in IDEs like IntelliJ and Eclipse, as well as via the CLI like so:\n\nIn IntelliJ, this is a VM parameter, not a program argument. When you hit run in IntelliJ (the green button), that sets up a run-time configuration. IJ starts a Java VM for you with the configurations you specify.\n\nWhat\u2019s the ideal amount to set to? That depends on how much RAM is on your computer. In general, allocate as much heap space as you think the JVM will need to get work done. Let\u2019s say you\u2019re on a 16G RAM laptop \u2014 allocate 8G of RAM to the JVM. A sound minimum on laptops with less RAM would be 3g, so\n\nIt may seem counterintuitive, but you want the min and max to be the same; i.e. should equal . If they are unequal, the JVM will progressively allocate more memory as needed until it reaches the max, and that process of gradual allocation slows things down. You want to pre-allocate it at the beginning. So\n\nIntelliJ will automatically specify the Java main class in question.\n\nAnother way to do this is by setting your environmental variables. Here, you would alter your hidden file, which adds environmental variables to bash. To see those variables, enter in the command line. To add more heap space, enter this command in your console:\n\nWe need to increase heap space because Deeplearning4j loads data in the background, which means we\u2019re taking more RAM in memory. By allowing more heap space for the JVM, we can cache more data in memory.\n\nA garbage collector is a program which runs on the JVM and gets rid of objects no longer used by a Java application. It is automatic memory management. Creating a new object in Java takes on-heap memory: A new Java object takes up 8 bytes of memory by default. So every new you create takes another 8 bytes.\n\nYou may need to alter the garbage collection algorithm that Java is using. This can be done via the command line like so:\n\nBetter garbage collection increases throughput. For a more detailed exploration of the issue, please read this InfoQ article.\n\nDL4J is tightly linked to the garbage collector. JavaCPP, the bridge between the JVM and C++, adheres to the heap space you set with and works extensively with off-heap memory. The off-heap memory will not surpass the amount of heap space you specify.\n\nJavaCPP, created by a Skymind engineer, relies on the garbage collector to tell it what has been done. We rely on the Java GC to tell us what to collect; the Java GC points at things, and we know how to de-allocate them with JavaCPP. This applies equally to how we work with GPUs.\n\nThe larger the batch size you use, the more RAM you\u2019re taking in memory.\n\nIn our repo, we don\u2019t make the ETL asynchronous, because the point of examples is to keep them simple. But for real-world problems, you need asynchronous ETL, and we\u2019ll show you how to do it with examples.\n\nData is stored on disk and disk is slow. That\u2019s the default. So you run into bottlenecks when loading data onto your harddrive. When optimizing throughput, the slowest component is always the bottleneck. For example, a distributed Spark job using three GPU workers and one CPU worker will have a bottleneck with the CPU. The GPUs have to wait for that CPU to finish.\n\nThe Deeplearning4j class hides the complexity of loading data on disk. The code for using any Datasetiterator will always be the same, invoking looks the same, but they work differently.\n\nHere\u2019s how the DatasetIterator is uniformly invoked for MNIST:\n\nYou can optimize by using an asychronous loader in the background. Java can do real multi-threading. It can load data in the background while other threads take care of compute. So you load data into the GPU at the same time that compute is being run. The neural net trains even as you grab new data from memory.\n\nThis is the relevant code, in particular the third line:\n\nThere are actually two types of asynchronous dataset iterators. The is what you would use most of the time. It\u2019s described in the Javadoc here.\n\nFor special cases such as recurrent neural nets applied to time series, or for computation graphs, you would use a , described in the Javadoc here.\n\nNotice in the code above that is another parameter to set. Normal batch size might be 1000 examples, but if you set to 3, it would pre-fetch 3,000 instances.\n\nIn Python, programmers are converting their data into pickles, or binary data objects. And if they\u2019re working with a smallish toy dataset, they\u2019re loading all those pickles into RAM. So they\u2019re effectively sidestepping a major task in dealing with larger datasets. At the same time, when benchmarking against Dl4j, they\u2019re not loading all the data onto RAM. So they\u2019re effectively comparing Dl4j speed for training computations + ETL against only training computation time for Python frameworks.\n\nBut Java has robust tools for moving big data, and if compared correctly, is much faster than Python. The Deeplearning4j community has reported up to 3700% increases in speed over Python frameworks, when ETL and computation are optimized.\n\nDeeplearning4j uses DataVec as it ETL and vectorization library. Unlike other deep-learning tools, DataVec does not force a particular format on your dataset. (Caffe forces you to use hdf5, for example.)\n\nWe try to be more flexible. That means you can point DL4J at raw photos, and it will load the image, run the transforms and put it into an NDArray to generate a dataset on the fly.\n\nBut if your training pipeline is doing that every time, Deeplearning4j will seem about 10x slower than other frameworks, because you\u2019re spending your time creating datasets. Every time you call , you\u2019re recreating a dataset, over and over again. We allow it to happen for ease of use, but we can show you how to speed things up. There are ways to make it just as fast.\n\nOne way is to pre-save the datasets, in a manner similar to the Python frameworks. (Pickles are pre-formatted data.) When you pre-save the dataset, you create a separate class.\n\nA talks to Datavec and outputs datasets for DL4J.\n\nLine 90 is where you see the asynchronous ETL. In this case, it\u2019s wrapping the pre-saved iterator, so you\u2019re taking advantage of both methods, with the asynch loading the pre-saved data in the background as the net trains.\n\nIf you are running inference benchmarks on CPUs, make sure you are using Deeplearning4j with Intel\u2019s MKL library, which is available via a clickwrap; i.e. Deeplearning4j does not bundle MKL like Anaconda, which is used by libraries like PyTorch.", 
        "title": "Deeplearning4j Benchmarks - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/tutorials", 
        "text": "Welcome to our deep-learning tutorials page. If you are just getting started with deep learning and Deeplearning4j, these tutorials will help clarify some of the concepts you will need to build neural networks. Image recognition, text processing and classification examples are provided here. Some of the examples include a written introduction along with code, while others include a screencast with voice over of the code being written. Enjoy!\n\nMNIST is the \u201cHello World\u201d of machine learning. With this tutorial, you can train on MNIST with a single-layer neural network.\n\nView the tutorial\n\n Just show me the code\n\nBy applying a more complex algorithm, Lenet, to MNIST, you can achieve 99 percent accuracy. Lenet is a deep convolutional network.\n\nJust show me the code\n\nWord2vec is a popular natural-language processing algorithm capable of creating neural word embeddings, which in turn can be fed into deeper neural networks.\n\nView the tutorial\n\n Just show me the code\n\nThis tutorial shows you how a multilayer perceptron can be used as a linear classifier.\n\nJust show me the code\n\nVideo lecture by instructor Tom Hanlon on Machine Learning. Tom provides an overview of how to build a simple neural net in this introductory tutorial. This screencast shows how to build a Linear Classifier using Deeplearning4j.\n\nXOR is a digital logic gate that implements an exclusive or. XOR means that a true output, or an output of 1, results if one, and only one, of the inputs to the gate is true.\n\nJust show me the code\n\nThis tutorial shows how to use Skymind\u2019s DataVec to ingest Comma Separated Values from a text file, convert the fields to numeric using a DataVec Transform Process in Spark, and save the modified data. Transforming non-numeric data to numeric data is a key preliminary step to using a Neural Network to analyze the data.\n\nThis tutorial is a series of videos and code examples that show a complete data pipeline.\n\nThe first example demonstrates using some of DataVec\u2019s tools to read a directory of images and generate a label for the image using ParentPathLabelGenerator. Once the data is read and labelled the image data is scaled so that the pixel values fall between 0 and 1, instead of 0 and 255.\n\nThis tutorial builds on the Image Ingestion and Labelling tutorial by taking the DataVec image pipeline and adding a Neural Network to train on the images. Topics covered include MultiLayerNetwork, DataSetIterator, Training the network, monitoring scores as the model trains.\n\nOnce you have a trained your network you may want to save the trained network for use in building an application. This tutorial demonstrates what is needed to save the trained model, and to load the trained model.\n\nOnce your network is trained and tested it is time to deploy the network in an application. This tutorial demonstrates Loading a training model, and adding a simple interface of a filechooser to allow the user to get the models opinion on what digit the image passed in might be. In the video I test a simple Feed Forward Neural Network that has been trained on the MNist dataset of handwritten digits, against an image of the digit 3 found in a google search.\n\nRecurrent nets are a type of artificial neural network designed to recognize patterns in sequences of data, such as text, genomes, handwriting, the spoken word, or numerical times series data emanating from sensors, stock markets and government agencies.\n\nLong Short-Term Memory Units provide a mechanism to allow a nueral net to learn from experience to classify, process and predict time series events.\n\nA more in depth discussion of RNN\u2019s including configuring your code to use RNN\u2019s in DL4J\n\nAn overview of loading VGG-16, testing and deploying as a web application\n\nFor people just getting started with deep learning, the following tutorials and videos provide an easy entrance to the fundamental ideas of deep neural networks:", 
        "title": "Deep Learning Tutorials - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/mnist-for-beginners", 
        "text": "\n\n\n\n MNIST is a database containing images of handwritten digits, with each image labeled by integer. It is used to benchmark the performance of machine learning algorithms. Deep learning performs quite well on MNIST, achieving more than 99.7% accuracy. We will use MNIST to train a neural network to look at each image and predict the digit. The first step is to install Deeplearning4j. GET STARTED WITH DEEP LEARNING\n\nThe MNIST dataset contains a training set of 60,000 examples, and a test set of 10,000 examples. The training set is used to teach the algorithm to predict the correct label, the integer, while the test set is used to check how accurately the trained network can make guesses. In the machine learning world, this is called supervised learning, because we have the correct answers for the images we\u2019re making guesses about. The training set can therefore act as a supervisor, or teacher, correcting the neural network when it guesses wrong.\n\nWe\u2019ve packaged the MNIST tutorial into Maven, so there\u2019s no need to write code. Please open IntelliJ to get started. (To download IntelliJ, see our Quickstart\u2026) Open the folder labeled . Go to the directories \u2192 \u2192 \u2192 \u2192 , and open the file . In this file, we will configure the neural network, train a model and evaluate the results. It will be helpful to view this code alongside the tutorial. final int numRows = 28; // The number of rows of a matrix. final int numColumns = 28; // The number of columns of a matrix. int outputNum = 10; // Number of possible outcomes (e.g. labels 0 through 9). int batchSize = 128; // How many examples to fetch with each step. int rngSeed = 123; // This random-number generator applies a seed to ensure that the same initial weights are used when training. We\u2019ll explain why this matters later. int numEpochs = 15; // An epoch is a complete pass through a given dataset. In our example, each MNIST image is 28x28 pixels, which means our input data is a 28 numRows x 28 numColumns matrix (matrices are the fundamental data structures of deep learning). Furthermore, MNIST contains 10 possible outcomes (the labels numbered 0 - 9) which is our outputNum. The batchSize and numEpochs have to be chosen based on experience; you learn what works through experimentation. A larger batch size results in faster training, while more epochs, or passes through the dataset, result in better accuracy. However, there are diminishing returns beyond a certain number of epochs, so there is a trade off between accuracy and training speed. In general, you\u2019ll need to experiment to discover the optimal values. We\u2019ve set reasonable defaults in this example. The class called is used to fetch the MNIST dataset. We create one dataset for training the model and another dataset for evaluating the accuracy of our model after training. The model, by the way, refers to the parameters of the neural network. Those parameters are coefficients that process signal from the input, and they are adjusted as the network learns until they can finally guess the correct label for each image; at that point, you have an accurate model.\n\nWe\u2019ll build a feedforward neural network based off a paper by Xavier Glorot and Yoshua Bengio. For our purposes, we\u2019ll start with a basic example with only one hidden layer. However, as a rule of thumb, the deeper your network (i.e. the more layers), the more complexity and nuance it can capture to produce accurate results. \n\n\n\n Keep this image in mind, because this is what we\u2019re constructing, a single layer neural network. For any neural network you build in Deeplearning4j, the foundation is the NeuralNetConfiguration class. This is where you configure hyperparameters, the quantities that define the architecture and how the algorithm learns. Intuitively, each hyperparameter is like one ingredient in a meal, a meal that can go very right, or very wrong\u2026 Luckily, you can adjust hyperparameters if they don\u2019t produce the right results. This parameter uses a specific, randomly generated weight initialization. If you run an example many times, and generate new, random weights each time you begin, then your net\u2019s results -- accuracy and F1 score -- may vary a great deal, because different initial weights can lead algorithms to different local minima in the errorscape. Keeping the same random weights allows you isolate the effect of adjusting other hyperparameters more clearly, while other conditions remain equal. Stochastic gradient descent (SGD) is a common method to optimize the cost function. To learn more about SGD and other optimization algorithms that help minimize error, we recommend Andrew Ng\u2019s Machine Learning course and the SGD definition in our glossary. This line sets the updater as nesterovs and the learning rate and the momentun. The learning rate, which is the size of the adjustments made to the weights with each iteration, the step size. A high learning rate makes a net traverse the errorscape quickly, but also makes it prone to overshoot the point of minimum error. A low learning rate is more likely to find the minimum, but it will do so very slowly, because it is taking small steps in adjusting the weights. Momentum is an additional factor in determining how fast an optimization algorithm converges on the optimum point. Momentum affects the direction that weights are adjusted in, so in the code we consider it a weight . Regularization is a technique to prevent what\u2019s called overfitting. Overfitting is when the model fits the training data really well, but performs poorly in real life as soon as it's exposed to data it hasn\u2019t seen before. We use L2 regularization, which prevents individual weights from having too much influence on the overall results. The list specifies the number of layers in the net; this function replicates your configuration n times and builds a layerwise configuration. Again, if any of the above was confusing, we recommend Andrew Ng\u2019s Machine Learning course. We won\u2019t go into the research behind each hyperparameter (i.e. activation, weightInit); we'll just attempt to give brief definitions of what they do. However, feel free to read the paper by Xavier Glorot and Yoshua Bengio to learn why these matter. \n\n .layer(0, new DenseLayer.Builder() .nIn(numRows * numColumns) // Number of input datapoints. .nOut(1000) // Number of output datapoints. .activation(\"relu\") // Activation function. .weightInit(WeightInit.XAVIER) // Weight initialization. .build()) .layer(1, new OutputLayer.Builder(LossFunction.NEGATIVELOGLIKELIHOOD) .nIn(1000) .nOut(outputNum) .activation(\"softmax\") .weightInit(WeightInit.XAVIER) .build()) .pretrain(false).backprop(true) .build(); So what exactly is the hidden layer? Each node (the circles) in the hidden layer represents a feature of a handwritten digit in the MNIST dataset. For example, imagine you are looking at the number 6. One node may represent rounded edges, another node may represent the intersection of curly lines, and so on and so forth. Such features are weighted by importance by the model's coefficients, and recombined in each hidden layer to help predict whether the handwritten number is indeed 6. The more layers of nodes you have, the more complexity and nuance they can capture to make better predictions. You could think of a layer as \"hidden\" because, while you can see the input entering a net, and the decisions coming out, it\u2019s difficult for humans to decipher how and why a neural net processes data on the inside. The parameters of a neural net model are simply long vectors of numbers, readable by machines.\n\n\n\n\n\n Accuracy - The percentage of MNIST images that were correctly identified by our model.\n\n Precision - The number of true positives divided by the number of true positives and false positives. \n\n Recall - The number of true positives divided by the number of true positives and the number of false negatives. \n\n F1 Score - Weighted average of precision and recall. \n\n Accuracy measures the model over all. Precision, recall and F1 measure a model\u2019s relevance. For example, it would be dangerous to predict that cancer will not reoccur (i.e. a false negative) as the person would not seek further treatment. Because of this, it would be wise to choose a model that avoids false negatives (i.e. a higher precision, recall and F1) even if the overall accuracy lower.", 
        "title": "Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/usingrnns", 
        "text": "This document outlines the specifics training features and the practicalities of how to use them in DeepLearning4J. This document assumes some familiarity with recurrent neural networks and their use - it is not an introduction to recurrent neural networks, and assumes some familiarity with their both their use and terminology. If you are new to RNNs, read A Beginner\u2019s Guide to Recurrent Networks and LSTMs before proceeding with this page.\n\nDL4J currently supports the following types of recurrent neural network\n\nJava documentation for each is available, GravesLSTM, BidirectionalGravesLSTM, BaseRecurrent\n\nConsider for the moment a standard feed-forward network (a multi-layer perceptron or \u2018DenseLayer\u2019 in DL4J). These networks expect input and output data that is two-dimensional: that is, data with \u201cshape\u201d [numExamples,inputSize]. This means that the data into a feed-forward network has \u2018numExamples\u2019 rows/examples, where each row consists of \u2018inputSize\u2019 columns. A single example would have shape [1,inputSize], though in practice we generally use multiple examples for computational and optimization efficiency. Similarly, output data for a standard feed-forward network is also two dimensional, with shape [numExamples,outputSize].\n\nConversely, data for RNNs are time series. Thus, they have 3 dimensions: one additional dimension for time. Input data thus has shape [numExamples,inputSize,timeSeriesLength], and output data has shape [numExamples,outputSize,timeSeriesLength]. This means that the data in our INDArray is laid out such that the value at position (i,j,k) is the jth value at the kth time step of the ith example in the minibatch. This data layout is shown below.\n\nWhen importing time series data using the class CSVSequenceRecordReader each line in the data files represents one time step with the earliest time series observation in the first row (or first row after header if present) and the most recent observation in the last row of the csv. Each feature time series is a separate column of the of the csv file. For example if you have five features in time series, each with 120 observations, and a training & test set of size 53 then there will be 106 input csv files(53 input, 53 labels). The 53 input csv files will each have five columns and 120 rows. The label csv files will have one column (the label) and one row.\n\nRnnOutputLayer is a type of layer used as the final layer with many recurrent neural network systems (for both regression and classification tasks). RnnOutputLayer handles things like score calculation, and error calculation (of prediction vs. actual) given a loss function etc. Functionally, it is very similar to the \u2018standard\u2019 OutputLayer class (which is used with feed-forward networks); however it both outputs (and expects as labels/targets) 3d time series data sets.\n\nConfiguration for the RnnOutputLayer follows the same design other layers: for example, to set the third layer in a MultiLayerNetwork to a RnnOutputLayer for classification:\n\nUse of RnnOutputLayer in practice can be seen in the examples, linked at the end of this document.\n\nTraining neural networks (including RNNs) can be quite computationally demanding. For recurrent neural networks, this is especially the case when we are dealing with long sequences - i.e., training data with many time steps.\n\nTruncated backpropagation through time (BPTT) was developed in order to reduce the computational complexity of each parameter update in a recurrent neural network. In summary, it allows us to train networks faster (by performing more frequent parameter updates), for a given amount of computational power. It is recommended to use truncated BPTT when your input sequences are long (typically, more than a few hundred time steps).\n\nConsider what happens when training a recurrent neural network with a time series of length 12 time steps. Here, we need to do a forward pass of 12 steps, calculate the error (based on predicted vs. actual), and do a backward pass of 12 time steps:\n\nFor 12 time steps, in the image above, this is not a problem. Consider, however, that instead the input time series was 10,000 or more time steps. In this case, standard backpropagation through time would require 10,000 time steps for each of the forward and backward passes for each and every parameter update. This is of course very computationally demanding.\n\nIn practice, truncated BPTT splits the forward and backward passes into a set of smaller forward/backward pass operations. The specific length of these forward/backward pass segments is a parameter set by the user. For example, if we use truncated BPTT of length 4 time steps, learning looks like the following:\n\nNote that the overall complexity for truncated BPTT and standard BPTT are approximately the same - both do the same number of time step during forward/backward pass. Using this method however, we get 3 parameter updates instead of one for approximately the same amount of effort. However, the cost is not exactly the same there is a small amount of overhead per parameter update.\n\nThe downside of truncated BPTT is that the length of the dependencies learned in truncated BPTT can be shorter than in full BPTT. This is easy to see: consider the images above, with a TBPTT length of 4. Suppose that at time step 10, the network needs to store some information from time step 0 in order to make an accurate prediction. In standard BPTT, this is ok: the gradients can flow backwards all the way along the unrolled network, from time 10 to time 0. In truncated BPTT, this is problematic: the gradients from time step 10 simply don\u2019t flow back far enough to cause the required parameter updates that would store the required information. This tradeoff is usually worth it, and (as long as the truncated BPTT lengths are set appropriately), truncated BPTT works well in practice.\n\nUsing truncated BPTT in DL4J is quite simple: just add the following code to your network configuration (at the end, before the final .build() in your network configuration)\n\nThe above code snippet will cause any network training (i.e., calls to MultiLayerNetwork.fit() methods) to use truncated BPTT with segments of length 100 steps.\n\nDL4J supports a number of related training features for RNNs, based on the idea of padding and masking. Padding and masking allows us to support training situations including one-to-many, many-to-one, as also support variable length time series (in the same mini-batch).\n\nSuppose we want to train a recurrent neural network with inputs or outputs that don\u2019t occur at every time step. Examples of this (for a single example) are shown in the image below. DL4J supports training networks for all of these situations:\n\nWithout masking and padding, we are restricted to the many-to-many case (above, left): that is, (a) All examples are of the same length, and (b) Examples have both inputs and outputs at all time steps.\n\nThe idea behind padding is simple. Consider two time series of lengths 50 and 100 time steps, in the same mini-batch. The training data is a rectangular array; thus, we pad (i.e., add zeros to) the shorter time series (for both input and output), such that the input and output are both the same length (in this example: 100 time steps).\n\nOf course, if this was all we did, it would cause problems during training. Thus, in addition to padding, we use a masking mechanism. The idea behind masking is simple: we have two additional arrays that record whether an input or output is actually present for a given time step and example, or whether the input/output is just padding.\n\nRecall that with RNNs, our minibatch data has 3 dimensions, with shape [miniBatchSize,inputSize,timeSeriesLength] and [miniBatchSize,outputSize,timeSeriesLength] for the input and output respectively. The padding arrays are then 2 dimensional, with shape [miniBatchSize,timeSeriesLength] for both the input and output, with values of 0 (\u2018absent\u2019) or 1 (\u2018present\u2019) for each time series and example. The masking arrays for the input and output are stored in separate arrays.\n\nFor a single example, the input and output masking arrays are shown below:\n\nFor the \u201cMasking not required\u201d cases, we could equivalently use a masking array of all 1s, which will give the same result as not having a mask array at all. Also note that it is possible to use zero, one or two masking arrays when learning RNNs - for example, the many-to-one case could have a masking array for the output only.\n\nIn practice: these padding arrays are generally created during the data import stage (for example, by the SequenceRecordReaderDatasetIterator \u2013 discussed later), and are contained within the DataSet object. If a DataSet contains masking arrays, the MultiLayerNetwork fit will automatically use them during training. If they are absent, no masking functionality is used.\n\nMask arrays are also important when doing scoring and evaluation (i.e., when evaluating the accuracy of a RNN classifier). Consider for example the many-to-one case: there is only a single output for each example, and any evaluation should take this into account.\n\nEvaluation using the (output) mask arrays can be used during evaluation by passing it to the following method:\n\nwhere labels are the actual output (3d time series), predicted is the network predictions (3d time series, same shape as labels), and outputMask is the 2d mask array for the output. Note that the input mask array is not required for evaluation.\n\nScore calculation will also make use of the mask arrays, via the MultiLayerNetwork.score(DataSet) method. Again, if the DataSet contains an output masking array, it will automatically be used when calculating the score (loss function - mean squared error, negative log likelihood etc) for the network.\n\nSequence classification is one common use of masking. The idea is that although we have a sequence (time series) as input, we only want to provide a single label for the entire sequence (rather than one label at each time step in the sequence).\n\nHowever, RNNs by design output sequences, of the same length of the input sequence. For sequence classification, masking allows us to train the network with this single label at the final time step - we essentially tell the network that there isn\u2019t actually label data anywhere except for the last time step.\n\nNow, suppose we\u2019ve trained our network, and want to get the last time step for predictions, from the time series output array. How do we do that?\n\nTo get the last time step, there are two cases to be aware of. First, when we have a single example, we don\u2019t actually need to use the mask arrays: we can just get the last time step in the output array:\n\nAssuming classification (same process for regression, however) the last line above gives us probabilities at the last time step - i.e., the class probabilities for our sequence classification.\n\nThe slightly more complex case is when we have multiple examples in the one minibatch (features array), where the lengths of each example differ. (If all are the same length: we can use the same process as above).\n\nIn this \u2018variable length\u2019 case, we need to get the last time step for each example separately. If we have the time series lengths for each example from our data pipeline, it becomes straightforward: we just iterate over examples, replacing the in the above code with the length of that example.\n\nIf we don\u2019t have the lengths of the time series directly, we need to extract them from the mask array.\n\nIf we have a labels mask array (which is a one-hot vector, like [0,0,0,1,0] for each time series):\n\nAlternatively, if we have only the features mask: One quick and dirty approach is to use this:\n\nTo understand what is happening here, note that originally we have a features mask like [1,1,1,1,0], from which we want to get the last non-zero element. So we map [1,1,1,1,0] -> [1,2,3,4,0], and then get the largest element (which is the last time step).\n\nIn either case, we can then do the following:\n\nRNN layers in DL4J can be combined with other layer types. For example, it is possible to combine DenseLayer and GravesLSTM layers in the same network; or combine Convolutional (CNN) layers and GravesLSTM layers for video.\n\nOf course, the DenseLayer and Convolutional layers do not handle time series data - they expect a different type of input. To deal with this, we need to use the layer preprocessor functionality: for example, the CnnToRnnPreProcessor and FeedForwardToRnnPreprocessor classes. See here for all preprocessors. Fortunately, in most situations, the DL4J configuration system will automatically add these preprocessors as required. However, the preprocessors can be added manually (overriding the automatic addition of preprocessors, for each layer).\n\nFor example, to manually add a preprocessor between layers 1 and 2, add the following to your network configuration: .\n\nAs with other types of neural networks, predictions can be generated for RNNs using the and methods. These methods can be useful in many circumstances; however, they have the limitation that we can only generate predictions for time series, starting from scratch each and every time.\n\nConsider for example the case where we want to generate predictions in a real-time system, where these predictions are based on a very large amount of history. It this case, it is impractical to use the output/feedForward methods, as they conduct the full forward pass over the entire data history, each time they are called. If we wish to make a prediction for a single time step, at every time step, these methods can be both (a) very costly, and (b) wasteful, as they do the same calculations over and over.\n\nFor these situations, MultiLayerNetwork provides four methods of note:\n\nThe rnnTimeStep() method is designed to allow forward pass (predictions) to be conducted efficiently, one or more steps at a time. Unlike the output/feedForward methods, the rnnTimeStep method keeps track of the internal state of the RNN layers when it is called. It is important to note that output for the rnnTimeStep and the output/feedForward methods should be identical (for each time step), whether we make these predictions all at once (output/feedForward) or whether these predictions are generated one or more steps at a time (rnnTimeStep). Thus, the only difference should be the computational cost.\n\nIn summary, the MultiLayerNetwork.rnnTimeStep() method does two things:\n\nFor example, suppose we want to use a RNN to predict the weather, one hour in advance (based on the weather at say the previous 100 hours as input). If we were to use the output method, at each hour we would need to feed in the full 100 hours of data to predict the weather for hour 101. Then to predict the weather for hour 102, we would need to feed in the full 100 (or 101) hours of data; and so on for hours 103+.\n\nAlternatively, we could use the rnnTimeStep method. Of course, if we want to use the full 100 hours of history before we make our first prediction, we still need to do the full forward pass:\n\nFor the first time we call rnnTimeStep, the only practical difference between the two approaches is that the activations/state of the last time step are stored - this is shown in orange. However, the next time we use the rnnTimeStep method, this stored state will be used to make the next predictions:\n\nThere are a number of important differences here:\n\nHowever, if you want to start making predictions for a new (entirely separate) time series: it is necessary (and important) to manually clear the stored state, using the method. This will reset the internal state of all recurrent layers in the network.\n\nIf you need to store or set the internal state of the RNN for use in predictions, you can use the rnnGetPreviousState and rnnSetPreviousState methods, for each layer individually. This can be useful for example during serialization (network saving/loading), as the internal network state from the rnnTimeStep method is not saved by default, and must be saved and loaded separately. Note that these get/set state methods return and accept a map, keyed by the type of activation. For example, in the LSTM model, it is necessary to store both the output activations, and the memory cell state.\n\nSome other points of note:\n\nData import for RNNs is complicated by the fact that we have multiple different types of data we could want to use for RNNs: one-to-many, many-to-one, variable length time series, etc. This section will describe the currently implemented data import mechanisms for DL4J.\n\nThe methods described here utilize the SequenceRecordReaderDataSetIterator class, in conjunction with the CSVSequenceRecordReader class from DataVec. This approach currently allows you to load delimited (tab, comma, etc) data from files, where each time series is in a separate file. This method also supports:\n\nNote that in all cases, each line in the data files represents one time step.\n\nSuppose we have 10 time series in our training data, represented by 20 files: 10 files for the input of each time series, and 10 files for the output/labels. For now, assume these 20 files all contain the same number of time steps (i.e., same number of rows).\n\nTo use the SequenceRecordReaderDataSetIterator and CSVSequenceRecordReader approaches, we first create two CSVSequenceRecordReader objects, one for input and one for labels:\n\nThis particular constructor takes the number of lines to skip (1 row skipped here), and the delimiter (comma character used here).\n\nSecond, we need to initialize these two readers, by telling them where to get the data from. We do this with an InputSplit object. Suppose that our time series are numbered, with file names \u201cmyInput_0.csv\u201d, \u201cmyInput_1.csv\u201d, \u2026, \u201cmyLabels_0.csv\u201d, etc. One approach is to use the NumberedFileInputSplit:\n\nIn this particular approach, the \u201c%d\u201d is replaced by the corresponding number, and the numbers 0 to 9 (both inclusive) are used.\n\nFinally, we can create our SequenceRecordReaderdataSetIterator:\n\nThis DataSetIterator can then be passed to MultiLayerNetwork.fit() to train the network.\n\nThe miniBatchSize argument specifies the number of examples (time series) in each minibatch. For example, with 10 files total, miniBatchSize of 5 would give us two data sets with 2 minibatches (DataSet objects) with 5 time series in each.\n\nFollowing on from the last example, suppose that instead of a separate files for our input data and labels, we have both in the same file. However, each time series is still in a separate file.\n\nAs of DL4J 0.4-rc3.8, this approach has the restriction of a single column for the output (either a class index, or a single real-valued regression output)\n\nIn this case, we create and initialize a single reader. Again, we are skipping one header row, and specifying the format as comma delimited, and assuming our data files are named \u201cmyData_0.csv\u201d, \u2026, \u201cmyData_9.csv\u201d:\n\nand are the same as the previous example. Here, specifies which column the labels are in. For example, if the labels are in the fifth column, use labelIndex = 4 (i.e., columns are indexed 0 to numColumns-1).\n\nFor regression on a single output value, we use:\n\nAgain, the numPossibleLabels argument is not used for regression.\n\nFollowing on from the previous two examples, suppose that for each example individually, the input and labels are of the same length, but these lengths differ between time series.\n\nWe can use the same approach (CSVSequenceRecordReader and SequenceRecordReaderDataSetIterator), though with a different constructor:\n\nThe argument here are the same as in the previous example, with the exception of the AlignmentMode.ALIGN_END addition. This alignment mode input tells the SequenceRecordReaderDataSetIterator to expect two things:\n\nNote that if the features and labels are always of the same length (as is the assumption in example 3), then the two alignment modes (AlignmentMode.ALIGN_END and AlignmentMode.ALIGN_START) will give identical outputs. The alignment mode option is explained in the next section.\n\nAlso note: that variable length time series always start at time zero in the data arrays: padding, if required, will be added after the time series has ended.\n\nUnlike examples 1 and 2 above, the DataSet objects produced by the above variableLengthIter instance will also include input and masking arrays, as described earlier in this document.\n\nWe can also use the AlignmentMode functionality in example 3 to implement a many-to-one RNN sequence classifier. Here, let us assume:\n\nIn fact, the same approach as in example 3 can do this:\n\nAlignment modes are relatively straightforward. They specify whether to pad the start or the end of the shorter time series. The diagram below shows how this works, along with the masking arrays (as discussed earlier in this document):\n\nThe one-to-many case (similar to the last case above, but with only one input) is done by using AlignmentMode.ALIGN_START.\n\nNote that in the case of training data that contains time series of different lengths, the labels and inputs will be aligned for each example individually, and then the shorter time series will be padded as required:\n\nIn some cases, you will have to do something that doesn\u2019t fit into a typical data import scenario. One option for this scenario is to implement a custom DataSetIterator. DataSetIterator is merely an interface for iterating over DataSet objects - objects that encapsulate the input and target INDArrays, plus (optionally) the input and labels mask arrays.\n\nNote however that this approach is quite low level: implementing a DataSetIterator requires you to manually create the required INDArrays for the input and the labels, as well as (if required) the input and labels mask arrays. However, this approach gives you a great degree of flexibility over exactly how data is loaded.\n\nFor example of this approach in practice, see the the iterator for the character example and for the Word2Vec movie review sentiment example.\n\nNote: When creating a custom DataSetIterator, it is important that your data arrays - the input features, the labels, and any mask arrays - are created in \u2018f\u2019 (fortran) order. See the ND4J user guide for details on array orders. In practice, this means using the Nd4j.create methods that allow you to specify the array order: . Though \u2018c\u2019 order arrays will also work, performance will be reduced due to the need to copy the arrays to \u2018f\u2019 order first, for certain operations.\n\nDL4J currently has the following recurrent neural network examples:", 
        "title": "Using Recurrent Neural Networks in DL4J - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/model-import-keras", 
        "text": "The module provides routines for importing neural network models originally configured and trained using Keras, a popular Python deep learning library that provides abstraction layers on top of TensorFlow, Theano and CNTK backends. You can learn more about saving Keras models on the Keras FAQ Page. Details about the Keras features matched in Deeplearning4j are through the link.\n\nOnce you have imported your model into DL4J, our full production stack is at your disposal. Please check here for a complete list of keras features supported through model import.\n\nBelow is a video tutorial demonstrating working code to load a Keras model into Deeplearning4j and validating the working network. Instructor Tom Hanlon provides an overview of a simple classifier over Iris data built in Keras with a Theano backend, and exported and loaded into Deeplearning4j:\n\nIf you have trouble viewing the video, please click here to view it on YouTube.\n\nEdit your pom.xml adding the following dependency\n\nUsing the Keras Model Import feature you have the following options. Note that Keras has two types of networks, and functional . Keras model is equivalent to DeepLearning4J\u2019s . Keras functional is equivalent to DeepLearning4J\u2019s .\n\nTo use this, you would save the model in Keras to a JSON file, the DeepLearning4J options available are.\n\nTo only load the model architecture or configuration, DL4J supports the following two methods.\n\nIn this case you would save both the JSON config and the weights from the trained model in Keras. The weights are saved in an H5 formatted file. In Keras you can save the weights and the model configuration into a single H5 file, or you can save the config in a separate file.\n\nThe network would be ready to use for inference by passing it input data, formatted, transformed, and normalized in the same manner that the original data was and calling network.output.\n\nIf you want to import a pre-trained model only for inference, then you should set . Unsupported training-only configurations generate warnings but model import will proceed.\n\nIf you want to import a model for training and want to ensure the resulting model matches a trained Keras model as closely as possible, then you should set enforceTrainingConfig=true. In that case, unsupported training-only configurations will throw an and stop model import.\n\nAn message indicates that you are attempting to import a Keras model configuration that is not currently supported in Deeplearning4j (either because model import does not cover it, or DL4J does not implement the model, layer, or feature).\n\nOnce you have imported your model we recommend our own modelserializer class for further saving and reloading of your model.\n\nYou can inquire further by visiting the DL4J gitter channel. You might consider filing a feature request via Github so that this missing functionality can be placed on the DL4J development roadmap or even sending us a pull request with the necessary changes!\n\nCheck back for frequent updates to both the model import module and to this page!\n\nVGG16 and other pre-trained models are widely used for demonstration purposes and for retraining for a specific use case. We are proud to announce support for VGG16 import along with some helper functions to properly format and normalize data for ingest, and helper functions to convert the numeric output to labelled text classes.\n\nIn addition to being able to import pre-trained Keras models, DeepLearning4j will actively add models to our own model zoo.\n\nKeras is a layer of abstraction that sits atop Python libraries like Theano, Tensorflow CNTK, providing an easier to use interface for deep learning.\n\nTo define a layer in a framework like Theano, you have to precisely define the weights, biases, activation functions and how your input data will be transformed into outputs. Moreover, you need to deal with backpropagation and updating those weights and biases. Keras wraps all that. It gives you prefab layers that encompass those calculations and updates.\n\nWith Keras, the only thing you define is the shape of the input, the shape of the output, and how you want to calculate the loss. Keras ensures that all the layers are the right size, and that the error gets backpropagated properly.\n\nMore information is also available here.", 
        "title": "Deeplearning4j Keras Model Import - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/datavec", 
        "text": "DataVec solves one of the most important obstacles to effective machine or deep learning: getting data into a format that neural nets can understand. Nets understand vectors. Vectorization is the first problem many data scientists will have to solve to start training their algorithms on data. Datavec should be used for 99% of your data transformations, if you are not sure if this applies to you, please consult the gitter. Datavec supports most data formats you could want out of the box, but you may also implement your own custom record reader as well.\n\nIf your data is in CSV (Comma Seperated Values) format stored in flat files that must be converted to numeric and ingested, or your data is a directory structure of labelled images then DataVec is the tool to help you organize that data for use in DeepLearning4J.\n\nPlease read this entire page, particularly the section Reading Records below, before working with DataVec.\n\nThis video describes the conversion of image data to a vector.\n\nIf data is numeric and appropriately formatted then CSVRecordReader may be satisfactory. If however your data has non-numeric fields such as strings representing boolean (T/F) or strings for labels then a Schema Transformation will be required. DataVec uses apache Spark to perform transform operations. *note you do not need to know the internals of Spark to be succesful with DataVec Transform\n\nA video tutorial of a simple DataVec transform along with code is available below.\n\nThe following code shows how to work with one example, raw images, transforming them into a format that will work well with DL4J and ND4J:\n\nThe RecordReader is a class in DataVec that helps convert the byte-oriented input into data that\u2019s oriented toward a record; i.e. a collection of elements that are fixed in number and indexed with a unique ID. Converting data to records is the process of vectorization. The record itself is a vector, each element of which is a feature.\n\nThe ImageRecordReader is a subclass of the RecordReader and is built to automatically take in 28 x 28 pixel images. Thus, LFW images are scaled to 28 pixels x 28 pixels. You can change dimensions to match your custom images by changing the parameters fed to the ImageRecordReader, as long as you make sure to adjust the hyperparameter, which will be equal to the product of image height x image width.\n\nOther parameters shown above include , which instructs the reader to append a label to the record, and , which is the array of supervised values (e.g. targets) used to validate neural net model results. Here are all the RecordReader extensions that come pre-built with DataVec (you can find them by right-clicking on in IntelliJ, clicking in the drop-down menu, and selection ):\n\nThe DataSetIterator is a Deeplearning4J class that traverses the elements of a list. Iterators pass through the data list, accesses each item sequentially, keeps track of how far it has progressed by pointing to its current element, and modifies itself to point to the next element with each new step in the traversal.\n\nThe DataSetIterator iterates through input datasets, fetching one or more new examples with each iteration, and loading those examples into a DataSet object that neural nets can work with. Note that ImageRecordReader produces image data with 4 dimensions that matches DL4J\u2019s expected activations layout. Thus, each 28x28 RGB image is represented as a 4d array, with dimensions [minibatch, channels, height, width] = [1, 3, 28, 28]. Note that the constructor line above also specifies the number of labels possible. Note also that ImageRecordReader does not normalize the image data, thus each pixel/channel value will be in the range 0 to 255 (and generally should be normalized separately - for example using ND4J\u2019s ImagePreProcessingScaler or another normalizer.\n\ncan take as parameters the specific recordReader you want (for images, sound, etc.) and the batch size. For supervised learning, it will also take a label index and the number of possible labels that can be applied to the input (for LFW, the number of labels is 5,749).\n\nFor a walkthrough of the other steps associated with moving data from DataVec to Deeplearning4j, you can read about how to build a customized image data pipeline here.\n\nRuns as both a local serial process and a MapReduce (MR engine on the roadmap) scale-out process with no code changes.", 
        "title": "DataVec - A Vectorization and ETL Library - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/updater", 
        "text": "This page and the explanations that follow assume that readers know how Stochastic Gradient Descent works.\n\nThe main difference among the updaters described below is how they treat the learning rate.\n\n(weights) is changed according to the gradient of the loss with respect to each theta.\n\nis the learning rate. If alpha is very small, convergence on an error minimum will be slow. If it is very large, the model will diverge away from the error minimum, and learning will cease.\n\nNow, the gradient of the loss (L) changes quickly after each iteration due to variance among training examples. Look at the convergence path below. The updater takes small steps, but those steps zig-zag back and forth on their way to an error minimum.\n\nTo stop the zig-zagging, we use momentum. Momentum applies its knowledge from previous steps to where the updater should go. To represent it, we use a new hyperparameter , or \u201cmu\u201d.\n\nWe\u2019ll use the concept of momentum again later. (Don\u2019t confuse it with moment, of which more below.)\n\nThe image above represents SGD using momentum.\n\nAdagrad scales alpha for each parameter according to the history of gradients (previous steps) for that parameter. That\u2019s basically done by dividing the current gradient in the update rule by the sum of previous gradients. As a result, when the gradient is very large, alpha is reduced, and vice-versa.\n\nThe only difference between RMSProp and Adagrad is that the term is calculated by exponentially decaying the average and not the sum of gradients.\n\nHere is called the second order moment of . Additionally, a first-order moment can also be introduced.\n\nAdding momentum, as in the first case\u2026\n\n\u2026and finally collecting a new as we did in the first example.\n\nAdaDelta also uses an exponentially decaying average of , which was our second moment of gradient. But without using the alpha we typically use as learning rate, it introduces , which is the second moment of .\n\nADAM uses both first-order moment mt and second-order moment , but they both decay over time. Step size is approximately . Step size will decrease as we approach the error minimum.", 
        "title": "Deeplearning4j Updaters Explained - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/welldressed-recommendation-engine", 
        "text": "I have a background in theater and IT.\n\nFor a few years, I ran an art-outsourcing studio aimed at the entertainment industry that created concept art and visual assets for games and movies. I specialized in designing characters. I\u2019ve always been a little bit of a pattern-focused kind of person, so my design work tended to be grounded in design harmony patterns that I studied and described into algorithmic and teachable concepts.\n\nBy way of experiment, I translated the algorithm to a program that could suggest color harmonies in sync with the weather. I added clothes to that program, which created Well Dressed.\n\nWell Dressed is an app that suggests clothes to wear or buy based on how you look, as well as on the weather, occasion and your budget. It debuted at Web Summit 2015 in Ireland, and it\u2019s a one-man operation (for now!). As a side-note, even though I\u2019m good with pattern-based thinking, I do not have an academic background. I lack patience and feel the need to create, rather than to completely understand what I\u2019m doing.\n\nTo build Well Dressed, I needed an many categories for different garment types, as I\u2019m dealing with style advice.\n\nHowever, data feeds from merchants offer few categorization options, and on top of that, every data feed is very different from the other. So I needed a solution that would let me analyze the information I could get with the data feeds, and somehow fit it into my database.\n\nMy first solution used keywords with a complex rule-and weight-system. It kind of worked, but it didn\u2019t work well enough. Accuracy was around 65%, and every day I had to recheck every new garment to make sure it was correct, costing me many hours I might have spent on marketing or product development.\n\nThe initial data comes from data feeds from stores worldwide.\n\nI decided to focus on Title, Description and Category/Keyword fields. Title and Description tend to give valuable, specific hints on what a garment actually is. Category works as a broad identification.\n\nI do not use image data for garment identification, which seems to counter the normal way to approach categorization of clothes. That\u2019s because fashion designers dislike repeating patterns yearly, and they are constantly looking for ways to visually blend different garment types (in hope of starting a trend): Casual shirts that look like hoodies, jogging pants that look like jeans, leather jackets that look like pea coats, etc.\n\nThe copy of the text tends to be the only true identifier of a garment type. However, I do use the images to identify a style and a design using Opencv.\n\nAll my data is already organized by gender and age before Deeplearning4j sees it.\n\nMy first step is to remove the brand name from the data. Brands occasionally have names that hold garment words, for example: Armani Jeans. Or they can be so present in the dataset, like Levi\u2019s, that the net will only see jeans if they are from Levi\u2019s\n\nEvery word is lowercase. I\u2019m not sure if this has an impact, but it feels sensible. I also remove all numbers and punctuation to make sure I\u2019m only dealing with words.\n\nI also remove stop words: both common stop words, and words that are natural to the industry like Mens, fashion, style, clothes, colors, etc.\n\nThe data pre-vectorisation looks like this:\n\nI use word2vec to create vectors. A word needs to show at least 10 times, and I get 40 vectors for each wordset. I then proceed to sum all vectors per title, description and category, resulting in 120 vectors. In theory, this seemed like a bad idea. I was opposed to this approach because I expected the resulting vectors to be too spread out to be of any use. In practice, however, it works very well.\n\nI need to do some unusual things with the data pipeline. The database has a lot of jeans and t-shirts, but very few tuxedoes and cummerbunds \u2013 which is to be expected, considering the different market sizes for each item. But to train properly, all the data still needs to be distributed evenly.\n\nThere are 84 garment types in the database. My approach is that every 84 entries in the dataset must have one random garment in it that fits the type. Within those 84 entries, I shuffle the order. This will create an evenly distributed dataset, but at the risk of very rare garments like cummerbunds becoming overfitted. It turned out to be a purely theoretical concern, though, because in practice it works great.\n\nMy full dataset contains 84,000 entries. I simply repeat the 84-random approach a thousand times to build the full dataset.\n\nAfter the set is done, I normalize it with a scalar. One problem here is that the scalar needs to be remade every time, because every training session the dataset is different due to the random factor.\n\nI decided to train many small specific nets rather than creating a big net that can classify any data.\n\nSo I train a net for each store. Since every store has its own way of creating their data feed, each store has its own patterns that are very easy to train for. Because the variation in the data is so small, I can create a highly accurate net quite fast. Other than store-oriented nets, I also create a net for each language; its accuracy is not as good as that of the store nets, as to be expected. But the purpose of the language-oriented net is different.\n\n\u2022 I use the store nets to classify new garment data. After classification, the garment can immediately be published to the app. \u2022 I use the language net to classify data in new stores. Then I can quickly create a good dataset for the store to use later in the creation of a store-specific net. Since I need to check every garment anyway, I do not mind a lower accuracy. And it speeds up the overall process immensely.\n\nA few things to point out:\n\nI use a variable label and batch size. Different stores tend to have different categories. But since the difference between the stores is rather small, it works well with one net design.\n\nA minibatch is 0.5% of all the records. The net takes about 200 epochs to train and produce an accurate model.\n\nPreviously, I used 9 vectors as input, but that didn\u2019t produce good results. When I scaled it up to 120 vectors, and when the number of nodes were between the number of features and labels, the results improved a lot.\n\nI still have some fine-tuning to do, but the results are quite good at the moment.", 
        "title": "The WellDressed Recommendation Engine - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/neuralnet-overview", 
        "text": "Neural networks are a set of algorithms, modeled loosely after the human brain, that are designed to recognize patterns. They interpret sensory data through a kind of machine perception, labeling or clustering raw input. The patterns they recognize are numerical, contained in vectors, into which all real-world data, be it images, sound, text or time series, must be translated.\n\nNeural networks help us cluster and classify. You can think of them as a clustering and classification layer on top of the data you store and manage. They help to group unlabeled data according to similarities among the example inputs, and they classify data when they have a labeled dataset to train on. (Neural networks can also extract features that are fed to other algorithms for clustering and classification; so you can think of deep neural networks as components of larger machine-learning applications involving algorithms for reinforcement learning, classification and regression.)\n\nWhat kind of problems does deep learning solve, and more importantly, can it solve yours? To know the answer, you need to ask yourself a few questions: What outcomes do I care about? Those outcomes are labels that could be applied to data: for example, or in an email filter, or in fraud detection, or in customer relationship management.\n\nThen ask: Do I have the data to accompany those labels? That is, can I find labeled data, or can I create a labeled dataset (with a service like AWS Mechanical Turk or Figure Eight or Mighty.ai) where spam has been labeled as spam, in order to teach an algorithm the correlation between labels and inputs?\n\nDeep learning maps inputs to outputs. It finds correlations. It is known as a \u201cuniversal approximator\u201d, because it can learn to approximate an unknown function between any input and any output , assuming they are related at all (by correlation or causation, for example). In the process of learning, a neural network finds the right , or the correct manner of transforming into , whether that be or . Here are a few examples of what deep learning can do.\n\nAll classification tasks depend upon labeled datasets; that is, humans must transfer their knowledge to the dataset in order for a neural to learn the correlation between labels and data. This is known as supervised learning.\n\nAny labels that humans can generate, any outcomes you care about and which correlate to data, can be used to train a neural network.\n\nClustering or grouping is the detection of similarities. Deep learning does not require labels to detect similarities. Learning without labels is called unsupervised learning. Unlabeled data is the majority of data in the world. One law of machine learning is: the more data an algorithm can train on, the more accurate it will be. Therefore, unsupervised learning has the potential to produce highly accurate models.\n\nWith classification, deep learning is able to establish correlations between, say, pixels in an image and the name of a person. You might call this a static prediction. By the same token, exposed to enough of the right data, deep learning is able to establish correlations between present events and future events. It can run regression between the past and the future. The future event is like the label in a sense. Deep learning doesn\u2019t necessarily care about time, or the fact that something hasn\u2019t happened yet. Given a time series, deep learning may read a string of number and predict the number most likely to occur next.\n\nThe better we can predict, the better we can prevent and pre-empt. As you can see, with neural networks, we\u2019re moving towards a world of fewer surprises. Not zero surprises, just marginally fewer. We\u2019re also moving toward a world of smarter agents that combine neural networks with other algorithms like reinforcement learning to attain goals.\n\nWith that brief overview of deep learning use cases, let\u2019s look at what neural nets are made of.\n\nDeep learning is the name we use for \u201cstacked neural networks\u201d; that is, networks composed of several layers.\n\nThe layers are made of nodes. A node is just a place where computation happens, loosely patterned on a neuron in the human brain, which fires when it encounters sufficient stimuli. A node combines input from the data with a set of coefficients, or weights, that either amplify or dampen that input, thereby assigning significance to inputs for the task the algorithm is trying to learn. (For example, which input is most helpful is classifying data without error?) These input-weight products are summed and the sum is passed through a node\u2019s so-called activation function, to determine whether and to what extent that signal progresses further through the network to affect the ultimate outcome, say, an act of classification.\n\nHere\u2019s a diagram of what one node might look like.\n\nA node layer is a row of those neuronlike switches that turn on or off as the input is fed through the net. Each layer\u2019s output is simultaneously the subsequent layer\u2019s input, starting from an initial input layer receiving your data.\n\nPairing adjustable weights with input features is how we assign significance to those features with regard to how the network classifies and clusters input.\n\nDeep-learning networks are distinguished from the more commonplace single-hidden-layer neural networks by their depth; that is, the number of node layers through which data passes in a multistep process of pattern recognition.\n\nEarlier versions of neural networks such as the first perceptrons were shallow, composed of one input and one output layer, and at most one hidden layer in between. More than three layers (including input and output) qualifies as \u201cdeep\u201d learning. So deep is a strictly defined, technical term that means more than one hidden layer.\n\nIn deep-learning networks, each layer of nodes trains on a distinct set of features based on the previous layer\u2019s output. The further you advance into the neural net, the more complex the features your nodes can recognize, since they aggregate and recombine features from the previous layer.\n\nThis is known as feature hierarchy, and it is a hierarchy of increasing complexity and abstraction. It makes deep-learning networks capable of handling very large, high-dimensional data sets with billions of parameters that pass through nonlinear functions.\n\nAbove all, these nets are capable of discovering latent structures within unlabeled, unstructured data, which is the vast majority of data in the world. Another word for unstructured data is raw media; i.e. pictures, texts, video and audio recordings. Therefore, one of the problems deep learning solves best is in processing and clustering the world\u2019s raw, unlabeled media, discerning similarities and anomalies in data that no human has organized in a relational database or ever put a name to.\n\nFor example, deep learning can take a million images, and cluster them according to their similarities: cats in one corner, ice breakers in another, and in a third all the photos of your grandmother. This is the basis of so-called smart photo albums.\n\nNow apply that same idea to other data types: Deep learning might cluster raw text such as emails or news articles. Emails full of angry complaints might cluster in one corner of the vector space, while satisfied customers, or spambot messages, might cluster in others. This is the basis of various messaging filters, and can be used in customer-relationship management (CRM). The same applies to voice messages. With time series, data might cluster around normal/healthy behavior and anomalous/dangerous behavior. If the time series data is being generated by a smart phone, it will provide insight into users\u2019 health and habits; if it is being generated by an autopart, it might be used to prevent catastrophic breakdowns.\n\nDeep-learning networks perform automatic feature extraction without human intervention, unlike most traditional machine-learning algorithms. Given that feature extraction is a task that can take teams of data scientists years to accomplish, deep learning is a way to circumvent the chokepoint of limited experts. It augments the powers of small data science teams, which by their nature do not scale.\n\nWhen training on unlabeled data, each node layer in a deep network learns features automatically by repeatedly trying to reconstruct the input from which it draws its samples, attempting to minimize the difference between the network\u2019s guesses and the probability distribution of the input data itself. Restricted Boltzmann machines, for examples, create so-called reconstructions in this manner.\n\nIn the process, these networks learn to recognize correlations between certain relevant features and optimal results \u2013 they draw connections between feature signals and what those features represent, whether it be a full reconstruction, or with labeled data.\n\nA deep-learning network trained on labeled data can then be applied to unstructured data, giving it access to much more input than machine-learning nets. This is a recipe for higher performance: the more data a net can train on, the more accurate it is likely to be. (Bad algorithms trained on lots of data can outperform good algorithms trained on very little.) Deep learning\u2019s ability to process and learn from huge quantities of unlabeled data give it a distinct advantage over previous algorithms.\n\nDeep-learning networks end in an output layer: a logistic, or softmax, classifier that assigns a likelihood to a particular outcome or label. We call that predictive, but it is predictive in a broad sense. Given raw data in the form of an image, a deep-learning network may decide, for example, that the input data is 90 percent likely to represent a person.\n\nOur goal in using a neural net is to arrive at the point of least error as fast as possible. We are running a race, and the race is around a track, so we pass the same points repeatedly in a loop. The starting line for the race is the state in which our weights are initialized, and the finish line is the state of those parameters when they are capable of producing accurate classifications and predictions.\n\nThe race itself involves many steps, and each of those steps resembles the steps before and after. Just like a runner, we will engage in a repetitive act over and over to arrive at the finish. Each step for a neural network involves a guess, an error measurement and a slight update in its weights, an incremental adjustment to the coefficients.\n\nA collection of weights, whether they are in their start or end state, is also called a model, because it is an attempt to model data\u2019s relationship to ground-truth labels, to grasp the data\u2019s structure. Models normally start out bad and end up less bad, changing over time as the neural network updates its parameters.\n\nThis is because a neural network is born in ignorance. It does not know which weights and biases will translate the input best to make the correct guesses. It has to start out with a guess, and then try to make better guesses sequentially as it learns from its mistakes. (You can think of a neural network as a miniature enactment of the scientific method, testing hypotheses and trying again \u2013 only it is the scientific method with a blindfold on.)\n\nHere is a simple explanation of what happens during learning with a feedforward neural network, the simplest architecture to explain.\n\nInput enters the network. The coefficients, or weights, map that input to a set of guesses the network makes at the end.\n\nWeighted input results in a guess about what that input is. The neural then takes its guess and compares it to a ground-truth about the data, effectively asking an expert \u201cDid I get this right?\u201d\n\nThe difference between the network\u2019s guess and the ground truth is its error. The network measures that error, and walks the error back over its model, adjusting weights to the extent that they contributed to the error.\n\nThe three pseudo-mathematical formulas above account for the three key functions of neural networks: scoring input, calculating loss and applying an update to the model \u2013 to begin the three-step process over again. A neural network is a corrective feedback loop, rewarding weights that support its correct guesses, and punishing weights that lead it to err.\n\nLet\u2019s linger on the first step above.\n\nDespite their biologically inspired name, artificial neural networks are nothing more than math and code, like any other machine-learning algorithm. In fact, anyone who understands linear regression, one of first methods you learn in statistics, can understand how a neural net works. In its simplest form, linear regression is expressed as\n\nwhere is the estimated output, X is the input, b is the slope and a is the intercept of a line on the vertical axis of a two-dimensional graph. (To make this more concrete: X could be radiation exposure and Y could be the cancer risk; X could be daily pushups and Y could be the total weight you can benchpress; X the amount of fertilizer and Y the size of the crop.) You can imagine that every time you add a unit to X, the dependent variable Y increases proportionally, no matter how far along you are on the X axis. That simple relation between two variables moving up or down together is a starting point.\n\nThe next step is to imagine multiple linear regression, where you have many input variables producing an output variable. It\u2019s typically expressed like this:\n\nNow, that form of multiple linear regression is happening at every node of a neural network. For each node of a single layer, input from each node of the previous layer is recombined with input from every other node. That is, the inputs are mixed in different proportions, according to their coefficients, which are different leading into each node of the subsequent layer. In this way, a net tests which combination of input is significant as it tries to reduce error.\n\nOnce you sum your node inputs to arrive at , it\u2019s passed through a non-linear function. Here\u2019s why: If every node merely performed multiple linear regression, would increase linearly and without limit as the X\u2019s increase, but that doesn\u2019t suit our purposes.\n\nWhat we are trying to build at each node is a switch (like a neuron\u2026) that turns on and off, depending on whether or not it should let the signal of the input pass through to affect the ultimate decisions of the network.\n\nWhen you have a switch, you have a classification problem. Does the input\u2019s signal indicate the node should classify it as enough, or not_enough, on or off? A binary decision can be expressed by 1 and 0, and logistic regression is a non-linear function that squashes input to translate it to a space between 0 and 1.\n\nThe nonlinear transforms at each node are usually s-shaped functions similar to logistic regression. They go by the names of sigmoid (the Greek word for \u201cS\u201d), tanh, hard tanh, etc., and they shaping the output of each node. The output of all nodes, each squashed into an s-shaped space between 0 and 1, is then passed as input to the next layer in a feed forward neural network, and so on until the signal reaches the final layer of the net, where decisions are made.\n\nThe name for one commonly used optimization function that adjusts weights according to the error they caused is called \u201cgradient descent.\u201d\n\nGradient is another word for slope, and slope, in its typical form on an x-y graph, represents how two variables relate to each other: rise over run, the change in money over the change in time, etc. In this particular case, the slope we care about describes the relationship between the network\u2019s error and a single weight; i.e. that is, how does the error vary as the weight is adjusted.\n\nTo put a finer point on it, which weight will produce the least error? Which one correctly represents the signals contained in the input data, and translates them to a correct classification? Which one can hear \u201cnose\u201d in an input image, and know that should be labeled as a face and not a frying pan?\n\nAs a neural network learns, it slowly adjusts many weights so that they can map signal to meaning correctly. The relationship between network Error and each of those weights is a derivative, dE/dw, that measures the degree to which a slight change in a weight causes a slight change in the error.\n\nEach weight is just one factor in a deep network that involves many transforms; the signal of the weight passes through activations and sums over several layers, so we use the chain rule of calculus to march back through the networks activations and outputs and finally arrive at the weight in question, and its relationship to overall error.\n\nThe chain rule in calculus states that\n\nIn a feedforward network, the relationship between the net\u2019s error and a single weight will look something like this:\n\nThat is, given two variables, Error and weight, that are mediated by a third variable, activation, through which the weight is passed, you can calculate how a change in weight affects a change in Error by first calculating how a change in activation affects a change in Error, and how a change in weight affects a change in activation.\n\nThe essence of learning in deep learning is nothing more than that: adjusting a model\u2019s weights in response to the error it produces, until you can\u2019t reduce the error any more.\n\nThe JavaDoc for updaters is part of the DeepLearning4J JavaDoc and is available here.\n\nThe activation function determines what output a node will generate base upon its input. Sigmoid activation functions had been very populare, ReLU is currently very popular. In DeepLearnging4J the activation function is set at the layer level and applies to all neurons in that layer.\n\nOn a deep neural network of many layers, the final layer has a particular role. When dealing with labeled input, the output layer classifies each example, applying the most likely label. Each node on the output layer represents one label, and that node turns on or off according to the strength of the signal it receives from the previous layer\u2019s input and parameters.\n\nEach output node produces two possible outcomes, the binary output values 0 or 1, because an input variable either deserves a label or it does not. After all, there is no such thing as a little pregnant.\n\nWhile neural networks working with labeled data produce binary output, the input they receive is often continuous. That is, the signals that the network receives as input will span a range of values and include any number of metrics, depending on the problem it seeks to solve.\n\nFor example, a recommendation engine has to make a binary decision about whether to serve an ad or not. But the input it bases its decision on could include how much a customer has spent on Amazon in the last week, or how often that customer visits the site.\n\nSo the output layer has to condense signals such as $67.59 spent on diapers, and 15 visits to a website, into a range between 0 and 1; i.e. a probability that a given input should be labeled or not.\n\nThe mechanism we use to convert continuous signals into binary output is called logistic regression. The name is unfortunate, since logistic regression is used for classification rather than regression in the linear sense that most people are familiar with. It calculates the probability that a set of inputs match the label.\n\nFor continuous inputs to be expressed as probabilities, they must output positive results, since there is no such thing as a negative probability. That\u2019s why you see input as the exponent of e in the denominator \u2013 because exponents force our results to be greater than zero. Now consider the relationship of e\u2019s exponent to the fraction 1/1. One, as we know, is the ceiling of a probability, beyond which our results can\u2019t go without being absurd. (We\u2019re 120% sure of that.)\n\nAs the input x that triggers a label grows, the expression e to the x shrinks toward zero, leaving us with the fraction 1/1, or 100%, which means we approach (without ever quite reaching) absolute certainty that the label applies. Input that correlates negatively with your output will have its value flipped by the negative sign on e\u2019s exponent, and as that negative signal grows, the quantity e to the x becomes larger, pushing the entire fraction ever closer to zero.\n\nNow imagine that, rather than having x as the exponent, you have the sum of the products of all the weights and their corresponding inputs \u2013 the total signal passing through your net. That\u2019s what you\u2019re feeding into the logistic regression layer at the output layer of a neural network classifier.\n\nWith this layer, we can set a decision threshold above which an example is labeled 1, and below which it is not. You can set different thresholds as you prefer \u2013 a low threshold will increase the number of false positives, and a higher one will increase the number of false negatives \u2013 depending on which side you would like to err.\n\nThe Loss Function is applied when building your output Layer.\n\nThe JavaDoc for the Loss Function is part of ND4J javadoc and is available [here.] (https://nd4j.org/doc/org/nd4j/linalg/api/ops/LossFunction.html)\n\nIn some circles, neural networks are thought of as \u201cbrute force\u201d AI, because they start with a blank slate and hammer their way through to an accurate model. They are effective, but to some eyes inefficient in their approach to modeling, which can\u2019t make assumptions about functional dependencies between output and input.\n\nThat said, gradient descent is not recombining every weight with every other to find the best match \u2013 its method of pathfinding shrinks the relevant weight space, and therefore the number of updates and required computation, by many orders of magnitude.\n\nTo train complex neural networks on very large datasets, a deep learning cluster using multiple chips, distributed over both GPUs and CPUs, is necessary if one is to train the network in a reasonable amount of time. Software engineers training those nets may avail themselves of GPUs in the cloud, or choose to depend on proprietary racks. Deeplearning4j scales out equally well on both, using Spark as an access layer to orchestrate multiple host threads over many cores. For support, please contact Skymind.\n\nFor people just getting started with deep learning, the following tutorials and videos provide an easy entrance to the fundamental ideas of deep neural networks:", 
        "title": "Introduction to Deep Neural Networks (Deep Learning) - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/questions", 
        "text": "We can\u2019t answer these questions for you, because the responses will be specific to the problem you seek to solve. But we hope this will serve as a useful checklist to clarify how you initially approach your choice of algorithms and tools:\n\nJava developers using Deeplearning4j are welcome to join our Gitter live chat, where our community helps answer these questions case by case.", 
        "title": "Questions to Ask When Applying Deep Learning - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/deeplearningforbeginners.html", 
        "text": "Where you start depends on what you already know.\n\nThe prerequisites for really understanding deep learning are linear algebra, calculus and statistics, as well as programming and some machine learning. The prerequisites for applying it are just learning how to deploy a model.\n\nIn the case of Deeplearning4j, you should know Java well and be comfortable with tools like the IntelliJ IDE and the automated build tool Maven. Skymind\u2019s SKIL also includes a managed Conda environment for machine learning tools using Python.\n\nBelow you\u2019ll find a list of resources. The sections are roughly organized in the order they will be useful.\n\nThe math involved with deep learning is basically linear algebra, calculus and probility, and if you have studied those at the undergraduate level, you will be able to understand most of the ideas and notation in deep-learning papers. If haven\u2019t studied those in college, never fear. There are many free resources available (and some on this website).\n\nIf you do not know how to program yet, you can start with Java, but you might find other languages easier. Python and Ruby resources can convey the basic ideas in a faster feedback loop. \u201cLearn Python the Hard Way\u201d and \u201cLearn to Program (Ruby)\u201d are two great places to start.\n\nIf you want to jump into deep-learning from here without Java, we recommend Theano and the various Python frameworks built atop it, including Keras and Lasagne.\n\nOnce you have programming basics down, tackle Java, the world\u2019s most widely used programming language. Most large organizations in the world operate on huge Java code bases. (There will always be Java jobs.) The big data stack \u2013 Hadoop, Spark, Kafka, Lucene, Solr, Cassandra, Flink \u2013 have largely been written for Java\u2019s compute environment, the JVM.\n\nWith that under your belt, we recommend you approach Deeplearning4j through its examples.\n\nYou can also download a free version of the Skymind Intelligence Layer, which supports Python, Java and Scala machine-learning and data science tools. SKIL is a machine-learning backend that works on prem and in the cloud, and can ship with your software to provide a machine learning model server.\n\nMost of what we know about deep learning is contained in academic papers. We\u2019ve linked to a number of them here.\n\nWhile individual courses have limits on what they can teach, the Internet does not. Most math and programming questions can be answered by Googling and searching sites like Stackoverflow and Math Stackexchange.", 
        "title": "Deep Learning for Beginners - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/use_cases", 
        "text": "Deep learning excels at identifying patterns in unstructured data, which most people know as media such as images, sound, video and text. Below is a list of sample use cases we\u2019ve run across, paired with the sectors to which they pertain.\n\nTraditional machine learning has the advantage of feature introspection \u2013 that is, it knows why it is classifying an input in one way or another, which is important for analytics. But that very advantage is what excludes it from working with unlabeled and unstructured data, as well as attaining the record-breaking accuracy of the latest deep learning models. Feature engineering is one of the major chokepoints of traditional machine learning, since so few people are able to do it well and quickly enough to adapt to changing data.\n\nFor cases where feature introspection is necessary (e.g. the law requires that you justify a decision to, say, deny a loan due to predicted credit risk), we recommend using a deep net in an ensemble with machine-learning algorithms, allowing each one to vote and relying on each for its strength. Alternatively, one can perform various analyses on the results of deep nets to form hypotheses about their decision-making.\n\nOne use of deep-learning networks is named-entity recognition, which is a way to extract from unstructured, unlabeled data certain types of information like people, places, companies or things. That information can then be stored in a structured schema to build, say, a list of addresses or serve as a benchmark for an identity validation engine.\n\nWith the proper data transforms, a deep network is capable of understanding audio signals. This can be used to identify snippets of sound in larger audio files, and transcribe the spoken word as text.\n\nObject recognition is an algorithm\u2019s ability to identify arbitrary objects \u2013 such as balls, animals, or even faces \u2013 within larger images. This is typically used in engineering applications to identify shapes for modeling purposes. It\u2019s also used by social networks for photo tagging. Facebook\u2019s Deep Face is a good example of a deep-learning application in this realm.\n\nHere\u2019s an example of faces clustered by a neural net:\n\nAdvances in reality capture and reality computing are making virtual and real worlds converge. One application of deep learning to this newly available data is to recognize and label objects in 3D environments, and in real life.\n\nFrom there, it\u2019s a short step to simulated semantics, in which machines learn the nature and constraints of objects in the world, through their virtual representations, and then bring that understanding to the language they generate and ingest. We believe that is one of many futures in store for neural nets.", 
        "title": "Deep Learning Use Cases - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/accuracy", 
        "text": "Deep learning has knocked down one record after another on benchmark dataset after benchmark dataset since 2006. In many competitions, the only algorithm deep learning is up against is itself. Below are a few reports and resources indicating the power and persistent advance of deep neural nets:\n\nHere\u2019s a list of the competitions that Juergen Schmidhuber and his team have won with their deep recurrent nets:\n\nWe could go on, but we\u2019d start to sound like a broken record. :)", 
        "title": "Deep Learning's Accuracy - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/ai-machinelearning-deeplearning", 
        "text": "You can think of deep learning, machine learning and artificial intelligence as a set of Russian dolls nested within each other, beginning with the smallest and working out. Deep learning is a subset of machine learning, and machine learning is a subset of AI, which is an umbrella term for any computer program that does something smart. In other words, all machine learning is AI, but not all AI is machine learning, and so forth.\n\nJohn McCarthy, widely recognized as one of the godfathers of AI, defined it as \u201cthe science and engineering of making intelligent machines.\u201d\n\nHere are a few other definitions of artificial intelligence:\n\nThere are a lot of ways to simulate human intelligence, and some methods are more intelligent than others.\n\nAI can be a pile of if-then statements, or a complex statistical model mapping raw sensory data to symbolic categories. The if-then statements are simply rules explicitly programmed by a human hand. Taken together, these if-then statements are sometimes called rules engines, expert systems, knowledge graphs or symbolic AI. Collectively, these are known as Good, Old-Fashioned AI (AI). The intelligence they mimic could be that of an accountant with knowledge of the tax code, who takes information you feed it, runs the information through a set of static rules, and gives your the amount of taxes you owe as a result.\n\nUsually, when a computer program designed by AI researchers actually succeeds at something \u2013 like winning at chess \u2013 many people say it\u2019s \u201cnot really intelligent\u201d, because the algorithm\u2019s internals are well understood. A wag would say that true AI is whatever computers can\u2019t do yet.\n\nMachine learning is a subset of AI. That is, all machine learning counts as AI, but not all AI counts as machine learning. For example, symbolic logic \u2013 rules engines, expert systems and knowledge graphs \u2013 could all be described as AI, and none of them are machine learning.\n\nOne aspect that separates machine learning from the knowledge graphs and expert systems is its ability to modify itself when exposed to more data; i.e. machine learning is dynamic and does not require human intervention to make certain changes. That makes it less brittle, and less reliant on human experts.\n\nIn 1959, Arthur Samuel, one of the pioneers of machine learning, defined machine learning as a \u201cfield of study that gives computers the ability to learn without being explicitly programmed.\u201d That is, machine-learning programs have not been explicitly entered into a computer, like the if-then statements above. Machine-learning programs, in a sense, adjust themselves in response to the data they\u2019re exposed to.\n\nSamuel taught a computer program to play checkers. His goal was to teach it to play checkers better than himself, which is obviously not something he could program explicitly. He succeeded, and in 1962 his program beat the checkers champion of the state of Connecticut.\n\nThe \u201clearning\u201d part of machine learning means that ML algorithms attempt to optimize along a certain dimension; i.e. they usually try to minimize error or maximize the likelihood of their predictions being true. This has three names: an error function, a loss function, or an objective function, because the algorithm has an objective\u2026 When someone says they are working with a machine-learning algorithm, you can get to the gist of its value by asking: What\u2019s the objective function?\n\nHow does one minimize error? Well, one way is to build a framework that multiplies inputs in order to make guesses as to the inputs\u2019 nature. Different outputs/guesses are the product of the inputs and the algorithm. Usually, the initial guesses are quite wrong, and if you are lucky enough to have ground-truth labels pertaining to the input, you can measure how wrong your guesses are by contrasting them with the truth, and then use that error to modify your algorithm. That\u2019s what neural networks do. They keep on measuring the error and modifying their parameters until they can\u2019t achieve any less error.\n\nThey are, in short, an optimization algorithm. If you tune them right, they minimize their error by guessing and guessing and guessing again.\n\nDeep learning is a subset of machine learning. Usually, when people use the term deep learning, they are referring to deep artificial neural networks, and somewhat less frequently to deep reinforcement learning.\n\nDeep artificial neural networks are a set of algorithms that have set new records in accuracy for many important problems, such as image recognition, sound recognition, recommender systems, etc. For example, deep learning is part of DeepMind\u2019s well-known AlphaGo algorithm, which beat the former world champion Lee Sedol at Go in early 2016, and the current world champion Ke Jie in early 2017. A more complete explanation of neural works is here.\n\nDeep is a technical term. It refers to the number of layers in a neural network. A shallow network has one so-called hidden layer, and a deep network has more than one. Multiple hidden layers allow deep neural networks to learn features of the data in a so-called feature hierarchy, because simple features (e.g. two pixels) recombine from one layer to the next, to form more complex features (e.g. a line). Nets with many layers pass input data (features) through more mathematical operations than nets with few layers, and are therefore more computationally intensive to train. Computational intensivity is one of the hallmarks of deep learning, and it is one reason why GPUs are in demand to train deep-learning models.\n\nSo you could apply the same definition to deep learning that Arthur Samuel did to machine learning \u2013 a \u201cfield of study that gives computers the ability to learn without being explicitly programmed\u201d \u2013 while adding that it tends to result in higher accuracy, require more hardware or training time, and perform exceptionally well on machine perception tasks that involved unstructured data such as blobs of pixels or text.", 
        "title": "What's the Difference Between Artificial Intelligence (AI), Machine Learning and Deep Learning? - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/data-for-deep-learning.html", 
        "text": "The minimum requirements to successfully apply deep learning depends on the problem you\u2019re trying to solve. In contrast to static, benchmark datasets like MNIST and CIFAR-10, real-world data is messy, varied and evolving, and that is the data practical deep learning solutions must deal with.\n\nDeep learning can be applied to any data type. The data types you work with, and the data you gather, will depend on the problem you\u2019re trying to solve.\n\nDeep learning can solve almost any problem of machine perception, including classifying data , clustering it, or making predictions about it.\n\nDeep learning is best applied to unstructured data like images, video, sound or text. An image is just a blob of pixels, a message is just a blob of text. This data is not organized in a typical, relational database by rows and columns. That makes it more difficult to specify its features manually.\n\nCommon use cases for deep learning include sentiment analysis, classifying images, predictive analytics, recommendation systems, anomaly detection and more.\n\nIf you are not sure whether deep learning makes sense for your use case, please get in touch.\n\nFor deep learning to succeed, your data needs to have certain characteristics.\n\nThe data you use to train your neural net must be directly relevant to your problem; that is, it must resemble as much as possible the real-world data you hope to process. Neural networks are born as blank slates, and they only learn what you teach them. If you want them to solve a problem involving certain kinds of data, like CCTV video, then you have to train them on CCTV video, or something similar to it. The training data should resemble the real-world data that they will classify in production.\n\nIf a client wants to build a deep-learning solution that classifies data, then they need to have a labeled dataset. That is, someone needs to apply labels to the raw data: \u201cThis image is a flower, that image is a panda.\u201d With time and tuning, this training dataset can teach a neural network to classify new images it has not seen before.\n\nNeural networks eat vectors of data and spit out decisions about those vectors. All data needs to be vectorized, and the vectors should be the same length when they enter the neural net. To get vectors of the same length, it\u2019s helpful to have, say, images of the same size (the same height and width). So sometimes you need to resize the images. This is called data pre-processing. We provide a tool called DataVec for that.\n\nThe data needs to be stored in a place that\u2019s easy to work with. A local file system, or HDFS (the Hadoop file system), or an S3 bucket on AWS, for example. If the data is stored in many different databases that are unconnected, you will have to build data pipelines. Building data pipelines and performing preprocessing can account for at least half the time you spend building deep-learning solutions.\n\nThe minimums vary with the complexity of the problem, but 100,000 instances in total, across all categories, is a good place to start.\n\nIf you have labeled data (i.e. categories A, B, C and D), it\u2019s preferable to have an evenly balanced dataset with 25,000 instances of each label; that is, 25,000 instances of A, 25,000 instances of B and so forth.\n\nIf you\u2019re wondering whether a deep learning solution can be built for your organization, please go through this checklist or get in touch. These are the things you need to consider when deploying a deep learning solution to production.\n\nFor people just getting started with deep learning, the following tutorials and videos provide an easy entrance to the fundamental ideas of deep neural networks:", 
        "title": "The Data You Need For Deep Learning - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/multinetwork", 
        "text": "A multilayer network is a stacked representation of a single-layer neural network. The input layer is tacked onto the first-layer neural network and a feed-forward network. Each subsequent layer after the input layer uses the output of the previous layer as its input.\n\nA multilayer network will accept the same kinds of inputs as a single-layer network. The multilayer network parameters are also typically the same as their single-layer network counterparts.\n\nThe output layer for a multilayer network is typically a logistic regression classifier, which sorts results into zeros and ones. This is a discriminatory layer used for classification of input features based on the final hidden layer of the deep network.\n\nA multilayer network is composed of the following kinds of layers:\n\nBelow are the parameters what you need to think about when training a network.\n\nThe learning rate, or step rate, is the rate at which a function steps through the search space. The typical value of the learning rate is between 0.001 and 0.1. Smaller steps mean longer training times, but can lead to more precise results.\n\nMomentum is an additional factor in determining how fast an optimization algorithm converges on the optimum point.\n\nIf you want to speed up the training, increase the momentum. But you should know that higher speeds can lower a model\u2019s accuracy.\n\nTo dig deeper, momentum is a variable between zero and one that is applied as a factor to the derivative of the rate of change of the matrix. It affects the change rate of the weights over time.\n\nL2 is the lambda discussed in this equation.\n\nFor pretraining \u2013 i.e. learning the features via reconstruction at each layer \u2013 a layer is trained and then the output is piped to the next layer.\n\nFinally, the logistic regression output layer is trained, and then back propagation happens for each layer.", 
        "title": "Creating deep-learning networks - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/neuralnetworktable", 
        "text": "The following table attempts to show the neural nets most useful for different problems.\n\nStart with Neural Nets Overview for additional information on common structures and some code snipets:\n\nNatural Language Processing (Almost) from Scratch; Ronan Collobert, Jason Weston, Leon Bottou, Michael Karlen, Koray Kavukcuoglu and Pavel Kuksa; NEC Laboratories America\n\nDeep Belief Networks for phone recognition; Abdel-rahman Mohamed, George Dahl, and Geoffrey Hinton; Department of Computer Science, University of Toronto\n\nAn Analysis of Gaussian-Binary Restricted Boltzmann Machines for Natural Images; Nan Wang, Jan Melchior and Laurenz Wiskott; Institut fuer Neuroinformatik and International Graduate School of Neuroscience", 
        "title": "How to Choose a Neural Network - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/lstm", 
        "text": "The purpose of this post is to give students of neural networks an intuition about the functioning of recurrent neural networks and purpose and structure of a prominent RNN variation, LSTMs.\n\nRecurrent nets are a type of artificial neural network designed to recognize patterns in sequences of data, such as text, genomes, handwriting, the spoken word, or numerical times series data emanating from sensors, stock markets and government agencies.\n\nThey are arguably the most powerful and useful type of neural network, applicable even to images, which can be decomposed into a series of patches and treated as a sequence.\n\nSince recurrent networks possess a certain type of memory, and memory is also part of the human condition, we\u2019ll make repeated analogies to memory in the brain.1\n\nTo understand recurrent nets, first you have to understand the basics of feedforward nets. Both of these networks are named after the way they channel information through a series of mathematical operations performed at the nodes of the network. One feeds information straight through (never touching a given node twice), while the other cycles it through a loop, and the latter are called recurrent.\n\nIn the case of feedforward networks, input examples are fed to the network and transformed into an output; with supervised learning, the output would be a label, a name applied to the input. That is, they map raw data to categories, recognizing patterns that may signal, for example, that an input image should be labeled \u201ccat\u201d or \u201celephant.\u201d\n\nA feedforward network is trained on labeled images until it minimizes the error it makes when guessing their categories. With the trained set of parameters (or weights, collectively known as a model), the network sallies forth to categorize data it has never seen. A trained feedforward network can be exposed to any random collection of photographs, and the first photograph it is exposed to will not necessarily alter how it classifies the second. Seeing photograph of a cat will not lead the net to perceive an elephant next.\n\nThat is, a feedforward network has no notion of order in time, and the only input it considers is the current example it has been exposed to. Feedforward networks are amnesiacs regarding their recent past; they remember nostalgically only the formative moments of training.\n\nRecurrent networks, on the other hand, take as their input not just the current input example they see, but also what they have perceived previously in time. Here\u2019s a diagram of an early, simple recurrent net proposed by Elman, where the BTSXPE at the bottom of the drawing represents the input example in the current moment, and CONTEXT UNIT represents the output of the previous moment.\n\nThe decision a recurrent net reached at time step affects the decision it will reach one moment later at time step . So recurrent networks have two sources of input, the present and the recent past, which combine to determine how they respond to new data, much as we do in life.\n\nRecurrent networks are distinguished from feedforward networks by that feedback loop connected to their past decisions, ingesting their own outputs moment after moment as input. It is often said that recurrent networks have memory.2 Adding memory to neural networks has a purpose: There is information in the sequence itself, and recurrent nets use it to perform tasks that feedforward networks can\u2019t.\n\nThat sequential information is preserved in the recurrent network\u2019s hidden state, which manages to span many time steps as it cascades forward to affect the processing of each new example. It is finding correlations between events separated by many moments, and these correlations are called \u201clong-term dependencies\u201d, because an event downstream in time depends upon, and is a function of, one or more events that came before. One way to think about RNNs is this: they are a way to share weights over time.\n\nJust as human memory circulates invisibly within a body, affecting our behavior without revealing its full shape, information circulates in the hidden states of recurrent nets. The English language is full of words that describe the feedback loops of memory. When we say a person is haunted by their deeds, for example, we are simply talking about the consequences that past outputs wreak on present time. The French call this \u201cLe pass\u00e9 qui ne passe pas,\u201d or \u201cThe past that does not pass away.\u201d\n\nThe hidden state at time step t is . It is a function of the input at the same time step , modified by a weight matrix (like the one we used for feedforward nets) added to the hidden state of the previous time step multiplied by its own hidden-state-to-hidden-state matrix , otherwise known as a transition matrix and similar to a Markov chain. The weight matrices are filters that determine how much importance to accord to both the present input and the past hidden state. The error they generate will return via backpropagation and be used to adjust their weights until error can\u2019t go any lower.\n\nThe sum of the weight input and hidden state is squashed by the function \u2013 either a logistic sigmoid function or tanh, depending \u2013 which is a standard tool for condensing very large or very small values into a logistic space, as well as making gradients workable for backpropagation.\n\nBecause this feedback loop occurs at every time step in the series, each hidden state contains traces not only of the previous hidden state, but also of all those that preceded for as long as memory can persist.\n\nGiven a series of letters, a recurrent will use the first character to help determine its perception of the second character, such that an initial might lead it to infer that the next letter will be , while an initial might lead it to infer that the next letter will be .\n\nSince recurrent nets span time, they are probably best illustrated with animation (the first vertical line of nodes to appear can be thought of as a feedforward network, which becomes recurrent as it unfurls over time).\n\nIn the diagram above, each is an input example, is the weights that filter inputs, is the activation of the hidden layer (a combination of weighted input and the previous hidden state), and is the output of the hidden layer after it has been transformed, or squashed, using a rectified linear or sigmoid unit.\n\nRemember, the purpose of recurrent nets is to accurately classify sequential input. We rely on the backpropagation of error and gradient descent to do so.\n\nBackpropagation in feedforward networks moves backward from the final error through the outputs, weights and inputs of each hidden layer, assigning those weights responsibility for a portion of the error by calculating their partial derivatives \u2013 \u2202E/\u2202w, or the relationship between their rates of change. Those derivatives are then used by our learning rule, gradient descent, to adjust the weights up or down, whichever direction decreases error.\n\nRecurrent networks rely on an extension of backpropagation called backpropagation through time, or BPTT. Time, in this case, is simply expressed by a well-defined, ordered series of calculations linking one time step to the next, which is all backpropagation needs to work.\n\nNeural networks, whether they are recurrent or not, are simply nested composite functions like . Adding a time element only extends the series of functions for which we calculate derivatives with the chain rule.\n\nTruncated BPTT is an approximation of full BPTT that is preferred for long sequences, since full BPTT\u2019s forward/backward cost per parameter update becomes very high over many time steps. The downside is that the gradient can only flow back so far due to that truncation, so the network can\u2019t learn dependencies that are as long as in full BPTT.\n\nLike most neural networks, recurrent nets are old. By the early 1990s, the vanishing gradient problem emerged as a major obstacle to recurrent net performance.\n\nJust as a straight line expresses a change in x alongside a change in y, the gradient expresses the change in all weights with regard to the change in error. If we can\u2019t know the gradient, we can\u2019t adjust the weights in a direction that will decrease error, and our network ceases to learn.\n\nRecurrent nets seeking to establish connections between a final output and events many time steps before were hobbled, because it is very difficult to know how much importance to accord to remote inputs. (Like great-great-*-grandparents, they multiply quickly in number and their legacy is often obscure.)\n\nThis is partially because the information flowing through neural nets passes through many stages of multiplication.\n\nEveryone who has studied compound interest knows that any quantity multiplied frequently by an amount slightly greater than one can become immeasurably large (indeed, that simple mathematical truth underpins network effects and inevitable social inequalities). But its inverse, multiplying by a quantity less than one, is also true. Gamblers go bankrupt fast when they win just 97 cents on every dollar they put in the slots.\n\nBecause the layers and time steps of deep neural networks relate to each other through multiplication, derivatives are susceptible to vanishing or exploding.\n\nExploding gradients treat every weight as though it were the proverbial butterfly whose flapping wings cause a distant hurricane. Those weights\u2019 gradients become saturated on the high end; i.e. they are presumed to be too powerful. But exploding gradients can be solved relatively easily, because they can be truncated or squashed. Vanishing gradients can become too small for computers to work with or for networks to learn \u2013 a harder problem to solve.\n\nBelow you see the effects of applying a sigmoid function over and over again. The data is flattened until, for large stretches, it has no detectable slope. This is analogous to a gradient vanishing as it passes through many layers.\n\nIn the mid-90s, a variation of recurrent net with so-called Long Short-Term Memory units, or LSTMs, was proposed by the German researchers Sepp Hochreiter and Juergen Schmidhuber as a solution to the vanishing gradient problem.\n\nLSTMs help preserve the error that can be backpropagated through time and layers. By maintaining a more constant error, they allow recurrent nets to continue to learn over many time steps (over 1000), thereby opening a channel to link causes and effects remotely. This is one of the central challenges to machine learning and AI, since algorithms are frequently confronted by environments where reward signals are sparse and delayed, such as life itself. (Religious thinkers have tackled this same problem with ideas of karma or divine reward, theorizing invisible and distant consequences to our actions.)\n\nLSTMs contain information outside the normal flow of the recurrent network in a gated cell. Information can be stored in, written to, or read from a cell, much like data in a computer\u2019s memory. The cell makes decisions about what to store, and when to allow reads, writes and erasures, via gates that open and close. Unlike the digital storage on computers, however, these gates are analog, implemented with element-wise multiplication by sigmoids, which are all in the range of 0-1. Analog has the advantage over digital of being differentiable, and therefore suitable for backpropagation.\n\nThose gates act on the signals they receive, and similar to the neural network\u2019s nodes, they block or pass on information based on its strength and import, which they filter with their own sets of weights. Those weights, like the weights that modulate input and hidden states, are adjusted via the recurrent networks learning process. That is, the cells learn when to allow data to enter, leave or be deleted through the iterative process of making guesses, backpropagating error, and adjusting weights via gradient descent.\n\nThe diagram below illustrates how data flows through a memory cell and is controlled by its gates.\n\nThere are a lot of moving parts here, so if you are new to LSTMs, don\u2019t rush this diagram \u2013 contemplate it. After a few minutes, it will begin to reveal its secrets.\n\nStarting from the bottom, the triple arrows show where information flows into the cell at multiple points. That combination of present input and past cell state is fed not only to the cell itself, but also to each of its three gates, which will decide how the input will be handled.\n\nThe black dots are the gates themselves, which determine respectively whether to let new input in, erase the present cell state, and/or let that state impact the network\u2019s output at the present time step. is the current state of the memory cell, and is the current input to it. Remember that each gate can be open or shut, and they will recombine their open and shut states at each step. The cell can forget its state, or not; be written to, or not; and be read from, or not, at each time step, and those flows are represented here.\n\nThe large bold letters give us the result of each operation.\n\nHere\u2019s another diagram for good measure, comparing a simple recurrent network (left) to an LSTM cell (right). The blue lines can be ignored; the legend is helpful.\n\nIt\u2019s important to note that LSTMs\u2019 memory cells give different roles to addition and multiplication in the transformation of input. The central plus sign in both diagrams is essentially the secret of LSTMs. Stupidly simple as it may seem, this basic change helps them preserve a constant error when it must be backpropagated at depth. Instead of determining the subsequent cell state by multiplying its current state with new input, they add the two, and that quite literally makes the difference. (The forget gate still relies on multiplication, of course.)\n\nDifferent sets of weights filter the input for input, output and forgetting. The forget gate is represented as a linear identity function, because if the gate is open, the current state of the memory cell is simply multiplied by one, to propagate forward one more time step.\n\nFurthermore, while we\u2019re on the topic of simple hacks, including a bias of 1 to the forget gate of every LSTM cell is also shown to improve performance. (Sutskever, on the other hand, recommends a bias of 5.)\n\nYou may wonder why LSTMs have a forget gate when their purpose is to link distant occurrences to a final output. Well, sometimes it\u2019s good to forget. If you\u2019re analyzing a text corpus and come to the end of a document, for example, you may have no reason to believe that the next document has any relationship to it whatsoever, and therefore the memory cell should be set to zero before the net ingests the first element of the next document.\n\nIn the diagram below, you can see the gates at work, with straight lines representing closed gates, and blank circles representing open ones. The lines and circles running horizontal down the hidden layer are the forget gates.\n\nIt should be noted that while feedforward networks map one input to one output, recurrent nets can map one to many, as above (one image to many words in a caption), many to many (translation), or many to one (classifying a voice).\n\nYou may also wonder what the precise value is of input gates that protect a memory cell from new data coming in, and output gates that prevent it from affecting certain outputs of the RNN. You can think of LSTMs as allowing a neural network to operate on different scales of time at once.\n\nLet\u2019s take a human life, and imagine that we are receiving various streams of data about that life in a time series. Geolocation at each time step is pretty important for the next time step, so that scale of time is always open to the latest information.\n\nPerhaps this human is a diligent citizen who votes every couple years. On democratic time, we would want to pay special attention to what they do around elections, before they return to making a living, and away from larger issues. We would not want to let the constant noise of geolocation affect our political analysis.\n\nIf this human is also a diligent daughter, then maybe we can construct a familial time that learns patterns in phone calls which take place regularly every Sunday and spike annually around the holidays. Little to do with political cycles or geolocation.\n\nOther data is like that. Music is polyrhythmic. Text contains recurrent themes at varying intervals. Stock markets and economies experience jitters within longer waves. They operate simultaneously on different time scales that LSTMs can capture.\n\nA gated recurrent unit (GRU) is basically an LSTM without an output gate, which therefore fully writes the contents from its memory cell to the larger net at each time step.\n\nA commented example of a Graves LSTM learning how to replicate Shakespearian drama, and implemented with Deeplearning4j, can be found here. The API is commented where it\u2019s not self-explanatory. If you have questions, please join us on Gitter.\n\nHere\u2019s what the LSTM configuration looks like:\n\nHere are a few ideas to keep in mind when manually optimizing hyperparameters for RNNs:\n\n1) While recurrent networks may seem like a far cry from general artificial intelligence, it\u2019s our belief that intelligence, in fact, is probably dumber than we thought. That is, with a simple feedback loop to serve as memory, we have one of the basic ingredients of consciousness \u2013 a necessary but insufficient component. Others, not discussed above, might include additional variables that represent the network and its state, and a framework for decisionmaking logic based on interpretations of data. The latter, ideally, would be part of a larger problem-solving loop that rewards success and punishes failure, much like reinforcement learning. Come to think of it, DeepMind already built that\u2026\n\n2) All neural networks whose parameters have been optimized have memory in a sense, because those parameters are the traces of past data. But in feedforward networks, that memory may be frozen in time. That is, after a network is trained, the model it learns may be applied to more data without further adapting itself. In addition, it is monolithic in the sense that the same memory (or set of weights) is applied to all incoming data. Recurrent networks, which also go by the name of dynamic (translation: \u201cchanging\u201d) neural networks, are distinguished from feedforward nets not so much by having memory as by giving particular weight to events that occur in a series. While those events do not need to follow each other immediately, they are presumed to be linked, however remotely, by the same temporal thread. Feedforward nets do not make such a presumption. They treat the world as a bucket of objects without order or time. It may be helpful to map two types of neural network to two types of human knowledge. When we are children, we learn to recognize colors, and we go through the rest of our lives recognizing colors wherever we see them, in highly varied contexts and independent of time. We only had to learn the colors once. That knowledge is like memory in feedforward nets; they rely on a past without scope, undefined. Ask them what colors they were fed five minutes ago and they don\u2019t know or care. They are short-term amnesiacs. On the other hand, we also learn as children to decipher the flow of sound called language, and the meanings we extract from sounds such as \u201ctoe\u201d or \u201croe\u201d or \u201cz\u201d are always highly dependent on the sounds preceding (and following) them. Each step of the sequence builds on what went before, and meaning emerges from their order. Indeed, whole sentences conspire to convey the meaning of each syllable within them, their redundant signals acting as a protection against ambient noise. That is similar to the memory of recurrent nets, which look to a particular slice of the past for help. Both types of nets bring the past, or different pasts, to bear in different ways.", 
        "title": "A Beginner's Guide to Recurrent Networks and LSTMs - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/convolutionalnets", 
        "text": "", 
        "title": "Redirecting\u2026"
    }, 
    {
        "url": "https://deeplearning4j.org/recurrentnetwork", 
        "text": "Recurrent nets are a powerful set of artificial neural network algorithms especially useful for processing sequential data such as sound, time series (sensor) data or written natural language. A version of recurrent networks was used by DeepMind in their work playing video games with autonomous agents.\n\nRecurrent nets differ from feedforward nets because they include a feedback loop, whereby output from step n-1 is fed back to the net to affect the outcome of step n, and so forth for each subsequent step. For example, if a net is exposed to a word letter by letter, and it is asked to guess each following letter, the first letter of a word will help determine what a recurrent net thinks the second letter will be, etc.\n\nThis differs from a feedforward network, which learns to classify each handwritten numeral of MNIST independently according to the pixels it is exposed to from a single example, without referring to the preceding example to adjust its predictions. Feedforward networks accept one input at a time, and produce one output. Recurrent nets don\u2019t face the same one-to-one constraint.\n\nWhile some forms of data, like images, do not seem to be sequential, they can be understood as sequences when fed into a recurrent net. Consider an image of a handwritten word. Just as recurrent nets process handwriting, converting each cursive image into a letter, and using the beginning of a word to guess how that word will end, so nets can treat part of any image like letters in a sequence. A neural net roving over a large picture may learn from each region what the neighboring regions are more likely to be.\n\nRecurrent nets and feedforward nets both \u201cremember\u201d something about the world, in a loose sense, by modeling the data they are exposed to. But they remember in very different ways. After training, feedforward net produces a static model of the data it has been shown, and that model can then accept new examples and accurately classify or cluster them.\n\nIn contrast, recurrent nets produce dynamic models \u2013 i.e. models that change over time \u2013 in ways that yield accurate classifications dependent of the context of the examples they\u2019re exposed to.\n\nTo be precise, recurrent models include the hidden state that determined the previous classification in a series. In each subsequent step, that hidden state is combined with the new step\u2019s input data to produce a) a new hidden state and then b) a new classification. Each hidden state is recycled to produce its modified successor.\n\nHuman memories are also context aware, recycling an awareness of previous states to properly interpret new data. For example, let\u2019s take two individuals. One is aware that she is near Jack\u2019s house. The other is aware that she has entered an airplane. They will interpret the sounds \u201cHi Jack!\u201d in two very different ways, precisely because they retain a hidden state affected by their short-term memories and preceding sensations.\n\nDifferent short-term memories should be recalled at different times, in order to assign the right meaning to current input. Some of those memories will have been forged recently, and other memories will have been forged many time steps before they are needed. The recurrent net that effectively associates memories and input remote in time is called a Long Short-Term Memory (LSTM), as much as that sounds like an oxymoron.\n\nRecurrent nets have predictive capacity. They grasp the structure of data dynamically over time, and they are used to predict the next element in a series. Those elements might be the next letters in a word, or the next words in a sentence (natural language generation); the next number in data from sensors, economic tables, stock price action, etc.\n\nSequential data also includes videos, and recurrent networks have been used for object and gesture tracking in videos in real-time.\n\nRecurrent nets, like other neural nets, are useful for clustering and anomaly detection. That is, they recognize similarities and dissimilarities by grouping examples in vector space and measuring their distance from each other. Modeling normal behavior, and flagging abnormalities, is applicable to healthcare data generated by wearables; home data generated by smart objects such as thermostats; market data generated by the movement of stocks and indices; personal financial data generated by account transactions (which may be used to identify fraud and money laundering).\n\nRecall that Deeplearning4j\u2019s multinetwork configuration lets you create a layer in the API simply by naming it. In this case, you create an LSTM.\n\nIn Deeplearning4j, normal LSTMs expect a matrix in which the first row, x_i, is given, and all subsequent rows, x_s, are what the neural network attempts to predict. This is a generative model, and there are no labels. There is no limit to the number of rows the matrix can contain, but all rows must have the same length.\n\nThe Graves LSTM, of which an example is forthcoming, is meant to be usesd in a multilayer network.\n\nFor example, input data could be:\n\nRecurrent neural networks \u201callow for both parallel and sequential computation, and in principle can compute anything a traditional computer can compute. Unlike traditional computers, however, RNN are similar to the human brain, which is a large feedback network of connected neurons that somehow can learn to translate a lifelong sensory input stream into a sequence of useful motor outputs. The brain is a remarkable role model as it can solve many problems current machines cannot yet solve.\u201d - Juergen Schmidhuber\n\nMuch research in recurrent nets has been led by Juergen Schmidhuber and his students, notably Sepp Hochreiter, who identified the vanishing gradient problem confronted by very deep networks and later invented Long Short-Term Memory (LSTM) recurrent nets, as well as Alex Graves, now at DeepMind. Two other researchers of note are Felix Gers, who invented LSTM forget gates, and Justin Bayer, who came up with a way to automatically evolve various kinds of LSTM topologies in a problem-specific fashion.", 
        "title": "Tutorial: Recurrent Networks and LSTMs in Java - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/word2vec", 
        "text": "Word2vec is a two-layer neural net that processes text. Its input is a text corpus and its output is a set of vectors: feature vectors for words in that corpus. While Word2vec is not a deep neural network, it turns text into a numerical form that deep nets can understand. Deeplearning4j implements a distributed form of Word2vec for Java and Scala, which works on Spark with GPUs.\n\nWord2vec\u2019s applications extend beyond parsing sentences in the wild. It can be applied just as well to genes, code, likes, playlists, social media graphs and other verbal or symbolic series in which patterns may be discerned.\n\nWhy? Because words are simply discrete states like the other data mentioned above, and we are simply looking for the transitional probabilities between those states: the likelihood that they will co-occur. So gene2vec, like2vec and follower2vec are all possible. With that in mind, the tutorial below will help you understand how to create neural embeddings for any group of discrete and co-occurring states.\n\nThe purpose and usefulness of Word2vec is to group the vectors of similar words together in vectorspace. That is, it detects similarities mathematically. Word2vec creates vectors that are distributed numerical representations of word features, features such as the context of individual words. It does so without human intervention.\n\nGiven enough data, usage and contexts, Word2vec can make highly accurate guesses about a word\u2019s meaning based on past appearances. Those guesses can be used to establish a word\u2019s association with other words (e.g. \u201cman\u201d is to \u201cboy\u201d what \u201cwoman\u201d is to \u201cgirl\u201d), or cluster documents and classify them by topic. Those clusters can form the basis of search, sentiment analysis and recommendations in such diverse fields as scientific research, legal discovery, e-commerce and customer relationship management.\n\nThe output of the Word2vec neural net is a vocabulary in which each item has a vector attached to it, which can be fed into a deep-learning net or simply queried to detect relationships between words.\n\nMeasuring cosine similarity, no similarity is expressed as a 90 degree angle, while total similarity of 1 is a 0 degree angle, complete overlap; i.e. Sweden equals Sweden, while Norway has a cosine distance of 0.760124 from Sweden, the highest of any other country.\n\nHere\u2019s a list of words associated with \u201cSweden\u201d using Word2vec, in order of proximity:\n\nThe nations of Scandinavia and several wealthy, northern European, Germanic countries are among the top nine.\n\nThe vectors we use to represent words are called neural word embeddings, and representations are strange. One thing describes another, even though those two things are radically different. As Elvis Costello said: \u201cWriting about music is like dancing about architecture.\u201d Word2vec \u201cvectorizes\u201d about words, and by doing so it makes natural language computer-readable \u2013 we can start to perform powerful mathematical operations on words to detect their similarities.\n\nSo a neural word embedding represents a word with numbers. It\u2019s a simple, yet unlikely, translation.\n\nWord2vec is similar to an autoencoder, encoding each word in a vector, but rather than training against the input words through reconstruction, as a restricted Boltzmann machine does, word2vec trains words against other words that neighbor them in the input corpus.\n\nIt does so in one of two ways, either using context to predict a target word (a method known as continuous bag of words, or CBOW), or using a word to predict a target context, which is called skip-gram. We use the latter method because it produces more accurate results on large datasets.\n\nWhen the feature vector assigned to a word cannot be used to accurately predict that word\u2019s context, the components of the vector are adjusted. Each word\u2019s context in the corpus is the teacher sending error signals back to adjust the feature vector. The vectors of words judged similar by their context are nudged closer together by adjusting the numbers in the vector.\n\nJust as Van Gogh\u2019s painting of sunflowers is a two-dimensional mixture of oil on canvas that represents vegetable matter in a three-dimensional space in Paris in the late 1880s, so 500 numbers arranged in a vector can represent a word or group of words.\n\nThose numbers locate each word as a point in 500-dimensional vectorspace. Spaces of more than three dimensions are difficult to visualize. (Geoff Hinton, teaching people to imagine 13-dimensional space, suggests that students first picture 3-dimensional space and then say to themselves: \u201cThirteen, thirteen, thirteen.\u201d :)\n\nA well trained set of word vectors will place similar words close to each other in that space. The words oak, elm and birch might cluster in one corner, while war, conflict and strife huddle together in another.\n\nSimilar things and ideas are shown to be \u201cclose\u201d. Their relative meanings have been translated to measurable distances. Qualities become quantities, and algorithms can do their work. But similarity is just the basis of many associations that Word2vec can learn. For example, it can gauge relations between words of one language, and map them to another.\n\nThese vectors are the basis of a more comprehensive geometry of words. Not only will Rome, Paris, Berlin and Beijing cluster near each other, but they will each have similar distances in vectorspace to the countries whose capitals they are; i.e. Rome - Italy = Beijing - China. And if you only knew that Rome was the capital of Italy, and were wondering about the capital of China, then the equation Rome -Italy + China would return Beijing. No kidding.\n\nLet\u2019s look at some other associations Word2vec can produce.\n\nInstead of the pluses, minus and equals signs, we\u2019ll give you the results in the notation of logical analogies, where means \u201cis to\u201d and means \u201cas\u201d; e.g. \u201cRome is to Italy as Beijing is to China\u201d = . In the last spot, rather than supplying the \u201canswer\u201d, we\u2019ll give you the list of words that a Word2vec model proposes, when given the first three elements:\n\nThis model was trained on the Google News vocab, which you can import and play with. Contemplate, for a moment, that the Word2vec algorithm has never been taught a single rule of English syntax. It knows nothing about the world, and is unassociated with any rules-based symbolic logic or knowledge graph. And yet it learns more, in a flexible and automated fashion, than most knowledge graphs will learn after a years of human labor. It comes to the Google News documents as a blank slate, and by the end of training, it can compute complex analogies that mean something to humans.\n\nYou can also query a Word2vec model for other assocations. Not everything has to be two analogies that mirror each other. (We explain how below\u2026.)\n\nBy building a sense of one word\u2019s proximity to other similar words, which do not necessarily contain the same letters, we have moved beyond hard tokens to a smoother and more general sense of meaning.\n\nWhile Word2vec refers to a family of related algorithms, this implementation uses Skip-Gram Negative Sampling.\n\nCreate a new project in IntelliJ using Maven. If you don\u2019t know how to do that, see our Quickstart page. Then specify these properties and dependencies in the POM.xml file in your project\u2019s root directory (You can check Maven for the most recent versions \u2013 please use those\u2026).\n\nNow create and name a new class in Java. After that, you\u2019ll take the raw sentences in your .txt file, traverse them with your iterator, and subject them to some sort of preprocessing, such as converting all words to lowercase.\n\nIf you want to load a text file besides the sentences provided in our example, you\u2019d do this:\n\nThat is, get rid of the and feed the absolute path of your file into the .\n\nIn bash, you can find the absolute file path of any directory by typing in your command line from within that same directory. To that path, you\u2019ll add the file name and voila.\n\nWord2vec needs to be fed words rather than whole sentences, so the next step is to tokenize the data. To tokenize a text is to break it up into its atomic units, creating a new token each time you hit a white space, for example.\n\nThat should give you one word per line.\n\nNow that the data is ready, you can configure the Word2vec neural net and feed in the tokens.\n\nThis configuration accepts a number of hyperparameters. A few require some explanation:\n\nAn example for uptraining your previously trained word vectors is here.\n\nThe next step is to evaluate the quality of your feature vectors.\n\nThe line will return the cosine similarity of the two words you enter. The closer it is to 1, the more similar the net perceives those words to be (see the Sweden-Norway example above). For example:\n\nWith , the words printed to the screen allow you to eyeball whether the net has clustered semantically similar words. You can set the number of nearest words you want with the second parameter of wordsNearest. For example:\n\nWe rely on TSNE to reduce the dimensionality of word feature vectors and project words into a two or three-dimensional space. The full DL4J/ND4J example for TSNE is here.\n\nYou\u2019ll want to save the model. The normal way to save models in Deeplearning4j is via the serialization utils (Java serialization is akin to Python pickling, converting an object into a series of bytes).\n\nThis will save the vectors to a file called that will appear in the root of the directory where Word2vec is trained. The output in the file should have one word per line, followed by a series of numbers that together are its vector representation.\n\nTo keep working with the vectors, simply call methods on like this:\n\nThe classic example of Word2vec\u2019s arithmetic of words is \u201cking - queen = man - woman\u201d and its logical extension \u201cking - queen + woman = man\u201d.\n\nThe example above will output the 10 nearest words to the vector , which should include . The first parameter for wordsNearest has to include the \u201cpositive\u201d words and , which have a + sign associated with them; the second parameter includes the \u201cnegative\u201d word , which is associated with the minus sign (positive and negative here have no emotional connotation); the third is the length of the list of nearest words you would like to see. Remember to add this to the top of the file: .\n\nAny number of combinations is possible, but they will only return sensible results if the words you query occurred with enough frequency in the corpus. Obviously, the ability to return similar words (or documents) is at the foundation of both search and recommendation engines.\n\nYou can reload the vectors into memory like this:\n\nYou can then use Word2vec as a lookup table:\n\nIf the word isn\u2019t in the vocabulary, Word2vec returns zeros.\n\nThe Google News Corpus model we use to test the accuracy of our trained nets is hosted on S3. Users whose current hardware takes a long time to train on large corpora can simply download it to explore a Word2vec model without the prelude.\n\nIf you trained with the C vectors or Gensimm, this line will import the model.\n\nRemember to add to your imported packages.\n\nWith large models, you may run into trouble with your heap space. The Google model may take as much as 10G of RAM, and the JVM only launches with 256 MB of RAM, so you have to adjust your heap space. You can do that either with a file (see our Troubleshooting section), or through IntelliJ itself:\n\nWords are read into the vector one at a time, and scanned back and forth within a certain range. Those ranges are n-grams, and an n-gram is a contiguous sequence of n items from a given linguistic sequence; it is the nth version of unigram, bigram, trigram, four-gram or five-gram. A skip-gram simply drops items from the n-gram.\n\nThe skip-gram representation popularized by Mikolov and used in the DL4J implementation has proven to be more accurate than other models, such as continuous bag of words, due to the more generalizable contexts generated.\n\nThis n-gram is then fed into a neural network to learn the significance of a given word vector; i.e. significance is defined as its usefulness as an indicator of certain larger meanings, or labels.\n\nPlease note : The code below may be outdated. For updated examples, please see our dl4j-examples repository on Github.\n\nNow that you have a basic idea of how to set up Word2Vec, here\u2019s one example of how it can be used with DL4J\u2019s API:\n\nAfter following the instructions in the Quickstart, you can open this example in IntelliJ and hit run to see it work. If you query the Word2vec model with a word isn\u2019t contained in the training corpus, it will return null.\n\nQ: I get a lot of stack traces like this\n\nA: Look inside the directory where you started your Word2vec application. This can, for example, be an IntelliJ project home directory or the directory where you typed Java at the command line. It should have some directories that look like:\n\nYou can shut down your Word2vec application and try to delete them.\n\nQ: Not all of the words from my raw text data are appearing in my Word2vec object\u2026\n\nA: Try to raise the layer size via .layerSize() on your Word2Vec object like so\n\nQ: How do I load my data? Why does training take forever?\n\nA: If all of your sentences have been loaded as one sentence, Word2vec training could take a very long time. That\u2019s because Word2vec is a sentence-level algorithm, so sentence boundaries are very important, because co-occurrence statistics are gathered sentence by sentence. (For GloVe, sentence boundaries don\u2019t matter, because it\u2019s looking at corpus-wide co-occurrence. For many corpora, average sentence length is six words. That means that with a window size of 5 you have, say, 30 (random number here) rounds of skip-gram calculations. If you forget to specify your sentence boundaries, you may load a \u201csentence\u201d that\u2019s 10,000 words long. In that case, Word2vec would attempt a full skip-gram cycle for the whole 10,000-word \u201csentence\u201d. In DL4J\u2019s implementation, a line is assumed to be a sentence. You need plug in your own SentenceIterator and Tokenizer. By asking you to specify how your sentences end, DL4J remains language-agnostic. UimaSentenceIterator is one way to do that. It uses OpenNLP for sentence boundary detection.\n\nQ: Why is there such a difference in performance when feeding whole documents as one \u201csentence\u201d vs splitting into Sentences?\n\nA:If average sentence contains 6 words, and window size is 5, maximum theoretical number of 10 skipgram rounds will be achieved on 0 words. Sentence isn\u2019t long enough to have full window set with words. Rough maximum number of 5 sg rounds is available there for all words in such sentence.\n\nBut if your \u201csentence\u201d is 1000k words length, you\u2019ll have 10 skipgram rounds for every word in this sentence, excluding the first 5 and last five. So, you\u2019ll have to spend WAY more time building model + cooccurrence statistics will be shifted due to the absense of sentence boundaries.\n\nQ: How does Word2Vec Use Memory?\n\nA: The major memory consumer in w2v is weghts matrix. Math is simple there: NumberOfWords x NumberOfDimensions x 2 x DataType memory footprint.\n\nSo, if you build w2v model for 100k words using floats, and 100 dimensions, your memory footprint will be 100k x 100 x 2 x 4 (float size) = 80MB RAM just for matri + some space for strings, variables, threads etc.\n\nIf you load pre-built model, it uses roughly 2 times less RAM then during build time, so it\u2019s 40MB RAM.\n\nAnd the most popular model used so far is Google News model. There\u2019s 3M words, and vector size 300. That gives us 3.6GB only to load model. And you have to add 3M of strings, that do not have constant size in java. So, usually that\u2019s something around 4-6GB for loaded model depending on jvm version/supplier, gc state and phase of the moon.\n\nQ: I did everything you said and the results still don\u2019t look right.\n\nA: Make sure you\u2019re not hitting into normalization issues. Some tasks, like wordsNearest(), use normalized weights by default, and others require non-normalized weights. Pay attention to this difference.\n\nGoogle Scholar keeps a running tally of the papers citing Deeplearning4j\u2019s implementation of Word2vec here.\n\nKenny Helsens, a data scientist based in Belgium, applied Deeplearning4j\u2019s implementation of Word2vec to the NCBI\u2019s Online Mendelian Inheritance In Man (OMIM) database. He then looked for the words most similar to alk, a known oncogene of non-small cell lung carcinoma, and Word2vec returned: \u201cnonsmall, carcinomas, carcinoma, mapdkd.\u201d From there, he established analogies between other cancer phenotypes and their genotypes. This is just one example of the associations Word2vec can learn on a large corpus. The potential for discovering new aspects of important diseases has only just begun, and outside of medicine, the opportunities are equally diverse.\n\nAndreas Klintberg trained Deeplearning4j\u2019s implementation of Word2vec on Swedish, and wrote a thorough walkthrough on Medium.\n\nWord2Vec is especially useful in preparing text-based data for information retrieval and QA systems, which DL4J implements with deep autoencoders.\n\nMarketers might seek to establish relationships among products to build a recommendation engine. Investigators might analyze a social graph to surface members of a single group, or other relations they might have to location or financial sponsorship.\n\nWord2vec is a method of computing vector representations of words introduced by a team of researchers at Google led by Tomas Mikolov. Google hosts an open-source version of Word2vec released under an Apache 2.0 license. In 2014, Mikolov left Google for Facebook, and in May 2015, Google was granted a patent for the method, which does not abrogate the Apache license under which it has been released.\n\nWhile words in all languages may be converted into vectors with Word2vec, and those vectors learned with Deeplearning4j, NLP preprocessing can be very language specific, and requires tools beyond our libraries. The Stanford Natural Language Processing Group has a number of Java-based tools for tokenization, part-of-speech tagging and named-entity recognition for languages such as Mandarin Chinese, Arabic, French, German and Spanish. For Japanese, NLP tools like Kuromoji are useful. Other foreign-language resources, including text corpora, are available here.\n\nLoading and saving GloVe models to word2vec can be done like so:\n\nDeeplearning4j has a class called SequenceVectors, which is one level of abstraction above word vectors, and which allows you to extract features from any sequence, including social media profiles, transactions, proteins, etc. If data can be described as sequence, it can be learned via skip-gram and hierarchic softmax with the AbstractVectors class. This is compatible with the DeepWalk algorithm, also implemented in Deeplearning4j.", 
        "title": "Word2Vec, Doc2vec & GloVe: Neural Word Embeddings for Natural Language Processing - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/restrictedboltzmannmachine", 
        "text": "Invented by Geoff Hinton, a Restricted Boltzmann machine is an algorithm useful for dimensionality reduction, classification, regression, collaborative filtering, feature learning and topic modeling. (For more concrete examples of how neural networks like RBMs can be employed, please see our page on use cases).\n\nGiven their relative simplicity and historical importance, restricted Boltzmann machines are the first neural network we\u2019ll tackle. In the paragraphs below, we describe in diagrams and plain language how they work.\n\nRBMs are shallow, two-layer neural nets that constitute the building blocks of deep-belief networks. The first layer of the RBM is called the visible, or input, layer, and the second is the hidden layer. (Editor\u2019s note: While RBMs are occasionally used, most practitioners in the machine-learning community have deprecated them in favor of generative adversarial networks or variational autoencoders.)\n\nEach circle in the graph above represents a neuron-like unit called a node, and nodes are simply where calculations take place. The nodes are connected to each other across layers, but no two nodes of the same layer are linked.\n\nThat is, there is no intra-layer communication \u2013 this is the restriction in a restricted Boltzmann machine. Each node is a locus of computation that processes input, and begins by making stochastic decisions about whether to transmit that input or not. (Stochastic means \u201crandomly determined\u201d, and in this case, the coefficients that modify inputs are randomly initialized.)\n\nEach visible node takes a low-level feature from an item in the dataset to be learned. For example, from a dataset of grayscale images, each visible node would receive one pixel-value for each pixel in one image. (MNIST images have 784 pixels, so neural nets processing them must have 784 input nodes on the visible layer.)\n\nNow let\u2019s follow that single pixel value, x, through the two-layer net. At node 1 of the hidden layer, x is multiplied by a weight and added to a so-called bias. The result of those two operations is fed into an activation function, which produces the node\u2019s output, or the strength of the signal passing through it, given input x.\n\nNext, let\u2019s look at how several inputs would combine at one hidden node. Each x is multiplied by a separate weight, the products are summed, added to a bias, and again the result is passed through an activation function to produce the node\u2019s output.\n\nBecause inputs from all visible nodes are being passed to all hidden nodes, an RBM can be defined as a symmetrical bipartite graph.\n\nSymmetrical means that each visible node is connected with each hidden node (see below). Bipartite means it has two parts, or layers, and the graph is a mathematical term for a web of nodes.\n\nAt each hidden node, each input x is multiplied by its respective weight w. That is, a single input x would have three weights here, making 12 weights altogether (4 input nodes x 3 hidden nodes). The weights between two layers will always form a matrix where the rows are equal to the input nodes, and the columns are equal to the output nodes.\n\nEach hidden node receives the four inputs multiplied by their respective weights. The sum of those products is again added to a bias (which forces at least some activations to happen), and the result is passed through the activation algorithm producing one output for each hidden node.\n\nIf these two layers were part of a deeper neural network, the outputs of hidden layer no. 1 would be passed as inputs to hidden layer no. 2, and from there through as many hidden layers as you like until they reach a final classifying layer. (For simple feed-forward movements, the RBM nodes function as an autoencoder and nothing more.)\n\nBut in this introduction to restricted Boltzmann machines, we\u2019ll focus on how they learn to reconstruct data by themselves in an unsupervised fashion (unsupervised means without ground-truth labels in a test set), making several forward and backward passes between the visible layer and hidden layer no. 1 without involving a deeper network.\n\nIn the reconstruction phase, the activations of hidden layer no. 1 become the input in a backward pass. They are multiplied by the same weights, one per internode edge, just as x was weight-adjusted on the forward pass. The sum of those products is added to a visible-layer bias at each visible node, and the output of those operations is a reconstruction; i.e. an approximation of the original input. This can be represented by the following diagram:\n\nBecause the weights of the RBM are randomly initialized, the difference between the reconstructions and the original input is often large. You can think of reconstruction error as the difference between the values of and the input values, and that error is then backpropagated against the RBM\u2019s weights, again and again, in an iterative learning process until an error minimum is reached.\n\nA more thorough explanation of backpropagation is here.\n\nAs you can see, on its forward pass, an RBM uses inputs to make predictions about node activations, or the probability of output given a weighted x: .\n\nBut on its backward pass, when activations are fed in and reconstructions, or guesses about the original data, are spit out, an RBM is attempting to estimate the probability of inputs given activations , which are weighted with the same coefficients as those used on the forward pass. This second phase can be expressed as .\n\nTogether, those two estimates will lead you to the joint probability distribution of inputs x and activations a, or .\n\nReconstruction does something different from regression, which estimates a continous value based on many inputs, and different from classification, which makes guesses about which discrete label to apply to a given input example.\n\nReconstruction is making guesses about the probability distribution of the original input; i.e. the values of many varied points at once. This is known as generative learning, which must be distinguished from the so-called discriminative learning performed by classification, which maps inputs to labels, effectively drawing lines between groups of data points.\n\nLet\u2019s imagine that both the input data and the reconstructions are normal curves of different shapes, which only partially overlap.\n\nTo measure the distance between its estimated probability distribution and the ground-truth distribution of the input, RBMs use Kullback Leibler Divergence. A thorough explanation of the math can be found on Wikipedia.\n\nKL-Divergence measures the non-overlapping, or diverging, areas under the two curves, and an RBM\u2019s optimization algorithm attempts to minimize those areas so that the shared weights, when multiplied by activations of hidden layer one, produce a close approximation of the original input. On the left is the probability distibution of a set of original input, p, juxtaposed with the reconstructed distribution q; on the right, the integration of their differences.\n\nBy iteratively adjusting the weights according to the error they produce, an RBM learns to approximate the original data. You could say that the weights slowly come to reflect the structure of the input, which is encoded in the activations of the first hidden layer. The learning process looks like two probability distributions converging, step by step.\n\nLet\u2019s talk about probability distributions for a moment. If you\u2019re rolling two dice, the probability distribution for all outcomes looks like this:\n\nThat is, 7s are the most likely because there are more ways to get to 7 (3+4, 1+6, 2+5) than there are ways to arrive at any other sum between 2 and 12. Any formula attempting to predict the outcome of dice rolls needs to take seven\u2019s greater frequency into account.\n\nOr take another example: Languages are specific in the probability distribution of their letters, because each language uses certain letters more than others. In English, the letters e, t and a are the most common, while in Icelandic, the most common letters are a, r and n. Attempting to reconstruct Icelandic with a weight set based on English would lead to a large divergence.\n\nIn the same way, image datasets have unique probability distributions for their pixel values, depending on the kind of images in the set. Pixels values are distributed differently depending on whether the dataset includes MNIST\u2019s handwritten numerals:\n\nor the headshots found in Labeled Faces in the Wild:\n\nImagine for a second an RBM that was only fed images of elephants and dogs, and which had only two output nodes, one for each animal. The question the RBM is asking itself on the forward pass is: Given these pixels, should my weights send a stronger signal to the elephant node or the dog node? And the question the RBM asks on the backward pass is: Given an elephant, which distribution of pixels should I expect?\n\nThat\u2019s joint probability: the simultaneous probability of x given a and of a given x, expressed as the shared weights between the two layers of the RBM.\n\nThe process of learning reconstructions is, in a sense, learning which groups of pixels tend to co-occur for a given set of images. The activations produced by nodes of hidden layers deep in the network represent significant co-occurrences; e.g. \u201cnonlinear gray tube + big, floppy ears + wrinkles\u201d might be one.\n\nIn the two images above, you see reconstructions learned by Deeplearning4j\u2019s implemention of an RBM. These reconstructions represent what the RBM\u2019s activations \u201cthink\u201d the original data looks like. Geoff Hinton refers to this as a sort of machine \u201cdreaming\u201d. When rendered during neural net training, such visualizations are extremely useful heuristics to reassure oneself that the RBM is actually learning. If it is not, then its hyperparameters, discussed below, should be adjusted.\n\nOne last point: You\u2019ll notice that RBMs have two biases. This is one aspect that distinguishes them from other autoencoders. The hidden bias helps the RBM produce the activations on the forward pass (since biases impose a floor so that at least some nodes fire no matter how sparse the data), while the visible layer\u2019s biases help the RBM learn the reconstructions on the backward pass.\n\nOnce this RBM learns the structure of the input data as it relates to the activations of the first hidden layer, then the data is passed one layer down the net. Your first hidden layer takes on the role of visible layer. The activations now effectively become your input, and they are multiplied by weights at the nodes of the second hidden layer, to produce another set of activations.\n\nThis process of creating sequential sets of activations by grouping features and then grouping groups of features is the basis of a feature hierarchy, by which neural networks learn more complex and abstract representations of data.\n\nWith each new hidden layer, the weights are adjusted until that layer is able to approximate the input from the previous layer. This is greedy, layerwise and unsupervised pre-training. It requires no labels to improve the weights of the network, which means you can train on unlabeled data, untouched by human hands, which is the vast majority of data in the world. As a rule, algorithms exposed to more data produce more accurate results, and this is one of the reasons why deep-learning algorithms are kicking butt.\n\nBecause those weights already approximate the features of the data, they are well positioned to learn better when, in a second step, you try to classify images with the deep-belief network in a subsequent supervised learning stage.\n\nWhile RBMs have many uses, proper initialization of weights to facilitate later learning and classification is one of their chief advantages. In a sense, they accomplish something similar to backpropagation: they push weights to model data well. You could say that pre-training and backprop are substitutable means to the same end.\n\nTo synthesize restricted Boltzmann machines in one diagram, here is a symmetrical bipartite and bidirectional graph:\n\nFor those interested in studying the structure of RBMs in greater depth, they are one type of undirectional graphical model, also called markov random field.\n\nThe variable is the number of times you run contrastive divergence. Contrastive divergence is the method used to calculate the gradient (the slope representing the relationship between a network\u2019s weights and its error), without which no learning can occur.\n\nEach time contrastive divergence is run, it\u2019s a sample of the Markov Chain composing the restricted Boltzmann machine. A typical value is 1.\n\nIn the above example, you can see how RBMs can be created as layers with a more general . After each dot you\u2019ll find an additional parameter that affects the structure and performance of a deep neural net. Most of those parameters are defined on this site.\n\nweightInit, or represents the starting value of the coefficients that amplify or mute the input signal coming into each node. Proper weight initialization can save you a lot of training time, because training a net is nothing more than adjusting the coefficients to transmit the best signals, which allow the net to classify accurately.\n\nactivationFunction refers to one of a set of functions that determine the threshold(s) at each node above which a signal is passed through the node, and below which it is blocked. If a node passes the signal through, it is \u201cactivated.\u201d\n\noptimizationAlgo refers to the manner by which a neural net minimizes error, or finds a locus of least error, as it adjusts its coefficients step by step. LBFGS, an acronym whose letters each refer to the last names of its multiple inventors, is an optimization algorithm that makes use of second-order derivatives to calculate the slope of gradient along which coefficients are adjusted.\n\nregularization methods such as l2 help fight overfitting in neural nets. Regularization essentially punishes large coefficients, since large coefficients by definition mean the net has learned to pin its results to a few heavily weighted inputs. Overly strong weights can make it difficult to generalize a net\u2019s model when exposed to new data.\n\nVisibleUnit/HiddenUnit refers to the layers of a neural net. The , or layer, is the layer of nodes where input goes in, and the is the layer where those inputs are recombined in more complex features. Both units have their own so-called transforms, in this case Gaussian for the visible and Rectified Linear for the hidden, which map the signal coming out of their respective layers onto a new space.\n\nlossFunction is the way you measure error, or the difference between your net\u2019s guesses and the correct labels contained in the test set. Here we use , which makes all errors positive so they can be summed and backpropagated.\n\nlearningRate, like momentum, affects how much the neural net adjusts the coefficients on each iteration as it corrects for error. These two parameters help determine the size of the steps the net takes down the gradient towards a local optimum. A large learning rate will make the net learn fast, and maybe overshoot the optimum. A small learning rate will slow down the learning, which can be inefficient.\n\nA continuous restricted Boltzmann machine is a form of RBM that accepts continuous input (i.e. numbers cut finer than integers) via a different type of contrastive divergence sampling. This allows the CRBM to handle things like image pixels or word-count vectors that are normalized to decimals between zero and one.\n\nIt should be noted that every layer of a deep-learning net requires four elements: the input, the coefficients, a bias and the transform (activation algorithm).\n\nThe input is the numeric data, a vector, fed to it from the previous layer (or as the original data). The coefficients are the weights given to various features that pass through each node layer. The bias ensures that some nodes in a layer will be activated no matter what. The transformation is an additional algorithm that squashes the data after it passes through each layer in a way that makes gradients easier to compute (and gradients are necessary for a net to learn).\n\nThose additional algorithms and their combinations can vary layer by layer.\n\nAn effective continuous restricted Boltzmann machine employs a Gaussian transformation on the visible (or input) layer and a rectified-linear-unit transformation on the hidden layer. That\u2019s particularly useful in facial reconstruction. For RBMs handling binary data, simply make both transformations binary ones.\n\nGaussian transformations do not work well on RBMs\u2019 hidden layers. The rectified-linear-unit transformations used instead are capable of representing more features than binary transformations, which we employ on deep-belief nets.\n\nYou can interpret RBMs\u2019 output numbers as percentages. Every time the number in the reconstruction is not zero, that\u2019s a good indication the RBM learned the input.\n\nIt should be noted that RBMs do not produce the most stable, consistent results of all shallow, feedforward networks. In many situations, a dense-layer autoencoder works better. Indeed, the industry is moving toward tools such as variational autoencoders and GANs.\n\nNext, we\u2019ll show you how to implement a deep-belief network, which is simply many restricted Boltzmann machines stacked on top of one another.", 
        "title": "A Beginner's Tutorial for Restricted Boltzmann Machines - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/deepautoencoder", 
        "text": "A deep autoencoder is composed of two, symmetrical deep-belief networks that typically have four or five shallow layers representing the encoding half of the net, and second set of four or five layers that make up the decoding half.\n\nThe layers are restricted Boltzmann machines, the building blocks of deep-belief networks, with several peculiarities that we\u2019ll discuss below. Here\u2019s a simplified schema of a deep autoencoder\u2019s structure, which we\u2019ll explain below.\n\nProcessing the benchmark dataset MNIST, a deep autoencoder would use binary transformations after each RBM. Deep autoencoders can also be used for other types of datasets with real-valued data, on which you would use Gaussian rectified transformations for the RBMs instead.\n\nLet\u2019s sketch out an example encoder:\n\nIf, say, the input fed to the network is 784 pixels (the square of the 28x28 pixel images in the MNIST dataset), then the first layer of the deep autoencoder should have 1000 parameters; i.e. slightly larger.\n\nThis may seem counterintuitive, because having more parameters than input is a good way to overfit a neural network.\n\nIn this case, expanding the parameters, and in a sense expanding the features of the input itself, will make the eventual decoding of the autoencoded data possible.\n\nThis is due to the representational capacity of sigmoid-belief units, a form of transformation used with each layer. Sigmoid belief units can\u2019t represent as much as information and variance as real-valued data. The expanded first layer is a way of compensating for that.\n\nThe layers will be 1000, 500, 250, 100 nodes wide, respectively, until the end, where the net produces a vector 30 numbers long. This 30-number vector is the last layer of the first half of the deep autoencoder, the pretraining half, and it is the product of a normal RBM, rather than an classification output layer such as Softmax or logistic regression, as you would normally see at the end of a deep-belief network.\n\nThose 30 numbers are an encoded version of the 28x28 pixel image. The second half of a deep autoencoder actually learns how to decode the condensed vector, which becomes the input as it makes its way back.\n\nThe decoding half of a deep autoencoder is a feed-forward net with layers 100, 250, 500 and 1000 nodes wide, respectively. Layer weights are initialized randomly.\n\nThe decoding half of a deep autoencoder is the part that learns to reconstruct the image. It does so with a second feed-forward net which also conducts back propagation. The back propagation happens through reconstruction entropy.\n\nAt the stage of the decoder\u2019s backpropagation, the learning rate should be lowered, or made slower: somewhere between 1e-3 and 1e-6, depending on whether you\u2019re handling binary or continuous data, respectively.\n\nAs we mentioned above, deep autoencoders are capable of compressing images into 30-number vectors.\n\nImage search, therefore, becomes a matter of uploading an image, which the search engine will then compress to 30 numbers, and compare that vector to all the others in its index.\n\nVectors containing similar numbers will be returned for the search query, and translated into their matching image.\n\nA more general case of image compression is data compression. Deep autoencoders are useful for semantic hashing, as discussed in this paper by Geoff Hinton.\n\nDeep autoencoders are useful in topic modeling, or statistically modeling abstract topics that are distributed across a collection of documents.\n\nThis, in turn, is an important step in question-answer systems like Watson.\n\nIn brief, each document in a collection is converted to a Bag-of-Words (i.e. a set of word counts) and those word counts are scaled to decimals between 0 and 1, which may be thought of as the probability of a word occurring in the doc.\n\nThe scaled word counts are then fed into a deep-belief network, a stack of restricted Boltzmann machines, which themselves are just a subset of feedforward-backprop autoencoders. Those deep-belief networks, or DBNs, compress each document to a set of 10 numbers through a series of sigmoid transforms that map it onto the feature space.\n\nEach document\u2019s number set, or vector, is then introduced to the same vector space, and its distance from every other document-vector measured. Roughly speaking, nearby document-vectors fall under the same topic.\n\nFor example, one document could be the \u201cquestion\u201d and others could be the \u201canswers,\u201d a match the software would make using vector-space measurements.\n\nA deep auto encoder can be built by extending Deeplearning4j\u2019s MultiLayerNetwork class.\n\nThe code would look something like this:\n\nTo construct a deep autoencoder, please make sure you have the most recent version of Deeplearning4j and its examples\n\nFor questions about Deep Autoencoders, contact us on Gitter.", 
        "title": "A Beginner's Guide to Deep Autoencoders - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/denoisingautoencoder", 
        "text": "An autoencoder is a neural network used for dimensionality reduction; that is, for feature selection and extraction. Autoencoders with more hidden layers than inputs run the risk of learning the identity function \u2013 where the output simply equals the input \u2013 thereby becoming useless.\n\nDenoising autoencoders are an extension of the basic autoencoder, and represent a stochastic version of it. Denoising autoencoders attempt to address identity-function risk by randomly corrupting input (i.e. introducing noise) that the autoencoder must then reconstruct, or denoise.\n\nThe amount of noise to apply to the input takes the form of a percentage. Typically, 30 percent, or 0.3, is fine, but if you have very little data, you may want to consider adding more.\n\nTo create the machine, you simply instantiate an AutoEncoder and set the corruptionLevel, or noise, as you can see in the example below.\n\nThat\u2019s how you set up a denoising autoencoder with one visible layer and one hidden layer using MNIST data. This net has a learning rate of 0.1, momentum of of 0.9, and utilizes reconstruction cross entropy as its loss function.\n\nNext, we\u2019ll show you a stacked denoising autoencoder, which is simply many denoising autoencoders strung together.", 
        "title": "Denoising Autoencoders - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/stackeddenoisingautoencoder", 
        "text": "A stacked denoising autoencoder is to a denoising autoencoder what a deep-belief network is to a restricted Boltzmann machine. A key function of SDAs, and deep learning more generally, is unsupervised pre-training, layer by layer, as input is fed through. Once each layer is pre-trained to conduct feature selection and extraction on the input from the preceding layer, a second stage of supervised fine-tuning can follow.\n\nA word on stochastic corruption in SDAs: Denoising autoencoders shuffle data around and learn about that data by attempting to reconstruct it. The act of shuffling is the noise, and the job of the network is to recognize the features within the noise that will allow it to classify the input. When a network is being trained, it generates a model, and measures the distance between that model and the benchmark through a loss function. Its attempts to minimize the loss function involve resampling the shuffled inputs and re-reconstructing the data, until it finds those inputs which bring its model closest to what it has been told is true.\n\nThe serial resamplings are based on a generative model to randomly provide data to be processed. This is known as a Markov Chain, and more specifically, a Markov Chain Monte Carlo algorithm that steps through the data set seeking a representative sampling of indicators that can be used to construct more and more complex features.\n\nIn Deeplearning4j, stacked denoising autoencoders are built by creating a that has autoencoders for its hidden layers. Those autoencoders have a . That\u2019s the \u201cnoise\u201d; the neural network learns to denoise the signal. Notice how is set to \u201ctrue\u201d.\n\nBy the same token, deep-belief networks are created as a that has restricted Boltzmann machines at each hidden layer. More generally, you can think of Deeplearning4j as usuing \u201cprimitives\u201d such as RBMs and autoencoders that allow you to construct various deep neural networks.", 
        "title": "Stacked Denoising AutoEncoders - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/generative-adversarial-network", 
        "text": "Generative adversarial networks (GANs) are deep neural net architectures comprised of two nets, pitting one against the other (thus the \u201cadversarial\u201d).\n\nGANs were introduced in a paper by Ian Goodfellow and other researchers at the University of Montreal, including Yoshua Bengio, in 2014. Referring to GANs, Facebook\u2019s AI research director Yann LeCun called adversarial training \u201cthe most interesting idea in the last 10 years in ML.\u201d\n\nGANs\u2019 potential is huge, because they can learn to mimic any distribution of data. That is, GANs can be taught to create worlds eerily similar to our own in any domain: images, music, speech, prose. They are robot artists in a sense, and their output is impressive \u2013 poignant even.\n\nTo understand GANs, you should know how generative algorithms work, and for that, contrasting them with discriminative algorithms is instructive. Discriminative algorithms try to classify input data; that is, given the features of a data instance, they predict a label or category to which that data belongs.\n\nFor example, given all the words in an email, a discriminative algorithm could predict whether the message is or . is one of the labels, and the bag of words gathered from the email are the features that constitute the input data. When this problem is expressed mathematically, the label is called and the features are called . The formulation is used to mean \u201cthe probability of y given x\u201d, which in this case would translate to \u201cthe probability that an email is spam given the words it contains.\u201d\n\nSo discriminative algorithms map features to labels. They are concerned solely with that correlation. One way to think about generative algorithms is that they do the opposite. Instead of predicting a label given certain features, they attempt to predict features given a certain label.\n\nThe question a generative algorithm tries to answer is: Assuming this email is spam, how likely are these features? While discriminative models care about the relation between and , generative models care about \u201chow you get x.\u201d They allow you to capture , the probability of given , or the probability of features given a class. (That said, generative algorithms can also be used as classifiers. It just so happens that they can do more than categorize input data.)\n\nAnother way to think about it is to distinguish discriminative from generative like this:\n\nOne neural network, called the generator, generates new data instances, while the other, the discriminator, evaluates them for authenticity; i.e. the discriminator decides whether each instance of data it reviews belongs to the actual training dataset or not.\n\nLet\u2019s say we\u2019re trying to do something more banal than mimic the Mona Lisa. We\u2019re going to generate hand-written numerals like those found in the MNIST dataset, which is taken from the real world. The goal of the discriminator, when shown an instance from the true MNIST dataset, is to recognize them as authentic.\n\nMeanwhile, the generator is creating new images that it passes to the discriminator. It does so in the hopes that they, too, will be deemed authentic, even though they are fake. The goal of the generator is to generate passable hand-written digits, to lie without being caught. The goal of the discriminator is to identify images coming from the generator as fake.\n\nHere are the steps a GAN takes:\n\nSo you have a double feedback loop:\n\nYou can think of a GAN as the combination of a counterfeiter and a cop in a game of cat and mouse, where the counterfeiter is learning to pass false notes, and the cop is learning to detect them. Both are dynamic; i.e. the cop is in training, too (maybe the central bank is flagging bills that slipped through), and each side comes to learn the other\u2019s methods in a constant escalation.\n\nThe discriminator network is a standard convolutional network that can categorize the images fed to it, a binomial classifier labeling images as real or fake. The generator is an inverse convolutional network, in a sense: While a standard convolutional classifier takes an image and downsamples it to produce a probability, the generator takes a vector of random noise and upsamples it to an image. The first throws away data through downsampling techniques like maxpooling, and the second generates new data.\n\nBoth nets are trying to optimize a different and opposing objective function, or loss function, in a zero-zum game. This is essentially an actor-critic model. As the discriminator changes its behavior, so does the generator, and vice versa. Their losses push against each other.\n\nIf you want to learn more about generating images, Brandon Amos wrote a great post about interpreting images as samples from a probability distribution.\n\nIt may be useful to compare generative adversarial networks to other neural networks, such as autoencoders and variational autoencoders.\n\nAutoencoders encode input data as vectors. They create a hidden, or compressed, representation of the raw data. They are useful in dimensionality reduction; that is, the vector serving as a hidden representation compresses the raw data into a smaller number of salient dimensions. Autoencoders can be paired with a so-called decoder, which allows you to reconstruct input data based on its hidden representation, much as you would with a restricted Boltzmann machine.\n\nVariational autoencoders are generative algorithm that add an additional constraint to encoding the input data, namely that the hidden representations are normalized. Variational autoencoders are capable of both compressing data like an autoencoder and synthesizing data like a GAN. However, while GANs generate data in fine, granular detail, images generated by VAEs tend to be more blurred. Deeplearning4j\u2019s examples include both autoencoders and variational autoencoders.\n\nYou can bucket generative algorithms into one of three types:\n\nWhen you train the discriminator, hold the generator values constant; and when you train the generator, hold the discriminator constant. Each should train against a static adversary. For example, this gives the generator a better read on the gradient it must learn by.\n\nBy the same token, pretraining the discriminator against MNIST before you start training the generator will establish a clearer gradient.\n\nEach side of the GAN can overpower the other. If the discriminator is too good, it will return values so close to 0 or 1 that the generator will struggle to read the gradient. If the generator is too good, it will persistently exploit weaknesses in the discriminator that lead to false negatives. This may be mitigated by the nets\u2019 respective learning rates.\n\nGANs take a long time to train. On a single GPU a GAN might take hours, and on a single CPU more than a day. While difficult to tune and therefore to use, GANs have stimulated a lot of interesting research and writing.\n\nHere\u2019s an example of a GAN coded in Keras, from which models can be imported to Deeplearning4j.\n\nNote: SKIL enables developers to build GANs. Deeplearning4j\u2019s latest 1.0.0-alpha release on Maven includes deconvolution and upsampling layers, which enable GANs.", 
        "title": "GAN: A Beginner\u2019s Guide to Generative Adversarial Networks - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/deepreinforcementlearning", 
        "text": "While neural networks are responsible for recent breakthroughs in problems like computer vision, machine translation and time series prediction \u2013 they can also combine with reinforcement learning algorithms to create something astounding like AlphaGo.\n\nReinforcement learning refers to goal-oriented algorithms, which learn how to attain a complex objective (goal) or maximize along a particular dimension over many steps; for example, maximize the points won in a game over many moves. They can start from a blank slate, and under the right conditions they achieve superhuman performance. Like a child incentivized by spankings and candy, these algorithms are penalized when they make the wrong decisions and rewarded when they make the right ones \u2013 this is reinforcement.\n\nReinforcement algorithms that incorporate deep learning can beat world champions at the game of Go as well as human experts playing numerous Atari video games. While that may sound trivial, it\u2019s a vast improvement over their previous accomplishments, and the state of the art is progressing rapidly.\n\nReinforcement learning solves the difficult problem of correlating immediate actions with the delayed returns they produce. Like humans, reinforcement learning algorithms sometimes have to wait a while to see the fruit of their decisions. They operate in a delayed return environment, where it can be difficult to understand which action leads to which outcome over many time steps.\n\nReinforcement learning algorithms can be expected to perform better and better in more ambiguous, real-life environments while choosing from an arbitrary number of possible actions, rather than from the limited options of a video game. That is, with time we expect them to be valuable to achieve goals in the real world.\n\nTwo reinforcement learning algorithms - Deep-Q learning and A3C - have been implemented in a Deeplearning4j library called RL4J. It can already play Doom.\n\nReinforcement learning can be understand using the concepts of agents, environments, states, actions and rewards, all of which we\u2019ll explain below. Capital letters tend to denote sets of things, and lower-case letters denote a specific instance of that thing; e.g. is all possible actions, while is a specific action contained in the set.\n\nSo environments are functions that transform an action taken in the current state into the next state and a reward; agents are functions that transform the new state and reward into the next action. We can know the agent\u2019s function, but we cannot know the function of the environment. It is a black box where we only see the inputs and outputs. Reinforcement learning represents an agent\u2019s attempt to approximate the environment\u2019s function, such that we can send actions into the black-box environment that maximize the rewards it spits out.\n\nIn the feedback loop above, the subscripts denote the time steps and , each of which refer to different states: the state at moment , and the state at moment . Unlike other forms of machine learning \u2013 such as supervised and unsupervised learning \u2013 reinforcement learning can only be thought about sequentially in terms of state-action pairs that occur one after the other.\n\nReinforcement learning judges actions by the results they produce. It is goal oriented, and its aim is to learn sequences of actions that will lead an agent to achieve its goal, or maximize its objective function. Here are some examples:\n\nHere\u2019s an example of an objective function for reinforcement learning; i.e. the way it defines its goal.\n\nWe are summing reward function r over t, which stands for time steps. So this objective function calculates all the reward we could obtain by running through, say, a game. Here, x is the state at a given time step, and a is the action taken in that state. r is the reward function for x and a. (We\u2019ll ignore \u03b3 for now.)\n\nReinforcement learning differs from both supervised and unsupervised learning by how it interprets inputs. We can illustrate their difference by describing what they learn about a \u201cthing.\u201d\n\nOne way to imagine an autonomous reinforcement learning agent would be as a blind person attempting to navigate the world with only their ears and a white cane. Agents have small windows that allow them to perceive their environment, and those windows may not even be the most appropriate way for them to perceive what\u2019s around them.\n\nThe goal of reinforcement learning is to pick the best known action for any given state, which means the actions have to be ranked, and assigned values relative to one another. Since those actions are state-dependent, what we are really gauging is the value of state-action pairs; i.e. an action taken from a certain state, something you did somewhere. Here are a few examples to demonstrate that the value and meaning of an action is contingent upon the state in which it is taken:\n\nWe map state-action pairs to the values we expect them to produce with the Q function, described above. The Q function takes as its input an agent\u2019s state and action, and maps them to probable rewards.\n\nReinforcement learning is the process of running the agent through sequences of state-action pairs, observing the rewards that result, and adapting the predictions of the Q function to those rewards until it accurately predicts the best path for the agent to take. That prediction is known as a policy.\n\nReinforcement learning is an attempt to model a complex probability distribution of rewards in relation to a very large number of state-action pairs. This is one reason reinforcement learning is paired with, say, a Markov decision process, a method to sample from a complex distribution to infer its properties. It closely resembles the problem that inspired Stan Ulam to invent the Monte Carlo method; namely, trying to infer the chances that a given hand of solitaire will turn out successful.\n\nAny statistical approach is essentially a confession of ignorance. The immense complexity of some phenomena (biological, political, sociological, or related to board games) make it impossible to reason from first principles. The only way to study them is through statistics, measuring superficial events and attempting to establish correlations between them, even when we do not understand the mechanism by which they relate. Reinforcement learning, like deep neural networks, is one such strategy, relying on sampling to extract information from data.\n\nAfter a little time spent employing something like a Markov decision process to approximate the probability distribution of reward over state-action pairs, a reinforcement learning algorithm may tend to repeat actions that lead to reward and cease to test alternatives. There is a tension between the exploitation of known rewards, and continued exploration to discover new actions that also lead to victory. Just as oil companies have the dual function of pumping crude out of known oil fields while drilling for new reserves, so too, reinforcement learning algorithms can be made to both exploit and explore to varying degrees, in order to ensure that they don\u2019t pass over rewarding actions at the expense of known winners.\n\nReinforcement learning is iterative. In its most interesting applications, it doesn\u2019t begin by knowing which rewards state-action pairs will produce. It learns those relations by running through states again and again, like athletes or musicians iterate through states in an attempt to improve their performance.\n\nYou could say that an algorithm is a method to more quickly aggregate the lessons of time. Reinforcement learning algorithms have a different relationship to time than humans do. An algorithm can run through the same states over and over again while experimenting with different actions, until it can infer which actions are best from which states. Effectively, algorithms enjoy their very own Groundhog Day, where they start out as dumb jerks and slowly get wise.\n\nSince humans never experience Groundhog Day outside the movie, reinforcement learning algorithms have the potential to learn more, and better, than humans. Indeed, the true advantage of these algorithms over humans stems not so much from their inherent nature, but from their ability to live in parallel on many chips at once, to train night and day without fatigue, and therefore to learn more. An algorithm trained on the game of Go, such as AlphaGo, will have played many more games of Go than any human could hope to complete in 100 lifetimes.2\n\nWhere do neural networks fit in? Neural networks are the agent that learns to map state-action pairs to rewards. Like all neural networks, they use coefficients to approximate the function relating inputs to outputs, and their learning consists to finding the right coefficients, or weights, by iteratively adjusting those weights along gradients that promise less error.\n\nIn reinforcement learning, convolutional networks can be used to recognize an agent\u2019s state; e.g. the screen that Mario is on, or the terrain before a drone. That is, they perform their typical task of image recognition.\n\nBut convolutional networks derive different interpretations from images in reinforcement learning than in supervised learning. In supervised learning, the network applies a label to an image; that is, it matches names to pixels.\n\nIn fact, it will rank the labels that best fit the image in terms of their probabilities. Shown an image of a donkey, it might decide the picture is 80% likely to be a donkey, 50% likely to be a horse, and 30% likely to be a dog.\n\nIn reinforcement learning, given an image that represents a state, a convolutional net can rank the actions possible to perform in that state; for example, it might predict that running right will return 5 points, jumping 7, and running left none.\n\nThe above image illustrates what a policy agent does, mapping a state to the best action.\n\nIf you recall, this is distinct from Q, which maps state action pairs to rewards.\n\nTo be more specific, Q maps state-action pairs to the highest combination of immediate reward with all future rewards that might be harvested by later actions in the trajectory. Here is the equation for Q, from Wikipedia:\n\nHaving assigned values to the expected rewards, the Q function simply selects the state-action pair with the highest so-called Q value.\n\nAt the beginning of reinforcement learning, the neural network coefficients may be initialized stochastically, or randomly. Using feedback from the environment, the neural net can use the difference between its expected reward and the ground-truth reward to adjust its weights and improve its interpretation of state-action pairs.\n\nThis feedback loop is analogous to the backpropagation of error in supervised learning. However, supervised learning begins with knowledge of the ground-truth labels the neural network is trying to predict. Its goal is to create a model that maps different images to their respective names.\n\nReinforcement learning relies on the environment to send it a scalar number in response to each new action. The rewards returned by the environment can be varied, delayed or affected by unknown variables, introducing noise to the feedback loop.\n\nThis leads us to a more complete expression of the Q function, which takes into account not only the immediate rewards produced by an action, but also the delayed rewards that may be returned several time steps deeper in the sequence.\n\nLike human beings, the Q function is recursive. Just as calling the wetware method contains within it another method , of which we are all the fruit, calling the Q function on a given state-action pair requires us to call a nested Q function to predict the value of the next state, which in turn depends on the Q function of the state after that, and so forth.\n\nRL4J examples are available here.\n\n1) It might be helpful to imagine a reinforcement learning algorithm in action, to paint it visually. Let\u2019s say the algorithm is learning to play the video game Super Mario. It\u2019s trying to get Mario through the game and acquire the most points. To do that, we can spin up lots of different Marios in parallel and run them through the space of all possible game states. It\u2019s as though you have 1,000 Marios all tunnelling through a mountain, and as they dig (e.g. as they decide again and again which action to take to affect the game environment), their experience-tunnels branch like the intricate and fractal twigs of a tree. The Marios\u2019 experience-tunnels are corridors of light cutting through the mountain. And as in life itself, one successful action may make it more likely that successful action is possible in a larger decision flow, propelling the winning Marios onward. You might also imagine, if each Mario is an agent, that in front of him is a heat map tracking the rewards he can associate with state-action pairs. (Imagine each state-action pair as have its own screen overlayed with heat from yellow to red. The many screens are assembled in a grid, like you might see in front of a Wall St. trader with many monitors. One action screen might be \u201cjump harder from this state\u201d, another might be \u201crun faster in this state\u201d and so on and so forth.) Since some state-action pairs lead to significantly more reward than others, and different kinds of actions such as jumping, squatting or running can be taken, the probability distribution of reward over actions is not a bell curve but instead complex, which is why Markov and Monte Carlo techniques are used to explore it, much as Stan Ulam explored winning Solitaire hands. That is, while it is difficult to describe the reward distribution in a formula, it can be sampled. Because the algorithm starts ignorant and many of the paths through the game-state space are unexplored, the heat maps will reflect their lack of experience; i.e. there could be blanks in the heatmap of the rewards they imagine, or they might just start with some default assumptions about rewards that will be adjusted with experience. The Marios are essentially reward-seeking missiles guided by those heatmaps, and the more times they run through the game, the more accurate their heatmap of potential future reward becomes. The heatmaps are basically probability distributions of reward over the state-action pairs possible from the Mario\u2019s current state.\n\n2) The correct analogy may actually be that a learning algorithm is like a species. Each simulation the algorithm runs as it learns could be considered an individual of the species. Just as knowledge from the algorithm\u2019s runs through the game is collected in the algorithm\u2019s model of the world, the individual humans of any group will report back via language, allowing the collective\u2019s model of the world, embodied in its texts, records and oral traditions, to become more intelligent (At least in the ideal case. The subversion and noise introduced into our collective models is a topic for another post, and probably for another website entirely.). This puts a finer point on why the contest between algorithms and individual humans, even when the humans are world champions, is unfair. We are pitting a civilization that has accumulated the wisdom of 10,000 lives against a single sack of flesh.", 
        "title": "A Beginner's Guide to Deep Reinforcement Learning - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/symbolicreasoning", 
        "text": "Deep learning has its discontents, and many of them look to other branches of AI when they hope for the future. Symbolic reasoning is one of those branches.\n\nThe two biggest flaws of deep learning are its lack of model interpretability (i.e. why did my model make that prediction?) and the amount of data that deep neural networks require in order to learn. They are data hungry.\n\nGeoff Hinton himself has expressed scepticism about whether backpropagation, the workhorse of deep neural nets, will be the way forward for AI.1\n\nResearch into so-called one-shot learning may address deep learning\u2019s data hunger, while deep symbolic learning, or enabling deep neural networks to manipulate, generate and otherwise cohabitate with concepts expressed in strings of characters, could help solve explainability, because, after all, humans communicate with signs and symbols, and that is what we desire from machines.2\n\nThe words sign and symbol derive from Latin and Greek words, respectively, that mean mark or token, as in \u201ctake this rose as a token of my esteem.\u201d Both words mean \u201cto stand for something else\u201d or \u201cto represent something else\u201d.\n\nThat something else could be a physical object, an idea, an event, you name it. For our purposes, the sign or symbol is a visual pattern, say a character or string of characters, in which meaning is embedded, and that sign or symbol is pointing at something else. It could be the variable , pointing at an unknown quantity, or it could be the word , which is pointing at the red, curling petals layered one over the other in a tight spiral at the end of a stalk of thorns.3\n\nThe signifier indicates the signified, like a finger pointing at the moon. Symbols compress sensory data in a way that enables humans, those beings of limited bandwidth, to share information.4\n\nCombinations of symbols that express their interrelations could be called reasoning, and when we humans string a bunch of signs together to express thought, as I am doing now, you might call it symbolic manipulation. Sometimes those symbolic relations are necessary and deductive, as with the formulas of pure math or the conclusions you might draw from a logical syllogism like this old Roman chestnut:\n\nOther times the symbols express lessons we derive inductively from our experiences of the world, as in: \u201cthe baby seems to prefer the pea-flavored goop (so for godssake let\u2019s make sure we keep some in the fridge),\u201d or E = mc2.\n\nSymbolic artificial intelligence, also known as Good, Old-Fashioned AI (GOFAI), was the dominant paradigm in the AI community from the post-War era until the late 1980s.\n\nImplementations of symbolic reasoning are called rules engines or expert systems or knowledge graphs. See Cyc for one of the longer-running examples. Google made a big one, too, which is what provides the information in the top box under your query when you search for something easy like the capital of Germany. These systems are essentially piles of nested if-then statements drawing conclusions about entities (human-readable concepts) and their relations (expressed in well understood semantics like is-a or lives-in ).\n\nImagine how Turbotax manages to reflect the US tax code \u2013 you tell it how much you earned and how many dependents you have and other contingencies, and it computes the tax you owe by law \u2013 that\u2019s an expert system.\n\nExternal concepts are added to the system by its programmer-creators, and that\u2019s more important than it sounds\u2026\n\nOne of the main differences between machine learning and traditional symbolic reasoning is where the learning happens. In machine- and deep-learning, the algorithm learns rules as it establishes correlations between inputs and outputs. In symbolic reasoning, the rules are created through human intervention. That is, to build a symbolic reasoning system, first humans must learn the rules by which two phenomena relate, and then hard-code those relationships into a static program. This difference is the subject of a well-known hacker koan:\n\nA hard-coded rule is a preconception. It is one form of assumption, and a strong one, while deep neural architectures contain other assumptions, usually about how they should learn, rather than what conclusion they should reach. The ideal, obviously, is to choose assumptions that allow a system to learn flexibly and produce accurate decisions about their inputs.\n\nOne of the main stumbling blocks of symbolic AI, or GOFAI, was the difficulty of revising beliefs once they were encoded in a rules engine. Expert systems are monotonic; that is, the more rules you add, the more knowledge is encoded in the system, but additional rules can\u2019t undo old knowledge. Monotonic basically means one direction; i.e. when one thing goes up, another thing goes up. Because machine learning algorithms can be retrained on new data, and will revise their parameters based on that new data, they are better at encoding tentative knowledge that can be retracted later if necessary.\n\nA second flaw in symbolic reasoning is that the computer itself doesn\u2019t know what the symbols mean; i.e. they are not necessarily linked to any other representations of the world in a non-symbolic way. Again, this stands in contrast to neural nets, which can link symbols to vectorized representations of the data, which are in turn just translations of raw sensory data. So the main challenge, when we think about GOFAI and neural nets, is how to ground symbols, or relate them to other forms of meaning that would allow computers to map the changing raw sensations of the world to symbols and then reason about them.\n\nHow can we fuse the ability of deep neural nets to learn probabilistic correlations from scratch alongside abstract and higher-order concepts, which are useful in compressing data and combining it in new ways? How can we learn to attach new meanings to concepts, and to use atomic concepts as elements in more complex and composable thoughts such as language allows us to express in all its natural plasticity?\n\nCombining symbolic reasoning with deep neural networks and deep reinforcement learning may help us address the fundamental challenges of reasoning, hierarchical representations, transfer learning, robustness in the face of adversarial examples, and interpretability (or explanatory power).\n\nLet\u2019s explore how they currently overlap and how they might. First of all, every deep neural net trained by supervised learning combines deep learning and symbolic manipulation, at least in a rudimentary sense. Because symbolic reasoning encodes knowledge in symbols and strings of characters. In supervised learning, those strings of characters are called labels, the categories by which we classify input data using a statistical model. The output of a classifier (let\u2019s say we\u2019re dealing with an image recognition algorithm that tells us whether we\u2019re looking at a pedestrian, a stop sign, a traffic lane line or a moving semi-truck), can trigger business logic that reacts to each classification. That business logic is one form of symbolic reasoning.\n\n1) Hinton, Yann LeCun and Andrew Ng have all suggested that work on unsupervised learning (learning from unlabeled data) will lead to our next breakthroughs.\n\n2) The two problems may overlap, and solving one could lead to solving the other, since a concept that helps explain a model will also help it recognize certain patterns in data using fewer examples.\n\n3) The weird thing about writing about signs, of course, is that in the confines of a text, we\u2019re just using one set of signs to describe another in the hopes that the reader will respond to the sensory evocation and supply the necessary analog memories of and . But you get my drift. (It gets even weirder when you consider that the sensory data perceived by our minds, and to which signs refer, are themselves signs of the thing in itself, which we cannot know.)\n\n4) According to science, the average American English speaker speaks at a rate of about 110\u2013150 words per minute (wpm). Just how much reality do you think will fit into a ten-minute transmission?\n\nThis page includes some recent, notable research that attempts to combine deep learning with symbolic learning to answer those questions.\n\nDeep reinforcement learning (DRL) brings the power of deep neural networks to bear on the generic task of trial-and-error learning, and its effectiveness has been convincingly demonstrated on tasks such as Atari video games and the game of Go. However, contemporary DRL systems inherit a number of shortcomings from the current generation of deep learning techniques. For example, they require very large datasets to work effectively, entailing that they are slow to learn even when such datasets are available. Moreover, they lack the ability to reason on an abstract level, which makes it difficult to implement high-level cognitive functions such as transfer learning, analogical reasoning, and hypothesis-based reasoning. Finally, their operation is largely opaque to humans, rendering them unsuitable for domains in which verifiability is important. In this paper, we propose an end-to-end reinforcement learning architecture comprising a neural back end and a symbolic front end with the potential to overcome each of these shortcomings. As proof-of-concept, we present a preliminary implementation of the architecture and apply it to several variants of a simple video game. We show that the resulting system \u2013 though just a prototype \u2013 learns effectively, and, by acquiring a set of symbolic rules that are easily comprehensible to humans, dramatically outperforms a conventional, fully neural DRL system on a stochastic variant of the game.\n\nArtificial Neural Networks are powerful function approximators capable of modelling solutions to a wide variety of problems, both supervised and unsupervised. As their size andexpressivity increases, so too does the variance of the model, yielding a nearly ubiquitous overfitting problem. Although mitigated by a variety of model regularisation methods, the common cure is to seek large amounts of training data\u2014which is not necessarily easily obtained\u2014that sufficiently approximates the data distribution of the domain we wish to test on. In contrast, logic programming methods such as Inductive Logic Programming offer an extremely data-efficient process by which models can be trained to reason on symbolic domains. However, these methods are unable to deal with the variety of domains neural networks can be applied to: they are not robust to noise in or mislabelling of inputs, and perhaps more importantly, cannot be applied to non-symbolic domains where the data is ambiguous, such as operating on raw pixels. In this paper, we propose a Differentiable Inductive Logic framework, which can not only solve tasks which traditional ILP systems are suited for, but shows a robustness to noise and error in the training data which ILP cannot cope with. Furthermore, as it is trained by backpropagation against a likelihood objective, it can be hybridised by connecting it with neural networks over ambiguous data in order to be applied to domains which ILP cannot address, while providing data efficiency and generalisation beyond what neural networks on their own can achieve.\n\nThe recent adaptation of deep neural network-based methods to reinforcement learning and planning domains has yielded remarkable progress on individual tasks. Nonetheless, progress on task-to-task transfer remains limited. In pursuit of efficient and robust generalization, we introduce the Schema Network, an object-oriented generative physics simulator capable of disentangling multiple causes of events and reasoning backward through causes to achieve goals. The richly structured architecture of the Schema Network can learn the dynamics of an environment directly from data. We compare Schema Networks with Asynchronous Advantage Actor-Critic and Progressive Networks on a suite of Breakout variations, reporting results on training efficiency and zero-shot generalization, consistently demonstrating faster, more robust learning and better transfer. We argue that generalizing from limited data and learning causal relationships are essential abilities on the path toward generally intelligent systems.\n\nWe introduce the Deep Symbolic Network (DSN) model, which aims at becoming the white-box version of Deep Neural Networks (DNN). The DSN model provides a simple, universal yet powerful structure, similar to DNN, to represent any knowledge of the world, which is transparent to humans. The conjecture behind the DSN model is that any type of real world objects sharing enough common features are mapped into human brains as a symbol. Those symbols are connected by links, representing the composition, correlation, causality, or other relationships between them, forming a deep, hierarchical symbolic network structure. Powered by such a structure, the DSN model is expected to learn like humans, because of its unique characteristics. First, it is universal, using the same structure to store any knowledge. Second, it can learn symbols from the world and construct the deep symbolic networks automatically, by utilizing the fact that real world objects have been naturally separated by singularities. Third, it is symbolic, with the capacity of performing causal deduction and generalization. Fourth, the symbols and the links between them are transparent to us, and thus we will know what it has learned or not - which is the key for the security of an AI system. Fifth, its transparency enables it to learn with relatively small data. Sixth, its knowledge can be accumulated. Last but not least, it is more friendly to unsupervised learning than DNN. We present the details of the model, the algorithm powering its automatic learning ability, and describe its usefulness in different use cases. The purpose of this paper is to generate broad interest to develop it within an open source project centered on the Deep Symbolic Network (DSN) model towards the development of general AI.\n\nAlthough deep learning has historical roots going back decades, neither the term \u201cdeep learning\u201d nor the approach was popular just over five years ago, when the field was reignited by papers such as Krizhevsky, Sutskever and Hinton\u2019s now classic (2012) deep network model of Imagenet. What has the field discovered in the five subsequent years? Against a background of considerable progress in areas such as speech recognition, image recognition, and game playing, and considerable enthusiasm in the popular press, I present ten concerns for deep learning, and suggest that deep learning must be supplemented by other techniques if we are to reach artificial general intelligence.\n\nThe tasks that an agent will need to solve often aren\u2019t known during training. However, if the agent knows which properties of the environment we consider im- portant, then after learning how its actions affect those properties the agent may be able to use this knowledge to solve complex tasks without training specifi- cally for them. Towards this end, we consider a setup in which an environment is augmented with a set of user defined attributes that parameterize the features of interest. We propose a model that learns a policy for transitioning between \u201cnearby\u201d sets of attributes, and maintains a graph of possible transitions. Given a task at test time that can be expressed in terms of a target set of attributes, and a current state, our model infers the attributes of the current state and searches over paths through attribute space to get a high level plan, and then uses its low level policy to execute the plan. We show in grid-world games and 3D block stacking that our model is able to generalize to longer, more complex tasks at test time even when it only sees short, simple tasks at train time. TL;DR: Compositional attribute-based planning that generalizes to long test tasks, despite being trained on short & simple tasks.\n\nWe investigate an unconventional direction of research that aims at converting neural networks, a class of distributed, connectionist, sub-symbolic models into a symbolic level with the ultimate goal of achieving AI interpretability and safety. To that end, we propose Object-Oriented Deep Learning, a novel computational paradigm of deep learning that adopts interpretable \u201cobjects/symbols\u201d as a basic representational atom instead of N-dimensional tensors (as in traditional \u201cfeature-oriented\u201d deep learning). For visual processing, each \u201cobject/symbol\u201d can explicitly package common properties of visual objects like its position, pose, scale, probability of being an object, pointers to parts, etc., providing a full spectrum of interpretable visual knowledge throughout all layers. It achieves a form of \u201csymbolic disentanglement\u201d, offering one solution to the important problem of disentangled representations and invariance. Basic computations of the network include predicting high-level objects and their properties from low-level objects and binding/aggregating relevant objects together. These computations operate at a more fundamental level than convolutions, capturing convolution as a special case while being significantly more general than it. All operations are executed in an input-driven fashion, thus sparsity and dynamic computation per sample are naturally supported, complementing recent popular ideas of dynamic networks and may enable new types of hardware accelerations. We experimentally show on CIFAR-10 that it can perform flexible visual processing, rivaling the performance of ConvNet, but without using any convolution. Furthermore, it can generalize to novel rotations of images that it was not trained for.", 
        "title": "A Beginner's Guide to Symbolic Reasoning (Symbolic AI) & Deep Learning - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/backpropagation.html", 
        "text": "To propagate is to transmit something (light, sound, motion or information) in a particular direction or through a particular medium. When we discuss backpropagation in deep learning, we are talking about the transmission of information, and that information relates to the error produced by the neural network.\n\nNeural networks are like new-born babies: They are created ignorant of the world, and it is only through exposure to the world, experiencing it, that their ignorance is slowly revised. Algorithms experience the world through data. So by training a neural network on a relevant dataset, we seek to decrease its ignorance. The way we measure progress is by monitoring the error produced by the network.\n\nThe knowledge of a neural network with regard to the world is captured by its weights, the parameters that alter input data as the signal flows through the neural network towards the final layer that will make a decision about that input. Those decisions are often wrong, because the parameters transforming the signal into a decision are wrong.\n\nSo the parameters of the neural network have a relationship with the error the net produces, and when the parameters change, presumably the error does, too. We change the parameters using an optimization algorithm called gradient descent, which is useful for finding the minimum of a function. We are seeking to minimize the error, which is also known as the loss function or the objective function.\n\nSo a neural propagates the signal of the input data forward through its parameters towards the moment of decision, and then backprogates information about the error through the network so that it can alter the parameters one step at a time.\n\nA gradient is a slope whose angle we can measure. Like all slopes, it can be expressed as a relationship between two variables: \u201cy over x\u201d, or rise over run. In this case, the is the error produced by the neural network, and is the parameter. So the gradient tells us the change we can expect in with regard to .\n\nTo obtain this information, we must use differential calculus, which enables us to measure instantaneous rates of change, which in this case is the tangent of a changing slope expressed the relationship of the parameter to the net\u2019s error.\n\nObviously, a neural network has many parameters, so what we\u2019re really measuring are the partial derivatives of each parameter\u2019s contribution to the total change in error.\n\nWhat\u2019s more, neural networks have parameters that process the input data sequentially, one after another. Therefore, backpropagation establishes the relationship between the neural network\u2019s error and the parameters of the net\u2019s last layer; then it establishes the relationship between the parameters of the neural net\u2019s last layer those the parameters of the second-to-last layer, and so forth, in an application of the chain rule of calculus.\n\nFor people just getting started with deep learning, the following tutorials and videos provide an easy entrance to the fundamental ideas of deep neural networks:", 
        "title": "Backpropagation & Deep Learning - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/data-sets-ml", 
        "text": "One of the hardest problems to solve in deep learning has nothing to do with neural nets: it\u2019s the problem of getting the right data in the right format.\n\nGetting the right data means gathering or identifying the data that correlates with the outcomes you want to predict; i.e. data that contains a signal about events you care about. The data needs to be aligned with the problem you\u2019re trying to solve. Kitten pictures are not very useful when you\u2019re building a facial identification system. Verifying that the data is aligned with the problem you seek to solve must be done by a data scientist. If you do not have the right data, then your efforts to build an AI solution must return to the data collection stage.\n\nThe right end format for deep learning is generally a tensor, or a multi-dimensional array. So data pipelines built for deep learning will generally convert all data \u2013 be it images, video, sound, voice, text or time series \u2013 into vectors and tensors to which linear algebra operations can be applied. That data frequently needs to be normalized, standardized and cleaned to increase its usefulness, and those are all steps in machine-learning ETL. Deeplearning4j offers the DataVec ETL tool to perform those data preprocessing tasks.\n\nDeep learning, and machine learning more generally, needs a good training set to work properly. Collecting and constructing the training set \u2013 a sizable body of known data \u2013 takes time and domain-specific knowledge of where and how to gather relevant information. The training set acts as the benchmark against which deep-learning nets are trained. That is what they learn to reconstruct before they\u2019re unleashed on data they haven\u2019t seen before.\n\nAt this stage, knowledgeable humans need to find the right raw data and transform it into a numerical representation that the deep-learning algorithm can understand, a tensor. Building a training set is, in a sense, pre-pre-training.\n\nTraining sets that require much time or expertise can serve as a proprietary edge in the world of data science and problem solving. The nature of the expertise is largely in telling your algorithm what matters to you by selecting what goes into the training set.\n\nIt involves telling a story \u2013 through the initial data you select \u2013 that will guide your deep-learning nets as they extract the significant features, both in the training set and in the raw data they\u2019ve been created to study.\n\nTo create a useful training set, you have to understand the problem you\u2019re solving; i.e. what you want your deep-learning nets to pay attention to, whicn outcomes you want to predict.\n\nMachine learning typically works with two data sets: training and test. All three should randomly sample a larger body of data.\n\nThe first set you use is the training set, the largest of the three. Running a training set through a neural network teaches the net how to weigh different features, adjusting them coefficients according to their likelihood of minimizing errors in your results.\n\nThose coefficients, also known as parameters, will be contained in tensors and together they are called the model, because they encode a model of the data they train on. They are the most important takeaways you will obtain from training a neural network.\n\nThe second set is your test set. It functions as a seal of approval, and you don\u2019t use it until the end. After you\u2019ve trained and optimized your data, you test your neural net against this final random sampling. The results it produces should validate that your net accurately recognizes images, or recognizes them at least [x] percentage of them.\n\nIf you don\u2019t get accurate predictions, go back to the training set, look at the hyperparameters you used to tune the network, as well as the quality of your data and look at your pre-processing techniques.\n\nNow that you have the overview, we\u2019ll show you how to create custom datasets.\n\nVarious repositories of open data sets that may be useful in training neural networks are available through the link.", 
        "title": "Data sets and machine learning - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/workspaces", 
        "text": "ND4J offers an additional memory-management model: workspaces. That allows you to reuse memory for cyclic workloads without the JVM Garbage Collector for off-heap memory tracking. In other words, at the end of the workspace loop, all s\u2019 memory content is invalidated. Workspaces are integrated into DL4J for training and inference.\n\nHere are some examples of how to use it with ND4J.\n\nThe basic idea is simple: You can do what you need within a workspace (or spaces), and if you want to get an INDArray out of it (i.e. to move result out of the workspace), you just call and you\u2019ll get an independent copy.\n\nFor DL4J users, workspaces provide better performance out of the box, and are enabled by default from 1.0.0-alpha onwards. Thus for most users, no explicit worspaces configuration is required.\n\nTo benefit from worspaces, they need to be enabled. You can configure the workspace mode using:\n\nThe difference between SEPARATE and SINGLE workspaces is a tradeoff between the performance & memory footprint:\n\nThat said, it\u2019s fine to use different modes for training & inference (i.e. use SEPARATE for training, and use SINGLE for inference, since inference only involves a feed-forward loop without backpropagation or updaters involved).\n\nWith workspaces enabled, all memory used during training will be reusable and tracked without the JVM GC interference. The only exclusion is the method that uses workspaces (if enabled) internally for the feed-forward loop. Subsequently, it detaches the resulting from the workspaces, thus providing you with independent which will be handled by the JVM GC.\n\nPlease note: After the 1.0.0-alpha release, workspaces in DL4J were refactored - SEPARATE/SINGLE modes have been deprecated, and users should use ENABLED instead.\n\nIf your training process uses workspaces, we recommend that you disable (or reduce the frequency of) periodic GC calls. That can be done like so:\n\nPut that somewhere before your call.\n\nFor , the workspace-mode configuration option was also added. As such, each of the trainer threads will use a separate workspace attached to the designated device.\n\nWe provide asynchronous prefetch iterators, and , which are usually used internally.\n\nThese iterators optionally use a special, cyclic workspace mode to obtain a smaller memory footprint. The size of the workspace, in this case, will be determined by the memory requirements of the first coming out of the underlying iterator, whereas the buffer size is defined by the user. The workspace will be adjusted if memory requirements change over time (e.g. if you\u2019re using variable-length time series).\n\nCaution: If you\u2019re using a custom iterator or the , please make sure you\u2019re not initializing something huge within the first call. Do that in your constructor to avoid undesired workspace growth.\n\nCaution: With being used, are supposed to be used before calling the DataSet. You are not supposed to store them, in any way, without the call. Otherwise, the memory used for within DataSet will be overwritten within eventually.\n\nIf for some reason you don\u2019t want your iterator to be wrapped into an asynchronous prefetch (e.g. for debugging purposes), special wrappers are provided: and . Basically, those are just thin wrappers that prevent prefetch.\n\nUsually, evaluation assumes use of the method, which essentially returns an detached from the workspace. In the case of regular evaluations during training, it might be better to use the built-in methods for evaluation. For example:\n\nThis piece of code will run a single cycle over , and it will update both (or less/more if required by your needs) implementations without any additional allocation.\n\nThere are also some situations, say, where you\u2019re short on RAM, and might want do release all workspaces created out of your control; e.g. during evaluation or training.\n\nThat could be done like so:\n\nThis method will destroy all workspaces that were created within the calling thread. If you\u2019ve created workspaces in some external threads on your own, you can use the same method in that thread, after the workspaces are no longer needed.\n\nIf workspaces are used incorrectly (such as a bug in a custom layer or data pipeline, for example), you may see an error message such as:\n\nFor more details on these exceptions, see ND4J User Guide - Workspaces", 
        "title": "Workspaces guide - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/customdatasets", 
        "text": "All input to the deep-learning nets \u2013 whether it\u2019s words, images or other data \u2013 must be transformed into numbers known as vectors, in a process called vectorization. A vector is simply a one-column matrix with an extendible number of rows.\n\nDataVec is an Apache2 Licensed open-sourced tool for machine learning ETL (Extract, Transform, Load) operations. The goal of DataVec is to transform raw data into usable vector formats across machine learning tools.\n\nDataVec provides tools to transform images into vectors, including labelling images based on directory name and structure. DataVec also provides tools to read CSV data and transform fields to the appropriate numeric format.\n\nDataVec examples are available in our examples.\n\nFor more information on DataVec is available here.", 
        "title": "Custom Datasets - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/csv-deep-learning", 
        "text": "It\u2019s useful to know how to load data from CSV files into neural nets, especially when dealing with time series. There\u2019s an easy way to do that with Deeplearning4j:\n\nThen configure the neural network and train it on the dataset. This example shows how.\n\nAnd here is the CSVRecordReader class.", 
        "title": "Loading Data From CSV's - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/opendata", 
        "text": "Here you\u2019ll find an organized list of interesting, high-quality datasets for machine learning research. We welcome your contributions for curating this list! You can find other lists of such datasets on Wikipedia, for example.\n\nGET STARTED WITH DEEP LEARNING FOR IMAGES\n\nGET STARTED WITH DEEP LEARNING FOR VIDEO\n\nGET STARTED WITH DEEP LEARNING FOR TEXT\n\nGET STARTED WITH DEEP LEARNING FOR GRAPHS\n\nGET STARTED WITH DEEP LEARNING FOR SOUND\n\nFor people just getting started with deep learning, the following tutorials and videos provide an easy entrance to the fundamental ideas of deep neural networks:\n\nThanks to deeplearning.net and Luke de Oliveira for many of these links and dataset descriptions. Any suggestions of open data sets we should include for the Deeplearning4j community are welcome!", 
        "title": "Open Datasets for Deep Learning & Machine Learning - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/image-data-pipeline.html#record", 
        "text": "Deeplearning4j\u2019s examples run on benchmark datasets that don\u2019t present any obstacles in the data pipeline, because we abstracted them away. But real-world users start with raw, messy data, which they need to preprocess, vectorize and use to train a neural net for clustering or classification.\n\nDataVec is our machine-learning vectorization library, and it is useful for customizing how you prepare data that a neural net can learn. (The Datavec Javadoc is here.)\n\nThis tutorial covers some key topics you may face when processing images. It covers Label Generation, Vectorization, and Configuring your Neural Net for ingesting images.\n\nA series of videos is available. The series involves a screencast of writing code to process a directory of images. Generate labels based on the path. Build a Neural Network to train on the images. Additional Videos in teh series include saving the Trained Network, loading the Trained Network, and testing against unseen images gathered from the internet.\n\nOur examples repo has an example of using ParentPathLabelGenerator. The class is ImagePipelineExample.java\n\nThe following code helps transform raw images into a format that will work well with DL4J and ND4J:\n\nThe RecordReader is a class in Datavec that helps convert the byte-oriented input into data that\u2019s oriented toward a record; i.e. a collection of elements that are fixed in number and indexed with a unique ID. Converting data to records is the process of vectorization. The record itself is a vector, each element of which is a feature.\n\nRead the DataVec JavaDoc for more information.\n\nThe ImageRecordReader is a subclass of the RecordReader and is built to automatically take in 28 x 28 pixel images. You can change dimensions to match your custom images by changing the parameters fed to the ImageRecordReader, as long as you make sure to adjust the hyperparameter, which will be equal to the product of image height x image width. A MultiLayerNetwork Configuration to ingest 28*28 images would have the\n\nIf a LabelGenerator is used the the call to ImageRecordReader includes the labelGenerator as a parameter.\n\nThe DataSetIterator is a Deeplearning4J class that traverses the elements of a list. Iterators pass through the data list, accesses each item sequentially, keeps track of how far it has progressed by pointing to its current element, and modifies itself to point to the next element with each new step in the traversal.\n\nThe DataSetIterator iterates through input datasets, fetching one or more (batchSize) new examples with each iteration, and loading those examples into a DataSet(INDArray) object that neural nets can work with. The line above also tells the RecordReaderDataSetIterator to convert the image to a straight line (e.g. vector) of elements, rather than a 28 x 28 grid (e.g. matrix); it also specifies the configuration of the labels.\n\ncan take as parameters the specific recordReader you want (for images, sound, etc.) and the batch size. For supervised learning, it will also take a label index and the number of possible labels that can be applied to the input (for LFW, the number of labels is 5,749).\n\nBelow is a neural net configuration example. Many of the hyperparameters are explained in the NeuralNetConfiguration Class glossary; thus, we\u2019ll summarize a few distinguishing characteristics.\n\nAt the end of the configuration, call build and pass the network\u2019s configuration into a MultiLayerNetwork object.\n\nTo set iteration listeners that show performance and help tune while training the neural net, use one of these examples:\n\nOnce the data is loaded, the model framework is built, train the model to fit the data. Call next on the data iterator to advance it through the data based on the batch size. It will return a certain amount of data based on batch size each time. The code below shows how to loop through the dataset iterator and then to run fit on the model in order to train it on the data.\n\nAfter the model has been trained, run data through it to test and evaluate its performance. Typically its a good idea to use cross validation by splitting up the dataset and using data the model hasn\u2019t seen before. In this case we have just show you below how to reset the current iterator, initialize the evaluation object and run data through it to get performance information.\n\nAn alternative approach to apply cross validation in this effort, would be to load all the data and split it up into a train and test set. Iris is a small enough dataset to load all the data and accomplish the split. Many datasets used for production neural nets are not. For the alternative approach in this example, use the following code:\n\nTo split test and train on large datasets, you\u2019ll have to iterate through both the test and training datasets. For the moment, we\u2019ll leave that to you.", 
        "title": "Customized Data Pipelines for Loading Images Into Deep Neural Networks - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/simple-image-load-transform", 
        "text": "Deeplearning4j\u2019s examples run on benchmark datasets that don\u2019t present any obstacles in the data pipeline, because we abstracted them away. But real-world users start with raw, messy data, which they need to preprocess, vectorize and use to train a neural net for clustering or classification.\n\nDataVec is our machine-learning vectorization library, and it is useful for customizing how you prepare data that a neural net can learn. (The DataVec Javadoc is here.)\n\nThis tutorial will walk through how to load an image dataset and carry out transforms on them. For the sake of simplicity this tutorial uses only 10 images from 3 of the classes in the Oxford flower dataset. Please do not copy paste the code below as they are only snippets for reference. Use the code from the full example here\n\nIn short, images in your dataset have to be organized in directories by class/label and these label/class directories live together in the parent directory.\n\nHere is the directory structure expected in general.\n\nIn this example the parentDir corresponds to and the subdirectories labelA, labelB, labelC all have 10 images each.\n\nPlease note that the images in your dataset do not have to be the same size. DataVec can do this for you. As you can see in this example images are all of different size and they are all resized to the height and width specified below\n\nThe advantage of neural nets is that they do not require you to manually feature engineer. However, there can be an advantage to transforming images to artificially increase the size of the dataset like in this winning kaggle entry http://benanne.github.io/2014/04/05/galaxy-zoo.html. Or you might want to crop an image down to only the relevant parts. An example would be detect a face and crop it down to size. DataVec has all the built in functionality/powerful features from OpenCV. A bare bones example that flips an image and then displays it is shown below:\n\nYou can chain transformations as shown below, write your own classes that will automate whatever features you want.\n\nIf you want to create a more sophisticated pipeline, you can use . You can create image transform pipelines, either sequentially or randomly.\n\nIn order to employ this complex pipeline, you can create a list, , then add ImageTransforms which you want to apply to the raw images with double values which is a probability. Note : the probability represents particular element in the pipeline will be executed. This is helpful for creating complex pipelines.\n\nHere is the code snippet of PipelineImageTransform.\n\nThe pipeline can also be randomly shuffled with each transform in order to increase the available dataset.\n\n If you set to true, you can get an image pipeline that executes each transformation at random.\n\nFFmpeg and OpenCV provide open source libraries for filtering and transforming images and video. Access to FFmpeg filters in versions 7.2 and above is available by adding the following to your pom.xml file, replacing the version with the current version.\n\nDeeplearning4j\u2019s neural nets take either a dataset or a dataset iterator to fit too. These are fundamental concepts for our framework. Please refer to other examples for how to use an iterator. Here is how you contruct a dataset iterator from an image record reader.\n\nThe DataSetIterator iterates through the input datasets via the recordReader, fetching one or more new examples with each iteration, and loading those examples into a DataSet object that neural nets can work with.\n\nThe DataSet passed by the DataIterator will be one or more arrays of pixel values. For example if we had specified our RecordReader with height of 10, width of 10 and channels of 1 for greyscale\n\nThen the DataSet returned would be a 10 by 10 collection of values between 0 and 255. With 0 for a black pixel and 255 for a white pixel. A value of 100 would be grey. If the image was color, then there would be three channels.\n\nIt may be useful to scale the image pixel value on a range of 0 to 1, rather than 0 to 255.\n\nThis can be done with the following code.", 
        "title": "Customized Data Pipelines for Loading Images Into Deep Neural Networks - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/datavecdoc/", 
        "text": "JavaScript is disabled on your browser.\n\nThis document is designed to be viewed using the frames feature. If you see this message, you are using a non-frame-capable web client. Link to Non-frame version.", 
        "title": "Generated Documentation (Untitled)"
    }, 
    {
        "url": "https://deeplearning4j.org/spark", 
        "text": "Deep learning is computationally intensive, so on very large datasets, speed matters. You can tackle the problem with faster hardware (usually GPUs), optimized code and some form of parallelism. Most people use Spark wrong when they\u2019re training deep neural networks, because Spark alone is not an efficient computation layer. It should be used for fast ETL, and the matrix manipulations should be moved down to faster, lower-level code, as Deeplearning4j does with ND4J and its underlying C++ library libnd4j.\n\nData parallelism shards large datasets and hands those pieces to separate neural networks, say, each on its own core. Deeplearning4j relies on Spark for this, training models in parallel and iteratively averages the parameters they produce in a central model. (Model parallelism, discussed here by Jeff Dean et al, allows models to specialize on separate patches of a large dataset without averaging.)\n\nNote that if you want a parameter server based approach (requires more setup!), please look at our new distributed page\n\nThis page assumes a working knowledge of Spark. If you are not familiar with setting up Spark clusters and running Spark jobs, this page will not teach you. Please consider studying Spark basics first, and then returning to this page. The Spark quick start is a great place to start with running Spark jobs.\n\nIf you want to run multiple models on the same server, consider using parallelwrapper instead.\n\nimplements the same concepts (parameter averaging and gradient sharing) optimized for a single server. You should use when you have a big box (64 cores or more) or multiple GPUs.\n\nNote that you can use multiple GPUs and cuDNN with Spark. The most difficult part of this will be cluster setup. It is not DL4J\u2019s responsibility, beyond being a Spark job.\n\nIf you have not run JVM-based Spark jobs before, we recommend building an uber JAR using the Maven Shade plugin .\n\nIf you would like a managed Spark cluster set up for you, please contact us. Various cloud services such as Elastic map reduce are another way of running and managing a Spark cluster.\n\nThe rest of this page covers the details for running a Spark job including how to customize the Spark job and how to use the Spark interface for DL4J.\n\nDeeplearning4j supports training neural networks on a Spark cluster, in order to accelerate network training.\n\nSimilar to DL4J\u2019s and classes, DL4J defines two classes for training neural networks on Spark:\n\nBecause these two classes are wrappers around the stardard single-machine classes, the network configuration process (i.e., creating a or ) is identical in both standard and distributed training. Distributed on Spark differs from local training in two respects, however: how data is loaded, and how training is set up (requiring some additional cluster-specific configuration).\n\nThe typical workflow for training a network on a Spark cluster (using spark-submit) is as follows.\n\nNote: For single machine training, Spark local can be used with DL4J, though this is not recommended (due to the synchronization and serialization overheads of Spark). Instead, consider the following:\n\nThe current version of DL4J uses a process of parameter averaging in order to train a network. Future versions may additionally include other distributed network training approaches.\n\nThe process of training a network using parameter averaging is conceptually quite simple:\n\nFor example, the diagram below shows the parameter averaging process with 5 workers (W1, \u2026, W5) and a parameter averaging frequency of 1. Just as with offline training, a training data set is split up into a number of subsets (generally known as minibatches, in the non-distributed setting); training proceeds over each split, with each worker getting a subset of the split. In practice, the number of splits is determined automatically, based on the training configuration (based on number of workers, averaging frequency and worker minibatch sizes - see configuration section).\n\nThis section shows the minimal set of components that you need in order to train a network on Spark. Details on the various approaches to loading data are forthcoming.\n\nDue to the spark networks being a wrapper around the multi layer network and computation graph apis, you have to obtain the underlying final network from the spark network after it is done training. The reason for this is due to the fact the data parallel training is actually averaging several networks at once during the training. This means that there is no \u201cone\u201d network until you get to the final output which is an averaged set of parameters accumulated across several workers.\n\nKnowing this, we should obtain the underlying reference on both SparkComputationGraph and SparkDl4jMultiLayer respectively using the methods getNetwork and getNetwork respectively for each kind of wrapper.\n\nYou\u2019ll note that the fit output returns the same underlying network as well directly. In that case youc an just use:\n\nA TrainingMaster in DL4J is an abstraction (interface) that allows for multiple different training implementations to be used with SparkDl4jMultiLayer and SparkComputationGraph.\n\nCurrently DL4J has one implementation, the ParameterAveragingTrainingMaster. This implements the parameter averaging process shown in the image above. To create one, use the builder pattern:\n\nThe ParameterAveragingTrainingMaster defines a number of configuration options that control how training is executed:\n\nTo use DL4J on Spark, you\u2019ll need to include the deeplearning4j-spark dependency:\n\nNote that is a Maven property with the value or and should match the version of Spark you are using.\n\nThe Deeplearning4j examples repo (old examples here) contains a number of Spark examples.\n\nThere are some issues to be aware of when running Deeplearning4j with CUDA on Spark, with cluster managers such as YARN.\n\nYARN is a commonly used cluster resource management/scheduling tool for Hadoop clusters. Running Deeplearning4j with GPUs on Spark+YARN is possible, but (unlike memory and CPU resources) YARN does not track/handle GPUs as a resource. Consequently, some additional steps are typically required to run DL4J with GPUs on YARN. This is problematic for two reasons:\n\nA workaround to this is to utilize node labels (which are available with YARN versions 2.6 or greater). Node labels provide a way to tag or group machines based on your own criteria - in this case, the presence or absence of GPUs. After enabling node labels in the YARN configuration, node labels can be created, and then can be assigned to machines in the cluster. Spark jobs can then be limited to using GPU-containing machines only, by specifying a node label.\n\nSome resources on node labels - and how to configure them - can be found at the following links:\n\nNote that multiple labels can be assigned to each node. Multiple labels can be utilized to obtain more fine-grained (albeit manual) control over job/node scheduling, and is a possible workaround to avoid YARN over-allocating GPU resources.\n\nThere are some issues to be aware of when using DL4J on Spark/YARN and GPUs.\n\nThree workarounds are available to avoid failures due to scheduling:\n\nFirst, use multiple labels to manually control scheduling, as discussed above.\n\nSecond, allocate sufficient resources (cores, memory) to the containers to ensure no other GPU-utilizing tasks are scheduled on each node.\n\nThird, it is possible to utilize containers (specifically, the Docker container executor), where the GPU is declared as being used in the container, via the cgroup - this ensures that GPU is not allocated to multiple tasks. A simpler approach is to use nvidia-docker containers, which handles this declaration for you.\n\nFurther details on the Docker container executor can be found here.\n\nApache Mesos is an another cluster resource management tool. Unlike YARN, Mesos has built-in support for Nvidia GPUs (since version 1.0.0), and does not have the GPU over-allocation issues of YARN. Consequently, (all other things being equal) running DL4J+GPUs on Mesos is easier than running the same task on YARN.\n\nApache Hadoop YARN is a commonly used resource manager for Hadoop clusters (Apache Mesos being an alternative). When submitting a job to a cluster via Spark submit, it is necessary to specify a small number of configuration options, such as the number of executors, the number of cores per executor and amount of memory for each executor.\n\nTo get the best performance out of DL4J when training on Spark (and to avoid exceeding memory limits), some additional memory configuration is required. This section explains why this is necessary, and how to do it in practice.\n\nDeeplearning4j is built upon the numerical computing library ND4J. The neural network implementations in DL4J are built using the matrix and vector operations in ND4J.\n\nOne key design aspect of ND4J is the fact that it utilizes off-heap memory management. This means that the memory allocated for INDArrays by ND4J is not allocated on on the JVM heap (as a standard Java object would be); instead, it is allocated in a separate pool of memory, outside of the JVM. This memory management is implemented using JavaCPP.\n\nOff-heap memory management provides a number of benefits. Most notably, it allows for efficient use of high-performance native (c++) code for numerical operations (using BLAS libraries such as OpenBLAS and Intel MKL, as well as the C++ library Libnd4j). Off-heap memory management is also necessary for efficient GPU operations with CUDA. If memory was allocated on the JVM heap (as it is in some other JVM BLAS implementations), it would be necessary to first copy the data from the JVM, perform the operations, and then copy the result back - adding both a memory and time overhead to each operation. Instead, ND4J can simply pass pointers around for numerical operations - entirely avoiding the data copying issue.\n\nThe important point here is that the on-heap (JVM) memory and off-heap (ND4J/JavaCPP) are two separate memory pools. It is possible to configure the size of each independently; by default, JavaCPP will allow the off-heap memory allocation to grow as large as the Runtime.maxMemory() setting (see: code) - this default is essentially equivalent to the size of the JVM \u2018Xmx\u2019 memory setting, used for configuring Java memory.\n\nTo manually control the maximum amount of off-heap memory that JavaCPP can allocate, we can set the system property. For a single JVM run locally, we would pass to limit the off-heap memory allocation to 1GB. We will see how to configure this for Spark on YARN in a later section.\n\nAs noted, YARN is a cluster resource manager. When submitting a compute task (such as DL4J Spark network training) to a YARN-managed cluster, it is YARN that is responsible for managing the allocation of a limited pool of resources (memory, CPU cores) to your job (and all other jobs). For more details on YARN and resource allocation, see this and this.\n\nThe key points for our purposes are as follows:\n\nThere are two key configuration options for controlling how much memory YARN will allocate to a container.\n\nBy default, the setting is equal to 10% of the executor memory, with a minimum of 384 MB. For more details, see the Apache Spark documentation for YARN.\n\nBecause of the extensive use of off-heap memory by ND4J, it is generally necessary to increase the memory overhead setting when training on Spark.\n\nTo recap the previous sections, when running distributed neural network training on Spark via YARN, it is necessary to do the following:\n\nWhen setting these values, there are some things to keep in mind. First, the sum of and must be less than the maximum amount of memory that YARN will allocate to a single container. You can generally find this limit in the YARN configuration or YARN resource manager web UI. If you exceed this limit, YARN is likely to reject your job.\n\nSecond, the value for should be strictly less than . Recall by default the memoryOverhead setting is 10% of the executor memory - this is because the JVM itself (and possibly other libraries) may require some off-heap memory. Consequently, we don\u2019t want JavaCPP to use up the entire non-JVM allocation of memory.\n\nThird, because DL4J/ND4J makes use off-heap memory for data, parameters and activations, we can afford to allocate less to the JVM (i.e., executor.memory) than we might otherwise do. Of course, we still require enough JVM memory for Spark itself (and any other libraries we are using), so we don\u2019t want to reduce this too much.\n\nHere\u2019s an example. Suppose we are running Spark training, and want to configure our memory as follows:\n\nThe total off-heap memory is 5+1=6GB; the total memory (JVM + off-heap/overhead) is 4+6=10GB, which is less than the YARN maximum allocation of 11GB. Note that the JavaCPP memory is specified in bytes, and 5GB is 5,368,709,120 bytes; YARN memory overhead is specified in MB, and 6GB is 6,144MB.\n\nThe arguments for Spark submit would be specified as follows:\n\nConfiguring Spark locality settings is an optional configuration option that can improve training performance.\n\nThe summary: adding to your Spark submit configuration may reduce training times, by scheduling the network fit operations to be started sooner.\n\nSpark has a number of configuration options for how it controls execution. One important component of this is the settings around locality. Locality, simply put, refers to where data is relative to where data can be processed. Suppose an executor is free, but data would have to be copied across the network, in order to process it. Spark must decide whether it should execute that network transfer, or if instead it should wait for an executor that has local access to the data to become free. By default, instead of transferring data immediately, Spark will wait a bit before transferring data across the network to a free executor. This default behaviour might work well for other tasks, but isn\u2019t an ideal fit for maximizing cluster utilization when training networks with Deeplearning4j.\n\nDeep learning is computationally intensive, and hence the amount of computation per input DataSet object is relatively high. Furthermore, during Spark training, DL4J ensures there is exactly one task (partition) per executor. Consequently, we are always better off immediately transferring data to a free executor, rather than waiting for another executor to become free. The computation time will outweigh any network transfer time. The way we can instruct Spark to do this is to add to our Spark submit configuration.\n\nFor more details, see the Spark Tuning Guide - Data Locality and Spark Configuration Guide.\n\nDeeplearning4j\u2019s Spark training implementation has the ability to collect performance information (such as how long it takes to create the inital network, receive broadcast data, perform network fitting operations, etc). This information can be useful to isolate and debug any performance issues when training a network with Deeplearning4j on Spark.\n\nTo collect and export these performance statistics, use the following:\n\nNote that as of Deeplearning4j version 0.6.0, the current HTML rendering implementation doesn\u2019t scale well to a large amount of stats: i.e., large clusters and long-running jobs. This is being worked on and will be improved in future releases.\n\nOne of the charts (Worker fit(DataSet) times) available via Spark Stats\n\nBy default, the Spark training performance stats rely on a Network Time Protocal (NTP) implementation to ensure that the event timestamps correspond across machines. Without this, there is no guarantee that clocks on each worker machine are accurate - they could be incorrect by an arbitrary/unknown amount. Without a NTP implementation, accurately plotting of timeline information (shown in the timeline figure above) is impossible.\n\nIt is possible to get errors like . Sometimes these failures are transient (later retries will work) and can be ignored. However, if the Spark cluster is configured such that one or more of the workers cannot access the internet (or specifically, the NTP server), all retries can fail.\n\nTwo solutions are available:\n\nTo use the system clock time source, add the following to Spark submit:\n\nSpark has some quirks regarding how it handles Java objects with large off-heap components, such as the DataSet and INDArray objects used in Deeplearning4j. This section explains the issues related to caching/persisting these objects.\n\nThe key points to know about are:\n\nOne of the way that Apache Spark improves performance is by allowing users to cache data in memory. This can be done using the or to store the contents in-memory, in deserialized (i.e., standard Java object) form. The basic idea is simple: if you persist a RDD, you can re-use it from memory (or disk, depending on configuration) without having to recalculate it. However, large RDDs may not entirely fit into memory. In this case, some parts of the RDD have to be recomputed or loaded from disk, depending on the storage level used. Furthermore, to avoid using too much memory, Spark will drop parts (blocks) of an RDD when required.\n\nThe main storage levels available in Spark are listed below. For an explanation of these, see the Spark Programming Guide.\n\nThe problem with Spark is how it handles memory. In particular, Spark will drop part of an RDD (a block) based on the estimated size of that block. The way Spark estimates the size of a block depends on the persistence level. For and persistence, this is done by walking the Java object graph - i.e., look at the fields in an object and recursively estimate the size of those objects. This process does not however take into account the off-heap memory used by Deeplearning4j or ND4J. For objects like DataSets and INDArrays (which are stored almost entirely off-heap), Spark significantly under-estimates the true size of the objects using this process. Furthermore, Spark considers only the amount of on-heap memory use when deciding whether to keep or drop blocks. Because DataSet and INDArray objects have a very small on-heap size, Spark will keep too many of them around with and persistence, resulting in off-heap memory being exhausted, causing out of memory issues.\n\nHowever, for and Spark stores blocks in serialized form, on the Java heap. The size of objects stored in serialized form can be estimated accurately by Spark (there is no off-heap memory component for the serialized objects) and consequently Spark will drop blocks when required - avoiding any out of memory issues.\n\nKryo is a serialization library commonly used with Apache Spark. It proposes to increase performance by reducing the amount of time taken to serialize objects. However, Kryo has difficulties working with the off-heap data structures in ND4J. To use Kryo serialization with ND4J on Apache Spark, it is necessary to set up some extra configuration for Spark. If Kryo is not correctly configured, it is possible to get NullPointerExceptions on some of the INDArray fields, due to incorrect serialization.\n\nTo use Kryo, add the appropriate nd4j-kryo dependency and configure the Spark configuration to use the Nd4j Kryo Registrator, as follows:\n\nNote that when using Deeplearning4j\u2019s SparkDl4jMultiLayer or SparkComputationGraph classes, a warning will be logged if the Kryo configuration is incorrect.\n\nReleases of DL4J available on Maven Cental are distributed with OpenBLAS. Thus this section does not apply to users who are using using versions of Deeplearning4j on Maven Central.\n\nIf DL4J is built from source with Intel MKL as the BLAS library, some additional configuration is required to make this work on Amazon Elastic MapReduce. When creating a cluster in EMR, to use Intel MKL it is necessary to provide some additional configuration.\n\nUnder the Create Cluster -> Advanced Options -> Edit Software Settings, add the following:\n\nWhen running a Spark on YARN cluster on Ubuntu 16.04 machines, chances are that after finishing a job, all processes owned by the user running Hadoop/YARN are killed. This is related to a bug in Ubuntu, which is documented at https://bugs.launchpad.net/ubuntu/+source/procps/+bug/1610499. There\u2019s also a Stackoverflow discussion about it at http://stackoverflow.com/questions/38419078/logouts-while-running-hadoop-under-ubuntu-16-04.\n\nCopy the /bin/kill binary from Ubuntu 14.04 and use that one instead.", 
        "title": "Deep Learning on Apache Spark - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/iterativereduce", 
        "text": "Understanding Iterative Reduce is easier if you start with its simpler predecessor, MapReduce.\n\nMapReduce is a technique for processing very large datasets simultaneously over many cores. Jeff Dean of Google introduced the method in a 2004 research paper, and Doug Cutting implemented a similar structure a year later at Yahoo. Cutting\u2019s project would eventually become Apache Hadoop. Both projects were created to batch index the Web, and have since found many other applications.\n\nThe word MapReduce refers to two methods derived from functional programming. Map is an operation that applies the same computation to every element in a list of values, producing a new list of values. Reduce is an operation that applies to a list of values and combines them into a smaller number of values.\n\nFor example, in their simplest form, map and reduce can understood with the example of a counting words in a vocab: given a vocabulary, map gives each instance of every word in the vocab a value of 1 in a key-value pair; reduce sums the 1s associated with each vocab word, creating the word count.\n\nMapReduce operates on a larger scale. Map breaks down a very large job by distributing data to many cores, and running the same operation(s) on those data shards. Reduce consolidates all those scattered and transformed shards into one dataset, gathering all the work into one place and applying an additional operation. Map explodes and Reduce collapses, like a star expands to become a Red Giant, and shrinks to a White Dwarf.\n\nWhile a single pass of MapReduce performs fine for many use cases, it is insufficient for machine- and deep learning, which are iterative in nature, since a model \u201clearns\u201d with an optimization algorithm that leads it to a point of minimal error over many steps.\n\nYou can think of Iterative MapReduce, also inspired by Jeff Dean, as a YARN framework that makes multiple passes on the data, rather than just one. While the architecture of Iterative Reduce is different from MapReduce, on a high level you can understand it as a sequence of map-reduce operations, where the output of MapReduce1 becomes the input of MapReduce2 and so forth.\n\nLet\u2019s say you have a deep-belief net that you want to train on a very large dataset to create a model that accurately classifies your inputs. A deep-belief net is composed of three functions: a scoring function that maps inputs to classifications; an error function that measures the difference between the model\u2019s guesses and the correct answer; and optimization algorithm that adjusts the parameters of your model until they make the guesses with the least amount of error.\n\nMap places all those operations on each core in your distributed system. Then it distributes batches of your very large input dataset over those many cores. On each core, a model is trained on the input it receives. Reduce takes all those models and averages the parameters, before sending the new, aggregate model back to each core. Iterative Reduce does that many times until learning plateaus and error ceases to shrink.\n\nThe image, created by Josh Patterson, below compares the two processes. On the left, you have a close-up of MapReduce; on the right, of Iterative. Each \u201cProcessor\u201d is a deep-belief network at work, learning on batches of a larger dataset; each \u201cSuperstep\u201d is an instance of parameter averaging, before the central model is redistributed to the rest of the cluster.\n\nBoth Hadoop and Spark are distributed run-times that perform a version of MapReduce and Iterative Reduce. Deeplearning4j works as a job within Hadoop/YARN or Spark. It can be called, run and provisioned as a YARN app, for example.\n\nIn Hadoop, Iterative Reduce workers sit on the their splits, or blocks of HDFS, and process data synchronously in parallel, before they send the their transformed parameters back to the master, where the parameters are averaged and used to update the model on each worker\u2019s core. With MapReduce, the map path goes away, but Iterative Reduce workers stay resident. This architecture is roughly similar to Spark.\n\nTo provide a little context about the state-of-the-art, both Google and Yahoo operate parameter servers that store billions of parameters which are then distributed to the cluster for processing. Google\u2019s is called the Google Brain, which was created by Andrew Ng and is now led by his student Quoc Le. Here\u2019s a rough picture of the Google production stack circa 2015 to show you how MapReduce fits in.\n\nDeeplearning4j considers distributed run-times to be interchangeable (but not necessarily equal); they are all simply a directory in a larger modular architecture that can be swapped in or out. This allows the overall project to evolve at different speeds, and separate run-times from other modules devoted to neural net algorithms on the one hand, and hardware on the other. Deeplearning4j users are also able to build a standalone distributed architecture via Akka, spinning out nodes on AWS.\n\nEvery form of scaleout including Hadoop and Spark is included in our scaleout repository.\n\nLines of Deeplearning4j code can be intermixed with Spark, for example, and DL4J operations will be distributed like any other.", 
        "title": "Iterative Reduce With DL4J on Hadoop and Spark - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/visualization", 
        "text": "Note: This information here pertains to DL4J versions 0.7.0 and later.\n\nDL4J Provides a user interface to visualize in your browser (in real time) the current network status and progress of training. The UI is typically used to help with tuning neural networks - i.e., the selection of hyperparameters (such as learning rate) to obtain good performance for a network.\n\nStep 1: Add the Deeplearning4j UI dependency to your project.\n\nNote the suffix: this is the Scala version (due to using the Play framework, a Scala library, for the backend). If you are not using other Scala libraries, either or is OK.\n\nStep 2: Enable the UI in your project\n\nThis is relatively straightforward:\n\nTo access the UI, open your browser and go to . You can set the port by using the system property: i.e., to use port 9001, pass the following to the JVM on launch:\n\nInformation will then be collected and routed to the UI when you call the method on your network.\n\nExample: See a UI example here\n\nThe full set of UI examples are available here.\n\nThe overview page (one of 3 available pages) contains the following information:\n\nNote that for the bottom two charts, these are displayed as the logarithm (base 10) of the values. Thus a value of -3 on the update: parameter ratio chart corresponds to a ratio of 10-3 = 0.001.\n\nThe ratio of parameters to updates is specifically the ratio of mean magnitudes of these values.\n\nSee the later section of this page on how to use these values in practice.\n\nThe model page contains a graph of the neural network layers, which operates as a selection mechanism. Click on a layer to display information for it.\n\nOn the right, the following charts are available, after selecting a layer:\n\nNote: parameters are labeled as follows: weights (W) and biases (b). For recurrent neural networks, W refers to the weights connecting the layer to the layer below, and RW refers to the recurrent weights (i.e., those between time steps).\n\nThe DL4J UI can be used with Spark. However, as of 0.7.0, conflicting dependencies mean that running the UI and Spark is the same JVM can be difficult.\n\nTwo alternatives are available:\n\nCollecting Stats for Later Offline Use\n\nThen, later you can load and display the saved information using:\n\nFirst, in the JVM running the UI (note this is the server):\n\nThis will require the or dependency. (NOTE THIS IS NOT THE CLIENT THIS IS YOUR SERVER - SEE BELOW FOR THE CLIENT WHICH USES: deeplearning4j-ui-model)\n\nClient (both spark and standalone neural networks using simple deeplearning4j-nn) Second, for your neural net (Note this example is for spark, but computation graph and multi layer network both have the equivalemtn setListeners method with the same usage, example found here):\n\nTo avoid dependency conflicts with Spark, you should use the dependency to get the StatsListener, not the full UI dependency.\n\nYou need to use the above method if you are on a newer scala version. See the linked example above for the client.\n\nNote: you should replace with the IP address of the machine running the user interface instance.\n\nHere\u2019s an excellent web page by Andrej Karpathy about visualizing neural net training. It is worth reading and understanding that page first.\n\nTuning neural networks is often more an art than a science. However, here\u2019s some ideas that may be useful:\n\nThe score vs. iteration should (overall) go down over time.\n\nOverview Page and Model Page - Using the Update: Parameter Ratio Chart\n\nThis chart can be used to detect vanishing or exploding activations (due to poor weight initialization, too much regularization, lack of data normalization, or too high a learning rate).\n\nThe layer parameters histogram is displayed for the most recent iteration only.\n\nThe layer update histogram is displayed for the most recent iteration only.\n\nThis chart simply shows the learning rates of the parameters of selected layer, over time.\n\nIf you are not using learning rate schedules, the chart will be flat. If you are using learning rate schedules, you can use this chart to track the current value of the learning rate (for each parameter), over time.\n\nWe rely on TSNE to reduce the dimensionality of word feature vectors and project words into a two or three-dimensional space. Here\u2019s some code for using TSNE with Word2Vec:\n\nA possible exception that can occur with the DL4J UI is the following:\n\nThis exception is not due to DL4J directly, but is due to a missing application.conf file, required by the Play framework (the library that DL4J\u2019s UI is based on). This is originally present in the deeplearning4j-play dependency: however, if an uber-jar (i.e., a JAR file with dependencies) is built (say, via ), it may not be copied over correctly. For example, using the has caused this exception for some users.\n\nThe recommended solution (for Maven) is to use the Maven Shade plugin to produce an uber-jar, configured as follows:\n\nThen, create your uber-jar with and run via . Note the \u201c-bin\u201d suffix for the generated JAR file: this includes all dependencies.\n\nNote also that this Maven Shade approach is configured for DL4J\u2019s examples repository.", 
        "title": "How to Visualize, Monitor and Debug Neural Network Learning - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/troubleshootingneuralnets", 
        "text": "Neural networks can be difficult to tune. If the network hyperparameters are poorly chosen, the network may learn slowly, or perhaps not at all. This page aims to provide some baseline steps you should take when tuning your network.\n\nMany of these tips have already been discussed in the academic literature. Our purpose is to consolidate them in one site and express them as clearly as possible.\n\nWhat\u2019s distribution of your data? Are you scaling it properly? As a general rule:\n\nNote that it\u2019s very important to use the exact same normalization method for both the training data and testing data.\n\nDeeplearning4j supports several different kinds of weight initializations with the weightInit parameter. These are set using the .weightInit(WeightInit) method in your configuration.\n\nYou need to make sure your weights are neither too big nor too small. Xavier weight initialization is usually a good choice for this. For networks with rectified linear (relu) or leaky relu activations, RELU weight initialization is a sensible choice.\n\nAn epoch is defined as a full pass of the data set.\n\nToo few epochs don\u2019t give your network enough time to learn good parameters; too many and you might overfit the training data. One way to choose the number of epochs is to use early stopping. Early stopping can also help to prevent the neural network from overfitting (i.e., can help the net generalize better to unseen data).\n\nThe learning rate is one of, if not the most important hyperparameter. If this is too large or too small, your network may learn very poorly, very slowly, or not at all. Typical values for the learning rate are in the range of 0.1 to 1e-6, though the optimal learning rate is usually data (and network architecture) specific. Some simple advice is to start by trying three different learning rates \u2013 1e-1, 1e-3, and 1e-6 \u2013 to get a rough idea of what it should be, before further tuning this. Ideally, they run models with different learning rates simultaneously to save time.\n\nThe usual approach to selecting an appropriate learning rate is to use DL4J\u2019s visualization interface to visualize the progress of training. You want to pay attention to both the loss over time, and the ratio of update magnitudes to parameter magnitudes (a ratio of approximately 1:1000 is a good place to start). For more information on tuning the learning rate, see this link.\n\nFor training neural networks in a distributed manner, you may need a different (frequently higher) learning rate compared to training the same network on a single machine.\n\nYou can optionally define a learning rate policy for your neural network. A policy will change the learning rate over time, achieving better results since the learning rate can \u201cslow down\u201d to find closer local minima for convergence. A common policy used is scheduling. See the LeNet example for a learning rate schedule used in practice.\n\nNote that if you\u2019re using multiple GPUs, this will affect your scheduling. For example, if you have 2x GPUs, then you will need to divide the iterations in your schedule by 2, since the throughput of your training process will be double, and the learning rate schedule is only applicable to the local GPU.\n\nThere are two aspects to be aware of, with regard to the choice of activation function.\n\nFirst, the activation function of the hidden (non-output) layers. As a general rule, \u2018relu\u2019 or \u2018leakyrelu\u2019 activations are good choices for this. Some other activation functions (tanh, sigmoid, etc) are more prone to vanishing gradient problems, which can make learning much harder in deep neural networks. However, for LSTM layers, the tanh activation function is still commonly used.\n\nSecond, regarding the activation function for the output layer: this is usually application specific. For classification problems, you generally want to use the softmax activation function, combined with the negative log likelihood / MCXENT (multi-class cross entropy). The softmax activation function gives you a probability distribution over classes (i.e., outputs sum to 1.0). For regression problems, the \u201cidentity\u201d activation function is frequently a good choice, in conjunction with the MSE (mean squared error) loss function.\n\nLoss functions for each neural network layer can either be used in pretraining, to learn better weights, or in classification (on the output layer) for achieving some result. (In the example above, classification happens in the override section.)\n\nYour net\u2019s purpose will determine the loss function you use. For pretraining, choose reconstruction entropy. For classification, use multiclass cross entropy.\n\nRegularization methods can help to avoid overfitting during training. Overfitting occurs when the network predicts the training set very well, but makes poor predictions on data the network has never seen. One way to think about overfitting is that the network memorizes the training data (instead of learning the general relationships in it).\n\nTo use l1/l2/dropout regularization, use .regularization(true) followed by .l1(x), .l2(y), .dropout(z) respectively. Note that z in dropout(z) is the probability of retaining an activation.\n\nA minibatch refers to the number of examples used at a time, when computing gradients and parameter updates. In practice (for all but the smallest data sets), it is standard to break your data set up into a number of minibatches.\n\nThe ideal minibatch size will vary. For example, a minibatch size of 10 is frequently too small for GPUs, but can work on CPUs. A minibatch size of 1 will allow a network to train, but will not reap the benefits of parallelism. 32 may be a sensible starting point to try, with minibatches in the range of 16-128 (sometimes smaller or larger, depending on the application and type of network) being common.\n\nIn DL4J, the term \u2018updater\u2019 refers to training mechanisms such as momentum, RMSProp, adagrad, and others. Using one of these methods can result in much faster network training companed to \u2018vanilla\u2019 stochastic gradient descent. You can set the updater using the .updater(Updater) configuration option.\n\nThe optimization algorithm is how updates are made, given the gradient. The simplest (and most commonly used) method is stochastic gradient descent (SGD), however DL4J also provides SGD with line search, conjugate gradient and LBFGS optimization algorithms. These latter algorithms are more powerful compared to SGD, but considerably more costly per parameter update due to a line search component, and aren\u2019t used as much in practice. Note that you can in principle combine any updater with any optimization algorithm.\n\nA good default choice in most cases is to use the stochastic gradient descent optimization algorithm combined with one of the momentum/rmsprop/adagrad updaters, with momentum frequently being used in practice. Note that for momentum, the updater is called NESTEROVS (a reference to the Nesterovs variant of momentum), and the momentum rate can be set by the .momentum(double) option.\n\nWhen training a neural network, it can sometimes be helpful to apply gradient normalization, to avoid the gradients being too large (the so-called exploding gradient problem, common in recurrent neural networks) or too small. This can be applied using the .gradientNormalization(GradientNormalization) and .gradientNormalizationThreshould(double) methods. For an example of gradient normalization see, GradientNormalization.java. The test code for that example is here.\n\nWhen training recurrent networks with long time series, it is generally advisable to use truncated backpropagation through time. With \u2018standard\u2019 backpropagation through time (the default in DL4J) the cost per parameter update can become prohibative. For more details, see this page and this glossary entry.\n\nWhen using a deep-belief network, pay close attention here. An RBM (the component of the DBN used for feature extraction) is stochastic and will sample from different probability distributions relative to the visible or hidden units specified.\n\nSee Geoff Hinton\u2019s definitive work, A Practical Guide to Training Restricted Boltzmann Machines, for a list of all of the different probability distributions.\n\nWhen creating hidden layers for autoencoders that perform compression, give them fewer neurons than your input data. If the hidden-layer nodes are too close to the number of input nodes, you risk reconstructing the identity function. Too many hidden-layer neurons increase the likelihood of noise and overfitting. For an input layer of 784, you might choose an initial hidden layer of 500, and a second hidden layer of 250. No hidden layer should be less than a quarter of the input layer\u2019s nodes. And the output layer will simply be the number of labels.\n\nLarger datasets require more hidden layers. Facebook\u2019s Deep Face uses nine hidden layers on what we can only presume to be an immense corpus. Many smaller datasets might only require three or four hidden layers, with their accuracy decreasing beyond that depth. As a rule: larger data sets contain more variation, which require more features/neurons for the net to obtain accurate results. Typical machine learning, of course, has one hidden layer, and those shallow nets are called Perceptrons.\n\nLarge datasets require that you pretrain your RBM several times. Only with multiple pretrainings will the algorithm learn to correctly weight features in the context of the dataset. That said, you can run the data in parallel or through a cluster to speed up the pretraining.\n\nQ. Why is my Neural Network throwing nan values?\n\nA. Backpropagation involves the multiplication of very small gradients, due to limited precision when representing real numbers values very close to zero can not be represented. The term for this issue is Arithmetic Underflow. If your Neural Network is throwing nan\u2019s then the solution is to retune your network to avoid the very small gradients. This is more likely an issue with deeper Neural Networks.\n\nYou can try using double data type but it\u2019s usually recommended to retune the net first.\n\nFollowing the basic tuning tips and monitoring the results is the way to ensure NAN doesn\u2019t show up anymore.", 
        "title": "Troubleshooting Neural Net Training - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/earlystopping", 
        "text": "When training neural networks, numerous decisions need to be made regarding the settings (hyperparameters) used, in order to obtain good performance. Once such hyperparameter is the number of training epochs: that is, how many full passes of the data set (epochs) should be used? If we use too few epochs, we might underfit (i.e., not learn everything we can from the training data); if we use too many epochs, we might overfit (i.e., fit the \u2018noise\u2019 in the training data, and not the signal).\n\nEarly stopping attempts to remove the need to manually set this value. It can also be considered a type of regularization method (like L1/L2 weight decay and dropout) in that it can stop the network from overfitting.\n\nThe idea behind early stopping is relatively simple:\n\nThis is shown graphically below:\n\nThe best model is the one saved at the time of the vertical dotted line - i.e., the model with the best accuracy on the test set.\n\nUsing DL4J\u2019s early stopping functionality requires you to provide a number of configuration options:\n\nAn example, with an epoch termination condition of maximum of 30 epochs, a maximum of 20 minutes training time, calculating the score every epoch, and saving the intermediate results to disk:\n\nThe source code for the built in termination classes are in this directory\n\nYou can of course implement your own iteration and epoch termination conditions.\n\nThe early stopping implementation described above will only work with a single device. However, provides similar functionality as early stopping and allows you to optimize for either multiple CPUs or GPUs. wraps your model in a class and performs localized distributed training.\n\nNote that doesn\u2019t support all of the functionality as its single device counterpart. It is not UI-compatible and may not work with complex iteration listeners. This is due to how the model is distributed and copied in the background.\n\nTestParallelEarlyStopping.java gives a good example of setting up parallel early stopping in different scenarios.", 
        "title": "Early Stopping - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/output", 
        "text": "The output of neural networks can be hard to interpret. You\u2019ll probably ask yourself questions like \u201cHow accurate is my model?\u201d or \u201cWhat do these numbers mean?\u201d Not only that, but each kind of neural network has a different kind of output.\n\nFor example, in unsupervised learning using a restricted boltzmann machine or a Variational AutoEncoder the network learns to reconstruct input. This is best illustrated by discussing images. The best way to interpret reconstruciton is to see how well an image is reconstructed after the data is \u201cnoised\u201d and fed into a neural network.\n\nAs you follow the MNIST tutorial, the best way to gauge your performace is to compare the initial image and network output side by side.\n\nAnother example would be multilayer networks. Multilayer networks are discriminatory classifiers; that is, they label things.\n\nIn machine learning, one metric used to determine how well a classifier performs is called the f1 score. The f1 score is a number between zero and one that explains how well the network performed during training. It is analogous to a percentage, with 1 being the equivalent of 100 percent predictive accuracy.\n\nDL4J has a class called Evaluation that will output f1 scores for you. The Evaluation class can also print out False Positive/negative rates, Confusion Matrix, and more.\n\nImagine each label is a binary matrix of 1 row and, say, 10 columns, with each column representing a number from one to 10. (The number of columns will actually vary with the number of possible outcomes, or labels.) There can only be a single 1 in this matrix, and it is located in the column representing the number labeled. That is, [0 1 0 0 0 0 0 0 0 0] means two, and so forth. Each label is then assigned a likelihood of how accurately it describes the input, according to the features recognized by your network. Those probabilities are the network\u2019s guesses. At the end of your test, you compare the highest-probability label with the actual number of the input. The aggregate of these comparisons is your accuracy rate, or f score.\n\nIn fact, you can enter any number of inputs into the network simultaneously. Each of them will be a row in your binary matrix. And the number of rows in your binary input matrix will be equal to the number of rows in your binary out matrix of guesses.\n\nThe Evaluation class is often used at the end of training to verify accuracy.", 
        "title": "Output Interpretation - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/spark-gpus", 
        "text": "Deeplearning4j trains deep neural networks on distributed GPUs using Spark. Specifically, we show the use of Spark to load data and GPUs to process images with cuDNN.\n\nDeeplearning4j includes libraries for the automatic tuning of neural networks, deployment of those neural-net models, visualization and integrations with other data pipeline tools that make dealing with data on production clusters much easier.\n\nThis post is a simple introduction to each of those technologies, which we\u2019ll define below. It looks at each individually, and at the end it shows with code how Deeplearning4j pulls them together in an image-processing example.\n\nIn this post, we will cover the below technologies and their interactions:\n\nAs an open-source, distributed run-time, Spark can orchestrate multiple host threads. It was the Apache Foundation\u2019s most popular project last year. Deeplearning4j only relies on Spark as a data-access layer for a cluster, since we have heavy computation needs that require more speed and capacity than Spark currently provides. It\u2019s basically fast ETL (extract transform load) or data storage and access for the hadoop ecosystem (HDFS or hadoop file system). The goal is to leverage hadoop\u2019s data locality mechanisms while speeding up compute with native computations.\n\nSpark accomplishes this via a construct called an RDD, or Resilient Distributed Dataset. The RDD construct provides us a functional interface to data partitioned across a cluster. Below you will see us use RDDs for loading data and passing a RDD of Dataset (a DL4J construct containing a feature matrix and a label matrix).\n\nNow, CUDA is NVIDIA\u2019s parallel computing platform and API model, a software layer that gives access to GPUs\u2019 lower-level instructions, and which works with C, C++ and FORTRAN. Deeplearning4j interacts with the GPU and CUDA via a mix of custom CUDA kernels and Java Native Interface.\n\ncuDNN stands for the CUDA Deep Neural Network Library, and it was created by the GPU maker NVIDIA. cuDNN is a library of primitives for standard deep learning routines: forward and backward convolution, pooling, normalization, and activation layers.\n\ncuDNN is one of the fastest libraries for deep convolutional networks (and more recently, for recurrent nets). It ranks at or near the top of several image-processing benchmarks conducted by Soumith Chintala of Facebook. Deeplearning4j wraps cuDNN via Java Native Interface, and gives the Java community easy access to it.\n\nDeeplearning4j is the most widely used open-source deep learning tool for the JVM, including the Java, Scala and Clojure communities. Its aim is to bring deep learning to the production stack, integrating tightly with popular big data frameworks like Hadoop and Spark. DL4J works with all major data types \u2013 images, text, time series and sound \u2013 and includes algorithms such as convolutional nets, recurrent nets like LSTMs, NLP tools like Word2Vec and Doc2Vec, and various types of autoencoders.\n\nDeeplearning4j is part of a set of open source libraries for building deep learning applications on the Java Virtual Machine. It is one of several open-source libraries maintained by Skymind engineers.\n\nDeeplearning4j also comes with built in Spark integration for handling distributed training of neural nets across a cluster. We use data parallelism (explained below) to scale out training on multiple computers leveraging a GPU (or 4) on each node. We use Spark for data access. We do this by training on Spark RDD partitions (portions of the data stored across a cluster)\n\nA distributed file system combined with an easy interface allows us to move compute to the data rather than the other way around, allowing us to benefit from an easy to setup way of doing distributed training without having to do a lot of the harder work ourselves.\n\nUsually. But we optimized communication by putting operations off heap. JavaCPP implements a class that makes it easy to do off-heap operations (i.e. data doesn\u2019t hit the garbage collector). This allows us to benefit from lower latency and memory management while benefiting from the managed garbage collector where it matters. This is the approach taken by many distributed systems frameworks and databases such as such as Apache Flink, Spark and Hbase.\n\nJava isn\u2019t good at linear algebra operations. They should be handled by C++, where we can benefit from hardware acceleration of floating-point operations. That\u2019s what libnd4j is for.\n\nThere are two main methods for the distributed training of neural networks: data parallelism and model parallelism.\n\nWith data parallelism, you subdivide a very large dataset into batches, and distribute those batches to parallel models running on separate hardware to train simultaneously.\n\nImagine training on an encyclopedia, subdividing it into batches of 10 pages, and distributing 10 batches to 10 models to train, then averaging the parameters of those trained models in one master model, and pushing the updated weights of the master model out to the distributed models. The model parameters are then averaged at the end of training to yield a single model.\n\nDeeplearning4j relies on data parallelism and uses Spark for distributed host thread orchestration across a cluster.\n\nSee references at the bottom of this post for some papers to dig in to.\n\nHere\u2019s an example of Deeplearning4j code that runs LeNet on Spark using GPUs.\n\nFirst we configure Spark and load the data:\n\nThen we configure the neural network:\n\nHere is a diagram of the neural net above:\n\nNote that above, we also have a more complex (but versatile) Computation Graph API for those familiar with other frameworks.\n\nAlso of note here is the builder pattern being used. Since Java doesn\u2019t have key word args, the fluent builder pattern is known as a best practice in Java land due to the complimenting tools such as IntelliJ for handling code completion. Despite its verbose nature, its also very easy to wrap in a more concise language such as Clojure or Scala.\n\nWe are going to release a Scala wrapper very similar to the Keras framework taking advantage of some of the nicer constructs of the scala language which should help usability quite a bit.\n\nThese configurations can also be defined via YAML or JSON.\n\nThen we tell Spark how to perform parameter averaging:\n\nAnd finally, we train the network by calling on .\n\nFinally if you want to go further:\n\nTo begin training deep neural networks on distributed GPUs on Spark, you will need to do two things.\n\nJoin us on gitter as well if you\u2019d like to speak to any of us live: https://gitter.im/deeplearning4j/deeplearning4j\n\nWhen running a Spark on YARN cluster on Ubuntu 16.04 machines, chances are that after finishing a job, all processes owned by the user running Hadoop/YARN are killed. This is related to a bug in Ubuntu, which is documented at https://bugs.launchpad.net/ubuntu/+source/procps/+bug/1610499. There\u2019s also a Stackoverflow discussion about it at http://stackoverflow.com/questions/38419078/logouts-while-running-hadoop-under-ubuntu-16-04.\n\nCopy the /bin/kill binary from Ubuntu 14.04 and use that one instead.\n\n[1] Training with intra-block parallel optimization and blockwise model-update filtering. In 2016\n\nPaul Tucker, Ke Yang, Quoc V Le, et al. Large scale distributed deep networks. In Advances in\n\n[4] Nikko Strom. Scalable distributed dnn training using commodity gpu cloud computing. In Six- teenth Annual Conference of the International Speech Communication Association, 2015. http:", 
        "title": "Running Deep Learning on Distributed GPUs With Spark - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/model-zoo", 
        "text": "As of 0.9.1, Deeplearning4j has a new native model zoo that can be accessed and instantiated directly from DL4J. Gone are the days of copying model configs from Github. The model zoo also includes pretrained weights for different datasets that are downloaded automatically and checked for integrity. \ud83d\ude80\n\nIf you want to use the new model zoo, you will need to add it as a dependency. A Maven POM would add the following:\n\nshould match the version of DL4J you are currently using, and should be the latest version available in Maven Central.\n\nOnce you\u2019ve successfully added the zoo dependency to your project, you can start to import and use models. Each model extends the abstract class and uses the interface. These classes provide methods that help you initialize either an empty, fresh network or a pretrained network.\n\nYou can instantly instantiate a model from the zoo using the method. For example, if you want to instantiate a fresh, untrained network of AlexNet you can use the following code:\n\nIf you want to tune parameters or change the optimization algorithm, you can obtain a reference to the underlying network configuration:\n\nSome models have pretrained weights available, and a small number of models are pretrained across different datasets. is an enumerator that outlines different weight types, which includes , , , and .\n\nFor example, you can initialize a VGG-16 model with ImageNet weights like so:\n\nAnd initialize another VGG16 model with weights trained on VGGFace:\n\nIf you\u2019re not sure whether a model contains pretrained weights, you can use the method which returns a boolean. Simply pass a enum to this method, which returns true if weights are available.\n\nNote that for convolutional models, input shape information follows the NCHW convention. So if a model\u2019s input shape default is , this means the model has 3 channels and height/width of 224.\n\nThe model zoo comes with well-known image recognition configurations in the deep learning community. The zoo also includes an LSTM for text generation, and a simple CNN for general image recognition.\n\nYou can find a complete list of models using this deeplearning4j-zoo Github link.\n\nThis includes ImageNet models such as VGG-16, ResNet-50, AlexNet, Inception-ResNet-v1, GoogLeNet, LeNet, and more.\n\nThe zoo comes with a couple additional features if you\u2019re looking to use the models for different use cases.\n\nThe allows you to select multiple models at once. This method was created for testing multiple models at once.\n\nWhen you use the selector, it returns a collection. Passing to would return a collection of type . However, if you wanted to initialize multiple models for a specific dataset, you can pass appropriate params to the selector like .\n\nLet\u2019s say you wanted to benchmark all of the models that had pretrained weights available for ImageNet. The following code allows you to select all of the convolutional models, check if weights are available, and then execute your own code:\n\nAlternatively, you can select specific models by doing:\n\nAside from passing certain configuration information to the constructor of a zoo model, you can also change its input shape using . NOTE: this applies to fresh configurations only, and will not affect pretrained models:\n\nPretrained models are perfect for transfer learning! You can read more about transfer learning using DL4J here.\n\nInitialization methods often have an additional parameter named . For the majority of users you will not need to use this; however, if you have a large machine that has \u201cbeefy\u201d specifications, you can pass . To learn more about workspaces, please see this section.", 
        "title": "Deeplearning4j Model Zoo - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/modelpersistence", 
        "text": "The is a class which handles loading and saving models. There are two methods for saving models shown in the examples through the link. The first example saves a normal multilayer network, the second one saves a computation graph.\n\nHere is a basic example with code to save a computation graph using the class, as well as an example of using ModelSerializer to save a neural net built using MultiLayer configuration.\n\nIf your model uses probabilities (i.e. DropOut/DropConnect), it may make sense to save it separately, and apply it after model is restored; i.e:\n\nThis will guarantee equal results between sessions/JVMs.", 
        "title": "Saving and Loading a Neural Network - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/tsne-visualization", 
        "text": "t-Distributed Stochastic Neighbor Embedding (t-SNE) is a data-visualization tool created by Laurens van der Maaten at Delft University of Technology.\n\nWhile it can be used for any data, t-SNE (pronounced Tee-Snee) is only really meaningful with labeled data, which clarify how the input is clustering. Below, you can see the kind of graphic you can generate in DL4J with t-SNE working on MNIST data.\n\nLook closely and you can see the numerals clustered near their likes, alongside the dots.", 
        "title": "t-SNE's Data Visualization - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/linear-regression", 
        "text": "", 
        "title": "Redirecting\u2026"
    }, 
    {
        "url": "https://deeplearning4j.org/usingrnns", 
        "text": "This document outlines the specifics training features and the practicalities of how to use them in DeepLearning4J. This document assumes some familiarity with recurrent neural networks and their use - it is not an introduction to recurrent neural networks, and assumes some familiarity with their both their use and terminology. If you are new to RNNs, read A Beginner\u2019s Guide to Recurrent Networks and LSTMs before proceeding with this page.\n\nDL4J currently supports the following types of recurrent neural network\n\nJava documentation for each is available, GravesLSTM, BidirectionalGravesLSTM, BaseRecurrent\n\nConsider for the moment a standard feed-forward network (a multi-layer perceptron or \u2018DenseLayer\u2019 in DL4J). These networks expect input and output data that is two-dimensional: that is, data with \u201cshape\u201d [numExamples,inputSize]. This means that the data into a feed-forward network has \u2018numExamples\u2019 rows/examples, where each row consists of \u2018inputSize\u2019 columns. A single example would have shape [1,inputSize], though in practice we generally use multiple examples for computational and optimization efficiency. Similarly, output data for a standard feed-forward network is also two dimensional, with shape [numExamples,outputSize].\n\nConversely, data for RNNs are time series. Thus, they have 3 dimensions: one additional dimension for time. Input data thus has shape [numExamples,inputSize,timeSeriesLength], and output data has shape [numExamples,outputSize,timeSeriesLength]. This means that the data in our INDArray is laid out such that the value at position (i,j,k) is the jth value at the kth time step of the ith example in the minibatch. This data layout is shown below.\n\nWhen importing time series data using the class CSVSequenceRecordReader each line in the data files represents one time step with the earliest time series observation in the first row (or first row after header if present) and the most recent observation in the last row of the csv. Each feature time series is a separate column of the of the csv file. For example if you have five features in time series, each with 120 observations, and a training & test set of size 53 then there will be 106 input csv files(53 input, 53 labels). The 53 input csv files will each have five columns and 120 rows. The label csv files will have one column (the label) and one row.\n\nRnnOutputLayer is a type of layer used as the final layer with many recurrent neural network systems (for both regression and classification tasks). RnnOutputLayer handles things like score calculation, and error calculation (of prediction vs. actual) given a loss function etc. Functionally, it is very similar to the \u2018standard\u2019 OutputLayer class (which is used with feed-forward networks); however it both outputs (and expects as labels/targets) 3d time series data sets.\n\nConfiguration for the RnnOutputLayer follows the same design other layers: for example, to set the third layer in a MultiLayerNetwork to a RnnOutputLayer for classification:\n\nUse of RnnOutputLayer in practice can be seen in the examples, linked at the end of this document.\n\nTraining neural networks (including RNNs) can be quite computationally demanding. For recurrent neural networks, this is especially the case when we are dealing with long sequences - i.e., training data with many time steps.\n\nTruncated backpropagation through time (BPTT) was developed in order to reduce the computational complexity of each parameter update in a recurrent neural network. In summary, it allows us to train networks faster (by performing more frequent parameter updates), for a given amount of computational power. It is recommended to use truncated BPTT when your input sequences are long (typically, more than a few hundred time steps).\n\nConsider what happens when training a recurrent neural network with a time series of length 12 time steps. Here, we need to do a forward pass of 12 steps, calculate the error (based on predicted vs. actual), and do a backward pass of 12 time steps:\n\nFor 12 time steps, in the image above, this is not a problem. Consider, however, that instead the input time series was 10,000 or more time steps. In this case, standard backpropagation through time would require 10,000 time steps for each of the forward and backward passes for each and every parameter update. This is of course very computationally demanding.\n\nIn practice, truncated BPTT splits the forward and backward passes into a set of smaller forward/backward pass operations. The specific length of these forward/backward pass segments is a parameter set by the user. For example, if we use truncated BPTT of length 4 time steps, learning looks like the following:\n\nNote that the overall complexity for truncated BPTT and standard BPTT are approximately the same - both do the same number of time step during forward/backward pass. Using this method however, we get 3 parameter updates instead of one for approximately the same amount of effort. However, the cost is not exactly the same there is a small amount of overhead per parameter update.\n\nThe downside of truncated BPTT is that the length of the dependencies learned in truncated BPTT can be shorter than in full BPTT. This is easy to see: consider the images above, with a TBPTT length of 4. Suppose that at time step 10, the network needs to store some information from time step 0 in order to make an accurate prediction. In standard BPTT, this is ok: the gradients can flow backwards all the way along the unrolled network, from time 10 to time 0. In truncated BPTT, this is problematic: the gradients from time step 10 simply don\u2019t flow back far enough to cause the required parameter updates that would store the required information. This tradeoff is usually worth it, and (as long as the truncated BPTT lengths are set appropriately), truncated BPTT works well in practice.\n\nUsing truncated BPTT in DL4J is quite simple: just add the following code to your network configuration (at the end, before the final .build() in your network configuration)\n\nThe above code snippet will cause any network training (i.e., calls to MultiLayerNetwork.fit() methods) to use truncated BPTT with segments of length 100 steps.\n\nDL4J supports a number of related training features for RNNs, based on the idea of padding and masking. Padding and masking allows us to support training situations including one-to-many, many-to-one, as also support variable length time series (in the same mini-batch).\n\nSuppose we want to train a recurrent neural network with inputs or outputs that don\u2019t occur at every time step. Examples of this (for a single example) are shown in the image below. DL4J supports training networks for all of these situations:\n\nWithout masking and padding, we are restricted to the many-to-many case (above, left): that is, (a) All examples are of the same length, and (b) Examples have both inputs and outputs at all time steps.\n\nThe idea behind padding is simple. Consider two time series of lengths 50 and 100 time steps, in the same mini-batch. The training data is a rectangular array; thus, we pad (i.e., add zeros to) the shorter time series (for both input and output), such that the input and output are both the same length (in this example: 100 time steps).\n\nOf course, if this was all we did, it would cause problems during training. Thus, in addition to padding, we use a masking mechanism. The idea behind masking is simple: we have two additional arrays that record whether an input or output is actually present for a given time step and example, or whether the input/output is just padding.\n\nRecall that with RNNs, our minibatch data has 3 dimensions, with shape [miniBatchSize,inputSize,timeSeriesLength] and [miniBatchSize,outputSize,timeSeriesLength] for the input and output respectively. The padding arrays are then 2 dimensional, with shape [miniBatchSize,timeSeriesLength] for both the input and output, with values of 0 (\u2018absent\u2019) or 1 (\u2018present\u2019) for each time series and example. The masking arrays for the input and output are stored in separate arrays.\n\nFor a single example, the input and output masking arrays are shown below:\n\nFor the \u201cMasking not required\u201d cases, we could equivalently use a masking array of all 1s, which will give the same result as not having a mask array at all. Also note that it is possible to use zero, one or two masking arrays when learning RNNs - for example, the many-to-one case could have a masking array for the output only.\n\nIn practice: these padding arrays are generally created during the data import stage (for example, by the SequenceRecordReaderDatasetIterator \u2013 discussed later), and are contained within the DataSet object. If a DataSet contains masking arrays, the MultiLayerNetwork fit will automatically use them during training. If they are absent, no masking functionality is used.\n\nMask arrays are also important when doing scoring and evaluation (i.e., when evaluating the accuracy of a RNN classifier). Consider for example the many-to-one case: there is only a single output for each example, and any evaluation should take this into account.\n\nEvaluation using the (output) mask arrays can be used during evaluation by passing it to the following method:\n\nwhere labels are the actual output (3d time series), predicted is the network predictions (3d time series, same shape as labels), and outputMask is the 2d mask array for the output. Note that the input mask array is not required for evaluation.\n\nScore calculation will also make use of the mask arrays, via the MultiLayerNetwork.score(DataSet) method. Again, if the DataSet contains an output masking array, it will automatically be used when calculating the score (loss function - mean squared error, negative log likelihood etc) for the network.\n\nSequence classification is one common use of masking. The idea is that although we have a sequence (time series) as input, we only want to provide a single label for the entire sequence (rather than one label at each time step in the sequence).\n\nHowever, RNNs by design output sequences, of the same length of the input sequence. For sequence classification, masking allows us to train the network with this single label at the final time step - we essentially tell the network that there isn\u2019t actually label data anywhere except for the last time step.\n\nNow, suppose we\u2019ve trained our network, and want to get the last time step for predictions, from the time series output array. How do we do that?\n\nTo get the last time step, there are two cases to be aware of. First, when we have a single example, we don\u2019t actually need to use the mask arrays: we can just get the last time step in the output array:\n\nAssuming classification (same process for regression, however) the last line above gives us probabilities at the last time step - i.e., the class probabilities for our sequence classification.\n\nThe slightly more complex case is when we have multiple examples in the one minibatch (features array), where the lengths of each example differ. (If all are the same length: we can use the same process as above).\n\nIn this \u2018variable length\u2019 case, we need to get the last time step for each example separately. If we have the time series lengths for each example from our data pipeline, it becomes straightforward: we just iterate over examples, replacing the in the above code with the length of that example.\n\nIf we don\u2019t have the lengths of the time series directly, we need to extract them from the mask array.\n\nIf we have a labels mask array (which is a one-hot vector, like [0,0,0,1,0] for each time series):\n\nAlternatively, if we have only the features mask: One quick and dirty approach is to use this:\n\nTo understand what is happening here, note that originally we have a features mask like [1,1,1,1,0], from which we want to get the last non-zero element. So we map [1,1,1,1,0] -> [1,2,3,4,0], and then get the largest element (which is the last time step).\n\nIn either case, we can then do the following:\n\nRNN layers in DL4J can be combined with other layer types. For example, it is possible to combine DenseLayer and GravesLSTM layers in the same network; or combine Convolutional (CNN) layers and GravesLSTM layers for video.\n\nOf course, the DenseLayer and Convolutional layers do not handle time series data - they expect a different type of input. To deal with this, we need to use the layer preprocessor functionality: for example, the CnnToRnnPreProcessor and FeedForwardToRnnPreprocessor classes. See here for all preprocessors. Fortunately, in most situations, the DL4J configuration system will automatically add these preprocessors as required. However, the preprocessors can be added manually (overriding the automatic addition of preprocessors, for each layer).\n\nFor example, to manually add a preprocessor between layers 1 and 2, add the following to your network configuration: .\n\nAs with other types of neural networks, predictions can be generated for RNNs using the and methods. These methods can be useful in many circumstances; however, they have the limitation that we can only generate predictions for time series, starting from scratch each and every time.\n\nConsider for example the case where we want to generate predictions in a real-time system, where these predictions are based on a very large amount of history. It this case, it is impractical to use the output/feedForward methods, as they conduct the full forward pass over the entire data history, each time they are called. If we wish to make a prediction for a single time step, at every time step, these methods can be both (a) very costly, and (b) wasteful, as they do the same calculations over and over.\n\nFor these situations, MultiLayerNetwork provides four methods of note:\n\nThe rnnTimeStep() method is designed to allow forward pass (predictions) to be conducted efficiently, one or more steps at a time. Unlike the output/feedForward methods, the rnnTimeStep method keeps track of the internal state of the RNN layers when it is called. It is important to note that output for the rnnTimeStep and the output/feedForward methods should be identical (for each time step), whether we make these predictions all at once (output/feedForward) or whether these predictions are generated one or more steps at a time (rnnTimeStep). Thus, the only difference should be the computational cost.\n\nIn summary, the MultiLayerNetwork.rnnTimeStep() method does two things:\n\nFor example, suppose we want to use a RNN to predict the weather, one hour in advance (based on the weather at say the previous 100 hours as input). If we were to use the output method, at each hour we would need to feed in the full 100 hours of data to predict the weather for hour 101. Then to predict the weather for hour 102, we would need to feed in the full 100 (or 101) hours of data; and so on for hours 103+.\n\nAlternatively, we could use the rnnTimeStep method. Of course, if we want to use the full 100 hours of history before we make our first prediction, we still need to do the full forward pass:\n\nFor the first time we call rnnTimeStep, the only practical difference between the two approaches is that the activations/state of the last time step are stored - this is shown in orange. However, the next time we use the rnnTimeStep method, this stored state will be used to make the next predictions:\n\nThere are a number of important differences here:\n\nHowever, if you want to start making predictions for a new (entirely separate) time series: it is necessary (and important) to manually clear the stored state, using the method. This will reset the internal state of all recurrent layers in the network.\n\nIf you need to store or set the internal state of the RNN for use in predictions, you can use the rnnGetPreviousState and rnnSetPreviousState methods, for each layer individually. This can be useful for example during serialization (network saving/loading), as the internal network state from the rnnTimeStep method is not saved by default, and must be saved and loaded separately. Note that these get/set state methods return and accept a map, keyed by the type of activation. For example, in the LSTM model, it is necessary to store both the output activations, and the memory cell state.\n\nSome other points of note:\n\nData import for RNNs is complicated by the fact that we have multiple different types of data we could want to use for RNNs: one-to-many, many-to-one, variable length time series, etc. This section will describe the currently implemented data import mechanisms for DL4J.\n\nThe methods described here utilize the SequenceRecordReaderDataSetIterator class, in conjunction with the CSVSequenceRecordReader class from DataVec. This approach currently allows you to load delimited (tab, comma, etc) data from files, where each time series is in a separate file. This method also supports:\n\nNote that in all cases, each line in the data files represents one time step.\n\nSuppose we have 10 time series in our training data, represented by 20 files: 10 files for the input of each time series, and 10 files for the output/labels. For now, assume these 20 files all contain the same number of time steps (i.e., same number of rows).\n\nTo use the SequenceRecordReaderDataSetIterator and CSVSequenceRecordReader approaches, we first create two CSVSequenceRecordReader objects, one for input and one for labels:\n\nThis particular constructor takes the number of lines to skip (1 row skipped here), and the delimiter (comma character used here).\n\nSecond, we need to initialize these two readers, by telling them where to get the data from. We do this with an InputSplit object. Suppose that our time series are numbered, with file names \u201cmyInput_0.csv\u201d, \u201cmyInput_1.csv\u201d, \u2026, \u201cmyLabels_0.csv\u201d, etc. One approach is to use the NumberedFileInputSplit:\n\nIn this particular approach, the \u201c%d\u201d is replaced by the corresponding number, and the numbers 0 to 9 (both inclusive) are used.\n\nFinally, we can create our SequenceRecordReaderdataSetIterator:\n\nThis DataSetIterator can then be passed to MultiLayerNetwork.fit() to train the network.\n\nThe miniBatchSize argument specifies the number of examples (time series) in each minibatch. For example, with 10 files total, miniBatchSize of 5 would give us two data sets with 2 minibatches (DataSet objects) with 5 time series in each.\n\nFollowing on from the last example, suppose that instead of a separate files for our input data and labels, we have both in the same file. However, each time series is still in a separate file.\n\nAs of DL4J 0.4-rc3.8, this approach has the restriction of a single column for the output (either a class index, or a single real-valued regression output)\n\nIn this case, we create and initialize a single reader. Again, we are skipping one header row, and specifying the format as comma delimited, and assuming our data files are named \u201cmyData_0.csv\u201d, \u2026, \u201cmyData_9.csv\u201d:\n\nand are the same as the previous example. Here, specifies which column the labels are in. For example, if the labels are in the fifth column, use labelIndex = 4 (i.e., columns are indexed 0 to numColumns-1).\n\nFor regression on a single output value, we use:\n\nAgain, the numPossibleLabels argument is not used for regression.\n\nFollowing on from the previous two examples, suppose that for each example individually, the input and labels are of the same length, but these lengths differ between time series.\n\nWe can use the same approach (CSVSequenceRecordReader and SequenceRecordReaderDataSetIterator), though with a different constructor:\n\nThe argument here are the same as in the previous example, with the exception of the AlignmentMode.ALIGN_END addition. This alignment mode input tells the SequenceRecordReaderDataSetIterator to expect two things:\n\nNote that if the features and labels are always of the same length (as is the assumption in example 3), then the two alignment modes (AlignmentMode.ALIGN_END and AlignmentMode.ALIGN_START) will give identical outputs. The alignment mode option is explained in the next section.\n\nAlso note: that variable length time series always start at time zero in the data arrays: padding, if required, will be added after the time series has ended.\n\nUnlike examples 1 and 2 above, the DataSet objects produced by the above variableLengthIter instance will also include input and masking arrays, as described earlier in this document.\n\nWe can also use the AlignmentMode functionality in example 3 to implement a many-to-one RNN sequence classifier. Here, let us assume:\n\nIn fact, the same approach as in example 3 can do this:\n\nAlignment modes are relatively straightforward. They specify whether to pad the start or the end of the shorter time series. The diagram below shows how this works, along with the masking arrays (as discussed earlier in this document):\n\nThe one-to-many case (similar to the last case above, but with only one input) is done by using AlignmentMode.ALIGN_START.\n\nNote that in the case of training data that contains time series of different lengths, the labels and inputs will be aligned for each example individually, and then the shorter time series will be padded as required:\n\nIn some cases, you will have to do something that doesn\u2019t fit into a typical data import scenario. One option for this scenario is to implement a custom DataSetIterator. DataSetIterator is merely an interface for iterating over DataSet objects - objects that encapsulate the input and target INDArrays, plus (optionally) the input and labels mask arrays.\n\nNote however that this approach is quite low level: implementing a DataSetIterator requires you to manually create the required INDArrays for the input and the labels, as well as (if required) the input and labels mask arrays. However, this approach gives you a great degree of flexibility over exactly how data is loaded.\n\nFor example of this approach in practice, see the the iterator for the character example and for the Word2Vec movie review sentiment example.\n\nNote: When creating a custom DataSetIterator, it is important that your data arrays - the input features, the labels, and any mask arrays - are created in \u2018f\u2019 (fortran) order. See the ND4J user guide for details on array orders. In practice, this means using the Nd4j.create methods that allow you to specify the array order: . Though \u2018c\u2019 order arrays will also work, performance will be reduced due to the need to copy the arrays to \u2018f\u2019 order first, for certain operations.\n\nDL4J currently has the following recurrent neural network examples:", 
        "title": "Using Recurrent Neural Networks in DL4J - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/compgraph", 
        "text": "This page describes how to build more complicated networks, using DL4J\u2019s Computation Graph functionality.\n\nNew Features!!! as of 7.3 Computation Graph will support a parameterless LossLayer and vertices for performing triplet loss.\n\nDL4J has two types of networks comprised of multiple layers:\n\nSpecifically, the ComputationGraph allows for networks to be built with the following features:\n\nAs a general rule, when building networks with a single input layer, a single output layer, and an input->a->b->c->output type connection structure: MultiLayerNetwork is usually the preferred network. However, everything that MultiLayerNetwork can do, ComputationGraph can do as well - though the configuration may be a little more complicated.\n\nExamples of some architectures that can be built using ComputationGraph include:\n\nThe basic idea is that in the ComputationGraph, the core building block is the GraphVertex, instead of layers. Layers (or, more accurately the LayerVertex objects), are but one type of vertex in the graph. Other types of vertices include:\n\nThese types of graph vertices are described briefly below.\n\nLayerVertex: Layer vertices (graph vertices with neural network layers) are added using the method. The first argument is the label for the layer, and the last arguments are the inputs to that layer. If you need to manually add an InputPreProcessor (usually this is unnecessary - see next section) you can use the method.\n\nInputVertex: Input vertices are specified by the method in your configuration. The strings used as inputs can be arbitrary - they are user-defined labels, and can be referenced later in the configuration. The number of strings provided define the number of inputs; the order of the input also defines the order of the corresponding INDArrays in the fit methods (or the DataSet/MultiDataSet objects).\n\nElementWiseVertex: Element-wise operation vertices do for example an element-wise addition or subtraction of the activations out of one or more other vertices. Thus, the activations used as input for the ElementWiseVertex must all be the same size, and the output size of the elementwise vertex is the same as the inputs.\n\nMergeVertex: The MergeVertex concatenates/merges the input activations. For example, if a MergeVertex has 2 inputs of size 5 and 10 respectively, then output size will be 5+10=15 activations. For convolutional network activations, examples are merged along the depth: so suppose the activations from one layer have 4 features and the other has 5 features (both with (4 or 5) x width x height activations), then the output will have (4+5) x width x height activations.\n\nSubsetVertex: The subset vertex allows you to get only part of the activations out of another vertex. For example, to get the first 5 activations out of another vertex with label \u201clayer1\u201d, you can use : this means that the 0th through 4th (inclusive) activations out of the \u201clayer1\u201d vertex will be used as output from the subset vertex.\n\nPreProcessorVertex: Occasionally, you might want to the functionality of an InputPreProcessor without that preprocessor being associated with a layer. The PreProcessorVertex allows you to do this.\n\nFinally, it is also possible to define custom graph vertices by implementing both a configuration and implementation class for your custom GraphVertex.\n\nSuppose we wish to build the following recurrent neural network architecture:\n\nFor the sake of this example, lets assume our input data is of size 5. Our configuration would be as follows:\n\nNote that in the .addLayer(\u2026) methods, the first string (\u201cL1\u201d, \u201cL2\u201d) is the name of that layer, and the strings at the end ([\u201cinput\u201d], [\u201cinput\u201d,\u201dL1\u201d]) are the inputs to that layer.\n\nConsider the following architecture:\n\nHere, the merge vertex takes the activations out of layers L1 and L2, and merges (concatenates) them: thus if layers L1 and L2 both have has 4 output activations (.nOut(4)) then the output size of the merge vertex is 4+4=8 activations.\n\nTo build the above network, we use the following configuration:\n\nIn multi-task learning, a neural network is used to make multiple independent predictions. Consider for example a simple network used for both classification and regression simultaneously. In this case, we have two output layers, \u201cout1\u201d for classification, and \u201cout2\u201d for regression.\n\nIn this case, the network configuration is:\n\nOne feature of the ComputationGraphConfiguration is that you can specify the types of input to the network, using the method in the configuration.\n\nThe setInputType method has two effects:\n\nFor example, if your network has 2 inputs, one being a convolutional input and the other being a feed-forward input, you would use\n\nThere are two types of data that can be used with the ComputationGraph.\n\nThe DataSet class was originally designed for use with the MultiLayerNetwork, however can also be used with ComputationGraph - but only if that computation graph has a single input and output array. For computation graph architectures with more than one input array, or more than one output array, DataSet and DataSetIterator cannot be used (instead, use MultiDataSet/MultiDataSetIterator).\n\nA DataSet object is basically a pair of INDArrays that hold your training data. In the case of RNNs, it may also include masking arrays (see this for more details). A DataSetIterator is essentially an iterator over DataSet objects.\n\nMultiDataSet is multiple input and/or multiple output version of DataSet. It may also include multiple mask arrays (for each input/output array) in the case of recurrent neural networks. As a general rule, you should use DataSet/DataSetIterator, unless you are dealing with multiple inputs and/or multiple outputs.\n\nThere are currently two ways to use a MultiDataSetIterator:\n\nThe RecordReaderMultiDataSetIterator provides a number of options for loading data. In particular, the RecordReaderMultiDataSetIterator provides the following functionality:\n\nSome basic examples on how to use the RecordReaderMultiDataSetIterator follow. You might also find these unit tests to be useful.\n\nSuppose we have a CSV file with 5 columns, and we want to use the first 3 as our input, and the last 2 columns as our output (for regression). We can build a MultiDataSetIterator to do this as follows:\n\nSuppose we have two separate CSV files, one for our inputs, and one for our outputs. Further suppose we are building a multi-task learning architecture, whereby have two outputs - one for classification. For this example, let\u2019s assume the data is as follows:\n\nIn this case, we can build our iterator as follows:", 
        "title": "Building Complex Network Architectures with Computation Graph - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/modelserver", 
        "text": "", 
        "title": "Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/devguide", 
        "text": "Note: Developers who are contributing will need to build from source. Follow our building locally guide before getting started.\n\nDeepLearning4j is perhaps the more visible project, there are a number of other projects that you should be familiar with - and may consider contributing to. These include:\n\nDeeplearning4j and ND4J are distributed under an Apache 2.0 license.\n\nThere are numerous ways to contribute to to DeepLearning4J (and related projects), depending on your interests and experince. Here\u2019s some ideas:\n\nThere are a number of different ways to find things to work on. These include:\n\nBefore you dive in, there\u2019s a few things you need to know. In particular, the tools we use:\n\nSome other tools you might find useful:\n\nAnd here\u2019s the typical workflow for contributing to any of our projects:\n\nHere\u2019s some guidelines and things to keep in mind:\n\nSo you want to contribute to one of the websites, such as deeplearning4j.org and nd4j.org? Great. These are also open source, and we welcome pull requests for them.\n\nHow does the website actually work? This is actually pretty straightforward. Consider for example deeplearning4j.org:\n\nMarkdown itself is relatively simple to pick up. If you are unfamiliar with it, have a look at the existing pages (.md files) in the gh-pages branch. You can use these as a guide to get started.\n\nDL4J is a very large and complicated piece of software. Giving a complete overview of how DL4J works is quite difficult. Though that is what this section attempts to do.\n\nSuppose you want to add a new type of layer to DL4J. Here\u2019s what you need to know.\n\nFirst, network configuration and network implementation (i.e., the math) are separated. Confusingly, they are both called layer:\n\nNow, to implement a new layer type, you need to implement all of the following:\n\nIn DL4J, we do not currently have symbolic automatic differentiation. This means that both the forward pass (predictions) and backward pass (backpropagation) code must be implemented manually.\n\nSome other things you should be aware of:\n\nSo, backprop. Let\u2019s start with the basics - an overview of the classes you need to know about:\n\nNow, let\u2019s step through what happens when you call MultiLayerNetwork.fit(DataSet) or MultiLayerNet.fit(DataSetIterator). We\u2019ll assume that we are doing backprop (not unsupervised pretraining).\n\nNow, we glossed over two important components: the calculation of the gradients, and their updating/modification.\n\nUpdating Gradients Updating gradients involves going from gradients for each parameter, to updates for each parameter. An \u2018update\u2019 is the gradients after we apply things like learning rates, momentum, L1/L2 regularization, gradient clipping and division by the minibatch size.\n\nThis functionality is implemented in BaseUpdater, and the various updater classes.", 
        "title": "Developer Guide for Deeplearning4j - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/architecture", 
        "text": "", 
        "title": ""
    }, 
    {
        "url": "https://deeplearning4j.org/features", 
        "text": "Here\u2019s a non-exhaustive list of Deeplearning4j\u2019s features. We\u2019ll be updating it as new nets and tools are added.\n\nSince Deeplearning4j is a composable framework, users can arrange shallow nets to create various types of deeper nets. Combining convolutional nets with recurrent nets, for example, is how Google accurately generated captions from images in late 2014.\n\nDL4J contains the following built-in vectorization algorithms:\n\nDL4J supports the following type of optimization algorithms:\n\nEach of these optimization algorithms may be paired with training features (known as \u2018updaters\u2019 in DL4J) such as:\n\nActivations functions are defined in ND4J here", 
        "title": "Deeplearning4j's Features - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/roadmap", 
        "text": "These priorities have been set by what the Skymind has seen demand for among clients and open-source community members. Contributors are welcome to add features whose priority they deem to be higher.\n\nThis is a work in progress. Last updated May 5, 2017.", 
        "title": "Deeplearning4j Roadmap - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/releasenotes", 
        "text": "Notable changes for upgrading codebases based on 0.6.0 to 0.7.0:", 
        "title": "Release Notes - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/doc", 
        "text": "JavaScript is disabled on your browser.\n\nThis document is designed to be viewed using the frames feature. If you see this message, you are using a non-frame-capable web client. Link to Non-frame version.", 
        "title": "Generated Documentation (Untitled)"
    }, 
    {
        "url": "https://deeplearning4j.org/nlp", 
        "text": "Although not designed to be comparable to tools such as Stanford CoreNLP or NLTK, deepLearning4J does include some core text processing tools that are described here.\n\nDeeplearning4j\u2019s NLP relies on ClearTK, an open-source machine learning and natural language processing framework for the Apache Unstructured Information Management Architecture, or UIMA. UIMA enables us to perform language identification, language-specific segmentation, sentence boundary detection and entity detection (proper nouns: persons, corporations, places and things).\n\nThere are several steps involved in processing natural language. The first is to iterate over your corpus to create a list of documents, which can be as short as a tweet, or as long as a newspaper article. This is performed by a SentenceIterator, which will appear like this:\n\nThe SentenceIterator encapsulates a corpus or text, organizing it, say, as one Tweet per line. It is responsible for feeding text piece by piece into your natural language processor. The SentenceIterator is not analogous to a similarly named class, the DatasetIterator, which creates a dataset for training a neural net. Instead it creates a collection of strings by segmenting a corpus.\n\nA Tokenizer further segments the text at the level of single words, also alternatively as n-grams. ClearTK contains the underlying tokenizers, such as parts of speech (PoS) and parse trees, which allow for both dependency and constituency parsing, like that employed by a recursive neural tensor network (RNTN).\n\nA Tokenizer is created and wrapped by a TokenizerFactory. The default tokens are words separated by spaces. The tokenization process also involves some machine learning to differentiate between ambibuous symbols like . which end sentences and also abbreviate words such as Mr. and vs.\n\nBoth Tokenizers and SentenceIterators work with Preprocessors to deal with anomalies in messy text like Unicode, and to render such text, say, as lowercase characters uniformly.\n\nEach document has to be tokenized to create a vocab, the set of words that matter for that document or corpus. Those words are stored in the vocab cache, which contains statistics about a subset of words counted in the document, the words that \u201cmatter\u201d. The line separating significant and insignifant words is mobile, but the basic idea of distinguishing between the two groups is that words occurring only once (or less than, say, five times) are hard to learn and their presence represents unhelpful noise.\n\nThe vocab cache stores metadata for methods such as Word2vec and Bag of Words, which treat words in radically different ways. Word2vec creates representations of words, or neural word embeddings, in the form of vectors that are hundreds of coefficients long. Those coefficients help neural nets predict the likelihood of a word appearing in any given context; for example, after another word. Here\u2019s Word2vec, configured:\n\nOnce you obtain word vectors, you can feed them into a deep net for classification, prediction, sentiment analysis and the like.", 
        "title": "Deeplearning4j's NLP Functionality - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/word2vec", 
        "text": "Word2vec is a two-layer neural net that processes text. Its input is a text corpus and its output is a set of vectors: feature vectors for words in that corpus. While Word2vec is not a deep neural network, it turns text into a numerical form that deep nets can understand. Deeplearning4j implements a distributed form of Word2vec for Java and Scala, which works on Spark with GPUs.\n\nWord2vec\u2019s applications extend beyond parsing sentences in the wild. It can be applied just as well to genes, code, likes, playlists, social media graphs and other verbal or symbolic series in which patterns may be discerned.\n\nWhy? Because words are simply discrete states like the other data mentioned above, and we are simply looking for the transitional probabilities between those states: the likelihood that they will co-occur. So gene2vec, like2vec and follower2vec are all possible. With that in mind, the tutorial below will help you understand how to create neural embeddings for any group of discrete and co-occurring states.\n\nThe purpose and usefulness of Word2vec is to group the vectors of similar words together in vectorspace. That is, it detects similarities mathematically. Word2vec creates vectors that are distributed numerical representations of word features, features such as the context of individual words. It does so without human intervention.\n\nGiven enough data, usage and contexts, Word2vec can make highly accurate guesses about a word\u2019s meaning based on past appearances. Those guesses can be used to establish a word\u2019s association with other words (e.g. \u201cman\u201d is to \u201cboy\u201d what \u201cwoman\u201d is to \u201cgirl\u201d), or cluster documents and classify them by topic. Those clusters can form the basis of search, sentiment analysis and recommendations in such diverse fields as scientific research, legal discovery, e-commerce and customer relationship management.\n\nThe output of the Word2vec neural net is a vocabulary in which each item has a vector attached to it, which can be fed into a deep-learning net or simply queried to detect relationships between words.\n\nMeasuring cosine similarity, no similarity is expressed as a 90 degree angle, while total similarity of 1 is a 0 degree angle, complete overlap; i.e. Sweden equals Sweden, while Norway has a cosine distance of 0.760124 from Sweden, the highest of any other country.\n\nHere\u2019s a list of words associated with \u201cSweden\u201d using Word2vec, in order of proximity:\n\nThe nations of Scandinavia and several wealthy, northern European, Germanic countries are among the top nine.\n\nThe vectors we use to represent words are called neural word embeddings, and representations are strange. One thing describes another, even though those two things are radically different. As Elvis Costello said: \u201cWriting about music is like dancing about architecture.\u201d Word2vec \u201cvectorizes\u201d about words, and by doing so it makes natural language computer-readable \u2013 we can start to perform powerful mathematical operations on words to detect their similarities.\n\nSo a neural word embedding represents a word with numbers. It\u2019s a simple, yet unlikely, translation.\n\nWord2vec is similar to an autoencoder, encoding each word in a vector, but rather than training against the input words through reconstruction, as a restricted Boltzmann machine does, word2vec trains words against other words that neighbor them in the input corpus.\n\nIt does so in one of two ways, either using context to predict a target word (a method known as continuous bag of words, or CBOW), or using a word to predict a target context, which is called skip-gram. We use the latter method because it produces more accurate results on large datasets.\n\nWhen the feature vector assigned to a word cannot be used to accurately predict that word\u2019s context, the components of the vector are adjusted. Each word\u2019s context in the corpus is the teacher sending error signals back to adjust the feature vector. The vectors of words judged similar by their context are nudged closer together by adjusting the numbers in the vector.\n\nJust as Van Gogh\u2019s painting of sunflowers is a two-dimensional mixture of oil on canvas that represents vegetable matter in a three-dimensional space in Paris in the late 1880s, so 500 numbers arranged in a vector can represent a word or group of words.\n\nThose numbers locate each word as a point in 500-dimensional vectorspace. Spaces of more than three dimensions are difficult to visualize. (Geoff Hinton, teaching people to imagine 13-dimensional space, suggests that students first picture 3-dimensional space and then say to themselves: \u201cThirteen, thirteen, thirteen.\u201d :)\n\nA well trained set of word vectors will place similar words close to each other in that space. The words oak, elm and birch might cluster in one corner, while war, conflict and strife huddle together in another.\n\nSimilar things and ideas are shown to be \u201cclose\u201d. Their relative meanings have been translated to measurable distances. Qualities become quantities, and algorithms can do their work. But similarity is just the basis of many associations that Word2vec can learn. For example, it can gauge relations between words of one language, and map them to another.\n\nThese vectors are the basis of a more comprehensive geometry of words. Not only will Rome, Paris, Berlin and Beijing cluster near each other, but they will each have similar distances in vectorspace to the countries whose capitals they are; i.e. Rome - Italy = Beijing - China. And if you only knew that Rome was the capital of Italy, and were wondering about the capital of China, then the equation Rome -Italy + China would return Beijing. No kidding.\n\nLet\u2019s look at some other associations Word2vec can produce.\n\nInstead of the pluses, minus and equals signs, we\u2019ll give you the results in the notation of logical analogies, where means \u201cis to\u201d and means \u201cas\u201d; e.g. \u201cRome is to Italy as Beijing is to China\u201d = . In the last spot, rather than supplying the \u201canswer\u201d, we\u2019ll give you the list of words that a Word2vec model proposes, when given the first three elements:\n\nThis model was trained on the Google News vocab, which you can import and play with. Contemplate, for a moment, that the Word2vec algorithm has never been taught a single rule of English syntax. It knows nothing about the world, and is unassociated with any rules-based symbolic logic or knowledge graph. And yet it learns more, in a flexible and automated fashion, than most knowledge graphs will learn after a years of human labor. It comes to the Google News documents as a blank slate, and by the end of training, it can compute complex analogies that mean something to humans.\n\nYou can also query a Word2vec model for other assocations. Not everything has to be two analogies that mirror each other. (We explain how below\u2026.)\n\nBy building a sense of one word\u2019s proximity to other similar words, which do not necessarily contain the same letters, we have moved beyond hard tokens to a smoother and more general sense of meaning.\n\nWhile Word2vec refers to a family of related algorithms, this implementation uses Skip-Gram Negative Sampling.\n\nCreate a new project in IntelliJ using Maven. If you don\u2019t know how to do that, see our Quickstart page. Then specify these properties and dependencies in the POM.xml file in your project\u2019s root directory (You can check Maven for the most recent versions \u2013 please use those\u2026).\n\nNow create and name a new class in Java. After that, you\u2019ll take the raw sentences in your .txt file, traverse them with your iterator, and subject them to some sort of preprocessing, such as converting all words to lowercase.\n\nIf you want to load a text file besides the sentences provided in our example, you\u2019d do this:\n\nThat is, get rid of the and feed the absolute path of your file into the .\n\nIn bash, you can find the absolute file path of any directory by typing in your command line from within that same directory. To that path, you\u2019ll add the file name and voila.\n\nWord2vec needs to be fed words rather than whole sentences, so the next step is to tokenize the data. To tokenize a text is to break it up into its atomic units, creating a new token each time you hit a white space, for example.\n\nThat should give you one word per line.\n\nNow that the data is ready, you can configure the Word2vec neural net and feed in the tokens.\n\nThis configuration accepts a number of hyperparameters. A few require some explanation:\n\nAn example for uptraining your previously trained word vectors is here.\n\nThe next step is to evaluate the quality of your feature vectors.\n\nThe line will return the cosine similarity of the two words you enter. The closer it is to 1, the more similar the net perceives those words to be (see the Sweden-Norway example above). For example:\n\nWith , the words printed to the screen allow you to eyeball whether the net has clustered semantically similar words. You can set the number of nearest words you want with the second parameter of wordsNearest. For example:\n\nWe rely on TSNE to reduce the dimensionality of word feature vectors and project words into a two or three-dimensional space. The full DL4J/ND4J example for TSNE is here.\n\nYou\u2019ll want to save the model. The normal way to save models in Deeplearning4j is via the serialization utils (Java serialization is akin to Python pickling, converting an object into a series of bytes).\n\nThis will save the vectors to a file called that will appear in the root of the directory where Word2vec is trained. The output in the file should have one word per line, followed by a series of numbers that together are its vector representation.\n\nTo keep working with the vectors, simply call methods on like this:\n\nThe classic example of Word2vec\u2019s arithmetic of words is \u201cking - queen = man - woman\u201d and its logical extension \u201cking - queen + woman = man\u201d.\n\nThe example above will output the 10 nearest words to the vector , which should include . The first parameter for wordsNearest has to include the \u201cpositive\u201d words and , which have a + sign associated with them; the second parameter includes the \u201cnegative\u201d word , which is associated with the minus sign (positive and negative here have no emotional connotation); the third is the length of the list of nearest words you would like to see. Remember to add this to the top of the file: .\n\nAny number of combinations is possible, but they will only return sensible results if the words you query occurred with enough frequency in the corpus. Obviously, the ability to return similar words (or documents) is at the foundation of both search and recommendation engines.\n\nYou can reload the vectors into memory like this:\n\nYou can then use Word2vec as a lookup table:\n\nIf the word isn\u2019t in the vocabulary, Word2vec returns zeros.\n\nThe Google News Corpus model we use to test the accuracy of our trained nets is hosted on S3. Users whose current hardware takes a long time to train on large corpora can simply download it to explore a Word2vec model without the prelude.\n\nIf you trained with the C vectors or Gensimm, this line will import the model.\n\nRemember to add to your imported packages.\n\nWith large models, you may run into trouble with your heap space. The Google model may take as much as 10G of RAM, and the JVM only launches with 256 MB of RAM, so you have to adjust your heap space. You can do that either with a file (see our Troubleshooting section), or through IntelliJ itself:\n\nWords are read into the vector one at a time, and scanned back and forth within a certain range. Those ranges are n-grams, and an n-gram is a contiguous sequence of n items from a given linguistic sequence; it is the nth version of unigram, bigram, trigram, four-gram or five-gram. A skip-gram simply drops items from the n-gram.\n\nThe skip-gram representation popularized by Mikolov and used in the DL4J implementation has proven to be more accurate than other models, such as continuous bag of words, due to the more generalizable contexts generated.\n\nThis n-gram is then fed into a neural network to learn the significance of a given word vector; i.e. significance is defined as its usefulness as an indicator of certain larger meanings, or labels.\n\nPlease note : The code below may be outdated. For updated examples, please see our dl4j-examples repository on Github.\n\nNow that you have a basic idea of how to set up Word2Vec, here\u2019s one example of how it can be used with DL4J\u2019s API:\n\nAfter following the instructions in the Quickstart, you can open this example in IntelliJ and hit run to see it work. If you query the Word2vec model with a word isn\u2019t contained in the training corpus, it will return null.\n\nQ: I get a lot of stack traces like this\n\nA: Look inside the directory where you started your Word2vec application. This can, for example, be an IntelliJ project home directory or the directory where you typed Java at the command line. It should have some directories that look like:\n\nYou can shut down your Word2vec application and try to delete them.\n\nQ: Not all of the words from my raw text data are appearing in my Word2vec object\u2026\n\nA: Try to raise the layer size via .layerSize() on your Word2Vec object like so\n\nQ: How do I load my data? Why does training take forever?\n\nA: If all of your sentences have been loaded as one sentence, Word2vec training could take a very long time. That\u2019s because Word2vec is a sentence-level algorithm, so sentence boundaries are very important, because co-occurrence statistics are gathered sentence by sentence. (For GloVe, sentence boundaries don\u2019t matter, because it\u2019s looking at corpus-wide co-occurrence. For many corpora, average sentence length is six words. That means that with a window size of 5 you have, say, 30 (random number here) rounds of skip-gram calculations. If you forget to specify your sentence boundaries, you may load a \u201csentence\u201d that\u2019s 10,000 words long. In that case, Word2vec would attempt a full skip-gram cycle for the whole 10,000-word \u201csentence\u201d. In DL4J\u2019s implementation, a line is assumed to be a sentence. You need plug in your own SentenceIterator and Tokenizer. By asking you to specify how your sentences end, DL4J remains language-agnostic. UimaSentenceIterator is one way to do that. It uses OpenNLP for sentence boundary detection.\n\nQ: Why is there such a difference in performance when feeding whole documents as one \u201csentence\u201d vs splitting into Sentences?\n\nA:If average sentence contains 6 words, and window size is 5, maximum theoretical number of 10 skipgram rounds will be achieved on 0 words. Sentence isn\u2019t long enough to have full window set with words. Rough maximum number of 5 sg rounds is available there for all words in such sentence.\n\nBut if your \u201csentence\u201d is 1000k words length, you\u2019ll have 10 skipgram rounds for every word in this sentence, excluding the first 5 and last five. So, you\u2019ll have to spend WAY more time building model + cooccurrence statistics will be shifted due to the absense of sentence boundaries.\n\nQ: How does Word2Vec Use Memory?\n\nA: The major memory consumer in w2v is weghts matrix. Math is simple there: NumberOfWords x NumberOfDimensions x 2 x DataType memory footprint.\n\nSo, if you build w2v model for 100k words using floats, and 100 dimensions, your memory footprint will be 100k x 100 x 2 x 4 (float size) = 80MB RAM just for matri + some space for strings, variables, threads etc.\n\nIf you load pre-built model, it uses roughly 2 times less RAM then during build time, so it\u2019s 40MB RAM.\n\nAnd the most popular model used so far is Google News model. There\u2019s 3M words, and vector size 300. That gives us 3.6GB only to load model. And you have to add 3M of strings, that do not have constant size in java. So, usually that\u2019s something around 4-6GB for loaded model depending on jvm version/supplier, gc state and phase of the moon.\n\nQ: I did everything you said and the results still don\u2019t look right.\n\nA: Make sure you\u2019re not hitting into normalization issues. Some tasks, like wordsNearest(), use normalized weights by default, and others require non-normalized weights. Pay attention to this difference.\n\nGoogle Scholar keeps a running tally of the papers citing Deeplearning4j\u2019s implementation of Word2vec here.\n\nKenny Helsens, a data scientist based in Belgium, applied Deeplearning4j\u2019s implementation of Word2vec to the NCBI\u2019s Online Mendelian Inheritance In Man (OMIM) database. He then looked for the words most similar to alk, a known oncogene of non-small cell lung carcinoma, and Word2vec returned: \u201cnonsmall, carcinomas, carcinoma, mapdkd.\u201d From there, he established analogies between other cancer phenotypes and their genotypes. This is just one example of the associations Word2vec can learn on a large corpus. The potential for discovering new aspects of important diseases has only just begun, and outside of medicine, the opportunities are equally diverse.\n\nAndreas Klintberg trained Deeplearning4j\u2019s implementation of Word2vec on Swedish, and wrote a thorough walkthrough on Medium.\n\nWord2Vec is especially useful in preparing text-based data for information retrieval and QA systems, which DL4J implements with deep autoencoders.\n\nMarketers might seek to establish relationships among products to build a recommendation engine. Investigators might analyze a social graph to surface members of a single group, or other relations they might have to location or financial sponsorship.\n\nWord2vec is a method of computing vector representations of words introduced by a team of researchers at Google led by Tomas Mikolov. Google hosts an open-source version of Word2vec released under an Apache 2.0 license. In 2014, Mikolov left Google for Facebook, and in May 2015, Google was granted a patent for the method, which does not abrogate the Apache license under which it has been released.\n\nWhile words in all languages may be converted into vectors with Word2vec, and those vectors learned with Deeplearning4j, NLP preprocessing can be very language specific, and requires tools beyond our libraries. The Stanford Natural Language Processing Group has a number of Java-based tools for tokenization, part-of-speech tagging and named-entity recognition for languages such as Mandarin Chinese, Arabic, French, German and Spanish. For Japanese, NLP tools like Kuromoji are useful. Other foreign-language resources, including text corpora, are available here.\n\nLoading and saving GloVe models to word2vec can be done like so:\n\nDeeplearning4j has a class called SequenceVectors, which is one level of abstraction above word vectors, and which allows you to extract features from any sequence, including social media profiles, transactions, proteins, etc. If data can be described as sequence, it can be learned via skip-gram and hierarchic softmax with the AbstractVectors class. This is compatible with the DeepWalk algorithm, also implemented in Deeplearning4j.", 
        "title": "Word2Vec, Doc2vec & GloVe: Neural Word Embeddings for Natural Language Processing - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/doc2vec", 
        "text": "The main purpose of Doc2Vec is associating arbitrary documents with labels, so labels are required. Doc2vec is an extension of word2vec that learns to correlate labels and words, rather than words with other words. Deeplearning4j\u2019s implentation is intended to serve the Java, Scala and Clojure communities.\n\nThe first step is coming up with a vector that represents the \u201cmeaning\u201d of a document, which can then be used as input to a supervised machine learning algorithm to associate documents with labels.\n\nIn the ParagraphVectors builder pattern, the method points to the labels to train on. In the example below, you can see labels related to sentiment analysis:\n\nHere\u2019s a full working example of classification with paragraph vectors:", 
        "title": "Doc2Vec, or Paragraph Vectors, in Deeplearning4j - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/textanalysis", 
        "text": "While images are inherently ambiguous, words belong to a set of semi-structured data known as language, which contains information about itself.\n\nOne way to view language is as a form of data compression, in which knowledge of the world is consolidated into a symbolic set. Like a lossy file or a serialized dataset, words are a compact rendering of something larger. You could argue, therefore, that words are a more promising field for deep learning than images, because you can get to the essence of them.\n\nThat said, textual analysis has presented many challenges for machine learning. Laborious, manual feature extraction is the main disadvantage of applying three-layer neural nets to textual analysis. In those cases, data scientists need to spend a great deal of time telling the algorithm what to pay attention to.\n\nA part-of-speech tag on a single word might be one feature they select, the fact that the word occurred might be another, and the number of times it appeared in a given text would be a third, each rule carefully and deliberately generated. That process leads to a high ratio of features per word, many of which can be redundant.\n\nOne of the chief advantages of deep learning is that feature creation is largely automated. To describe what it does exactly, we\u2019ll first describe feature extraction in more depth.\n\nA text fed into a neural network passes through several stages of analysis. The first is sentence segementation, in which the software finds the sentence boundaries within the text. The second is tokenization, in which the software finds individual words. In the third stage, parts-of-speech tags are attached to those words, and in the fourth, they are grouped according to their stems or concepts, in a process known as lemmatization. That is, words such as be, been and is will be grouped since they represent the same verb idea.\n\nThe neural net called Word2vec goes as far as lemmatization. Lemmas simply extend features based on stems, which is a process deep learning does in other ways automatically.\n\nBefore we turn to Word2vec, however, we\u2019ll cover a slightly simpler algorithm, Bag of Words.", 
        "title": "Textual analysis and deep learning - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/bagofwords-tf-idf", 
        "text": "Bag of Words (BoW) is an algorithm that counts how many times a word appears in a document. Those word counts allow us to compare documents and gauge their similarities for applications like search, document classification and topic modeling. BoW is a method for preparing text for input in a deep-learning net.\n\nBoW lists words with their word counts per document. In the table where the words and documents effectively become vectors are stored, each row is a word, each column is a document and each cell is a word count. Each of the documents in the corpus is represented by columns of equal length. Those are wordcount vectors, an output stripped of context.\n\nBefore they\u2019re fed to the neural net, each vector of wordcounts is normalized such that all elements of the vector add up to one. Thus, the frequencies of each word is effectively converted to represent the probabilities of those words\u2019 occurrence in the document. Probabilities that surpass certain levels will activate nodes in the net and influence the document\u2019s classification.\n\nTerm-frequency-inverse document frequency (TF-IDF) is another way to judge the topic of an article by the words it contains. With TF-IDF, words are given weight \u2013 TF-IDF measures relevance, not frequency. That is, wordcounts are replaced with TF-IDF scores across the whole dataset.\n\nFirst, TF-IDF measures the number of times that words appear in a given document (that\u2019s term frequency). But because words such as \u201cand\u201d or \u201cthe\u201d appear frequently in all documents, those are systematically discounted. That\u2019s the inverse-document frequency part. The more documents a word appears in, the less valuable that word is as a signal. That\u2019s intended to leave only the frequent AND distinctive words as markers. Each word\u2019s TF-IDF relevance is a normalized data format that also adds up to one.\n\nThose marker words are then fed to the neural net as features in order to determine the topic covered by the document that contains them.\n\nSetting up a BoW looks something like this:\n\nWhile simple, TF-IDF is incredibly powerful, and contributes to such ubiquitous and useful tools as Google search.\n\nBoW is different from Word2vec, which we\u2019ll cover next. The main difference is that Word2vec produces one vector per word, whereas BoW produces one number (a wordcount). Word2vec is great for digging into documents and identifying content and subsets of content. Its vectors represent each word\u2019s context, the ngrams of which it is a part. BoW is good for classifying documents as a whole.", 
        "title": "Bag of Words - TF-IDF - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/sentenceiterator", 
        "text": "A sentence iterator is used in both Word2vec and Bag of Words.\n\nIt feeds bits of text into a neural network in the form of vectors, and also covers the concept of documents in text processing.\n\nIn natural-language processing, a document or sentence is typically used to encapsulate a context which an algorithm should learn.\n\nA few examples include analyzing Tweets and full-blown news articles. The purpose of the sentence iterator is to divide text into processable bits. Note the sentence iterator is input agnostic. So bits of text (a document) can come from a file system, the Twitter API or Hadoop.\n\nDepending on how input is processed, the output of a sentence iterator will then be passed to a tokenizer for the processing of individual tokens, which are usually words, but could also be ngrams, skipgrams or other units. The tokenizer is created on a per-sentence basis by a tokenizer factory. The tokenizer factory is what is passed into a text-processing vectorizer.\n\nSome typical examples are below:\n\nThis assumes that each line in a file is a sentence.\n\nYou can also do list of strings as sentence as follows:\n\nThis will assume that each string is a sentence (document). Remember this could be a list of Tweets or articles \u2013 both are applicable.\n\nYou can iterate over files as follows:\n\nThis will parse the files line by line and return individual sentences on each one.\n\nFor anything complex, we recommend an actual machine-learning level pipeline, represented by the UimaSentenceIterator.\n\nThe UimaSentenceIterator is capable of tokenization, part-of-speech tagging and lemmatization, among other things. The UimaSentenceIterator iterates over a set of files and can segment sentences. You can customize its behavior based on the AnalysisEngine passed into it.\n\nThe AnalysisEngine is the UIMA concept of a text-processing pipeline. DeepLearning4j comes with standard analysis engines for all of these common tasks, allowing you to customize which text is being passed in and how you define sentences. The AnalysisEngines are thread-safe versions of the opennlp pipelines. We also include cleartk-based pipelines for handling common tasks.\n\nFor those using UIMA or curious about it, this employs the cleartk type system for tokens, sentences, and other annotations within the type system.\n\nYou can also instantiate directly:\n\nFor those familiar with Uima, this uses Uimafit extensively to create analysis engines. You can also create custom sentence iterators by extending SentenceIterator.", 
        "title": "Sentence iterator - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/tokenization", 
        "text": "Tokenization is the process of breaking text down into individual words. Word windows are also composed of tokens. Word2Vec can output text windows that comprise training examples for input into neural nets, as seen here.\n\nHere\u2019s an example of tokenization done with DL4J tools:\n\nThe above snippet creates a tokenizer capable of stemming.\n\nIn Word2Vec, that\u2019s the recommended a way of creating a vocabulary, because it averts various vocabulary quirks, such as the singular and plural of the same noun being counted as two different words.", 
        "title": "Tokenization - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/vocabcache", 
        "text": "The vocabulary cache, or vocab cache, is a mechanism for handling general-purpose natural-language tasks in Deeplearning4j, including normal TF-IDF, word vectors and certain information-retrieval techniques. The goal of the vocab cache is to be a one-stop shop for text vectorization, encapsulating techniques common to bag of words and word vectors, among others.\n\nVocab cache handles storage of tokens, word-count frequencies, inverse-document frequencies and document occurrences via an inverted index. The InMemoryLookupCache is the reference implementation.\n\nIn order to use a vocab cache as you iterate over text and index tokens, you need to figure out if the tokens should be included in the vocab. The criterion is usually if tokens occur with more than a certain pre-configured frequency in the corpus. Below that frequency, an individual token isn\u2019t a vocab word, and it remains just a token.\n\nWe track tokens as well. In order to track tokens, do the following:\n\nWhen you want to add a vocab word, do the following:\n\nAdding the word to the index sets the index. Then you declare it as a vocab word. (Declaring it as a vocab word will pull the word from the index.)", 
        "title": "How the Vocab Cache Works - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/eigenvector", 
        "text": "This post introduces eigenvectors and their relationship to matrices in plain language and without a great deal of math. It builds on those ideas to explain covariance, principal component analysis, and information entropy.\n\nThe eigen in eigenvector comes from German, and it means something like \u201cvery own.\u201d For example, in German, \u201cmein eigenes Auto\u201d means \u201cmy very own car.\u201d So eigen denotes a special relationship between two things. Something particular, characteristic and definitive. This car, or this vector, is mine and not someone else\u2019s.\n\nMatrices, in linear algebra, are simply rectangular arrays of numbers, a collection of scalar values between brackets, like a spreadsheet. All square matrices (e.g. 2 x 2 or 3 x 3) have eigenvectors, and they have a very special relationship with them, a bit like Germans have with their cars.\n\nWe\u2019ll define that relationship after a brief detour into what matrices do, and how they relate to other numbers.\n\nMatrices are useful because you can do things with them like add and multiply. If you multiply a vector v by a matrix A, you get another vector b, and you could say that the matrix performed a linear transformation on the input vector.\n\nIt maps one vector v to another, b.\n\nWe\u2019ll illustrate with a concrete example. (You can see how this type of matrix multiply, called a dot product, is performed here.)\n\nSo A turned v into b. In the graph below, we see how the matrix mapped the short, low line v, to the long, high one, b.\n\nYou could feed one positive vector after another into matrix A, and each would be projected onto a new space that stretches higher and farther to the right.\n\nImagine that all the input vectors v live in a normal grid, like this:\n\nAnd the matrix projects them all into a new space like the one below, which holds the output vectors b:\n\nHere you can see the two spaces juxtaposed:\n\nAnd here\u2019s an animation that shows the matrix\u2019s work transforming one space to another:\n\nYou can imagine a matrix like a gust of wind, an invisible force that produces a visible result. And a gust of wind must blow in a certain direction. The eigenvector tells you the direction the matrix is blowing in.\n\nSo out of all the vectors affected by a matrix blowing through one space, which one is the eigenvector? It\u2019s the one that that changes length but not direction; that is, the eigenvector is already pointing in the same direction that the matrix is pushing all vectors toward. An eigenvector is like a weathervane. An eigenvane, as it were.\n\nThe definition of an eigenvector, therefore, is a vector that responds to a matrix as though that matrix were a scalar coefficient. In this equation, A is the matrix, x the vector, and lambda the scalar coefficient, a number like 5 or 37 or pi.\n\nYou might also say that eigenvectors are axes along which linear transformation acts, stretching or compressing input vectors. They are the lines of change that represent the action of the larger matrix, the very \u201cline\u201d in linear transformation.\n\nNotice we\u2019re using the plural \u2013 axes and lines. Just as a German may have a Volkswagen for grocery shopping, a Mercedes for business travel, and a Porsche for joy rides (each serving a distinct purpose), square matrices can have as many eigenvectors as they have dimensions; i.e. a 2 x 2 matrix could have two eigenvectors, a 3 x 3 matrix three, and an n x n matrix could have n eigenvectors, each one representing its line of action in one dimension.1\n\nBecause eigenvectors distill the axes of principal force that a matrix moves input along, they are useful in matrix decomposition; i.e. the diagonalization of a matrix along its eigenvectors. Because those eigenvectors are representative of the matrix, they perform the same task as the autoencoders employed by deep neural networks.\n\nPCA is a tool for finding patterns in high-dimensional data such as images. Machine-learning practitioners sometimes use PCA to preprocess data for their neural networks. By centering, rotating and scaling data, PCA prioritizes dimensionality (allowing you to drop some low-variance dimensions) and can improve the neural network\u2019s convergence speed and the overall quality of results.\n\nTo get to PCA, we\u2019re going to quickly define some basic statistical ideas \u2013 mean, standard deviation, variance and covariance \u2013 so we can weave them together later. Their equations are closely related.\n\nMean is simply the average value of all x\u2019s in the set X, which is found by dividing the sum of all data points by the number of data points, n.\n\nStandard deviation, as fun as that sounds, is simply the square root of the average square distance of data points to the mean. In the equation below, the numerator contains the sum of the differences between each datapoint and the mean, and the denominator is simply the number of data points (minus one), producing the average distance.\n\nVariance is the measure of the data\u2019s spread. If I take a team of Dutch basketball players and measure their height, those measurements won\u2019t have a lot of variance. They\u2019ll all be grouped above six feet.\n\nBut if I throw the Dutch basketball team into a classroom of psychotic kindergartners, then the combined group\u2019s height measurements will have a lot of variance. Variance is the spread, or the amount of difference that data expresses.\n\nVariance is simply standard deviation squared, and is often expressed as s^2.\n\nFor both variance and standard deviation, squaring the differences between data points and the mean makes them positive, so that values above and below the mean don\u2019t cancel each other out.\n\nLet\u2019s assume you plotted the age (x axis) and height (y axis) of those individuals (setting the mean to zero) and came up with an oblong scatterplot:\n\nPCA attempts to draw straight, explanatory lines through data, like linear regression.\n\nEach straight line represents a \u201cprincipal component,\u201d or a relationship between an independent and dependent variable. While there are as many principal components as there are dimensions in the data, PCA\u2019s role is to prioritize them.\n\nThe first principal component bisects a scatterplot with a straight line in a way that explains the most variance; that is, it follows the longest dimension of the data. (This happens to coincide with the least error, as expressed by the red lines\u2026) In the graph below, it slices down the length of the \u201cbaguette.\u201d\n\nThe second principal component cuts through the data perpendicular to the first, fitting the errors produced by the first. There are only two principal components in the graph above, but if it were three-dimensional, the third component would fit the errors from the first and second principal components, and so forth.\n\nWhile we introduced matrices as something that transformed one set of vectors into another, another way to think about them is as a description of data that captures the forces at work upon it, the forces by which two variables might relate to each other as expressed by their variance and covariance.\n\nImagine that we compose a square matrix of numbers that describe the variance of the data, and the covariance among variables. This is the covariance matrix. It is an empirical description of data we observe.\n\nFinding the eigenvectors and eigenvalues of the covariance matrix is the equivalent of fitting those straight, principal-component lines to the variance of the data. Why? Because eigenvectors trace the principal lines of force, and the axes of greatest variance and covariance illustrate where the data is most susceptible to change.\n\nThink of it like this: If a variable changes, it is being acted upon by a force known or unknown. If two variables change together, in all likelihood that is either because one is acting upon the other, or they are both subject to the same hidden and unnamed force.\n\nWhen a matrix performs a linear transformation, eigenvectors trace the lines of force it applies to input; when a matrix is populated with the variance and covariance of the data, eigenvectors reflect the forces that have been applied to the given. One applies force and the other reflects it.\n\nEigenvalues are simply the coefficients attached to eigenvectors, which give the axes magnitude. In this case, they are the measure of the data\u2019s covariance. By ranking your eigenvectors in order of their eigenvalues, highest to lowest, you get the principal components in order of significance.\n\nFor a 2 x 2 matrix, a covariance matrix might look like this:\n\nThe numbers on the upper left and lower right represent the variance of the x and y variables, respectively, while the identical numbers on the lower left and upper right represent the covariance between x and y. Because of that identity, such matrices are known as symmetrical. As you can see, the covariance is positive, since the graph near the top of the PCA section points up and to the right.\n\nIf two variables increase and decrease together (a line going up and to the right), they have a positive covariance, and if one decreases while the other increases, they have a negative covariance (a line going down and to the right).\n\nNotice that when one variable or the other doesn\u2019t move at all, and the graph shows no diagonal motion, there is no covariance whatsoever. Covariance answers the question: do these two variables dance together? If one remains null while the other moves, the answer is no.\n\nAlso, in the equation below, you\u2019ll notice that there is only a small difference between covariance and variance.\n\nThe great thing about calculating covariance is that, in a high-dimensional space where you can\u2019t eyeball intervariable relationships, you can know how two variables move together by the positive, negative or non-existent character of their covariance. (Correlation is a kind of normalized covariance, with a value between -1 and 1.)\n\nTo sum up, the covariance matrix defines the shape of the data. Diagonal spread along eigenvectors is expressed by the covariance, while x-and-y-axis-aligned spread is expressed by the variance.\n\nCausality has a bad name in statistics, so take this with a grain of salt:\n\nWhile not entirely accurate, it may help to think of each component as a causal force in the Dutch basketball player example above, with the first principal component being age; the second possibly gender; the third nationality (implying nations\u2019 differing healthcare systems), and each of those occupying its own dimension in relation to height. Each acts on height to different degrees. You can read covariance as traces of possible cause.\n\nBecause the eigenvectors of the covariance matrix are orthogonal to each other, they can be used to reorient the data from the x and y axes to the axes represented by the principal components. You re-base the coordinate system for the dataset in a new space defined by its lines of greatest variance.\n\nThe x and y axes we\u2019ve shown above are what\u2019s called the basis of a matrix; that is, they provide the points of the matrix with x, y coordinates. But it is possible to recast a matrix along other axes; for example, the eigenvectors of a matrix can serve as the foundation of a new set of coordinates for the same matrix. Matrices and vectors are animals in themselves, independent of the numbers linked to a specific coordinate system like x and y.\n\nIn the graph above, we show how the same vector v can be situated differently in two coordinate systems, the x-y axes in black, and the two other axes shown by the red dashes. In the first coordinate system, v = (1,1), and in the second, v = (1,0), but v itself has not changed. Vectors and matrices can therefore be abstracted from the numbers that appear inside the brackets.\n\nThis has profound and almost spiritual implications, one of which is that there exists no natural coordinate system, and mathematical objects in n-dimensional space are subject to multiple descriptions. (Changing matrices\u2019 bases also makes them easier to manipulate.)\n\nA change of basis for vectors is roughly analogous to changing the base for numbers; i.e. the quantity nine can be described as 9 in base ten, as 1001 in binary, and as 100 in base three (i.e. 1, 2, 10, 11, 12, 20, 21, 22, 100 <\u2013 that is \u201cnine\u201d). Same quantity, different symbols; same vector, different coordinates.\n\nIn information theory, the term entropy refers to information we don\u2019t have (normally people define \u201cinformation\u201d as what they know, and jargon has triumphed once again in turning plain language on its head to the detriment of the uninitiated). The information we don\u2019t have about a system, its entropy, is related to its unpredictability: how much it can surprise us.\n\nIf you know that a certain coin has heads embossed on both sides, then flipping the coin gives you absolutely no information, because it will be heads every time. You don\u2019t have to flip it to know. We would say that two-headed coin contains no information, because it has no way to surprise you.\n\nA balanced, two-sided coin does contain an element of surprise with each coin toss. And a six-sided die, by the same argument, contains even more surprise with each roll, which could produce any one of six results with equal frequency. Both those objects contain information in the technical sense.\n\nNow let\u2019s imagine the die is loaded, it comes up \u201cthree\u201d on five out of six rolls, and we figure out the game is rigged. Suddenly the amount of surprise produced with each roll by this die is greatly reduced. We understand a trend in the die\u2019s behavior that gives us greater predictive capacity.\n\nUnderstanding the die is loaded is analogous to finding a principal component in a dataset. You simply identify an underlying pattern.\n\nThat transfer of information, from what we don\u2019t know about the system to what we know, represents a change in entropy. Insight decreases the entropy of the system. Get information, reduce entropy. This is information gain. And yes, this type of entropy is subjective, in that it depends on what we know about the system at hand. (Fwiw, information gain is synonymous with Kullback-Leibler divergence, which we explored briefly in this tutorial on restricted Boltzmann machines.)\n\nSo each principal component cutting through the scatterplot represents a decrease in the system\u2019s entropy, in its unpredictability.\n\nIt so happens that explaining the shape of the data one principal component at a time, beginning with the component that accounts for the most variance, is similar to walking data through a decision tree. The first component of PCA, like the first if-then-else split in a properly formed decision tree, will be along the dimension that reduces unpredictability the most.\n\nYou can see how we do eigenvectors in ND4J, a numerical computing library for the JVM that handles n-dimensional arrays, broadly inspired by Numpy. ND4J has Java and Scala APIs, runs on Hadoop and Spark, and is roughly twice as fast as Numpy/Cython on very large matrix operations.\n\n1) In some cases, matrices may not have a full set of eigenvectors; they can have at most as many linearly independent eigenvectors as their respective order, or number of dimensions.", 
        "title": "Eigenvectors, Eigenvalues, PCA, Covariance and Entropy - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/thoughtvectors", 
        "text": "\u201cThought vector\u201d is a term popularized by Geoffrey Hinton, the prominent deep-learning researcher now at Google, which is using vectors based on natural language to improve its search results.\n\nA thought vector is like a word vector, which is typically a vector of 300-500 numbers that represent a word. A word vector represents a word\u2019s meaning as it relates to other words (its context) with a single column of numbers.\n\nThat is, the word is embedded in a vector space using a shallow neural network like word2vec, which learns to generate the word\u2019s context through repeated guesses.\n\nA thought vector, therefore, is a vectorized thought, and the vector represents one thought\u2019s relations to others. A thought vector is trained to generate a thought\u2019s context. Just as a words are linked by grammar (a sentence is just a path drawn across words), so thoughts are linked by a chain of reasoning, a logical path of sorts.\n\nSo training an algorithm to represent any thought in its relation to others might be called the artificial construction of common sense. Given one thought, a neural network might predict the thoughts that are likely to follow, much like recurrent neural networks do with characters and words. Conversation as search.\n\nHinton, in a 2015 speech to the Royal Society in London, said this:\n\nLet\u2019s pause for a moment and consider what Hinton is saying.\n\nTraditional, rules-based AI, a pile of if-then statements locking brittle symbols into hard-coded relationships with others, is not flexible enough to represent the world without near infinite amounts of human intervention. Symbolic logic and knowledge graphs may establish strict relations between entities, but those relations are unlikely to adapt quickly to the new.\n\nHinton is saying that, rather than hard-code the logical leaps that lead an AI from one thought to another, we can simply feed neural nets enough text \u2013 enough trains of thought \u2013 that they will eventually be able to mimic the thoughts expressed there, and generate their own thought trains, the context of the thoughts they\u2019ve been fed.\n\nThis affects how well algorithms will understand natural-language queries at search engines like Google, and it will also go beyond pure search.\n\nWith the ability to associate thoughts comes the ability to converse. Thought vectors could serve as the basis for chatbots, personal assistants, and other agents whose purpose is to augment and entertain human beings. That\u2019s the good side. The bad side is that, on the Internet, you really won\u2019t know who\u2019s a dog, or in this case, a bot.\n\nIf we define thought vectors loosely, we could say they are already being used to represent similar sentences in different languages, which is useful in machine translation. (In fact, improving Google Translate was one the goals that brought thought vectors about.) They are therefore independent of any particular language.\n\nThought vectors can also represent images, which makes them more general than, and independent of, language alone. Thus the term thought, a concept more general that the textual or visual mediums by which it is expressed. The problem with thought vectors, even if we limit ourselves to words, is that their number increases exponentially with the words used to express them. Thoughts are combinatorial. What\u2019s more, one sentence may contain many states, or discrete elements of thought; e.g. x is-a y, or b has-a c. So every sentence might contain and mingle several thoughts.\n\nThis is important, because when we vectorize words, we index those words in a lookup table. In the massive matrix of all words, each word is a vector, and that vector is a row in the matrix. (Each column represents a feature of the word, which in a low-dimensional space would be 300-500 columns.)\n\nGiven that neural networks are already taxing current hardware to its limits, the exponentially larger costs of manipulating a dense matrix containing all thought vectors looks impractical. For now.\n\nThe autonomous conversational agents, or chatbots, so much in the news will probably require proper thought vectors to determine the most appropriate response in a given conversation. For the moment, those chatbots are unable to deliver useful, interesting and plausible responses in a complex conversation of any duration. Our best tools tend to serve up hard answers, like Watson winning at Jeopardy, or 3-5 words sentences, as you find with Google Smart Reply. As of mid-2016, we\u2019re not capable of much more than that.\n\nThe future of this branch of AI will depend on advances in hardware, as well as advances in thought vectorization, or capturing thoughts with numbers in novel ways. (How do we discretize sentences? What are the fundamental units of thought?)\n\nA word should be said about semantic structure. It\u2019s possible to embed dependency and constituency based parsing in vectors. In fact, interesting work is being done at Stanford, Cornell and University of Texas, among other schools.\n\nAdvances in theory and hardware, in turn, will give us other tools to tackle natural language processing and machine conceptualization, the missing link between symbolic logic, which is abstract, and machine percetion via deep learning, which is processing concrete instances of, say, images or sounds.\n\nOn a biological level, thoughts are literally graphs, graphs that capture the connections and action between neurons. Those graphs can represent a network of neurons whose connections fire in different ways over time as synapses fire, a dynamic flow of graphs.\n\nHere are a few of the approaches that are being made to thought vectorization:", 
        "title": "Thought Vectors, Deep Learning & the Future of AI - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/markovchainmontecarlo", 
        "text": "Markov Chain Monte Carlo is a method to sample from a population with a complicated probability distribution. OK.\n\nThere are just a few possible outcomes, and we assume H and T are equally likely. Another word for outcomes is states, as in: what is the end state of the coin flip?\n\nInstead of attempting to measure the probability of states such as heads or tails, we could try to estimate the distribution of and over an unknown earth, where land and water would be states. Or the reading level of children in a school system, where each reading level from 1 through 10 is a state.\n\nMarkov Chain Monte Carlo (MCMC) is a mathematical method that draws samples randomly from a black-box to approximate the probability distribution of attributes over a range of objects (the height of men, the names of babies, the outcomes of events like coin tosses, the reading levels of school children, the rewards resulting from certain actions) or the futures of states. You could say it\u2019s a large-scale statistical method for guess-and-check.\n\nMCMC methods help gauge the distribution of an outcome or statistic you\u2019re trying to predict, by randomly sampling from a complex probabilistic space.\n\nAs with all statistical techniques, we sample from a distribution when we don\u2019t know the function to succinctly describe the relation to two variables (actions and rewards). MCMC helps us approximate a black-box probability distribution.\n\nWith a little more jargon, you might say it\u2019s a simulation using a pseudo-random number generator to produce samples covering many possible outcomes of a given system. The method goes by the name \u201cMonte Carlo\u201d because the capital of Monaco, a coastal enclave bordering southern France, is known for its casinos and games of chance, where winning and losing are a matter of probabilities. It\u2019s \u201cJames Bond math.\u201d\n\nLet\u2019s say you\u2019re a gambler in the saloon of a Gold Rush town and you roll a suspicious die without knowing if it is fair or loaded. You roll a six-sided die a hundred times, count the number of times you roll a four, and divide by a hundred. That gives you the probability of four in the total distribution. If it\u2019s close to 16.7 (1/6 * 100), the die is probably fair.\n\nMonte Carlo looks at the results of rolling the die many times and tallies the results to determine the probabilities of different states. It is an inductive method, drawing from experience. The die has a state space of six, one for each side.\n\nThe states in question can vary. Instead of games of chance, the states might be letters in the Roman alphabet, which has a state space of 26. (\u201ce\u201d happens to be the most frequently occurring letter in the English language\u2026.) They might be stock prices, weather conditions (rainy, sunny, overcast), notes on a scale, electoral outcomes, or pixel colors in a JPEG file. These are all systems of discrete states that can occur in seriatim, one after another. Here are some other ways Monte Carlo is used:\n\nAt a more abstract level, where words mean almost anything at all, a system is a set of things connected together (you might even call it a graph, where each state is a vertex, and each transition is an edge). It\u2019s a set of states, where each state is a condition of the system. But what are states?\n\nSo states are an abstraction used to describe these discrete, separable, things. A group of those states bound together by transitions is a system. And those systems have structure, in that some states are more likely to occur than others (ocean, land), or that some states are more likely to follow others.\n\nWe are more like to read the sequence Paris -> France than Paris -> Texas, although both series exist, just as we are more likely to drive from Los Angeles to Las Vegas than from L.A. to Slab City, although both places are nearby.\n\nA list of all possible states is known as the \u201cstate space.\u201d The more states you have, the larger the state space gets, and the more complex your combinatorial problem becomes.\n\nSince states can occur one after another, it may make sense to traverse the state space, moving from one to the next. A Markov chain is a probabilistic way to traverse a system of states. It traces a series of transitions from one state to another. It\u2019s a random walk across a graph.\n\nEach current state may have a set of possible future states that differs from any other. For example, you can\u2019t drive straight from Atlanta to Seattle - you\u2019ll need to hit other states in between. We are all, always, in such corridors of probabilities; from each state, we face an array of possible future states, which in turn offer an array of future states two degrees away from the start, changing with each step as the state tree unfolds. New possibilites open up, others close behind us. Since we generally don\u2019t have enough compute to explore every possible state of a game tree for complex games like go, one trick that organizations like DeepMind use is Monte Carlo Tree Search to narrow the beam of possibilities to only those states that promise the most likely reward.\n\nTraversing a Markov chain, you\u2019re not sampling with a God\u2019s-eye view any more like a conquering alien. You are in the middle of things, groping your way toward one of several possible future states step by probabilistic step, through a Markov Chain.\n\nWhile our journeys across a state space may seem unique, like road trips across America, an infinite number of road trips would slowly give us a picture of the country as a whole, and the network that links its cities together. This is known as an equilibrium distribution. That is, given infinite random walks through a state space, you can come to know how much total time would be spent in any given state. If this condition holds, you can use Monte Carlo methods to initiate randoms \u201cdraws\u201d, or walks through the state space, in order to sample it.\n\nMarkov chains have a particular property: oblivion, or forgetting.\n\nThat is, they have no long-term memory. They know nothing beyond the present, which means that the only factor determining the transition to a future state is a Markov chain\u2019s current state. You could say the \u201cm\u201d in Markov stands for \u201cmemoryless\u201d: A woman with amnesia pacing through the rooms of a house without knowing why.\n\nOr you might say that Markov Chains assume the entirety of the past is encoded in the present, so we don\u2019t need to know anything more than where we are to infer where we will be next.2\n\nFor an excellent interactive demo of Markov Chains, see the visual explanation on this site.\n\nSo imagine the current state as the input data, and the distribution of attributes related to those states (perhaps that attribute is reward, or perhaps it is simply the most likely future states), as the output. From each state in the system, by sampling you can determine the probability of what will happen next, doing so recursively at each step of the walk through the system\u2019s states.\n\nWhen they call it a state space, they\u2019re not joking. You can picture it, just like you can picture land and water, each one of them a probability as much as they are a physical thing. Unfold a six-sided die and you have a flattened state space in six equal pieces, shapes on a plane. Line up the letters by their frequency for 11 different languages, and you get 11 different state spaces:\n\nFive letters account for half of all characters occurring in Italian, but only a third of Swedish, if you\u2019re just dropping socks from the sky.\n\nIf you wanted to look at the English language alone, you would get this set of histograms. Here, probabilities are defined by a line traced across the top, and the area under the line can be measured with a calculus operation called integration, the opposite of a derivative.\n\nMCMC can be used in the context of deep reinforcement learning to sample from the array of possible actions available in any given state. For more information, please see our page on Deep Reinforcement Learning.\n\n1) You could say that life itself is too complex to know in its entirety, confronted as we are with imperfect information about what\u2019s happening and the intent of others. You could even say that each individual human is a random draw that the human species took from the complex probability distribution of life, and that we are each engaged in a somewhat random walk. Literature is one way we overcome the strict constraints on the information available to us through direct experience, the limited bandwidth of our hours and sensory organs and organic processing power. Simulations mediated by books expose us to other random walks and build up some predictive capacity about states we have never physically encountered. Which brings us to the fundamental problems confronted by science: How can we learn what we don\u2019t know? How can we test what we think we know? How can we say what we want to know? (Expressed that in a way an algorithm can understand.) How can we guess smarter? (Random guesses are pretty inefficient\u2026)\n\n2) It\u2019s interesting to think about how these ways of thinking translate, or fail to translate, to real life. For example, while we might agree that the entirety of the past is encoded in the present moment, we cannot know the entirety of the present moment. The walk of any individual agent through life will reveal certain elements of the past at time step 1, and others at time step 10, and others still will not be revealed at all because in life we are faced with imperfect information \u2013 unlike in Go or Chess. Recurrent neural nets are structurally Markovian, in that the tensors passed forward through their hidden units contain everything the network needs to know about the past. LSTMs are thought to be more effective at retaining information about a larger state space (more of the past), than other algorithms such as Hidden Markov Models; i.e. they decrease how imperfect the information is upon which they base their decisions.", 
        "title": "Markov Chain Monte Carlo and Machine Learning - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/unsupervised-learning", 
        "text": "Unlike supervised learning, with unsupervised learning, we are working without a labeled dataset. What we generally learn, in the absence of a label, is how to reconstruct the input data using a representation, or embedding.\n\nThe features learned by deep neural networks can be used for the purposes of classification, clustering and regression.\n\nNeural nets are simply universal approximators using non-linearities. They produce \u201cgood\u201d features by learning to reconstruct data through pretraining or through backpropagation. In the latter case, neural nets plug into arbitrary loss functions to map inputs to outputs.\n\nThe features learned by neural networks can be fed into any variety of other algorithms, including traditional machine-learning algorithms that group input, softmax/logistic regression that classifies it, or simple regression that predicts a value.\n\nSo you can think of neural networks as feature-producers that plug modularly into other functions. For example, you could make a convolutional neural network learn image features on ImageNet with supervised training, and then you could take the activations/features learned by that neural network and feed it into a second algorithm that would learn to group images.\n\nHere is a list of use cases for features generated by neural networks:\n\nt-distributed stochastic neighbor embedding (T-SNE) is an algorithm used to reduce high-dimensional data into two or three dimensions, which can then be represented in a scatterplot. T-SNE is used for finding latent trends in data. Deeplearning4j relies on T-SNE for some visualizations, and it is an interesting end point for neural network features. For more information and downloads, see this page on T-SNE.\n\nRenders - Deeplearning4j relies on visual renders as heuristics to monitor how well a neural network is learning. That is, renders are used to debug. They help us visualize activations over time, and activations over time are an indicator of what and how much the network is learning.\n\nK-Means is an algorithm used for automatically labeling activations based on their raw distances from other input in a vector space. There is no target or loss function; k-means picks so-called centroids. K-means creates centroids through a repeated averaging of all the data points. K-means classifies new data by its proximity to a given centroid. Each centroid is associated with a label. This is an example of unsupervised learning (learning lacking a loss function) that applies labels.\n\nTransfer learning takes the activations of one neural network and puts them to use as features for another algorithm or classifier. For example, you can take the model of a ConvNet trained on ImageNet, and pass fresh images through it into another algorithm, such as K-Nearest Neighbor. The strict definition of transfer learning is just that: taking the model trained on one set of data, and plugging it into another problem.\n\nMike Depies has written a tutorial about how to combine Deeplearning4j and K-Nearest Neighbor here.\n\nThis algorithm serves the purposes of classification and regression, and relies on a kd-tree. A kd-tree is a data structure for storing a finite set of points from a k-dimensional space. It partitions a space of arbitrary dimensions into a tree, which may also be called a vantage point tree. kd-trees subdivide a space with a tree structure, and you navigate the tree to find the closest points. The label associated with the closest points is applied to input.\n\nLet your input and training examples be vectors. Training vectors might be arranged in a binary tree like so:\n\nIf you were to visualize those nodes in two dimensions, partitioning space at each branch, then the kd-tree would look like this:\n\nNow, let\u2019s saw you place a new input, X, in the tree\u2019s partitioned space. This allows you to identify both the parent and child of that space within the tree. The X then constitutes the center of a circle whose radius is the distance to the child node of that space. By definition, only other nodes within the circle\u2019s circumference can be nearer.\n\nAnd finally, if you want to make art with kd-trees, you could do a lot worse than this:\n\nThe underlying implementation for K Nearest Neighbors is the VP Tree, which we\u2019ve implemented here. You can think of it as a search engine for coordinate spaces.", 
        "title": "A Beginner's Guide to Unsupervised Learning - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/generative-adversarial-network", 
        "text": "Generative adversarial networks (GANs) are deep neural net architectures comprised of two nets, pitting one against the other (thus the \u201cadversarial\u201d).\n\nGANs were introduced in a paper by Ian Goodfellow and other researchers at the University of Montreal, including Yoshua Bengio, in 2014. Referring to GANs, Facebook\u2019s AI research director Yann LeCun called adversarial training \u201cthe most interesting idea in the last 10 years in ML.\u201d\n\nGANs\u2019 potential is huge, because they can learn to mimic any distribution of data. That is, GANs can be taught to create worlds eerily similar to our own in any domain: images, music, speech, prose. They are robot artists in a sense, and their output is impressive \u2013 poignant even.\n\nTo understand GANs, you should know how generative algorithms work, and for that, contrasting them with discriminative algorithms is instructive. Discriminative algorithms try to classify input data; that is, given the features of a data instance, they predict a label or category to which that data belongs.\n\nFor example, given all the words in an email, a discriminative algorithm could predict whether the message is or . is one of the labels, and the bag of words gathered from the email are the features that constitute the input data. When this problem is expressed mathematically, the label is called and the features are called . The formulation is used to mean \u201cthe probability of y given x\u201d, which in this case would translate to \u201cthe probability that an email is spam given the words it contains.\u201d\n\nSo discriminative algorithms map features to labels. They are concerned solely with that correlation. One way to think about generative algorithms is that they do the opposite. Instead of predicting a label given certain features, they attempt to predict features given a certain label.\n\nThe question a generative algorithm tries to answer is: Assuming this email is spam, how likely are these features? While discriminative models care about the relation between and , generative models care about \u201chow you get x.\u201d They allow you to capture , the probability of given , or the probability of features given a class. (That said, generative algorithms can also be used as classifiers. It just so happens that they can do more than categorize input data.)\n\nAnother way to think about it is to distinguish discriminative from generative like this:\n\nOne neural network, called the generator, generates new data instances, while the other, the discriminator, evaluates them for authenticity; i.e. the discriminator decides whether each instance of data it reviews belongs to the actual training dataset or not.\n\nLet\u2019s say we\u2019re trying to do something more banal than mimic the Mona Lisa. We\u2019re going to generate hand-written numerals like those found in the MNIST dataset, which is taken from the real world. The goal of the discriminator, when shown an instance from the true MNIST dataset, is to recognize them as authentic.\n\nMeanwhile, the generator is creating new images that it passes to the discriminator. It does so in the hopes that they, too, will be deemed authentic, even though they are fake. The goal of the generator is to generate passable hand-written digits, to lie without being caught. The goal of the discriminator is to identify images coming from the generator as fake.\n\nHere are the steps a GAN takes:\n\nSo you have a double feedback loop:\n\nYou can think of a GAN as the combination of a counterfeiter and a cop in a game of cat and mouse, where the counterfeiter is learning to pass false notes, and the cop is learning to detect them. Both are dynamic; i.e. the cop is in training, too (maybe the central bank is flagging bills that slipped through), and each side comes to learn the other\u2019s methods in a constant escalation.\n\nThe discriminator network is a standard convolutional network that can categorize the images fed to it, a binomial classifier labeling images as real or fake. The generator is an inverse convolutional network, in a sense: While a standard convolutional classifier takes an image and downsamples it to produce a probability, the generator takes a vector of random noise and upsamples it to an image. The first throws away data through downsampling techniques like maxpooling, and the second generates new data.\n\nBoth nets are trying to optimize a different and opposing objective function, or loss function, in a zero-zum game. This is essentially an actor-critic model. As the discriminator changes its behavior, so does the generator, and vice versa. Their losses push against each other.\n\nIf you want to learn more about generating images, Brandon Amos wrote a great post about interpreting images as samples from a probability distribution.\n\nIt may be useful to compare generative adversarial networks to other neural networks, such as autoencoders and variational autoencoders.\n\nAutoencoders encode input data as vectors. They create a hidden, or compressed, representation of the raw data. They are useful in dimensionality reduction; that is, the vector serving as a hidden representation compresses the raw data into a smaller number of salient dimensions. Autoencoders can be paired with a so-called decoder, which allows you to reconstruct input data based on its hidden representation, much as you would with a restricted Boltzmann machine.\n\nVariational autoencoders are generative algorithm that add an additional constraint to encoding the input data, namely that the hidden representations are normalized. Variational autoencoders are capable of both compressing data like an autoencoder and synthesizing data like a GAN. However, while GANs generate data in fine, granular detail, images generated by VAEs tend to be more blurred. Deeplearning4j\u2019s examples include both autoencoders and variational autoencoders.\n\nYou can bucket generative algorithms into one of three types:\n\nWhen you train the discriminator, hold the generator values constant; and when you train the generator, hold the discriminator constant. Each should train against a static adversary. For example, this gives the generator a better read on the gradient it must learn by.\n\nBy the same token, pretraining the discriminator against MNIST before you start training the generator will establish a clearer gradient.\n\nEach side of the GAN can overpower the other. If the discriminator is too good, it will return values so close to 0 or 1 that the generator will struggle to read the gradient. If the generator is too good, it will persistently exploit weaknesses in the discriminator that lead to false negatives. This may be mitigated by the nets\u2019 respective learning rates.\n\nGANs take a long time to train. On a single GPU a GAN might take hours, and on a single CPU more than a day. While difficult to tune and therefore to use, GANs have stimulated a lot of interesting research and writing.\n\nHere\u2019s an example of a GAN coded in Keras, from which models can be imported to Deeplearning4j.\n\nNote: SKIL enables developers to build GANs. Deeplearning4j\u2019s latest 1.0.0-alpha release on Maven includes deconvolution and upsampling layers, which enable GANs.", 
        "title": "GAN: A Beginner\u2019s Guide to Generative Adversarial Networks - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/deepreinforcementlearning", 
        "text": "While neural networks are responsible for recent breakthroughs in problems like computer vision, machine translation and time series prediction \u2013 they can also combine with reinforcement learning algorithms to create something astounding like AlphaGo.\n\nReinforcement learning refers to goal-oriented algorithms, which learn how to attain a complex objective (goal) or maximize along a particular dimension over many steps; for example, maximize the points won in a game over many moves. They can start from a blank slate, and under the right conditions they achieve superhuman performance. Like a child incentivized by spankings and candy, these algorithms are penalized when they make the wrong decisions and rewarded when they make the right ones \u2013 this is reinforcement.\n\nReinforcement algorithms that incorporate deep learning can beat world champions at the game of Go as well as human experts playing numerous Atari video games. While that may sound trivial, it\u2019s a vast improvement over their previous accomplishments, and the state of the art is progressing rapidly.\n\nReinforcement learning solves the difficult problem of correlating immediate actions with the delayed returns they produce. Like humans, reinforcement learning algorithms sometimes have to wait a while to see the fruit of their decisions. They operate in a delayed return environment, where it can be difficult to understand which action leads to which outcome over many time steps.\n\nReinforcement learning algorithms can be expected to perform better and better in more ambiguous, real-life environments while choosing from an arbitrary number of possible actions, rather than from the limited options of a video game. That is, with time we expect them to be valuable to achieve goals in the real world.\n\nTwo reinforcement learning algorithms - Deep-Q learning and A3C - have been implemented in a Deeplearning4j library called RL4J. It can already play Doom.\n\nReinforcement learning can be understand using the concepts of agents, environments, states, actions and rewards, all of which we\u2019ll explain below. Capital letters tend to denote sets of things, and lower-case letters denote a specific instance of that thing; e.g. is all possible actions, while is a specific action contained in the set.\n\nSo environments are functions that transform an action taken in the current state into the next state and a reward; agents are functions that transform the new state and reward into the next action. We can know the agent\u2019s function, but we cannot know the function of the environment. It is a black box where we only see the inputs and outputs. Reinforcement learning represents an agent\u2019s attempt to approximate the environment\u2019s function, such that we can send actions into the black-box environment that maximize the rewards it spits out.\n\nIn the feedback loop above, the subscripts denote the time steps and , each of which refer to different states: the state at moment , and the state at moment . Unlike other forms of machine learning \u2013 such as supervised and unsupervised learning \u2013 reinforcement learning can only be thought about sequentially in terms of state-action pairs that occur one after the other.\n\nReinforcement learning judges actions by the results they produce. It is goal oriented, and its aim is to learn sequences of actions that will lead an agent to achieve its goal, or maximize its objective function. Here are some examples:\n\nHere\u2019s an example of an objective function for reinforcement learning; i.e. the way it defines its goal.\n\nWe are summing reward function r over t, which stands for time steps. So this objective function calculates all the reward we could obtain by running through, say, a game. Here, x is the state at a given time step, and a is the action taken in that state. r is the reward function for x and a. (We\u2019ll ignore \u03b3 for now.)\n\nReinforcement learning differs from both supervised and unsupervised learning by how it interprets inputs. We can illustrate their difference by describing what they learn about a \u201cthing.\u201d\n\nOne way to imagine an autonomous reinforcement learning agent would be as a blind person attempting to navigate the world with only their ears and a white cane. Agents have small windows that allow them to perceive their environment, and those windows may not even be the most appropriate way for them to perceive what\u2019s around them.\n\nThe goal of reinforcement learning is to pick the best known action for any given state, which means the actions have to be ranked, and assigned values relative to one another. Since those actions are state-dependent, what we are really gauging is the value of state-action pairs; i.e. an action taken from a certain state, something you did somewhere. Here are a few examples to demonstrate that the value and meaning of an action is contingent upon the state in which it is taken:\n\nWe map state-action pairs to the values we expect them to produce with the Q function, described above. The Q function takes as its input an agent\u2019s state and action, and maps them to probable rewards.\n\nReinforcement learning is the process of running the agent through sequences of state-action pairs, observing the rewards that result, and adapting the predictions of the Q function to those rewards until it accurately predicts the best path for the agent to take. That prediction is known as a policy.\n\nReinforcement learning is an attempt to model a complex probability distribution of rewards in relation to a very large number of state-action pairs. This is one reason reinforcement learning is paired with, say, a Markov decision process, a method to sample from a complex distribution to infer its properties. It closely resembles the problem that inspired Stan Ulam to invent the Monte Carlo method; namely, trying to infer the chances that a given hand of solitaire will turn out successful.\n\nAny statistical approach is essentially a confession of ignorance. The immense complexity of some phenomena (biological, political, sociological, or related to board games) make it impossible to reason from first principles. The only way to study them is through statistics, measuring superficial events and attempting to establish correlations between them, even when we do not understand the mechanism by which they relate. Reinforcement learning, like deep neural networks, is one such strategy, relying on sampling to extract information from data.\n\nAfter a little time spent employing something like a Markov decision process to approximate the probability distribution of reward over state-action pairs, a reinforcement learning algorithm may tend to repeat actions that lead to reward and cease to test alternatives. There is a tension between the exploitation of known rewards, and continued exploration to discover new actions that also lead to victory. Just as oil companies have the dual function of pumping crude out of known oil fields while drilling for new reserves, so too, reinforcement learning algorithms can be made to both exploit and explore to varying degrees, in order to ensure that they don\u2019t pass over rewarding actions at the expense of known winners.\n\nReinforcement learning is iterative. In its most interesting applications, it doesn\u2019t begin by knowing which rewards state-action pairs will produce. It learns those relations by running through states again and again, like athletes or musicians iterate through states in an attempt to improve their performance.\n\nYou could say that an algorithm is a method to more quickly aggregate the lessons of time. Reinforcement learning algorithms have a different relationship to time than humans do. An algorithm can run through the same states over and over again while experimenting with different actions, until it can infer which actions are best from which states. Effectively, algorithms enjoy their very own Groundhog Day, where they start out as dumb jerks and slowly get wise.\n\nSince humans never experience Groundhog Day outside the movie, reinforcement learning algorithms have the potential to learn more, and better, than humans. Indeed, the true advantage of these algorithms over humans stems not so much from their inherent nature, but from their ability to live in parallel on many chips at once, to train night and day without fatigue, and therefore to learn more. An algorithm trained on the game of Go, such as AlphaGo, will have played many more games of Go than any human could hope to complete in 100 lifetimes.2\n\nWhere do neural networks fit in? Neural networks are the agent that learns to map state-action pairs to rewards. Like all neural networks, they use coefficients to approximate the function relating inputs to outputs, and their learning consists to finding the right coefficients, or weights, by iteratively adjusting those weights along gradients that promise less error.\n\nIn reinforcement learning, convolutional networks can be used to recognize an agent\u2019s state; e.g. the screen that Mario is on, or the terrain before a drone. That is, they perform their typical task of image recognition.\n\nBut convolutional networks derive different interpretations from images in reinforcement learning than in supervised learning. In supervised learning, the network applies a label to an image; that is, it matches names to pixels.\n\nIn fact, it will rank the labels that best fit the image in terms of their probabilities. Shown an image of a donkey, it might decide the picture is 80% likely to be a donkey, 50% likely to be a horse, and 30% likely to be a dog.\n\nIn reinforcement learning, given an image that represents a state, a convolutional net can rank the actions possible to perform in that state; for example, it might predict that running right will return 5 points, jumping 7, and running left none.\n\nThe above image illustrates what a policy agent does, mapping a state to the best action.\n\nIf you recall, this is distinct from Q, which maps state action pairs to rewards.\n\nTo be more specific, Q maps state-action pairs to the highest combination of immediate reward with all future rewards that might be harvested by later actions in the trajectory. Here is the equation for Q, from Wikipedia:\n\nHaving assigned values to the expected rewards, the Q function simply selects the state-action pair with the highest so-called Q value.\n\nAt the beginning of reinforcement learning, the neural network coefficients may be initialized stochastically, or randomly. Using feedback from the environment, the neural net can use the difference between its expected reward and the ground-truth reward to adjust its weights and improve its interpretation of state-action pairs.\n\nThis feedback loop is analogous to the backpropagation of error in supervised learning. However, supervised learning begins with knowledge of the ground-truth labels the neural network is trying to predict. Its goal is to create a model that maps different images to their respective names.\n\nReinforcement learning relies on the environment to send it a scalar number in response to each new action. The rewards returned by the environment can be varied, delayed or affected by unknown variables, introducing noise to the feedback loop.\n\nThis leads us to a more complete expression of the Q function, which takes into account not only the immediate rewards produced by an action, but also the delayed rewards that may be returned several time steps deeper in the sequence.\n\nLike human beings, the Q function is recursive. Just as calling the wetware method contains within it another method , of which we are all the fruit, calling the Q function on a given state-action pair requires us to call a nested Q function to predict the value of the next state, which in turn depends on the Q function of the state after that, and so forth.\n\nRL4J examples are available here.\n\n1) It might be helpful to imagine a reinforcement learning algorithm in action, to paint it visually. Let\u2019s say the algorithm is learning to play the video game Super Mario. It\u2019s trying to get Mario through the game and acquire the most points. To do that, we can spin up lots of different Marios in parallel and run them through the space of all possible game states. It\u2019s as though you have 1,000 Marios all tunnelling through a mountain, and as they dig (e.g. as they decide again and again which action to take to affect the game environment), their experience-tunnels branch like the intricate and fractal twigs of a tree. The Marios\u2019 experience-tunnels are corridors of light cutting through the mountain. And as in life itself, one successful action may make it more likely that successful action is possible in a larger decision flow, propelling the winning Marios onward. You might also imagine, if each Mario is an agent, that in front of him is a heat map tracking the rewards he can associate with state-action pairs. (Imagine each state-action pair as have its own screen overlayed with heat from yellow to red. The many screens are assembled in a grid, like you might see in front of a Wall St. trader with many monitors. One action screen might be \u201cjump harder from this state\u201d, another might be \u201crun faster in this state\u201d and so on and so forth.) Since some state-action pairs lead to significantly more reward than others, and different kinds of actions such as jumping, squatting or running can be taken, the probability distribution of reward over actions is not a bell curve but instead complex, which is why Markov and Monte Carlo techniques are used to explore it, much as Stan Ulam explored winning Solitaire hands. That is, while it is difficult to describe the reward distribution in a formula, it can be sampled. Because the algorithm starts ignorant and many of the paths through the game-state space are unexplored, the heat maps will reflect their lack of experience; i.e. there could be blanks in the heatmap of the rewards they imagine, or they might just start with some default assumptions about rewards that will be adjusted with experience. The Marios are essentially reward-seeking missiles guided by those heatmaps, and the more times they run through the game, the more accurate their heatmap of potential future reward becomes. The heatmaps are basically probability distributions of reward over the state-action pairs possible from the Mario\u2019s current state.\n\n2) The correct analogy may actually be that a learning algorithm is like a species. Each simulation the algorithm runs as it learns could be considered an individual of the species. Just as knowledge from the algorithm\u2019s runs through the game is collected in the algorithm\u2019s model of the world, the individual humans of any group will report back via language, allowing the collective\u2019s model of the world, embodied in its texts, records and oral traditions, to become more intelligent (At least in the ideal case. The subversion and noise introduced into our collective models is a topic for another post, and probably for another website entirely.). This puts a finer point on why the contest between algorithms and individual humans, even when the humans are world champions, is unfair. We are pitting a civilization that has accumulated the wisdom of 10,000 lives against a single sack of flesh.", 
        "title": "A Beginner's Guide to Deep Reinforcement Learning - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/scala", 
        "text": "Scala programmers seeking to build machine learning solutions can use Deeplearning4j\u2019s Scala API ScalNet or work with the Java framework using the pattern.\n\nSkymind\u2019s numerical computing library, ND4J (n-dimensional arrays for the JVM), comes with a Scala API, ND4S. Our full walkthrough of Deeplearning4j\u2019s Apache Spark integration is here. Our examples include a number of tutorials using Scala notebooks with Zepellin.\n\nScala is one of the most exciting languages to be created in the 21st century. It is a multi-paradigm language that fully supports functional, object-oriented, imperative and concurrent programming. It also has a strong type system, and from our point of view, strong type is a convenient form of self-documenting code.\n\nScala works on the JVM and has access to the riches of the Java ecosystem, but it is less verbose than Java. As we employ it for ND4J, its syntax is strikingly similar to Python, a language that many data scientists are comfortable with. Like Python, Scala makes programmers happy, but like Java, it is quite fast.\n\nFinally, Apache Spark is written in Scala, and any library that purports to work on distributed run times should at the very least be able to interface with Spark. Deeplearning4j and ND4J go a step further, because they work in a Spark cluster, and boast Scala APIs called ScalNet and ND4S.\n\nWe believe Scala\u2019s many strengths will lead it to dominate numerical computing, as well as deep learning. We think that will happen on Spark. And we have tried to build the tools to make it happen now.\n\nDeeplearning4j depends on Apache Spark for fast ETL. While many machine-learning tools rely on Spark for computation, this is in fact quite inefficient, and slows down neural net training. The trick to using Apache Spark is pushing the computation to a numerical computing library like ND4J, and its underlying C++ code.", 
        "title": "Scala, Apache Spark and Deeplearning4j - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/compare-dl4j-torch7-pylearn", 
        "text": "", 
        "title": "Redirecting\u2026"
    }, 
    {
        "url": "https://deeplearning4j.org/glossary", 
        "text": "The intent of this glossary is to provide clear definitions of the technical terms specific to deep artificial neural networks. It is a work in progress.\n\nAn activation, or activation function, for a neural network is defined as the mapping of the input to the output via a non-linear transform function at each \u201cnode\u201d, which is simply a locus of computation within the net. Each layer in a neural net consists of many nodes, and the number of nodes in a layer is known as its width.\n\nActivation algorithms are the gates that determine, at each node in the net, whether and to what extent to transmit the signal the node has received from the previous layer. A combination of weights (coefficients) and biases work on the input data from the previous layer to determine whether that signal surpasses a given treshhold and is deemed significant. Those weights and biases are slowly updated as the neural net minimizes its error; i.e. the level of nodes\u2019 activation change in the course of learning. Deeplearning4j includes activation functions such as sigmoid, relu, tanh and ELU. These activation functions allow neural networks to make complex boundary decisions for features at various levels of abstraction.\n\nAdadelta is an updater, or learning algorithm, related to gradient descent. Unlike SGD, which applies the same learning rate to all parameters of the network, Adadelta adapts the learning rate per parameter.\n\nAdagrad, short for adaptive gradient, is an updater or learning algorithm that adjust the learning rate for each parameter in the net by monitoring the squared gradients in the course of learning. It is a substitute for SGD, and can be useful when processing sparse data.\n\nAdam (Gibson) co-created Deeplearning4j. :) Adam is also an updater, similar to rmsprop, which uses a running average of the gradient\u2019s first and second moment plus a bias-correction term.\n\nAffine is a fancy word for a fully connected layer in a neural network. \u201cFully connected\u201d means that all the nodes of one layer connect to all the nodes of the subsequent layer. A restricted Boltzmann machine, for example, is a fully connected layer. Convolutional networks use affine layers interspersed with both their namesake convolutional layers (which create feature maps based on convolutions) and downsampling layers, which throw out a lot of data and only keep the maximum value. \u201cAffine\u201d derives from the Latin , which means bordering or connected with. Each connection, in an affine layer, is a passage whereby input is multiplied by a weight and added to a bias before it accumulates with all other inputs at a given node, the sum of which is then passed through an activation function: e.g. , or .\n\nAlexNet is a deep convolutional network named after Alex Krizhevsky, a former student of Geoff Hinton\u2019s at the University of Toronto, now at Google. AlexNet was used to win ILSVRC 2012, and foretold a wave of deep convolutional networks that would set new records in image recognition. AlexNet is now a standard architecture: it contains five convolutional layers, three of which are followed by max-pooling (downsampling) layers, two fully connected (affine) layers \u2013 all of which ends in a softmax layer. Here is Deeplearning4j\u2019s implementation of AlexNet.\n\nAttention models \u201cattend\u201d to specific parts of an image in sequence, one after another. By relying on a sequence of glances, they capture visual structure, much like the human eye is believed to function with foveation. This visual processing, which relies on a recurrent network to process sequential data, can be contrasted with other machine vision techniques that process a whole image in a single, forward pass.\n\nAutoencoders are at the heart of representation learning. They encode input, usually by compressing large vectors into smaller vectors that capture their most significant features; that is, they are useful for data compression (dimensionality reduction) as well as data reconstruction for unsupervised learning. A restricted Boltzmann machine is a type of autoencoder, and in fact, autoencoders come in many flavors, including Variational Autoencoders, Denoising Autoencoders and Sequence Autoencoders. Variational autoencoders have replaced RBMs in many labs because they produce more stable results. Denoising autoencoders provide a form of regularization by introducing Gaussian noise into the input, which the network learns to ignore in search of the true signal.\n\nTo calculate the gradient the relate weights to error, we use a technique known as backpropagation, which is also referred to as the backward pass of the network. Backpropagation is a repeated application of chain rule of calculus for partial derivatives. The first step is to calculate the derivatives of the objective function with respect to the output units, then the derivatives of the output of the last hidden layer to the input of the last hidden layer; then the input of the last hidden layer to the weights between it and the penultimate hidden layer, etc. Here\u2019s a derivation of backpropagation. And here\u2019s Yann LeCun\u2019s important paper on the subject.\n\nA special form of backpropagation is called backpropagation through time, or BPTT, which is specifically useful for recurrent networks analyzing text and time series. With BPTT, each time step of the RNN is the equivalent of a layer in a feed-forward network. To backpropagate over many time steps, BPTT can be truncated for the purpose of efficiency. Truncated BPTT limits the time steps over which error is propagated.\n\nBatch Normalization does what is says: it normalizes mini-batches as they\u2019re fed into a neural-net layer. Batch normalization has two potential benefits: it can accelerate learning because it allows you to employ higher learning rates, and also regularizes that learning.\n\nBayes\u2019 Theorem is a mathematical framework for integrating new evidence with prior beliefs. For example, suppose you\u2019re sitting in your quiet suburban home and you hear something that sounds like a lion roaring. You have some prior beliefs that lions are unlikely to be near your house, so you figure that it\u2019s probably not a lion. Probably it\u2019s some weird machine of your neighbor\u2019s that just happens to sound like a lion, or some kids pranking you by playing lion noises, or something. You end up believing that there\u2019s probably no lion nearby, but you do have a slightly higher probability of there being a lion nearby than you had before you heard the roaring noise. Bayes\u2019 Theorem is just this kind of reasoning converted to math. source\n\nA Bidirectional RNN is composed of two RNNs that process data in opposite directions. One reads a given sequence from start to finish; the other reads it from finish to start. Bidirectional RNNs are employed in NLP for translation problems, among other use cases. Deeplearning4j\u2019s implementation of bidirectional Graves LSTMs is here.\n\nThe process of transforming data in to a set of zeros and ones. An example would be gray-scaling an image by transforming a picture from the 0-255 spectrum to a 0-1 spectrum.\n\n\u201cA Boltzmann machine learns internal (not defined by the user) concepts that help to explain (that can generate) the observed data. These concepts are captured by random variables (called hidden units) that have a joint distribution (statistical dependencies) among themselves and with the data, and that allow the learner to capture highly non-linear and complex interactions between the parts (observed random variables) of any observed example (like the pixels in an image). You can also think of these higher-level factors or hidden units as another, more abstract, representation of the data. The Boltzmann machine is parametrized through simple two-way interactions between every pair of random variable involved (the observed ones as well as the hidden ones).\u201d - Yoshua Bengio\n\nChannel is a word used when speaking of convolutional networks. ConvNets treat color images as volumes; that is, an image has height, width and depth. The depth is the number of channels, which coincide with how you encode colors. RGB images have three channels, for red, green and blue respectively.\n\nUsed in classification a Class refers to a label applied to a group of records sharing similar characteristics.\n\nAlso known as an error matrix or contingency table. Confusions matrices allow you to see if your algorithm is systematically confusing two labels, by contrasting your net\u2019s predictions against a benchmark.\n\n\u201cContrastive divergence is a recipe for training undirected graphical models (a class of probabilistic models used in machine learning). It relies on an approximation of the gradient (a good direction of change for the parameters) of the log-likelihood (the basic criterion that most probabilistic learning algorithms try to optimize) based on a short Markov chain (a way to sample from probabilistic models) started at the last example seen. It has been popularized in the context of Restricted Boltzmann Machines (Hinton & Salakhutdinov, 2006, Science), the latter being the first and most popular building block for deep learning algorithms.\u201d ~Yoshua Bengio\n\nConvolutional networks are a deep neural network that is currently the state-of-the-art in image processing. They are setting new records in accuracy every year on widely accepted benchmark contests like ImageNet.\n\nFrom the Latin convolvere, \u201cto convolve\u201d means to roll together. For mathematical purposes, a convolution is the integral measuring how much two functions overlap as one passes over the other. Think of a convolution as a way of mixing two functions by multiplying them: a fancy form of multiplication.\n\nImagine a tall, narrow bell curve standing in the middle of a graph. The integral is the area under that curve. Imagine near it a second bell curve that is shorter and wider, drifting slowly from the left side of the graph to the right. The product of those two functions\u2019 overlap at each point along the x-axis is their convolution. So in a sense, the two functions are being \u201crolled together.\u201d\n\nIt turns out two vectors are just 66% of a triangle, so let\u2019s do a quick trig review.\n\nTrigonometric functions like sine, cosine and tangent are ratios that use the lengths of a side of a right triangle (opposite, adjacent and hypotenuse) to compute the shape\u2019s angles. By feeding the sides into ratios like these\n\nwe can also know the angles at which those sides intersect. Remember SOH-CAH-TOA?\n\nDifferences between word vectors, as they swing around the origin like the arms of a clock, can be thought of as differences in degrees.\n\nAnd similar to ancient navigators gauging the stars by a sextant, we will measure the angular distance between words using something called cosine similarity. You can think of words as points of light in a dark canopy, clustered together in constellations of meaning.\n\nTo find that distance knowing only the word vectors, we need the equation for vector dot multiplication (multiplying two vectors to produce a single, scalar value).\n\nIn Java, you can think of the formula to measure cosine similarity like this:\n\nCosine is the angle attached to the origin, which makes it useful here. (We normalize the measurements so they come out as percentages, where 1 means that two vectors are equal, and 0 means they are perpendicular, bearing no relation to each other.)\n\nTraining a neural network on a very large dataset requires some form of parallelism, of which there are two types: data parallelism and model parallelism.\n\nLet\u2019s say you have a very large image dataset of 1,000,000 faces. Those faces can be divided into batches of 10, and then 10 separate batches can be dispatched simultaneously to 10 different convolutional networks, so that 100 instances can be processed at once. The 10 different CNNs would then train on a batch, calculate the error on that batch, and update their parameters based on that error. Then, using parameter averaging, the 10 CNNs would update a central, master CNN that would take the average of their updated paramters. This process would repeat until the entire dataset has been exhausted. For more information, please see our page on iterative reduce.\n\nModel parallelism is another way to accelerate neural net training on very large datasets. Here, instead of sending batches of faces to separate neural networks, let\u2019s imagine a different kind of image: an enormous map of the earth. Model parallelism would divide that enormous map into regions, and it would park a separate CNN on each region, to train on only that area and no other. Then, as each enormous map was peeled off the dataset to train the neural networks, it would be broken up and different patches of it would be sent to train on separate CNNs. No parameter averaging necessary here.\n\nData science is the discipline of drawing conclusions from data using computation. There are three core aspects of effective data analysis: exploration, prediction, and inference.\n\nA deep-belief network is a stack of restricted Boltzmann machines, which are themselves a feed-forward autoencoder that learns to reconstruct input layer by layer, greedily. Pioneered by Geoff Hinton and crew. Because a DBN is deep, it learns a hierarchical representation of input. Because DBNs learn to reconstruct that data, they can be useful in unsupervised learning.\n\nThe Nupic community has a good explanation of distributed representations here. Other good explanations can be found on this Quora page.\n\nDownpour stochastic gradient descent is an asynchronous stochastic gradient descent procedure, employed by Google among others, that expands the scale and increases the speed of training deep-learning networks.\n\nDropout is a hyperparameter used for regularization in neural networks. Like all regularization techniques, its purpose is to prevent overfitting. Dropout randomly makes nodes in the neural network \u201cdrop out\u201d by setting them to zero, which encourages the network to rely on other features that act as signals. That, in turn, creates more generalizable representations of data.\n\nDropConnect is a generalization of Dropout for regularizing large fully-connected layers within neural networks. Dropout sets a randomly selected subset of activations to zero at each layer. DropConnect, in contrast, sets a randomly selected subset of weights within the network to zero.\n\nAn embedding is a representation of input, or an encoding. For example, a neural word embedding is a vector that represents that word. The word is said to be embedded in vector space. Word2vec and GloVe are two techniques used to train word embeddings to predict a word\u2019s context. Because an embedding is a form of representation learning, we can \u201cembed\u201d any data type, including sounds, images and time series.\n\nIn machine-learning parlance, an epoch is a complete pass through a given dataset. That is, by the end of one epoch, your neural network \u2013 be it a restricted Boltzmann machine, convolutional net or deep-belief network \u2013 will have been exposed to every record to example within the dataset once. Not to be confused with an iteration, which is simply one update of the neural net model\u2019s parameters. Many iterations can occur before an epoch is over. Epoch and iteration are only synonymous if you update your parameters once for each pass through the whole dataset.\n\nData is loaded from disk or other sources into memory with the proper transforms such as binarization and normalization. Broadly, you can think of a datapipeline as the process over gathering data from disparate sources and locations, putting it into a form that your algorithms can learn from, and then placing it in a data structure that they can iterate through.\n\nThe f1 score is a number between zero and one that explains how well the network performed during training. It is analogous to a percentage, with 1 being the best score and zero the worst. f1 is basically the probability that your net\u2019s guesses are correct.\n\nAccuracy measures how often you get the right answer, while f1 scores are a measure of accuracy. For example, if you have 100 fruit \u2013 99 apples and 1 orange \u2013 and your model predicts that all 100 items are apples, then it is 99% accurate. But that model failed to identify the difference between apples and oranges. f1 scores help you judge whether a model is actually doing well as classifying when you have an imbalance in the categories you\u2019re trying to tag.\n\nAn f1 score is an average of both precision and recall. More specifically, it is a type of average called the harmonic mean, which tends to be less than the arithmetic or geometric means. Recall answers: \u201cGiven a positive example, how likely is the classifier going to detect it?\u201d It is the ratio of true positives to the sum of true positives and false negatives.\n\nPrecision answers: \u201cGiven a positive prediction from the classifier, how likely is it to be correct ?\u201d It is the ratio of true positives to the sum of true positives and false positives.\n\nFor f1 to be high, both recall and precision of the model have to be high.\n\nA neural network that takes the initial input and triggers the activation of each layer of the network successively, without circulating. Feed-forward nets contrast with recurrent and recursive nets in that feed-forward nets never let the output of one node circle back to the same or previous nodes.\n\nA Gaussian, or normal, distribution, is a continuous probability distribution that represents the probability that any given observation will occur on different points of a range. Visually, it resembles what\u2019s usually called a Bell curve.\n\nGenerative Adversarial Networks (GANs) are a tool to conduct unsupervised learning, essentially pitting a generative net against a discriminative net. The first net tries to fool the second by mimicking the probability distribution of a training dataset in order to fool the discriminative net into judging that the generated data instance actually belongs to the training set.\n\nGloVe is a generalization of Tomas Mikolov\u2019s word2vec algorithms, a technique for creating neural word embeddings. It was first presented at NIPS by Jeffrey Pennington, Richard Socher and Christopher Manning of Stanford\u2019s NLP department. Deeplearning4j\u2019s implementation of GloVe is here.\n\nThe gradient is a derivative, which you will know from differential calculus. That is, it\u2019s the ratio of the rate of change of a neural net\u2019s parameters and the error it produces, as it learns how to reconstruct a dataset or make guesses about labels. The process of minimizing error is called gradient descent. Descending a gradient has two aspects: choosing the direction to step in (momentum) and choosing the size of the step (learning rate).\n\nGradient Clipping is one way to solve the problem of exploding gradients. Exploding gradients arise in deep networks when gradients associating weights and the net\u2019s error become too large. Exploding gradients are frequently encountered in RNNs dealing with long-term dependencies. One way to clip gradients is to normalize them when the L2 norm of a parameter vector surpasses a given threshhold.\n\nAn Epoch is a complete pass through all the training data. A neural network is trained until the error rate is acceptable, and this will often take multiple passes through the complete data set.\n\nnote An iteration is when parameters are updated and is typically less than a full pass. For example if BatchSize is 100 and data size is 1,000 an epoch will have 10 iterations. If trained for 30 epochs there will be 300 iterations.\n\nA directed graphical model is another name for a Bayesian net, which represents the probabilistic relationships between the variables represented by its nodes.\n\nA GRU is a pared-down LSTM. GRUs rely on gating mechanisms to learn long-range dependencies while sidestepping the vanishing gradient problem. They include reset and update gates to decide when to update the GRUs memory at each time step.\n\nHighway networks are an architecture introduced by Schmidhuber et al to let information flow unhindered across several RNN layers on so-called \u201cinformation highways.\u201d The architecture uses gating units that learn to regulate the flow of information through the net. Highway networks with hundreds of layers can be trained directly using SGD, which means they can support very deep architectures.\n\n\u201cA hyperplane in an n-dimensional Euclidean space is a flat, n-1 dimensional subset of that space that divides the space into two disconnected parts. What does that mean intuitively?\n\nFirst think of the real line. Now pick a point. That point divides the real line into two parts (the part above that point, and the part below that point). The real line has 1 dimension, while the point has 0 dimensions. So a point is a hyperplane of the real line.\n\nNow think of the two-dimensional plane. Now pick any line. That line divides the plane into two parts (\u201cleft\u201d and \u201cright\u201d or maybe \u201cabove\u201d and \u201cbelow\u201d). The plane has 2 dimensions, but the line has only one. So a line is a hyperplane of the 2d plane. Notice that if you pick a point, it doesn\u2019t divide the 2d plane into two parts. So one point is not enough.\n\nNow think of a 3d space. Now to divide the space into two parts, you need a plane. Your plane has two dimensions, your space has three. So a plane is the hyperplane for a 3d space.\n\nOK, now we\u2019ve run out of visual examples. But suppose you have a space of n dimensions. You can write down an equation describing an n-1 dimensional object that divides the n-dimensional space into two pieces. That\u2019s a hyperplane.\u201d -Quora\n\nICML, or the International Conference for Machine Learning, is a well-known and well attended machine-learning conference.\n\nThe ImageNet Large Scale Visual Recognition Challenge is the formal name for ImageNet, a yearly contest held to solicit and evalute the best techniques in image recognition. Deep convolutional architectures have driven error rates on the ImageNet competition from 30% to less than 5%, which means they now have human-level accuracy.\n\nAn iteration is an update of weights after analysing a batch of input records. See Epoch for clarification.\n\nGoogle\u2019s LeNet architecture is a deep convolutional network. It won ILSVRC in 2014, and introduced techniques for paring the size of a CNN, thus increasing computational efficiency.\n\nLSTMs are a form of recurrent neural network invented in the 1990s by Sepp Hochreiter and Juergen Schmidhuber, and now widely used for image, sound and time series analysis, because they help solve the vanishing gradient problem by using a memory gates. Alex Graves made significant improvements to the LSTM with what is now known as the Graves LSTM, which Deeplearning4j implements here.\n\nLog likelihood is related to the statistical idea of the likelihood function. Likelihood is a function of the parameters of a statistical model. \u201cThe probability of some observed outcomes given a set of parameter values is referred to as the likelihood of the set of parameter values given the observed outcomes.\u201d\n\nLogistic regression behaves like an on-off switch, and is usually used for classification problems; e.g. does this data instance belong to category X or not? It produces a value between 1 and 0 that corresponds to the probability that the data belongs to a given class.\n\n\u201cSay you have a coin and you\u2019re not sure it\u2019s \u201cfair.\u201d So you want to estimate the \u201ctrue\u201d probability it will come up heads. Call this probability P, and code the outcome of a coin flip as 1 if it\u2019s heads and 0 if it\u2019s tails. You flip the coin four times and get 1, 0, 0, 0 (i.e., 1 heads and 3 tails). What is the likelihood that you would get these outcomes, given P? Well, the probability of heads is P, as we defined it above. That means the probability of tails is (1 - P). So the probability of 1 heads and 3 tails is P * (1 - P)3 [Edit: We call this the \u201clikelihood\u201d of the data]. If we \u201cguess\u201d that the coin is fair, that\u2019s saying P = 0.5, so the likelihood of the data is L = .5 * (1 - .5)3 = .0625. What if we guess that P = 0.45? Then L = .45 * (1 - .45)3 = ~.075. So P = 0.45 is actually a better estimate than P = 0.5, because the data are \u201cmore likely\u201d to have occurred if P = 0.45 than if P = 0.5. At P = 0.4, the likelihood is 0.4 * (1 - 0.4)3 = .0864. At P = 0.35, the likelihood is 0.35 * (1 - 0.35)3 = .096. In this case, it turns out that the value of P that maximizes the likelihood is P = 0.25. So that\u2019s our \u201cmaximum likelihood\u201d estimate for P. In practice, max likelihood is harder to estimate than this (with predictors and various assumptions about the distribution of the data and error terms), but that\u2019s the basic concept behind it.\u201d \u2013u/jacknbox\n\nSo in a sense, probability is treated as an unseen, internal property of the data. A parameter. And likelihood is a measure of how well the outcomes recorded in the data match our hypothesis about their probability; i.e. our theory about how the data is produced. The better our theory of the data\u2019s probability, the higher the likelihood of a given set of outcomes.\n\nIn neural networks, the model is the collection of weights and biases that transform input into output. A neural network is a set of algorithms that update models such that the models guess with less error as they learn. A model is a symbolic, logical or mathematical machine whose purpose is to deduce output from input. If a model\u2019s assumptions are correct, then one must necessarily believe its conclusions. Neural networks produced trained models that can be deployed to process, classify, cluster and make predictions about data.\n\nMNIST is the \u201chello world\u201d of deep-learning datasets. Everyone uses MNIST to test their neural networks, just to see if the net actually works at all. MNIST contains 60,000 training examples and 10,000 test examples of the handwritten numerals 0-9. These images are 28x28 pixels, which means they require 784 nodes on the first input layer of a neural network. MNIST is available for download here. Here is an example of training a DBN on MNIST with Deeplearning4j.\n\nThe goal of training is to improve the \u201cscore\u201d for the output, or the overall error rate. The Web UI will present a graph of the score for each iteration. For text-based console output of the score, you would use ScoreIterationListener\n\nMomentum also known as Nesterov\u2019s momentum, influences the speed of learning. It causes the model to converge faster to a point of minimal error. Momentum adjusts the size of the next step, the weight update, based on the previous step\u2019s gradient. That is, it takes the gradient\u2019s history and multiplies it. Before each new step, a provisional gradient is calculated by taking partial derivatives from the model, and the hyperparameters are applied to it to produce a new gradient. Momentum influences the gradient your model uses for the next step.\n\nMulti-Layer Perceptrons are perhaps the oldest form of deep neural network. They consist of multiple, fully connected feedforward layers. Examples of Deeplearning4j\u2019s multilayer perceptrons can be seen here.\n\nNeural machine translation maps one language to another using neural networks. Typically, recurrent neural networks are use to ingest a sequence from the input language and output a sequence in the target language.\n\nNoise-contrastive estimation offers a balance of computational and statistical efficiency. It is used to train classifiers with many classes in the output layer. It replaces the softmax probability density function, an approximation of a maximum likelihood estimator that is cheaper computationally.\n\nA function that maps input on a nonlinear scale such as sigmoid or tanh. By definition, a nonlinear function\u2019s output is not directly proportional to its input.\n\nThe process of transforming the data to span a range from 0 to 1. Standardizing the range of input data makes it easier for algorithms to learned from that data.\n\nWhile deep learning and opject oriented programming don\u2019t necessarily go together, Deeplearning4j is written in Java following the principles of OOP. In object-oriented programming, you create so-called objects, which are generally abstract nouns representing a part in a larger symbolic machine (e.g. in Deeplearning4j, the object class DataSetIterator traverses across datasets and feeds parts of those datasets into another process, iteratively, piece by piece).\n\nDatasetIterator is actually the name of a class of object. In any particular object-oriented program, you would create a particular instance of that general class, calling it, say, \u2018iter\u2019 like this:\n\nEvery object is really just a data structure that combines fields containing data and methods that act on the data in those fields.\n\nThe way you talk about those fields and methods is with the dot operator , and parentheses that contain parameters. For example, if you wrote , then you\u2019d be telling the DataSetIterator to go across a dataset processing 5 instances of that data (say 5 images or records) at a time, where is the method you call, and 5 is the parameter you pass into it.\n\nYou can learn more about DataSetIterator and other classes in Deeplearning4j in our Javadoc.\n\nAlso called a loss function or a cost function, an objective function defines what success looks like when an algorithm learns. It is a measure of the difference between a neural net\u2019s guess and the ground truth; that is, the error. Measuring that error is a precondition to updating the neural net in such a way that its guesses generate less error. The error resulting from the loss function is fed into backpropagation in order to update the weights and biases that process input in the neural network.\n\nUsed in classification and bag of words. The label for each example is all 0s, except for a 1 at the index of the actual class to which the example belongs. For BOW, the one represents the word encountered.\n\nBelow is an example of one-hot encoding for the phrase \u201cThe quick brown fox\u201d\n\nPooling, max pooling and average pooling are terms that refer to downsampling or subsampling within a convolutional network. Downsampling is a way of reducing the amount of data flowing through the network, and therefore decreasing the computational cost of the network. Average pooling takes the average of several values. Max pooling takes the greatest of several values. Max pooling is currently the preferred type of downsampling layer in convolutional networks.\n\nProbability densities are used in unsupervised learning, with algorithms such as autoencoders, VAEs and GANs.\n\n\u201cA probability density essentially says \u201cfor a given variable (e.g. radius) what, at that particular value, is the likelihood of encountering an event or an object (e.g. an electron)?\u201d So if I\u2019m at the nucleus of a atom and I move to, say, one Angstrom away, at one Angstrom there is a certain likelihood I will spot an electron. But we like to not just ask for the probability at one point; we\u2019d sometimes like to find the probability for a range of points: What is the probability of finding an electron between the nucleus and one Angstrom, for example. So we add up (\u201cintegrate\u201d) the probability from zero to one Angstrom. For the sake of convenience, we sometimes employ \u201cnormalization\u201d; that is, we require that adding up all the probabilities over every possible value will give us 1.00000000 (etc).\u201d \u2013u/beigebox\n\n\u201cA probability distribution is a mathematical function and/or graph that tells us how likely something is to happen.\n\nSo, for example, if you\u2019re rolling two dice and you want to find the likelihood of each possible number you can get, you could make a chart that looks like this. As you can see, you\u2019re most likely to get a 7, then a 6, then an 8, and so on. The numbers on the left are the percent of the time where you\u2019ll get that value, and the ones on the right are a fraction (they mean the same thing, just different forms of the same number). The way that it you use the distribution to find the likelihood of each outcome is this:\n\nThere are 36 possible ways for the two dice to land. There are 6 combinations that get you 7, 5 that get you 6/8, 4 that get you 5/9, and so on. So, the likelihood of each one happening is the number of possible combinations that get you that number divided by the total number of possible combinations. For 7, it would be 6/36, or 1/6, which you\u2019ll notice is the same as what we see in the graph. For 8, it\u2019s 5/36, etc. etc.\n\nThe key thing to note here is that the sum of all of the probabilities will equal 1 (or, 100%). That\u2019s really important, because it\u2019s absolutely essential that there be a result of rolling the two die every time. If all the percentages added up to 90%, what the heck is happening that last 10% of the time?\n\nSo, for more complex probability distributions, the way that the distribution is generated is more involved, but the way you read it is the same. If, for example, you see a distribution that looks like this, you know that you\u2019re going to get a value of \u03bc 40% (corresponding to .4 on the left side) of the time whenever you do whatever the experiment or test associated with that distribution.\n\nThe percentages in the shaded areas are also important. Just like earlier when I said that the sum of all the probabilities has to equal 1 or 100%, the area under the curve of a probability distribution has to equal 1, too. You don\u2019t need to know why that is (it involves calculus), but it\u2019s worth mentioning. You can see that the graph I linked is actually helpfully labeled; the reason they do that is to show you that you what percentage of the time you\u2019re going to end up somewhere in that area.\n\nSo, for example, about 68% of the time, you\u2019ll end up between -1\u03c3 and 1\u03c3.\u201d \u2013u/corpuscle634\n\nA radial basis function network calculates absolute values such as Euclidean distances as part of its activation function. It acts as a function approximator, much like other neural networks.\n\nAfter applying Gaussian noise, a kind of statistical white noise, to the data, this objective function punishes the network for any result that is not closer to the original input. That signal prompts the network to learn different features in an attempt to reconstruct the input better and minimize error.\n\nRectified linear units, or reLU, are a non-linear activation function widely applied in neural networks because they deal well with the vanishing gradient problem. They can be expressed so: , where activation is set to zero if the output does not surpass a minimum threshhold, and activation increases linearly above that threshhold.\n\nRecurrent neural networks are just a special form of shared weights. While \u201ca multilayer perceptron (MLP) can only map from input to output vectors, whereas an RNN can in principle map from the entire history of previous inputs to each output. Indeed, the equivalent result to the universal approximation theory for MLPs is that an RNN with a sufficient number of hidden units can approximate any measurable sequence-to-sequence mapping to arbitrary accuracy (Hammer, 2000). The key point is that the recurrent connections allow a \u2018memory\u2019 of previous inputs to persist in the network\u2019s internal state, which can then be used to influence the network output. The forward pass of an RNN is the same as that of an MLP with a single hidden layer, except that activations arrive at the hidden layer from both the current external input and the hidden layer activations one step back in time. \u201c -Graves\n\n\u201cIf you imagine a neural net as a 2D graph, a RNN is a 3D graph where the topology of every 2D slice is a duplicate of the original non recurrent network. Every slice has connections going to the next slice, these inter-slice connections also have the same topology. The inter-slice connections represent connections in time, going into the future. So when you are performing a back propagation step, you might step into the prior layer, and/or you might also step into the the prior time step.\u201d \u2013link\n\nRecursive neural networks learn data with structural hierarchies, such as text arranged grammatically, much like recurrent neural networks learn data structured by its occurance in time. Their chief use is in natural-language processing, and they are associated with Richard Socher of Stanford\u2019s NLP lab.\n\nReinforcement learning is a branch of machine learning that is goal oriented; that is, reinforcement learning algorithms have as their objective to maximize a reward, often over the course of many decisions. Unlike deep neural networks, reinforcement learning is not differentiable.\n\nRepresentation learning is learning the best representation of input. A vector, for example, can \u201crepresent\u201d an image. Training a neural network will adjust the vector\u2019s elements to represent the image better, or lead to better guesses when a neural network is fed the image. The neural net might train to guess the image\u2019s name, for instance. Deep learning means that several layers of representations are stacked atop one another, and those representations are increasingly abstract; i.e. the initial, low-level representations are granular, and may represent pixels, while the higher representations will stand for combinations of pixels, and then combinations of combinations, and so forth.\n\nMicrosoft Research used deep Residual Networks to win ImageNet in 2015. ResNets create \u201cshortcuts\u201d across several layers (deep resnets have 150 layers), allowing the net to learn so-called residual mappings. ResNets are similar to nets with Highway Layers, although they\u2019re data independent. Microsoft Research created ResNets by generating by different deep networks automatically and relying on hyperparameter optimization.\n\nRestricted Boltzmann machines are Boltzmann machines that are constrained to feed input forward symmetrically, which means all the nodes of one layer must connect to all the nodes of the subsequent layer. Stacked RBMs are known as a deep-belief network, and are used to learn how to reconstruct data layer by layer. Introduced by Geoff Hinton, RBMs were partially responsible for the renewed interest in deep learning that began circa 2006. In many labs, they have been replaced with more stable layers such as Variational Autoencoders.\n\nRMSProp is an optimization algorithm like Adagrad. In contrast to Adagrad, it relies on a decay term to prevent the learning rate from decreasing too rapidly.\n\nMeasurement of the overall error rate of the model. The score of the model will be displayed graphically in the webui or it can be displayed the console by using ScoreIterationListener\n\nSerialization is how you translate data structures or object state into storable formats. Deeplearning4j\u2019s nets are serialized, which means they can operate on devices with limited memory.\n\nThe prerequisite to a definition of skipgrams is one of ngrams. An n-gram is a contiguous sequence of n items from a given sequence of text or speech. A unigram represents one \u201citem,\u201d a bigram two, a trigram three and so forth. Skipgrams are ngrams in which the items are not necessarily contiguous. This can be illustrated best with a few examples. Skipping is a form of noise, in the sense of noising and denoising, which allows neural nets to better generalize their extraction of features. See how skipgrams are implemented in Word2vec.\n\nSoftmax is a function used as the output layer of a neural network that classifies input. It converts vectors into class probabilities. Softmax normalizes the vector of scores by first exponentiating and then dividing by a constant.\n\nStochastic Gradient Descent optimizes gradient descent and minimizes the loss function during network training.\n\nStochastic is simply a synonym for \u201crandom.\u201d A stochastic process is a process that involves a random variable, such as randomly initialized weights. Stochastic derives from the Greek word stochazesthai, \u201cto guess or aim at\u201d. Stochastic processes describe the evolution of, say, a random set of variables, and as such, they involve some indeterminacy \u2013 quite the opposite of having a precisely predicted processes that are deterministic, and have just one outcome.\n\nThe stochastic element of a learning process is a form of search. Random weights represent a hypothesis, an attempt, or a guess that one tests. The results of that search are recorded in the form of a weight adjustment, which effectively shrinks the search space as the parameters move toward a position of less error.\n\nNeural-network gradients are calculated using backpropagation. SGD is usually used with minibatches, such that parameters are updated based on the average error generated by the instances of a whole batch.\n\nWhile support-vector machines are not neural networks, they are an important algorithm that deserves explanation:\n\nHere is an example of tensor along dimension (TAD):\n\nThe vanishing gradient problem is a challenge the confront backpropagation over many layers. Backpropagation establishes the relationship between a given weight and the error of a neural network. It does so through the chain rule of calculus, calculating how the change in a given weight along a gradient affects the change in error. However, in very deep neural networks, the gradient that relates the weight change to the error change can become very small. So small that updates in the net\u2019s parameters hardly change the net\u2019s guesses and error; so small, in fact, that it is difficult to know in which direction the weight should be adjusted to diminish error. Non-linear activation functions such as sigmoid and tanh make the vanishing gradient problem particularly difficult, because the activation funcion tapers off at both ends. This has led to the widespread adoption of rectified linear units (reLU) for activations in deep nets. It was in seeking to solve the vanishing gradient problem that Sepp Hochreiter and Juergen Schmidhuber invented a form of recurrent network called an LSTM in the 1990s. The inverse of the vanishing gradient problem, in which the gradient is impossibly small, is the exploding gradient problem, in which the gradient is impossibly large (i.e. changing a weight has too much impact on the error.)\n\nTransfer learning is when a system can recognize and apply knowledge and skills learned in previous domains or tasks to novel domains or tasks. That is, if a model is trained on image data to recognize one set of categories, transfer learning applies if that same model is capable, with minimal additional training, or recognizing a different set of categories. For example, trained on 1,000 celebrity faces, a transfer learning model can be taught to recognize members of your family by swapping in another output layer with the nodes \u201cmom\u201d, \u201cdad\u201d, \u201celder brother\u201d, \u201cyounger sister\u201d and training that output layer on the new classifications.\n\nWord2vec and other neural networks represent input as vectors.\n\nA vector is a data structure with at least two components, as opposed to a scalar, which has just one. For example, a vector can represent velocity, an idea that combines speed and direction: wind velocity = (50mph, 35 degrees North East). A scalar, on the other hand, can represent something with one value like temperature or height: 50 degrees Celsius, 180 centimeters.\n\nTherefore, we can represent two-dimensional vectors as arrows on an x-y graph, with the coordinates x and y each representing one of the vector\u2019s values.\n\nTwo vectors can relate to one another mathematically, and similarities between them (and therefore between anything you can vectorize, including words) can be measured with precision.\n\nAs you can see, these vectors differ from one another in both their length, or magnitude, and in their angle, or direction. The angle is what concerns us here.\n\nVGG is a deep convolutional architecture that won the benchmark ImageNet competition in 2014. A VGG architecture is composed of 16\u201319 weight layers and uses small convolutional filters. Deeplearning4j\u2019s implementations of two VGG architectures are here.\n\nTomas Mikolov\u2019s neural networks, known as Word2vec, have become widely used because they help produce state-of-the-art word embeddings. Word2vec is a two-layer neural net that processes text. Its input is a text corpus and its output is a set of vectors: feature vectors for words in that corpus. While Word2vec is not a deep neural network, it turns text into a numerical form that deep nets can understand. Word2vec\u2019s applications extend beyond parsing sentences in the wild. It can be applied just as well to genes, code, playlists, social media graphs and other verbal or symbolic series in which patterns may be discerned. Deeplearning4j implements a distributed form of Word2vec for Java and Scala, which works on Spark with GPUs.\n\nThe Xavier initialization is based on the work of Xavier Glorot and Yoshua Bengio in their paper \u201cUnderstanding the difficulty of training deep feedforward neural networks.\u201d An explanation can be found here. Weights should be initialized in a way that promotes \u201clearning\u201d. The wrong weight initialization will make gradients too large or too small, and make it difficult to update the weights. Small weights lead to small activations, and large weights lead to large ones. Xavier weight initialization considers the distribution of output activations with regard to input activations. Its purpose is to maintain same distribution of activations, so they aren\u2019t too small (mean zero but with small variance) or too large (mean zero but with large variance). DL4J\u2019s implementation of Xavier weight initialization aligns with the Glorot Bengio paper, . Where would be the number of units sending input to k, and would be the number of units recieving output from k.", 
        "title": "Deep Learning and Neural Network Glossary - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/graphdata", 
        "text": "", 
        "title": "Redirecting\u2026"
    }, 
    {
        "url": "https://deeplearning4j.org/symbolicreasoning", 
        "text": "Deep learning has its discontents, and many of them look to other branches of AI when they hope for the future. Symbolic reasoning is one of those branches.\n\nThe two biggest flaws of deep learning are its lack of model interpretability (i.e. why did my model make that prediction?) and the amount of data that deep neural networks require in order to learn. They are data hungry.\n\nGeoff Hinton himself has expressed scepticism about whether backpropagation, the workhorse of deep neural nets, will be the way forward for AI.1\n\nResearch into so-called one-shot learning may address deep learning\u2019s data hunger, while deep symbolic learning, or enabling deep neural networks to manipulate, generate and otherwise cohabitate with concepts expressed in strings of characters, could help solve explainability, because, after all, humans communicate with signs and symbols, and that is what we desire from machines.2\n\nThe words sign and symbol derive from Latin and Greek words, respectively, that mean mark or token, as in \u201ctake this rose as a token of my esteem.\u201d Both words mean \u201cto stand for something else\u201d or \u201cto represent something else\u201d.\n\nThat something else could be a physical object, an idea, an event, you name it. For our purposes, the sign or symbol is a visual pattern, say a character or string of characters, in which meaning is embedded, and that sign or symbol is pointing at something else. It could be the variable , pointing at an unknown quantity, or it could be the word , which is pointing at the red, curling petals layered one over the other in a tight spiral at the end of a stalk of thorns.3\n\nThe signifier indicates the signified, like a finger pointing at the moon. Symbols compress sensory data in a way that enables humans, those beings of limited bandwidth, to share information.4\n\nCombinations of symbols that express their interrelations could be called reasoning, and when we humans string a bunch of signs together to express thought, as I am doing now, you might call it symbolic manipulation. Sometimes those symbolic relations are necessary and deductive, as with the formulas of pure math or the conclusions you might draw from a logical syllogism like this old Roman chestnut:\n\nOther times the symbols express lessons we derive inductively from our experiences of the world, as in: \u201cthe baby seems to prefer the pea-flavored goop (so for godssake let\u2019s make sure we keep some in the fridge),\u201d or E = mc2.\n\nSymbolic artificial intelligence, also known as Good, Old-Fashioned AI (GOFAI), was the dominant paradigm in the AI community from the post-War era until the late 1980s.\n\nImplementations of symbolic reasoning are called rules engines or expert systems or knowledge graphs. See Cyc for one of the longer-running examples. Google made a big one, too, which is what provides the information in the top box under your query when you search for something easy like the capital of Germany. These systems are essentially piles of nested if-then statements drawing conclusions about entities (human-readable concepts) and their relations (expressed in well understood semantics like is-a or lives-in ).\n\nImagine how Turbotax manages to reflect the US tax code \u2013 you tell it how much you earned and how many dependents you have and other contingencies, and it computes the tax you owe by law \u2013 that\u2019s an expert system.\n\nExternal concepts are added to the system by its programmer-creators, and that\u2019s more important than it sounds\u2026\n\nOne of the main differences between machine learning and traditional symbolic reasoning is where the learning happens. In machine- and deep-learning, the algorithm learns rules as it establishes correlations between inputs and outputs. In symbolic reasoning, the rules are created through human intervention. That is, to build a symbolic reasoning system, first humans must learn the rules by which two phenomena relate, and then hard-code those relationships into a static program. This difference is the subject of a well-known hacker koan:\n\nA hard-coded rule is a preconception. It is one form of assumption, and a strong one, while deep neural architectures contain other assumptions, usually about how they should learn, rather than what conclusion they should reach. The ideal, obviously, is to choose assumptions that allow a system to learn flexibly and produce accurate decisions about their inputs.\n\nOne of the main stumbling blocks of symbolic AI, or GOFAI, was the difficulty of revising beliefs once they were encoded in a rules engine. Expert systems are monotonic; that is, the more rules you add, the more knowledge is encoded in the system, but additional rules can\u2019t undo old knowledge. Monotonic basically means one direction; i.e. when one thing goes up, another thing goes up. Because machine learning algorithms can be retrained on new data, and will revise their parameters based on that new data, they are better at encoding tentative knowledge that can be retracted later if necessary.\n\nA second flaw in symbolic reasoning is that the computer itself doesn\u2019t know what the symbols mean; i.e. they are not necessarily linked to any other representations of the world in a non-symbolic way. Again, this stands in contrast to neural nets, which can link symbols to vectorized representations of the data, which are in turn just translations of raw sensory data. So the main challenge, when we think about GOFAI and neural nets, is how to ground symbols, or relate them to other forms of meaning that would allow computers to map the changing raw sensations of the world to symbols and then reason about them.\n\nHow can we fuse the ability of deep neural nets to learn probabilistic correlations from scratch alongside abstract and higher-order concepts, which are useful in compressing data and combining it in new ways? How can we learn to attach new meanings to concepts, and to use atomic concepts as elements in more complex and composable thoughts such as language allows us to express in all its natural plasticity?\n\nCombining symbolic reasoning with deep neural networks and deep reinforcement learning may help us address the fundamental challenges of reasoning, hierarchical representations, transfer learning, robustness in the face of adversarial examples, and interpretability (or explanatory power).\n\nLet\u2019s explore how they currently overlap and how they might. First of all, every deep neural net trained by supervised learning combines deep learning and symbolic manipulation, at least in a rudimentary sense. Because symbolic reasoning encodes knowledge in symbols and strings of characters. In supervised learning, those strings of characters are called labels, the categories by which we classify input data using a statistical model. The output of a classifier (let\u2019s say we\u2019re dealing with an image recognition algorithm that tells us whether we\u2019re looking at a pedestrian, a stop sign, a traffic lane line or a moving semi-truck), can trigger business logic that reacts to each classification. That business logic is one form of symbolic reasoning.\n\n1) Hinton, Yann LeCun and Andrew Ng have all suggested that work on unsupervised learning (learning from unlabeled data) will lead to our next breakthroughs.\n\n2) The two problems may overlap, and solving one could lead to solving the other, since a concept that helps explain a model will also help it recognize certain patterns in data using fewer examples.\n\n3) The weird thing about writing about signs, of course, is that in the confines of a text, we\u2019re just using one set of signs to describe another in the hopes that the reader will respond to the sensory evocation and supply the necessary analog memories of and . But you get my drift. (It gets even weirder when you consider that the sensory data perceived by our minds, and to which signs refer, are themselves signs of the thing in itself, which we cannot know.)\n\n4) According to science, the average American English speaker speaks at a rate of about 110\u2013150 words per minute (wpm). Just how much reality do you think will fit into a ten-minute transmission?\n\nThis page includes some recent, notable research that attempts to combine deep learning with symbolic learning to answer those questions.\n\nDeep reinforcement learning (DRL) brings the power of deep neural networks to bear on the generic task of trial-and-error learning, and its effectiveness has been convincingly demonstrated on tasks such as Atari video games and the game of Go. However, contemporary DRL systems inherit a number of shortcomings from the current generation of deep learning techniques. For example, they require very large datasets to work effectively, entailing that they are slow to learn even when such datasets are available. Moreover, they lack the ability to reason on an abstract level, which makes it difficult to implement high-level cognitive functions such as transfer learning, analogical reasoning, and hypothesis-based reasoning. Finally, their operation is largely opaque to humans, rendering them unsuitable for domains in which verifiability is important. In this paper, we propose an end-to-end reinforcement learning architecture comprising a neural back end and a symbolic front end with the potential to overcome each of these shortcomings. As proof-of-concept, we present a preliminary implementation of the architecture and apply it to several variants of a simple video game. We show that the resulting system \u2013 though just a prototype \u2013 learns effectively, and, by acquiring a set of symbolic rules that are easily comprehensible to humans, dramatically outperforms a conventional, fully neural DRL system on a stochastic variant of the game.\n\nArtificial Neural Networks are powerful function approximators capable of modelling solutions to a wide variety of problems, both supervised and unsupervised. As their size andexpressivity increases, so too does the variance of the model, yielding a nearly ubiquitous overfitting problem. Although mitigated by a variety of model regularisation methods, the common cure is to seek large amounts of training data\u2014which is not necessarily easily obtained\u2014that sufficiently approximates the data distribution of the domain we wish to test on. In contrast, logic programming methods such as Inductive Logic Programming offer an extremely data-efficient process by which models can be trained to reason on symbolic domains. However, these methods are unable to deal with the variety of domains neural networks can be applied to: they are not robust to noise in or mislabelling of inputs, and perhaps more importantly, cannot be applied to non-symbolic domains where the data is ambiguous, such as operating on raw pixels. In this paper, we propose a Differentiable Inductive Logic framework, which can not only solve tasks which traditional ILP systems are suited for, but shows a robustness to noise and error in the training data which ILP cannot cope with. Furthermore, as it is trained by backpropagation against a likelihood objective, it can be hybridised by connecting it with neural networks over ambiguous data in order to be applied to domains which ILP cannot address, while providing data efficiency and generalisation beyond what neural networks on their own can achieve.\n\nThe recent adaptation of deep neural network-based methods to reinforcement learning and planning domains has yielded remarkable progress on individual tasks. Nonetheless, progress on task-to-task transfer remains limited. In pursuit of efficient and robust generalization, we introduce the Schema Network, an object-oriented generative physics simulator capable of disentangling multiple causes of events and reasoning backward through causes to achieve goals. The richly structured architecture of the Schema Network can learn the dynamics of an environment directly from data. We compare Schema Networks with Asynchronous Advantage Actor-Critic and Progressive Networks on a suite of Breakout variations, reporting results on training efficiency and zero-shot generalization, consistently demonstrating faster, more robust learning and better transfer. We argue that generalizing from limited data and learning causal relationships are essential abilities on the path toward generally intelligent systems.\n\nWe introduce the Deep Symbolic Network (DSN) model, which aims at becoming the white-box version of Deep Neural Networks (DNN). The DSN model provides a simple, universal yet powerful structure, similar to DNN, to represent any knowledge of the world, which is transparent to humans. The conjecture behind the DSN model is that any type of real world objects sharing enough common features are mapped into human brains as a symbol. Those symbols are connected by links, representing the composition, correlation, causality, or other relationships between them, forming a deep, hierarchical symbolic network structure. Powered by such a structure, the DSN model is expected to learn like humans, because of its unique characteristics. First, it is universal, using the same structure to store any knowledge. Second, it can learn symbols from the world and construct the deep symbolic networks automatically, by utilizing the fact that real world objects have been naturally separated by singularities. Third, it is symbolic, with the capacity of performing causal deduction and generalization. Fourth, the symbols and the links between them are transparent to us, and thus we will know what it has learned or not - which is the key for the security of an AI system. Fifth, its transparency enables it to learn with relatively small data. Sixth, its knowledge can be accumulated. Last but not least, it is more friendly to unsupervised learning than DNN. We present the details of the model, the algorithm powering its automatic learning ability, and describe its usefulness in different use cases. The purpose of this paper is to generate broad interest to develop it within an open source project centered on the Deep Symbolic Network (DSN) model towards the development of general AI.\n\nAlthough deep learning has historical roots going back decades, neither the term \u201cdeep learning\u201d nor the approach was popular just over five years ago, when the field was reignited by papers such as Krizhevsky, Sutskever and Hinton\u2019s now classic (2012) deep network model of Imagenet. What has the field discovered in the five subsequent years? Against a background of considerable progress in areas such as speech recognition, image recognition, and game playing, and considerable enthusiasm in the popular press, I present ten concerns for deep learning, and suggest that deep learning must be supplemented by other techniques if we are to reach artificial general intelligence.\n\nThe tasks that an agent will need to solve often aren\u2019t known during training. However, if the agent knows which properties of the environment we consider im- portant, then after learning how its actions affect those properties the agent may be able to use this knowledge to solve complex tasks without training specifi- cally for them. Towards this end, we consider a setup in which an environment is augmented with a set of user defined attributes that parameterize the features of interest. We propose a model that learns a policy for transitioning between \u201cnearby\u201d sets of attributes, and maintains a graph of possible transitions. Given a task at test time that can be expressed in terms of a target set of attributes, and a current state, our model infers the attributes of the current state and searches over paths through attribute space to get a high level plan, and then uses its low level policy to execute the plan. We show in grid-world games and 3D block stacking that our model is able to generalize to longer, more complex tasks at test time even when it only sees short, simple tasks at train time. TL;DR: Compositional attribute-based planning that generalizes to long test tasks, despite being trained on short & simple tasks.\n\nWe investigate an unconventional direction of research that aims at converting neural networks, a class of distributed, connectionist, sub-symbolic models into a symbolic level with the ultimate goal of achieving AI interpretability and safety. To that end, we propose Object-Oriented Deep Learning, a novel computational paradigm of deep learning that adopts interpretable \u201cobjects/symbols\u201d as a basic representational atom instead of N-dimensional tensors (as in traditional \u201cfeature-oriented\u201d deep learning). For visual processing, each \u201cobject/symbol\u201d can explicitly package common properties of visual objects like its position, pose, scale, probability of being an object, pointers to parts, etc., providing a full spectrum of interpretable visual knowledge throughout all layers. It achieves a form of \u201csymbolic disentanglement\u201d, offering one solution to the important problem of disentangled representations and invariance. Basic computations of the network include predicting high-level objects and their properties from low-level objects and binding/aggregating relevant objects together. These computations operate at a more fundamental level than convolutions, capturing convolution as a special case while being significantly more general than it. All operations are executed in an input-driven fashion, thus sparsity and dynamic computation per sample are naturally supported, complementing recent popular ideas of dynamic networks and may enable new types of hardware accelerations. We experimentally show on CIFAR-10 that it can perform flexible visual processing, rivaling the performance of ConvNet, but without using any convolution. Furthermore, it can generalize to novel rotations of images that it was not trained for.", 
        "title": "A Beginner's Guide to Symbolic Reasoning (Symbolic AI) & Deep Learning - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/deeplearningpapers", 
        "text": "Deep learning of the tissue-regulated splicing code; Michael K. K. Leung, Hui Yuan Xiong, Leo J. Lee and Brendan J. Frey\n\nThe human splicing code reveals new insights into the genetic determinants of disease; Hui Y. Xiong et al\n\nAdaptive Step-Size for Online Temporal Difference Learning; William Dabney and Andrew G. Barto; University of Massachusetts Amherst\n\nNotes on Convolutional Neural Networks; Jake Bouvrie; Center for Biological and Computational Learning, Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology\n\nNatural Language Processing (Almost) from Scratch; Ronan Collobert, Jason Weston, Leon Bottou, Michael Karlen, Koray Kavukcuoglu and Pavel Kuksa; NEC Laboratories America\n\nDeep Belief Networks for phone recognition; Abdel-rahman Mohamed, George Dahl, and Geoffrey Hinton; Department of Computer Science, University of Toronto\n\nReducing the Dimensionality of Data with Neural Networks; G. E. Hinton and R. R. Salakhutdinov; 28 July 2006 vol. 313 Science\n\nUsing Very Deep Autoencoders for Content-Based Image Retrieval; Alex Krizhevsky and Geoffrey E. Hinton; University of Toronto, Dept of Computer Science\n\nAnalysis of Recurrent Neural Networks with Application to Speaker Independent Phoneme Recognition; Esko O. Dijk; University of Twente, Department of Electrical Engineering\n\nA fast learning algorithm for deep belief nets; Geoffrey E. Hinton and Simon Osindero, Department of Computer Science University of Toronto; Yee-Whye Teh, Department of Computer Science, National University of Singapore\n\nLearning Deep Architectures for AI; Yoshua Bengio; Foundations and Trends in Machine Learning, Vol. 2, No. 1 (2009)\n\nAn Analysis of Gaussian-Binary Restricted Boltzmann Machines for Natural Images; Nan Wang, Jan Melchior and Laurenz Wiskott; Institut fuer Neuroinformatik and International Graduate School of Neuroscience\n\nIPAM Summer School 2012 Tutorial on: Deep Learning; Geoffrey Hinton; Canadian Institute for Advanced Research & Department of Computer Science, University of Toronto\n\nA Practical Guide to Training Restricted Boltzmann Machines; Geo\ufb00rey Hinton; Department of Computer Science, University of Toronto\n\nHogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent; Feng Niu, Benjamin Recht, Christopher Re and Stephen J. Wright; Computer Sciences Department, University of Wisconsin-Madison\n\nImproved Learning of Gaussian-Bernoulli Restricted Boltzmann Machines; KyungHyun Cho, Alexander Ilin, and Tapani Raiko; Department of Information and Computer Science, Aalto University School of Science, Finland\n\nConvolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations; Honglak Lee, Roger Grosse, Rajesh Ranganath and Andrew Y. Ng; Computer Science Department, Stanford University\n\nRecti\ufb01ed Linear Units Improve Restricted Boltzmann Machines; Vinod Nair and Geo\ufb00rey E. Hinton; Department of Computer Science, University of Toronto\n\nDistributed Training Strategies for the Structured Perceptron; Ryan McDonald, Keith Hall and Gideon Mann; Google\n\nDeep Learning for NLP (without magic); Richard Socher and Christopher Manning; Stanford University\n\nDeep Learning Made Easier by Linear Transformations in Perceptrons; Tapani Raiko, Harri Valpola and Yann LeCun; Aalto University and New York University\n\nTraining Restricted Boltzmann Machines on Word Observations; George E. Dahl, Ryan P. Adams and Hugo Larochelle; University of Toronto, Harvard University and Universit\u00e9 de Sherbrooke\n\nRepresentational Power of Restricted Boltzmann Machines and Deep Belief Networks; Nicolas Le Roux and Yoshua Bengio; Universit\u00e9 de Montr\u00e9al\n\nRobust Boltzmann Machines for Recognition and Denoising; Yichuan Tang, Ruslan Salakhutdinov and Geoffrey Hinton; University of Toronto\n\nSemantic hashing; Ruslan Salakhutdinov and Geoffrey Hinton; Department of Computer Science, University of Toronto\n\nRecursive Deep Models for Semantic Compositionality Over a Sentiment Treebank; Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng and Christopher Potts; Stanford University\n\nOpinion Mining and Sentiment Analysis; Bo Pang and Lillian Lee; Yahoo Research; Foundations and Trends in Information Retrieval\n\nStochastic Pooling for Regularization of Deep Convolutional Neural Networks; Matthew D. Zeiler and Rob Fergus; Courant Institute, New York University\n\nSymmetry breaking in non-monotonic neural nets; G. Boffetta, R. Monasson and R. Zecchina; Journal of Physics A: Mathematical and General\n\nPhone Recognition Using Restricted Boltzmann Machines; Abdel-rahman Mohamed and Geoffrey Hinton; University of Toronto\n\nWhy Does Unsupervised Pre-training Help Deep Learning?; Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent and Samy Bengio; Universit\u00e9 de Montr\u00e9eal and Google Research\n\nTraining Restricted Boltzmann Machines on Word Observations; George E. Dahl, Ryan P. Adams and Hugo Larochelle; University of Toronto, Harvard University and Universit\u00e9 de Sherbrooke\n\nVisually Debugging Restricted Boltzmann Machine Training with a 3D Example; Jason Yosinski and Hod Lipson; Cornell University\n\nA Few Useful Things to Know about Machine Learning; Pedro Domingos, University of Washington\n\nOn Chomsky and the Two Cultures of Statistical Learning; Peter Norvig\n\nHow transferable are features in deep neural networks?; Jason Yosinski, Jeff Clune, Yoshua Bengio and Hod Lipson\n\nBackpropagation Through Time: What It Does and How to Do It; Paul Werbos\n\nLearning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation; Cho et al\n\nExplorations in Parallel Distributed Processing: A Handbook of Models, Programs, and Exercises; James L. McClelland\n\nAn Introduction to MCMC for Machine Learning\n\nUsing Neural Networks for Modeling and Representing Natural Languages\n\nRecursive Deep Models for Semantic Compositionality Over a Sentiment Treebank;\n\n Socher et al. 2013. Introduces Recursive Neural Tensor Network. Uses a parse tree.\n\nDistributed Representations of Sentences and Documents\n\n Le; Mikolov. 2014. Introduces Paragraph Vector. Concatenates and averages pretrained, fixed word vectors to create vectors for sentences, paragraphs and documents. Also known as paragraph2vec. Doesn\u2019t use a parse tree.\n\nDeep Recursive Neural Networks for Compositionality in Language; \n\n Irsoy & Cardie. 2014. Uses Deep Recursive Neural Networks. Uses a parse tree.\n\nImproved Semantic Representations From Tree-Structured Long Short-Term Memory Networks; Tai et al. 2015 Introduces Tree LSTM. Uses a parse tree.\n\nA Neural Network Approach to Context-Sensitive Generation of Conversational Responses; Sordoni 2015. Generates responses to tweets. Uses Recurrent Neural Network Language Model (RLM) architecture of (Mikolov et al., 2010).\n\nTowards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks; Weston 2015. Classifies QA tasks. Expands on Memory Networks.\n\nA Neural Conversation Model; Vinyals, Le 2015. Uses LSTM RNNs to generate conversational responses. Uses seq2seq framework.", 
        "title": "Deep Learning Resources (Papers, Online Courses, Books) - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/deeplearningtranslated", 
        "text": "Deep learning goes by several names in English as well as other languages. A fuller, more precise term is deep, artificial neural networks.\n\nBelow, we\u2019ll define the following terms in order for each language.", 
        "title": "Deep Learning in Other Languages - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/machine-learning-demos.html", 
        "text": "", 
        "title": "Machine Learning and Deep Learning Demos - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/machine-learning-library-software-links.html", 
        "text": "", 
        "title": "Machine Learning and Deep Learning Software Links - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/machine-learning-research-groups-labs.html", 
        "text": "For people just getting started with deep learning, the following tutorials and videos provide an easy entrance to the fundamental ideas of deep neural networks:", 
        "title": "Machine Learning Research Groups & Labs - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/cn/index", 
        "text": "", 
        "title": "Deeplearning4j\u662f\u4ec0\u4e48\uff1f - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/ja-index", 
        "text": "", 
        "title": "Deeplearning 4 j \u306e\u30af\u30a4\u30c3\u30af\u30b9\u30bf\u30fc\u30c8\u30ac\u30a4\u30c9 - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }, 
    {
        "url": "https://deeplearning4j.org/kr-index", 
        "text": "", 
        "title": "Deeplearning4j\ub780? - Deeplearning4j: Open-source, Distributed Deep Learning for the JVM"
    }
]